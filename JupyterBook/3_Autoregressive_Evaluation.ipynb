{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d6920b-1452-45a8-8751-0b7d6e8e8fd0",
   "metadata": {},
   "source": [
    "# IQNx4: Chapter 3: Autoregressive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c623666-defa-4463-adfe-f7667c7d385d",
   "metadata": {},
   "source": [
    "## 3.1: Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31994c2b-a804-4348-a5a9-8bd8c89ff64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using torch version 1.11.0.post2\n",
      "matplotlib version=  3.5.3\n",
      "using (optional) optuna version 3.0.0\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "using torch version 1.11.0.post2\n",
      "matplotlib version=  3.5.3\n",
      "using (optional) optuna version 3.0.0\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n",
      "DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "using DATA_DIR=/home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4164329/2263036294.py:54: MatplotlibDeprecationWarning: Support for setting an rcParam that expects a str value to a non-str value is deprecated since 3.5 and support will be removed two minor releases later.\n",
      "  mp.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]  # for \\text command\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import scipy as sp; import scipy.stats as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"using torch version {torch.__version__}\")\n",
    "# use numba's just-in-time compiler to speed things up\n",
    "# from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "\n",
    "print(\"matplotlib version= \", mp.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reset matplotlib stle/parameters\n",
    "# reset matplotlib parameters to their defaults\n",
    "# plt.style.use('seaborn-deep')\n",
    "# mp.rcParams['agg.path.chunksize'] = 10000\n",
    "font_legend = 15\n",
    "font_axes = 15\n",
    "# %matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from IPython.display import Image, display\n",
    "# from importlib import import_module\n",
    "# import plotly\n",
    "try:\n",
    "    import optuna\n",
    "\n",
    "    print(f\"using (optional) optuna version {optuna.__version__}\")\n",
    "except Exception:\n",
    "    print(\"optuna is only used for hyperparameter tuning, not critical!\")\n",
    "    pass\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# import sympy as sy\n",
    "# import ipywidgets as wid;\n",
    "\n",
    "# update fonts\n",
    "font = {\"family\": \"serif\", \"size\": 10}\n",
    "mp.rc(\"font\", **font)\n",
    "\n",
    "# set usetex = False if LaTex is not\n",
    "# available on your system or if the\n",
    "# rendering is too slow\n",
    "mp.rcParams.update({\"text.usetex\": True})\n",
    "# plt.rcParams['text.usetex'] = True\n",
    "mp.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]  # for \\text command\n",
    "\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "# seed = 128\n",
    "# rnd  = np.random.RandomState(seed)\n",
    "# sometimes jupyter doesnt initialize MathJax automatically for latex, so do this:\n",
    "\n",
    "try:\n",
    "    IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "    print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "    utils_dir = os.path.join(IQN_BASE, 'utils/')\n",
    "    sys.path.append(utils_dir)\n",
    "    import utils\n",
    "\n",
    "    # usually its not recommended to import everything from a module, but we know\n",
    "    # whats in it so its fine\n",
    "    # from utils import *\n",
    "    print(\"DATA directory also properly set, in %s\" % os.environ[\"DATA_DIR\"])\n",
    "except Exception:\n",
    "    # IQN_BASE=os.getcwd()\n",
    "    print(\n",
    "        \"\"\"\\nBASE directory not properly set. Read repo README.    If you need a function from utils, use the decorator below, or add utils to sys.path\\n\n",
    "    You can also do \n",
    "    os.environ['IQN_BASE']=<ABSOLUTE PATH FOR THE IQN REPO>\n",
    "    or\n",
    "    os.environ['IQN_BASE']=os.getcwd()\"\"\"\n",
    "    )\n",
    "    pass\n",
    "\n",
    "\n",
    "IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "utils_dir = os.path.join(IQN_BASE, \"utils/\")\n",
    "sys.path.append(utils_dir)\n",
    "# usually its not recommended to import everything from a module, but we know\n",
    "# whats in it so its fine\n",
    "\n",
    "# or use joblib for caching on disk\n",
    "from joblib import Memory\n",
    "\n",
    "\n",
    "################################### CONFIGURATIONS ###################################\n",
    "DATA_DIR = os.environ[\"DATA_DIR\"]\n",
    "print(f\"using DATA_DIR={DATA_DIR}\")\n",
    "JUPYTER = False\n",
    "use_subsample = False\n",
    "# use_subsample=True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "memory = Memory(DATA_DIR)\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p(p_T)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "    \"RecoDataeta\": \"$p(\\eta)$\",\n",
    "    \"RecoDataphi\": \"$p(\\phi)$\",\n",
    "    \"RecoDatam\": \"$p(m)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "}\n",
    "\n",
    "loss_y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p_T^{reco}$\",\n",
    "    \"RecoDataeta\": \"$\\eta^{reco}$\",\n",
    "    \"RecoDataphi\": \"$\\phi^{reco}$\",\n",
    "    \"RecoDatam\": \"$m^{reco}$\",\n",
    "}\n",
    "\n",
    "\n",
    "################################### SET DATA CONFIGURATIONS ###################################\n",
    "X = [\"genDatapT\", \"genDataeta\", \"genDataphi\", \"genDatam\", \"tau\"]\n",
    "\n",
    "# set order of training:\n",
    "# pT_first: pT->>m->eta->phi\n",
    "# m_first: m->pT->eta->phi\n",
    "\n",
    "\n",
    "ORDER = \"m_First\"\n",
    "\n",
    "if ORDER == \"m_First\":\n",
    "    FIELDS = {\n",
    "        \"RecoDatam\": {\n",
    "            \"inputs\": X,\n",
    "            \"xlabel\": r\"$m$ (GeV)\",\n",
    "            \"ylabel\": \"$m^{reco}$\",\n",
    "            \"xmin\": 0,\n",
    "            \"xmax\": 25,\n",
    "        },\n",
    "        \"RecoDatapT\": {\n",
    "            \"inputs\": [\"RecoDatam\"] + X,\n",
    "            \"xlabel\": r\"$p_T$ (GeV)\",\n",
    "            \"ylabel\": \"$p_T^{reco}$\",\n",
    "            \"xmin\": 20,\n",
    "            \"xmax\": 80,\n",
    "        },\n",
    "        \"RecoDataeta\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\"] + X,\n",
    "            \"xlabel\": r\"$\\eta$\",\n",
    "            \"ylabel\": \"$\\eta^{reco}$\",\n",
    "            \"xmin\": -5,\n",
    "            \"xmax\": 5,\n",
    "        },\n",
    "        \"RecoDataphi\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\", \"RecoDataeta\"] + X,\n",
    "            \"xlabel\": r\"$\\phi$\",\n",
    "            \"ylabel\": \"$\\phi^{reco}$\",\n",
    "            \"xmin\": -3.2,\n",
    "            \"xmax\": 3.2,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Load and explore raw (unscaled) dataframes\n",
    "\n",
    "\n",
    "all_variable_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "]\n",
    "all_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "    \"tau\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb2b41-f128-45d0-993a-cfdd421b768d",
   "metadata": {},
   "source": [
    "## 3.2: Needed Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29814968-1591-409f-9655-42a962b5eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3c5cc4-9f36-4642-b4c2-9fc9e3150955",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Load unscaled dataframes ###################################\n",
    "@memory.cache\n",
    "def load_raw_data():\n",
    "    \"\"\"Dont use AUTOREGRESSIVE_DIST_NAME for training of any variable. \n",
    "    For mass evaluation: dont use AUTOREGRESSIVE_DIST_NAME. For pT evaluation use AUTOREGRESSIVE_DIST_NAME \n",
    "    as the distribution predicted by mass, etc.  \"\"\"\n",
    "    print(f\"\\nSUBSAMPLE = {SUBSAMPLE}\\n\")\n",
    "    raw_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    raw_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"validation_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    raw_test_data = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"test_data_10M_2.csv\"), \n",
    "    usecols=all_cols, \n",
    "    nrows=SUBSAMPLE\n",
    "    )\n",
    "\n",
    "    print(\"\\n RAW TRAIN DATA\\n\")\n",
    "    print(raw_train_data.shape)\n",
    "    raw_train_data.describe()  # unscaled\n",
    "    print(\"\\n RAW TEST DATA\\n\")\n",
    "    print(raw_test_data.shape)\n",
    "    raw_test_data.describe()  # unscaled\n",
    "\n",
    "    return raw_train_data, raw_test_data, raw_valid_data\n",
    "\n",
    "\n",
    "########## Generate scaled data###############\n",
    "# scaled_train_data = L_scale_df(raw_train_data, title='scaled_train_data_10M_2.csv',\n",
    "#                              save=True)\n",
    "# print('\\n\\n')\n",
    "# scaled_test_data = L_scale_df(raw_test_data,  title='scaled_test_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "# print('\\n\\n')\n",
    "\n",
    "# scaled_valid_data = L_scale_df(raw_valid_data,  title='scaled_valid_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "\n",
    "# explore_data(df=scaled_train_data, title='Braden Kronheim-L-scaled Dataframe', scaled=True)\n",
    "\n",
    "################ Load scaled data##############\n",
    "@utils.time_type_of_func(tuning_or_training=\"loading\")\n",
    "# @memory.cache\n",
    "def load_scaled_dataframes():\n",
    "    print(\"SCALED TRAIN DATA\")\n",
    "    scaled_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    print(\"TRAINING FEATURES\\n\", scaled_train_data.head())\n",
    "\n",
    "    scaled_test_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_test_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    scaled_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_valid_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    return scaled_train_data, scaled_test_data, scaled_valid_data\n",
    "\n",
    "\n",
    "#######################################\n",
    "#\n",
    "# # print('\\nTESTING FEATURES\\n', scaled_test_data.head())\n",
    "\n",
    "# print('\\ntrain set shape:',  scaled_train_data.shape)\n",
    "# print('\\ntest set shape:  ', scaled_test_data.shape)\n",
    "# # print('validation set shape:', valid_data.shape)\n",
    "# @memory.cache\n",
    "def get_train_scale_dict(USE_BRADEN_SCALING):\n",
    "    if USE_BRADEN_SCALING==True:\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(scaled_train_data)\n",
    "        print(\"BRADEN SCALING DICTIONARY\")\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    else:\n",
    "        print(\"NORMAL UNSCALED DICTIONARY\")\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(raw_train_data)\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    return TRAIN_SCALE_DICT\n",
    "\n",
    "\n",
    "################################ SPLIT###########\n",
    "# @memory.cache\n",
    "def T(variable, scaled_df):\n",
    "    if variable == \"pT\":\n",
    "        L_pT_gen = scaled_df[\"genDatapT\"]\n",
    "        L_pT_reco = scaled_df[\"RecoDatapT\"]\n",
    "        target = (L_pT_reco + 10) / (L_pT_gen + 10)\n",
    "    if variable == \"eta\":\n",
    "        L_eta_gen = scaled_df[\"genDataeta\"]\n",
    "        L_eta_reco = scaled_df[\"RecoDataeta\"]\n",
    "        target = (L_eta_reco + 10) / (L_eta_gen + 10)\n",
    "    if variable == \"phi\":\n",
    "        L_phi_gen = scaled_df[\"genDataphi\"]\n",
    "        L_phi_reco = scaled_df[\"RecoDataphi\"]\n",
    "        target = (L_phi_reco + 10) / (L_phi_gen + 10)\n",
    "    if variable == \"m\":\n",
    "        L_m_gen = scaled_df[\"genDatam\"]\n",
    "        L_m_reco = scaled_df[\"RecoDatam\"]\n",
    "        target = (L_m_reco + 10) / (L_m_gen + 10)\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def split_t_x(df, target, input_features):\n",
    "    \"\"\"Get teh target as the ratio, according to the T equation\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_train_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def split_t_x_test(df, target, input_features):\n",
    "    \"\"\"Get teh target as the ratio, according to the T equation\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_test_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# @memory.cache\n",
    "def normal_split_t_x(df, target, input_features):\n",
    "    # change from pandas dataframe format to a numpy\n",
    "    # array of the specified types\n",
    "    # t = np.array(df[target])\n",
    "    t = np.array(df[target])\n",
    "    x = np.array(df[input_features])\n",
    "    return t, x\n",
    "\n",
    "\n",
    "################ Apply Z scaling############\n",
    "def z(x):\n",
    "    eps = 1e-20\n",
    "    return (x - np.mean(x)) / (np.std(x) + eps)\n",
    "\n",
    "\n",
    "def z_inverse(xprime, x):\n",
    "    return xprime * np.std(x) + np.mean(x)\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z2(x, mean, std):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x ([type]): [description]\n",
    "        mean ([type]): [description]\n",
    "        std ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    scaled = (x - mean) / (std + eps)\n",
    "    return np.array(scaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "def z_inverse(xprime, x):\n",
    "    unscaled = xprime * np.std(x) + np.mean(x)\n",
    "    return np.array(unscaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z_inverse2(xprime, train_mean, train_std):\n",
    "    \"\"\"mean original train mean, std: original. Probably not needed\"\"\"\n",
    "    return xprime * train_std + train_mean\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x):\n",
    "    \"\"\"TO ensure this z scaling is only applied once to the training features, we use a generator.\n",
    "    This doesn't change the shapes of anything, just applies z to all the feature columns other than tau\"\"\"\n",
    "    NFEATURES = train_x.shape[1]\n",
    "    for i in range(NFEATURES - 1):\n",
    "        variable = list(TRAIN_SCALE_DICT)[i]\n",
    "        train_mean = float(TRAIN_SCALE_DICT[variable][\"mean\"])\n",
    "        train_std = float(TRAIN_SCALE_DICT[variable][\"std\"])\n",
    "        train_x[:, i] = z2(train_x[:, i], mean=train_mean, std=train_std)\n",
    "        test_x[:, i] = z2(test_x[:, i], mean=train_mean, std=train_std)\n",
    "        valid_x[:, i] = z2(valid_x[:, i], mean=train_mean, std=train_std)\n",
    "    yield train_x\n",
    "    yield test_x\n",
    "    yield valid_x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_targets(train_t, test_t, valid_t):\n",
    "    train_mean = np.mean(train_t)\n",
    "    train_std = np.std(train_t)\n",
    "    train_t_ = z2(train_t, mean=train_mean, std=train_std)\n",
    "    test_t_ = z2(test_t, mean=train_mean, std=train_std)\n",
    "    valid_t_ = z2(valid_t, mean=train_mean, std=train_std)\n",
    "\n",
    "    yield train_t_\n",
    "    yield test_t_\n",
    "    yield valid_t_\n",
    "\n",
    "\n",
    "# @utils.debug\n",
    "def save_model(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "# @utils.debug\n",
    "def load_model(PATH, PARAMS):\n",
    "    # n_layers = int(BEST_PARAMS[\"n_layers\"])\n",
    "    # hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "    # dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "    # optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "    # learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "    # batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "    model = utils.RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    # OR\n",
    "    # model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED! Also, use dictionary \".pth\" which has both the model state dict and the PARAMS dict\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def simple_eval(model, test_x_z_scaled):\n",
    "    model.eval()\n",
    "    # evaluate on the scaled features\n",
    "    valid_x_tensor = torch.from_numpy(test_x_z_scaled).float()\n",
    "    # valid_x_tensor=torch.from_numpy(train_x).float()\n",
    "    pred = model(valid_x_tensor)\n",
    "    p = pred.detach().numpy()\n",
    "    # if USE_BRADEN_SCALING:\n",
    "    #     fig, ax = plt.subplots(1,1)\n",
    "    #     label=FIELDS[target]['ylabel']\n",
    "    #     ax.hist(p, label=f'Predicted post-z ratio for {label}', alpha=0.4, density=True)\n",
    "    #     # orig_ratio = z(T('m', scaled_df=scaled_train_data))\n",
    "    #     orig_ratio = z(T('m', scaled_df=scaled_test_data))\n",
    "    #     print(orig_ratio[:5])\n",
    "    #     ax.hist(orig_ratio, label = f'original post-z ratio for {label}', alpha=0.4,density=True)\n",
    "    #     ax.grid()\n",
    "    #     set_axes(ax, xlabel='predicted $T$')\n",
    "    # print('predicted ratio shape: ', p.shape)\n",
    "    return p\n",
    "\n",
    "def get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME):\n",
    "        \n",
    "    print(f'Test (evaluation) Data is Autoregressive, loading {AUTOREGRESSIVE_DIST_NAME}')\n",
    "    eval_data = pd.read_csv(\n",
    "        os.path.join(\n",
    "            IQN_BASE,\n",
    "            \"JupyterBook\",\n",
    "            \"Cluster\",\n",
    "            \"EVALUATE\",\n",
    "            AUTOREGRESSIVE_DIST_NAME,\n",
    "        )\n",
    "    )\n",
    "    return eval_data\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def get_hist(label):\n",
    "    \"\"\"label could be \"pT\", \"eta\", \"phi\", \"m\" \"\"\"\n",
    "    predicted_label_counts, label_edges = np.histogram(\n",
    "        JETS_DICT[\"Predicted_RecoData\" + label][\"dist\"],\n",
    "        range=JETS_DICT[\"Predicted_RecoData\" + label][\"range\"],\n",
    "        bins=bins,\n",
    "    )\n",
    "    real_label_counts, _ = np.histogram(\n",
    "        JETS_DICT[\"Real_RecoData\" + label][\"dist\"],\n",
    "        range=JETS_DICT[\"Real_RecoData\" + label][\"range\"],\n",
    "        bins=bins,\n",
    "    )\n",
    "    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2\n",
    "\n",
    "    return real_label_counts, predicted_label_counts, label_edges\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def get_hist_simple(predicted_dist, target):\n",
    "    \n",
    "    range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "    bins=50\n",
    "    predicted_label_counts, label_edges = np.histogram(\n",
    "        predicted_dist, range=range_, bins=bins\n",
    "    )\n",
    "    \n",
    "    \n",
    "    real_label_counts, _ = np.histogram(REAL_DIST, range=range_, bins=bins)\n",
    "    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2\n",
    "    return real_label_counts, predicted_label_counts, label_edges\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def plot_one(\n",
    "    target, real_edges, real_counts, predicted_counts, save_plot=False, PARAMS=None, JUPYTER=True\n",
    "):\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2, 1, figsize=(3.5 * 3 / 2.5, 3.8), gridspec_kw={\"height_ratios\": [2, 0.5]}\n",
    "    )\n",
    "    ax1.step(\n",
    "        real_edges, real_counts / norm_data, where=\"mid\", color=\"k\", linewidth=0.5\n",
    "    )  # step real_count_pt\n",
    "    ax1.step(\n",
    "        real_edges,\n",
    "        predicted_counts / norm_IQN,\n",
    "        where=\"mid\",\n",
    "        color=\"#D7301F\",\n",
    "        linewidth=0.5,\n",
    "    )  # step predicted_count_pt\n",
    "    ax1.scatter(\n",
    "        real_edges,\n",
    "        real_counts / norm_data,\n",
    "        label=\"reco\",\n",
    "        color=\"k\",\n",
    "        facecolors=\"none\",\n",
    "        marker=\"o\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax1.scatter(\n",
    "        real_edges,\n",
    "        predicted_counts / norm_IQN,\n",
    "        label=\"predicted\",\n",
    "        color=\"#D7301F\",\n",
    "        marker=\"x\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax1.set_xlim(range_)\n",
    "    ax1.set_ylim(0, max(predicted_counts / norm_IQN) * 1.1)\n",
    "    ax1.set_ylabel(\"counts\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    ratio = (predicted_counts / norm_IQN) / (real_counts / norm_data)\n",
    "    ax2.scatter(\n",
    "        real_edges, ratio, color=\"r\", marker=\"x\", s=5, linewidth=0.5\n",
    "    )  # PREDICTED (IQN)/Reco (Data)\n",
    "    ax2.scatter(\n",
    "        real_edges,\n",
    "        ratio / ratio,\n",
    "        color=\"k\",\n",
    "        marker=\"o\",\n",
    "        facecolors=\"none\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax2.set_xlim(range_)\n",
    "    ax2.set_xlabel(FIELDS[target][\"xlabel\"])\n",
    "    ax2.set_ylabel(\n",
    "        r\"$\\frac{\\textnormal{predicted}}{\\textnormal{reco}}$\"\n",
    "        #    , fontsize=10\n",
    "    )\n",
    "    ax2.set_ylim((YLIM))\n",
    "    ax2.set_xlim(range_)\n",
    "    ax2.set_yticklabels([0.8, 1.0, 1.2])\n",
    "    if JUPYTER==True:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(wspace=0.5, hspace=0.2)\n",
    "        fig.subplots_adjust(wspace=0.0, hspace=0.1)\n",
    "        plt.axis('off')\n",
    "\n",
    "    # plt.gca().set_position([0, 0, 1, 1])\n",
    "    if save_plot:\n",
    "        plot_filename = utils.get_model_filename(target, PARAMS).split(\".dict\")[0] + \".png\"\n",
    "        plt.savefig(\n",
    "            os.path.join(IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", plot_filename)\n",
    "        )\n",
    "\n",
    "    \n",
    "    # fig.show()\n",
    "    # plt.show();\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.gca().set_position([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3336b47e-6e8a-43ba-a93a-5067d6b44dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data only once, and with caching!\n",
    "raw_train_data, raw_test_data, raw_valid_data = load_raw_data()\n",
    "\n",
    "# Load scaled data\n",
    "# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d40d613-52be-4b51-ab10-822df7976083",
   "metadata": {},
   "source": [
    "## 3.3: Evaluate Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c5cef-a246-45ea-9ee4-d4d22452a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n",
      "spliting data for RecoDatam\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 5)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 5)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 5)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 3.27223764e+01  6.98189368e-04 -8.95543973e-04  6.96116528e+00\n",
      "  5.00485136e-01] [15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]\n",
      "[ 3.26952341e+01 -1.78188172e-03 -3.83090331e-04  6.96299435e+00\n",
      "  4.99915289e-01] [14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]\n",
      "5.5514112643334546 2.664124544901276\n",
      "5.555567451922438 2.664339857066051\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[ 1.81700902e-03  1.12510099e-03 -2.82526482e-04 -6.57626105e-04\n",
      "  5.00485136e-01] [1.01748627 0.9999745  0.99989115 0.99987283 0.28852734]\n",
      "[ 1.61650249e-15 -1.10134124e-18 -3.12905257e-17  5.01036723e-15\n",
      "  4.99915289e-01] [1.         1.         1.         1.         0.28867295]\n",
      "-0.0015599314696879494 0.9999191874249058\n",
      "6.996216939114674e-16 1.0000000000000002\n",
      "<class 'str'>\n",
      "This model was trained for 500000 iteration, which is  32.0 epochs\n"
     ]
    }
   ],
   "source": [
    "# TODO: plot the loss curves (train and valid) of all 4 networks on the same plot,\n",
    "# and with the learning rate plotted on the same plot but on a different y axis \n",
    "# (x axis being iteration, with marks indicating epochs)\n",
    "\n",
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "########################################################################################\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING==True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = split_t_x(\n",
    "        df=scaled_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "        df=raw_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "        df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = normal_split_t_x(\n",
    "        df=raw_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "######################################################\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "\n",
    "NFEATURES = train_x.shape[1]\n",
    "TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "# to features\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "###########################################################\n",
    "# Get the  parameters for this model and training\n",
    "PARAMS_m = {\n",
    "\"n_layers\": int(4),\n",
    "\"hidden_size\": int(5),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(0.5),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(512),\n",
    "    'n_iterations': int(5e5),\n",
    "}\n",
    "\n",
    "optimizer_name = PARAMS_m[\"optimizer_name\"]\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS = PARAMS_m[\"n_iterations\"]\n",
    "BATCHSIZE = PARAMS_m[\"batch_size\"]\n",
    "comment = \"\"\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(\n",
    "    f\"This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs\"\n",
    ")\n",
    "\n",
    "# 'Trained_IQNx4_%s_TUNED.dict' % target\n",
    "filename_model = utils.get_model_filename(target, PARAMS_m)\n",
    "# OR, if you know a model filename directly, you can also specify it, \n",
    "# BUT, if you pull a trained model explicitly, you have to make sure its parameters in the PARAMS dictionary above match\n",
    "# Nominal one is 'Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict', also in backup\n",
    "# filename_model='Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict'\n",
    "# filename_model='Trained_IQNx4_RecoDatapT_10_layer6_hiddenLeakyReLU_activation512_batchsize300_Kiteration.dict'\n",
    "trained_models_dir = \"trained_models\"\n",
    "utils.mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE,  # the loaction of the repo\n",
    "    \"JupyterBook\",  # up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\",\n",
    "    \"TRAIN\",\n",
    "    trained_models_dir,  # /trained_models\n",
    "    filename_model,  # utils.get_model_filename has the saved file format\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "# Get predicted distribution\n",
    "p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "\n",
    "range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "bins = 50\n",
    "REAL_RAW_DATA = raw_test_data\n",
    "\n",
    "YLIM = (0.8, 1.2)\n",
    "###########GET REAL DIST###########\n",
    "REAL_RAW_DATA = REAL_RAW_DATA[\n",
    "    [\"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\", \"RecoDatam\"]\n",
    "]\n",
    "REAL_RAW_DATA.columns = [\"realpT\", \"realeta\", \"realphi\", \"realm\"]\n",
    "REAL_DIST = REAL_RAW_DATA[\"realm\"]\n",
    "norm_data = REAL_RAW_DATA.shape[0]\n",
    "#############GET EVALUATION DIST#############\n",
    "raw_test_data.describe()\n",
    "m_reco = raw_test_data[\"RecoDatam\"]\n",
    "m_gen = raw_test_data[\"genDatam\"]\n",
    "# plt.hist(m_reco,label=r'$m_{gen}^{test \\ data}$');plt.legend();plt.show()\n",
    "\n",
    "\n",
    "def descale_Braden_scaled_prediction(label, p):\n",
    "    \"\"\"Label could be m. p is the outcome of the model evaluation, e.g. \n",
    "    IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "    p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "    \n",
    "    \"\"\"\n",
    "    # make sure you've set braden scaling global variable to use this function.\n",
    "    assert USE_BRADEN_SCALING==True\n",
    "    orig_ratio = T(label, scaled_df=scaled_train_data)\n",
    "    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))\n",
    "    L_obs = L(orig_observable=m_gen, label=label)\n",
    "    z_inv_f = z_inv_f.flatten()\n",
    "    print(z_inv_f.shape)\n",
    "\n",
    "    factor = (z_inv_f * (L_obs + 10)) - 10\n",
    "    label_pred = L_inverse(L_observable=factor, label=label)\n",
    "    return label_pred\n",
    "    \n",
    "    \n",
    "m_pred = z_inverse2(\n",
    "    xprime=p,\n",
    "    train_mean=TRAIN_SCALE_DICT[target][\"mean\"],\n",
    "    train_std=TRAIN_SCALE_DICT[target][\"std\"],\n",
    ")\n",
    "m_pred = m_pred.flatten()\n",
    "\n",
    "# Get histogram of predicted distribution\n",
    "real_label_counts_m, predicted_label_counts_m, label_edges_m = get_hist_simple(\n",
    "    predicted_dist=m_pred, target=target\n",
    ")\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "# Get evaluation data\n",
    "eval_data = pd.read_csv(DATA_DIR + \"/test_data_10M_2.csv\")\n",
    "ev_features = features\n",
    "eval_data = eval_data[ev_features]\n",
    "# save new distribution (m) in the eval data as autoregressive eval for next IQN\n",
    "eval_data[target] = m_pred\n",
    "\n",
    "new_cols = [target] + features\n",
    "eval_data = eval_data.reindex(columns=new_cols)\n",
    "print(\"EVALUATION DATA NEW INDEX\\n\", eval_data.head())\n",
    "\n",
    "eval_data.to_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load this saved predited autoregressive distribution\n",
    "AUTOREGRESSIVE_DIST = pd.read_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]\n",
    "# get normalization values\n",
    "norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]\n",
    "norm_IQN = norm_autoregressive\n",
    "print(\n",
    "    \"norm_data\",\n",
    "    norm_data,\n",
    "    \"\\nnorm IQN\",\n",
    "    norm_IQN,\n",
    "    \"\\nnorm_autoregressive\",\n",
    "    norm_autoregressive,\n",
    ")\n",
    "\n",
    "# Finally, plot predicted distribution\n",
    "\n",
    "plot_one(\n",
    "    target=target,\n",
    "    real_edges=label_edges_m,\n",
    "    real_counts=real_label_counts_m,\n",
    "    predicted_counts=predicted_label_counts_m,\n",
    "    save_plot=True,\n",
    "    PARAMS=PARAMS_m\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df205105-a24d-4694-a182-ef84c08bfd4c",
   "metadata": {},
   "source": [
    "## 3.4: Evaluate $p_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed548cae-19f9-44d7-b30f-1328fd8fb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['RecoDatam', 'genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatapT\n",
      "USING NEW DATASET\n",
      "\n",
      "spliting autoregressive evaluation data for RecoDatapT\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 6)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[ 2.59587    29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [ 6.25659    41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [ 6.11213    35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [ 4.17483    26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 6)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 6)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 5.55141126e+00  3.27223764e+01  6.98189368e-04 -8.95543973e-04\n",
      "  6.96116528e+00  5.00485136e-01] [ 2.66412454 15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]\n",
      "[ 5.55556745e+00  3.26952341e+01 -1.78188172e-03 -3.83090331e-04\n",
      "  6.96299435e+00  4.99915289e-01] [ 2.66433986 14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]\n",
      "32.881453465999996 16.02400426348493\n",
      "32.86720151648752 15.829355769531851\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[-1.81710707e+00  1.48455353e+01  5.96132264e-04 -2.50379668e+00\n",
      " -1.63658184e+00  5.00485136e-01] [0.17834627 6.89519305 1.2152514  0.65207164 0.17568487 0.28852734]\n",
      "[-1.81682884e+00  1.48332220e+01 -7.71183141e-04 -2.50361243e+00\n",
      " -1.63646629e+00  4.99915289e-01] [0.17836068 6.77669391 1.21528238 0.65214262 0.17570722 0.28867295]\n",
      "0.0009003493079555966 1.0122966781963252\n",
      "-1.2048033681821834e-15 1.0000000000000002\n",
      "<class 'str'>\n",
      "This model was trained for 500000 iteration, which is  64.0 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (3): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.3)\n",
      "    (5): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (6): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (9): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.3)\n",
      "    (11): Linear(in_features=6, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "EVALUATION DATA NEW INDEX\n",
      "    RecoDatam  RecoDatapT  genDatapT  genDataeta  genDataphi  genDatam  \\\n",
      "0    5.80270     44.3274    43.6113    0.824891    -1.26949   5.93310   \n",
      "1    5.80270     44.3274    43.6113    0.824891    -1.26949   5.93310   \n",
      "2    4.81403     27.4750    26.0153    3.529970     1.55495   7.41270   \n",
      "3    7.06425     33.8797    28.4944   -1.159650     1.82602   7.84157   \n",
      "4    4.08061     23.3141    21.9840    2.747660     2.03085   5.18315   \n",
      "\n",
      "        tau  \n",
      "0  0.250046  \n",
      "1  0.847493  \n",
      "2  0.851995  \n",
      "3  0.052378  \n",
      "4  0.542549  \n",
      "norm_data 1000000 \n",
      "norm IQN 1000000 \n",
      "norm_autoregressive 1000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 302.4x273.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = \"RecoDatapT\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "PREVIOUS_AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime_pT_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "raw_train_data, raw_test_data, raw_valid_data = load_raw_data()\n",
    "\n",
    "\n",
    "# Load scaled data\n",
    "# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()\n",
    "\n",
    "\n",
    "################################################## Load Evaluation Data\n",
    "#eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "# Or test on actual test (evaluation) data for development\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING == True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "        \n",
    "    ##### WHAT MATTERS IS TEST (EVALUATION)\n",
    "\n",
    "    test_t, test_x = split_t_x(\n",
    "        df=scaled_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting autoregressive evaluation data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "            df=raw_train_data, target=target, input_features=features\n",
    "        )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "        df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    ##### WHAT MATTERS IS TEST (EVALUATION)\n",
    "    test_t, test_x = normal_split_t_x(\n",
    "        df=raw_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "######################################################\n",
    "# Replace test_x with eval_data\n",
    "\n",
    "# ev_features = features\n",
    "# eval_data_df = eval_data[ev_features]\n",
    "# eval_data = np.array(eval_data_df)\n",
    "# test_x = eval_data\n",
    "\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "    \n",
    "# eval_data=raw_train_data[:raw_test_data.shape[0]]\n",
    "# test_x = np.array\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "\n",
    "NFEATURES = train_x.shape[1]\n",
    "# GET EVALUATION DATASET\n",
    "# eval_data= get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME=PREVIOUS_AUTOREGRESSIVE_DIST_NAME)\n",
    "# test_x = np.array(eval_data[features])\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "# to features\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "###########################################################\n",
    "# Get the  parameters for this model and training\n",
    "PARAMS_pT =  {\n",
    "\"n_layers\": int(4),\n",
    "\"hidden_size\": int(6),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(1e-2),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(1024),\n",
    "    'n_iterations': int(5e5),\n",
    "}\n",
    "\n",
    "optimizer_name = PARAMS_pT[\"optimizer_name\"]\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS = PARAMS_pT[\"n_iterations\"]\n",
    "BATCHSIZE = PARAMS_pT[\"batch_size\"]\n",
    "comment = \"\"\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(\n",
    "    f\"This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs\"\n",
    ")\n",
    "\n",
    "\n",
    "filename_model = utils.get_model_filename(target, PARAMS_pT)\n",
    "# filename_model = 'Trained_IQNx4_RecoDatapT_ 13_layer6_hiddenLeakyReLU_activation1024_batchsize200_Kiteration.dict'\n",
    "# filename_model = 'Trained_IQNx4_RecoDatapT_10_layer6_hiddenLeakyReLU_activation512_batchsize300_Kiteration.dict'\n",
    "trained_models_dir = \"trained_models\"\n",
    "utils.mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE,  # the loaction of the repo\n",
    "    \"JupyterBook\",  # up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\",\n",
    "    \"TRAIN\",\n",
    "    trained_models_dir,  # /trained_models\n",
    "    filename_model,  # utils.get_model_filename has the saved file format\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "IQN_pT = load_model(PATH_model, PARAMS_pT)\n",
    "# Get predicted distribution\n",
    "p = simple_eval(IQN_pT, test_x_z_scaled)\n",
    "\n",
    "range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "bins = 50\n",
    "REAL_RAW_DATA = raw_test_data\n",
    "\n",
    "YLIM = (0.8, 1.2)\n",
    "###########GET REAL DIST###########\n",
    "REAL_RAW_DATA = REAL_RAW_DATA[\n",
    "    [\"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\", \"RecoDatam\"]\n",
    "]\n",
    "REAL_RAW_DATA.columns = [\"realpT\", \"realeta\", \"realphi\", \"realm\"]\n",
    "REAL_DIST = REAL_RAW_DATA[\"realpT\"]\n",
    "norm_data = REAL_RAW_DATA.shape[0]\n",
    "#############GET EVALUATION DIST#############\n",
    "raw_test_data.describe()\n",
    "pT_reco = raw_test_data[\"RecoDatapT\"]\n",
    "pT_gen = raw_test_data[\"genDatapT\"]\n",
    "# plt.hist(m_reco,label=r'$m_{gen}^{test \\ data}$');plt.legend();plt.show()\n",
    "\n",
    "def descale_Braden_scaled_prediction(label, p):\n",
    "    \"\"\"Label could be m. p is the outcome of the model evaluation, e.g. \n",
    "    IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "    p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "\n",
    "    \"\"\"\n",
    "    # make sure you've set braden scaling global variable to use this function.\n",
    "    assert USE_BRADEN_SCALING==True\n",
    "    orig_ratio = T(label, scaled_df=scaled_train_data)\n",
    "    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))\n",
    "    L_obs = L(orig_observable=pT_gen, label=label)\n",
    "    z_inv_f = z_inv_f.flatten()\n",
    "    print(z_inv_f.shape)\n",
    "\n",
    "    factor = (z_inv_f * (L_obs + 10)) - 10\n",
    "    label_pred = L_inverse(L_observable=factor, label=label)\n",
    "    return label_pred\n",
    "\n",
    "\n",
    "pT_pred = z_inverse2(\n",
    "    xprime=p,\n",
    "    train_mean=TRAIN_SCALE_DICT[target][\"mean\"],\n",
    "    train_std=TRAIN_SCALE_DICT[target][\"std\"],\n",
    ")\n",
    "pT_pred = pT_pred.flatten()\n",
    "\n",
    "# Get histogram of predicted distribution\n",
    "real_label_counts_pT, predicted_label_counts_pT, label_edges_pT = get_hist_simple(\n",
    "    predicted_dist=pT_pred, target=target\n",
    ")\n",
    "\n",
    "# Get evaluation data as test data for development\n",
    "\n",
    "eval_data_df=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')#[features]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_cols = [\"RecoDatam\", target] + X\n",
    "eval_data_df = eval_data_df.reindex(columns=new_cols)\n",
    "print(\"EVALUATION DATA NEW INDEX\\n\", eval_data_df.head())\n",
    "# save \n",
    "eval_data_df.to_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load this saved predited autoregressive distribution\n",
    "AUTOREGRESSIVE_DIST = pd.read_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]\n",
    "# get normalization values\n",
    "norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]\n",
    "norm_IQN = norm_autoregressive\n",
    "print(\n",
    "    \"norm_data\",\n",
    "    norm_data,\n",
    "    \"\\nnorm IQN\",\n",
    "    norm_IQN,\n",
    "    \"\\nnorm_autoregressive\",\n",
    "    norm_autoregressive,\n",
    ")\n",
    "\n",
    "# Finally, plot predicted distribution\n",
    "plot_one(\n",
    "    target=target,\n",
    "    real_edges=label_edges_pT,\n",
    "    real_counts=real_label_counts_pT,\n",
    "    predicted_counts=predicted_label_counts_pT,\n",
    "    save_plot=True,\n",
    "    PARAMS=PARAMS_pT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48801bd-6864-425d-b4cd-fbe5fdf89790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
