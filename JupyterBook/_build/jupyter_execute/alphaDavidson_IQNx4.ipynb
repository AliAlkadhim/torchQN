{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwdrt7e8bYrb",
    "outputId": "a2783344-212f-4d00-9bc0-240e4956b918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "\n",
      " b''\n",
      "Downloading Environment\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess as sb\n",
    "COLAB=True\n",
    "if COLAB==True:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "    #pwd=/content\n",
    "  sb.run('mkdir -p drive/MyDrive/IQNx4/utils', shell=True)\n",
    "  os.environ[\"IQN_BASE\"] = os.path.join(os.getcwd(), 'drive', 'MyDrive', 'IQNx4')\n",
    "  os.environ[\"DATA_DIR\"] = os.path.join(os.getcwd(), 'drive', 'MyDrive', 'IQNx4')\n",
    "  sp=sb.Popen('tree drive/MyDrive', shell=True, stdout=sb.PIPE, stderr=sb.PIPE)\n",
    "  out,err=sp.communicate(); print('\\n', out)\n",
    "  print('Downloading Environment')#sudo apt update && sudo apt upgrade && \n",
    "  sb.run('sudo apt-get update && sudo apt install wget pandoc dvipng texlive-xetex texlive-fonts-recommended cm-super', shell=True)\n",
    "  #download utils.py from github\n",
    "  utils_url='https://raw.githubusercontent.com/AliAlkadhim/torchQN/master/utils/utils.py'\n",
    "  sb.run('wget %s drive/MyDrive/IQNx4/utils' % utils_url, shell=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbJPQvuiHIQx",
    "outputId": "e41b0529-78ec-4d80-a6ab-f02b3e5ba879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 19 01:38:05 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   53C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc3GyfBBHdPz",
    "outputId": "4be790c7-b201-4542-f7b4-b54a4c138679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:                    x86_64\n",
      "CPU op-mode(s):                  32-bit, 64-bit\n",
      "Byte Order:                      Little Endian\n",
      "Address sizes:                   46 bits physical, 48 bits virtual\n",
      "CPU(s):                          4\n",
      "On-line CPU(s) list:             0-3\n",
      "Thread(s) per core:              2\n",
      "Core(s) per socket:              2\n",
      "Socket(s):                       1\n",
      "NUMA node(s):                    1\n",
      "Vendor ID:                       GenuineIntel\n",
      "CPU family:                      6\n",
      "Model:                           85\n",
      "Model name:                      Intel(R) Xeon(R) CPU @ 2.00GHz\n",
      "Stepping:                        3\n",
      "CPU MHz:                         2000.186\n",
      "BogoMIPS:                        4000.37\n",
      "Hypervisor vendor:               KVM\n",
      "Virtualization type:             full\n",
      "L1d cache:                       64 KiB\n",
      "L1i cache:                       64 KiB\n",
      "L2 cache:                        2 MiB\n",
      "L3 cache:                        38.5 MiB\n",
      "NUMA node0 CPU(s):               0-3\n",
      "Vulnerability Itlb multihit:     Not affected\n",
      "Vulnerability L1tf:              Mitigation; PTE Inversion\n",
      "Vulnerability Mds:               Vulnerable; SMT Host state unknown\n",
      "Vulnerability Meltdown:          Vulnerable\n",
      "Vulnerability Mmio stale data:   Vulnerable\n",
      "Vulnerability Retbleed:          Vulnerable\n",
      "Vulnerability Spec store bypass: Vulnerable\n",
      "Vulnerability Spectre v1:        Vulnerable: __user pointer sanitization and use\n",
      "                                 rcopy barriers only; no swapgs barriers\n",
      "Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PB\n",
      "                                 RSB-eIBRS: Not affected\n",
      "Vulnerability Srbds:             Not affected\n",
      "Vulnerability Tsx async abort:   Vulnerable\n",
      "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtr\n",
      "                                 r pge mca cmov pat pse36 clflush mmx fxsr sse s\n",
      "                                 se2 ss ht syscall nx pdpe1gb rdtscp lm constant\n",
      "                                 _tsc rep_good nopl xtopology nonstop_tsc cpuid \n",
      "                                 tsc_known_freq pni pclmulqdq ssse3 fma cx16 pci\n",
      "                                 d sse4_1 sse4_2 x2apic movbe popcnt aes xsave a\n",
      "                                 vx f16c rdrand hypervisor lahf_lm abm 3dnowpref\n",
      "                                 etch invpcid_single ssbd ibrs ibpb stibp fsgsba\n",
      "                                 se tsc_adjust bmi1 hle avx2 smep bmi2 erms invp\n",
      "                                 cid rtm mpx avx512f avx512dq rdseed adx smap cl\n",
      "                                 flushopt clwb avx512cd avx512bw avx512vl xsaveo\n",
      "                                 pt xsavec xgetbv1 xsaves arat md_clear arch_cap\n",
      "                                 abilities\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8B8H9e-icVC7",
    "outputId": "9269ab6c-d3c8-4e7c-8a37-93cfb9928747"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using torch version 1.13.1+cu116\n",
      "matplotlib version=  3.2.2\n",
      "optuna is only used for hyperparameter tuning, not critical!\n",
      "\n",
      "BASE directoy properly set =  /content/drive/MyDrive/IQNx4\n",
      "using torch version 1.13.1+cu116\n",
      "matplotlib version=  3.2.2\n",
      "optuna is only used for hyperparameter tuning, not critical!\n",
      "BASE directoy properly set =  /content/drive/MyDrive/IQNx4\n",
      "DATA directory also properly set, in /content/drive/MyDrive/IQNx4\n",
      "using DATA_DIR=/content/drive/MyDrive/IQNx4\n",
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import scipy as sp; import scipy.stats as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"using torch version {torch.__version__}\")#old torch version: 1.9.0\n",
    "# use numba's just-in-time compiler to speed things up\n",
    "# from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "\n",
    "print(\"matplotlib version= \", mp.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reset matplotlib stle/parameters\n",
    "import matplotlib as mpl\n",
    "\n",
    "# reset matplotlib parameters to their defaults\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use(\"seaborn-deep\")\n",
    "mp.rcParams[\"agg.path.chunksize\"] = 10000\n",
    "font_legend = 15\n",
    "font_axes = 15\n",
    "%matplotlib inline\n",
    "import sys\n",
    "#or use joblib for caching on disk\n",
    "from joblib import  Memory\n",
    "# from IPython.display import Image, display\n",
    "# from importlib import import_module\n",
    "# import plotly\n",
    "try:\n",
    "    import optuna\n",
    "\n",
    "    print(f\"using (optional) optuna version {optuna.__version__}\")\n",
    "except Exception:\n",
    "    print(\"optuna is only used for hyperparameter tuning, not critical!\")\n",
    "    pass\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# import sympy as sy\n",
    "# import ipywidgets as wid;\n",
    "\n",
    "\n",
    "# try:\n",
    "#     IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "#     print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "#     utils_dir = os.path.join(IQN_BASE, \"utils/\")\n",
    "#     sys.path.append(utils_dir)\n",
    "#     import utils\n",
    "\n",
    "#     # usually its not recommended to import everything from a module, but we know\n",
    "#     # whats in it so its fine\n",
    "#     from utils import *\n",
    "\n",
    "#     print(\"DATA directory also properly set, in %s\" % os.environ[\"DATA_DIR\"])\n",
    "# except Exception:\n",
    "#     # IQN_BASE=os.getcwd()\n",
    "#     print(\n",
    "#         \"\"\"\\nBASE directory not properly set. Read repo README.    If you need a function from utils, use the decorator below, or add utils to sys.path\\n\n",
    "#     You can also do \n",
    "#     os.environ['IQN_BASE']=<ABSOLUTE PATH FOR THE IQN REPO>\n",
    "#     or\n",
    "#     os.environ['IQN_BASE']=os.getcwd()\"\"\"\n",
    "#     )\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "# @debug\n",
    "# def get_model_params_simple():\n",
    "#     dropout=0.2\n",
    "#     n_layers = 2\n",
    "#     n_hidden=32\n",
    "#     starting_learning_rate=1e-3\n",
    "#     print('n_iterations, n_layers, n_hidden, starting_learning_rate, dropout')\n",
    "#     return n_iterations, n_layers, n_hidden, starting_learning_rate, dropout\n",
    "\n",
    "\n",
    "# update fonts\n",
    "FONTSIZE = 14\n",
    "font = {\"family\": \"serif\", \"weight\": \"normal\", \"size\": FONTSIZE}\n",
    "mp.rc(\"font\", **font)\n",
    "\n",
    "# set usetex = False if LaTex is not\n",
    "# available on your system or if the\n",
    "# rendering is too slow\n",
    "mp.rc(\"text\", usetex=True)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "# seed = 128\n",
    "# rnd  = np.random.RandomState(seed)\n",
    "# sometimes jupyter doesnt initialize MathJax automatically for latex, so do this:\n",
    "#######\n",
    "\n",
    "\n",
    "\n",
    "IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "print(\"\\nBASE directoy properly set = \", IQN_BASE)\n",
    "utils_dir = os.path.join(IQN_BASE, \"utils/\")\n",
    "sys.path.append(utils_dir)\n",
    "import utils\n",
    "\n",
    "# usually its not recommended to import everything from a module, but we know\n",
    "# whats in it so its fine\n",
    "from utils import *\n",
    "\n",
    "DATA_DIR = os.environ[\"DATA_DIR\"]\n",
    "print(f\"using DATA_DIR={DATA_DIR}\")\n",
    "\n",
    "memory = Memory(DATA_DIR)\n",
    "################################### SET DATA CONFIGURATIONS ###################################\n",
    "\n",
    "y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p(p_T)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "    \"RecoDataeta\": \"$p(\\eta)$\",\n",
    "    \"RecoDataphi\": \"$p(\\phi)$\",\n",
    "    \"RecoDatam\": \"$p(m)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "}\n",
    "\n",
    "loss_y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p_T^{reco}$\",\n",
    "    \"RecoDataeta\": \"$\\eta^{reco}$\",\n",
    "    \"RecoDataphi\": \"$\\phi^{reco}$\",\n",
    "    \"RecoDatam\": \"$m^{reco}$\",\n",
    "}\n",
    "\n",
    "X = [\"genDatapT\", \"genDataeta\", \"genDataphi\", \"genDatam\", \"tau\"]\n",
    "\n",
    "# set order of training:\n",
    "# pT_first: pT->>m->eta->phi\n",
    "# m_first: m->pT->eta->phi\n",
    "\n",
    "\n",
    "ORDER = \"m_First\"\n",
    "\n",
    "if ORDER == \"m_First\":\n",
    "    FIELDS = {\n",
    "        \"RecoDatam\": {\n",
    "            \"inputs\": X,\n",
    "            \"xlabel\": r\"$m$ (GeV)\",\n",
    "            \"ylabel\": \"$m^{reco}$\",\n",
    "            \"xmin\": 0,\n",
    "            \"xmax\": 25,\n",
    "        },\n",
    "        \"RecoDatapT\": {\n",
    "            \"inputs\": [\"RecoDatam\"] + X,\n",
    "            \"xlabel\": r\"$p_T$ (GeV)\",\n",
    "            \"ylabel\": \"$p_T^{reco}$\",\n",
    "            \"xmin\": 20,\n",
    "            \"xmax\": 80,\n",
    "        },\n",
    "        \"RecoDataeta\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\"] + X,\n",
    "            \"xlabel\": r\"$\\eta$\",\n",
    "            \"ylabel\": \"$\\eta^{reco}$\",\n",
    "            \"xmin\": -5,\n",
    "            \"xmax\": 5,\n",
    "        },\n",
    "        \"RecoDataphi\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\", \"RecoDataeta\"] + X,\n",
    "            \"xlabel\": r\"$\\phi$\",\n",
    "            \"ylabel\": \"$\\phi^{reco}$\",\n",
    "            \"xmin\": -3.2,\n",
    "            \"xmax\": 3.2,\n",
    "        },\n",
    "    }\n",
    "elif ORDER== \"phi_first\":\n",
    "    FIELDS = {\n",
    "        \"RecoDataphi\": {\n",
    "        \"inputs\": X,\n",
    "        \"xlabel\": r\"$\\phi$\",\n",
    "        \"ylabel\": \"$\\phi^{reco}$\",\n",
    "        \"xmin\": -3.2,\n",
    "        \"xmax\": 3.2,\n",
    "    },\n",
    "\n",
    "    \"RecoDatam\": {\n",
    "        \"inputs\": ['RecoDataphi'] + X,\n",
    "        \"xlabel\": r\"$m$ (GeV)\",\n",
    "        \"ylabel\": \"$m^{reco}$\",\n",
    "        \"xmin\": 0,\n",
    "        \"xmax\": 25,\n",
    "    },\n",
    "    \"RecoDatapT\": {\n",
    "        \"inputs\": [\"RecoDataphi\", \"RecoDatam\"] + X,\n",
    "        \"xlabel\": r\"$p_T$ (GeV)\",\n",
    "        \"ylabel\": \"$p_T^{reco}$\",\n",
    "        \"xmin\": 20,\n",
    "        \"xmax\": 80,\n",
    "    },\n",
    "    \"RecoDataeta\": {\n",
    "        \"inputs\": [\"RecoDataphi\", \"RecoDatam\", \"RecoDatapT\"]  + X,\n",
    "        \"xlabel\": r\"$\\eta$\",\n",
    "        \"ylabel\": \"$\\eta^{reco}$\",\n",
    "        \"xmin\": -5,\n",
    "        \"xmax\": 5,\n",
    "    },\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "# Load and explore raw (unscaled) dataframes\n",
    "\n",
    "\n",
    "all_variable_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "]\n",
    "all_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "    \"tau\",\n",
    "]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device='cpu'\n",
    "print(f'device is {device}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpvxgbaZgrf5"
   },
   "source": [
    "# 2.2: Load Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KWdnn-f7cgqC"
   },
   "outputs": [],
   "source": [
    "################################### Load unscaled dataframes ###################################\n",
    "################################### Load unscaled dataframes ###################################\n",
    "@memory.cache\n",
    "def load_raw_data():\n",
    "    \"\"\"Load raw train, test, and validation raw (unscaled) dataframes, in that order.\n",
    "\n",
    "    Returns:\n",
    "        list(pandas.DataFrame): train, test, valid raw datafranes\n",
    "    \"\"\"\n",
    "    print(f'SUBSAMPLE = {SUBSAMPLE}')\n",
    "    raw_train_data=pd.read_csv(os.path.join(DATA_DIR,'train_data_10M_2.csv'),\n",
    "                        usecols=all_cols,\n",
    "                        nrows=SUBSAMPLE\n",
    "                        )\n",
    "\n",
    "    raw_test_data=pd.read_csv(os.path.join(DATA_DIR,'test_data_10M_2.csv'),\n",
    "                        usecols=all_cols,\n",
    "                        nrows=SUBSAMPLE\n",
    "                        )\n",
    "\n",
    "    raw_valid_data=pd.read_csv(os.path.join(DATA_DIR,'validation_data_10M_2.csv'),\n",
    "                        usecols=all_cols,\n",
    "                        nrows=SUBSAMPLE\n",
    "                        )\n",
    "\n",
    "\n",
    "    print('\\n RAW TRAIN DATA SHAPE\\n')\n",
    "    print(raw_train_data.shape)\n",
    "    print('\\n RAW TRAIN DATA\\n')\n",
    "    raw_train_data.describe()#unscaled\n",
    "    print('\\n RAW TEST DATA\\ SHAPEn')\n",
    "    print(raw_test_data.shape)\n",
    "    print('\\n RAW TEST DATA\\n')\n",
    "    raw_test_data.describe()#unscaled\n",
    "\n",
    "    return raw_train_data, raw_test_data, raw_valid_data\n",
    "\n",
    "\n",
    "########## Generate scaled data###############\n",
    "# scaled_train_data = L_scale_df(raw_train_data, title='scaled_train_data_10M_2.csv',\n",
    "#                              save=True)\n",
    "# print('\\n\\n')\n",
    "# scaled_test_data = L_scale_df(raw_test_data,  title='scaled_test_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "# print('\\n\\n')\n",
    "\n",
    "# scaled_valid_data = L_scale_df(raw_valid_data,  title='scaled_valid_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "\n",
    "# explore_data(df=scaled_train_data, title='Braden Kronheim-L-scaled Dataframe', scaled=True)\n",
    "\n",
    "################ Load scaled data##############\n",
    "@utils.time_type_of_func(tuning_or_training='loading')\n",
    "@memory.cache\n",
    "def load_scaled_dataframes():\n",
    "    \"\"\"Load L-scaled train, test and validation according to Braden scaling, in that order.\n",
    "\n",
    "    Returns:\n",
    "        list(pandas.DataFarme): L-scaled train, test, validation dataframes, in that order.\n",
    "    \"\"\"\n",
    "    # print(\"SCALED TRAIN DATA\")\n",
    "    scaled_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    # print(\"TRAINING FEATURES\\n\", scaled_train_data.head())\n",
    "\n",
    "    scaled_test_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_test_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    scaled_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_valid_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    return scaled_train_data, scaled_test_data, scaled_valid_data\n",
    "\n",
    "# print('\\nTESTING FEATURES\\n', test_data_m.head())\n",
    "\n",
    "# print('\\ntrain set shape:',  train_data_m.shape)\n",
    "# print('\\ntest set shape:  ', test_data_m.shape)\n",
    "# # print('validation set shape:', valid_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "def get_train_scale_dict(USE_BRADEN_SCALING):\n",
    "    \"\"\"Get a dictionary containing mean and standard deviation of each gen and reco feature. \n",
    "\n",
    "    Args:\n",
    "        USE_BRADEN_SCALING (bool): Whether you wish to use the Braden scaling. If True, it uses the L-scaled train dataframe. If False, it uses the unscaled dataframe.\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of floats containing mean and standard deviation of each gen and reco feature. \n",
    "    \"\"\"\n",
    "    if USE_BRADEN_SCALING==True:\n",
    "        TRAIN_SCALE_DICT = get_scaling_info(scaled_train_data)\n",
    "        print('BRADEN SCALING DICTIONARY')\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    else:\n",
    "        print('NORMAL UNSCALED DICTIONARY')\n",
    "        TRAIN_SCALE_DICT = get_scaling_info(raw_train_data)\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    return TRAIN_SCALE_DICT\n",
    "\n",
    "\n",
    "class LR_Cooler:\n",
    "    def __init__(self, starting_lr: float, total_iterations: int, iter_: int) -> float:\n",
    "        self.starting_lr=starting_lr\n",
    "        self.iter_=iter_\n",
    "        self.total_iterations= total_iterations\n",
    "    def exponential_decay(self):\n",
    "        return self.starting_lr * (np.exp(-  self.iter_/1e5 ))\n",
    "    def exponential_decay_2(self):\n",
    "        decay_rate=1e-3\n",
    "        return self.starting_lr * np.exp(- decay_rate* self.iter)\n",
    "    \n",
    "    def fractional_decay(self):\n",
    "        final_time = 1\n",
    "        return self.starting_lr/(self.iter + final_time) \n",
    "\n",
    "################################ SPLIT###########\n",
    "# Currently need the split function again here\n",
    "# @memory.cache\n",
    "def split_t_x(df, target, input_features):\n",
    "    \"\"\"Get the target as the ratio, according to the T equation.\n",
    "    \n",
    "    Returns:\n",
    "    list(numpy.array): list of numpy array of target and training features\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_train_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "# @memory.cache\n",
    "def normal_split_t_x(df, target, input_features):\n",
    "    \"\"\"splot dataframe into targets and feature arrays.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe of train, test or validation data.\n",
    "        target (str): Choice of \"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\",\"RecoDatam\" as target.\n",
    "        input_features (list(str)): list of training features labels\n",
    "\n",
    "    Returns:\n",
    "    list(numpy.array): list of numpy array of target and training features\n",
    " \"\"\"\n",
    "    # change from pandas dataframe format to a numpy \n",
    "    # array of the specified types\n",
    "    # t = np.array(df[target])\n",
    "    t = np.array(df[target])\n",
    "    x = np.array(df[input_features])\n",
    "    return t, x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ Apply Z scaling############\n",
    "def z(x):\n",
    "    \"\"\"Simple z-score standardization. Used for targets\"\"\"\n",
    "    eps = 1e-20\n",
    "    return (x - np.mean(x)) / (np.std(x) + eps)\n",
    "def z_inverse(xprime, x):\n",
    "    return xprime * np.std(x) + np.mean(x)\n",
    "\n",
    "# @memory.cache\n",
    "def z2(x, mean, std):\n",
    "    \"\"\"\n",
    "    The main z score function. Args:\n",
    "        x (numpy.array): feature 1-D array\n",
    "        mean (float): mean of the feature (in the training set)\n",
    "        std (float): standard deviation of the feature (in the training set)\n",
    "\n",
    "    Returns:\n",
    "        numpy.array: z-score-scaled 1-D feature\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    scaled = (x - mean) / (std + eps)\n",
    "    return np.array(scaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z_inverse2(xprime, train_mean, train_std):\n",
    "    \"\"\"\n",
    "        The main z score de-scaling function. \n",
    "        \n",
    "        Args:\n",
    "        xprime (numpy.array): z-score-scaled feature 1-D array\n",
    "        train_mean (float): mean of the feature (in the training set)\n",
    "        train_std (float): standard deviation of the feature (in the training set)\n",
    "        \"\"\"\n",
    "    return xprime * train_std + train_mean\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x):\n",
    "    \"\"\"TO ensure this z scaling is only applied once to the training features, we use a generator.\n",
    "    This doesn't change the shapes of anything, just applies z to all the feature columns other than tau.\n",
    "    \n",
    "    Args:\n",
    "    TRAIN_SCALE_DICT (dict(float)): dictionary of train set mean and standard deviation values\n",
    "    train_x (numpy.array): 2-D numpy array of training features\n",
    "    test_x (numpy.array):  2-D numpy array of test features\n",
    "    valid_x (numpy.array):  2-D numpy array of validation features\n",
    "    \"\"\"\n",
    "    for i in range(NFEATURES - 1):\n",
    "        variable = list(TRAIN_SCALE_DICT)[i]\n",
    "        train_mean = float(TRAIN_SCALE_DICT[variable][\"mean\"])\n",
    "        train_std = float(TRAIN_SCALE_DICT[variable][\"std\"])\n",
    "        train_x[:, i] = z2(train_x[:, i], mean=train_mean, std=train_std)\n",
    "        test_x[:, i] = z2(test_x[:, i], mean=train_mean, std=train_std)\n",
    "        valid_x[:, i] = z2(valid_x[:, i], mean=train_mean, std=train_std)\n",
    "    yield train_x\n",
    "    yield test_x\n",
    "    yield valid_x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_targets(train_t, test_t, valid_t):\n",
    "    \"\"\"apply z-score scaling to target columns\n",
    "\n",
    "    Args:\n",
    "        train_t (numpy.array): target column in the training set\n",
    "        test_t (numpy.array): target column in the test set\n",
    "        valid_t (numpy.array): target column in the validation set\n",
    "\n",
    "    Yields:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    train_mean = np.mean(train_t)\n",
    "    train_std = np.std(train_t)\n",
    "    train_t_ = z2(train_t, mean=train_mean, std=train_std)\n",
    "    test_t_ = z2(test_t, mean=train_mean, std=train_std)\n",
    "    valid_t_ = z2(valid_t, mean=train_mean, std=train_std)\n",
    "\n",
    "    yield train_t_\n",
    "    yield test_t_\n",
    "    yield valid_t_\n",
    "\n",
    "\n",
    "\n",
    "# check that it looks correct\n",
    "# fig = plt.figure(figsize=(10, 4))\n",
    "# ax = fig.add_subplot(autoscale_on=True)\n",
    "# ax.grid()\n",
    "# for i in range(NFEATURES):\n",
    "#     ax.hist(train_x[:,i], alpha=0.35, label=f'feature {i}' )\n",
    "#     # set_axes(ax=ax, xlabel=\"Transformed features X' \",title=\"training features post-z score: X'=z(L(X))\")\n",
    "# ax.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "######### Get beset hyperparameters\n",
    "# tuned_dir = os.path.join(IQN_BASE,'best_params')\n",
    "# tuned_filename=os.path.join(tuned_dir,'best_params_mass_%s_trials.csv' % str(int(n_trials)))\n",
    "# BEST_PARAMS = pd.read_csv(os.path.join(IQN_BASE, 'best_params','best_params_Test_Trials.csv'))\n",
    "# BEST_PARAMS=pd.read_csv(tuned_filename)\n",
    "# print(BEST_PARAMS)\n",
    "class RegularizedRegressionModel(nn.Module):\n",
    "    \"\"\"Used for hyperparameter tuning\"\"\"\n",
    "\n",
    "    # inherit from the super class\n",
    "    def __init__(\n",
    "        self,\n",
    "        nfeatures,\n",
    "        ntargets,\n",
    "        nlayers,\n",
    "        hidden_size,\n",
    "        dropout_1,\n",
    "        dropout_2,\n",
    "        activation,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(nlayers):\n",
    "            if len(layers) == 0:\n",
    "                # nlayers is number of hidden layers+1, since there is always an input layer and an output layer\n",
    "                # INPUT LAYER\n",
    "                # inital layer has to have size of (input features, output_nodes),\n",
    "                # its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                # here we choose its output layer as the hidden size (fully connected)\n",
    "                # ALPHA DROPOUT\n",
    "                # layers.append(nn.AlphaDropout(dropout_1))\n",
    "\n",
    "                layer = nn.Linear(nfeatures, hidden_size)\n",
    "                torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                layers.append(layer)\n",
    "                # batch normalization\n",
    "                # layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                # dropout should have higher values in deeper layers\n",
    "                # layers.append(nn.Dropout(dropout_1))#Use small dropout for 1st layers & larger dropout for later layers. In both cases, the larger he model the larger the dropout.\n",
    "                # When model is in training, apply dropout. When using model for inference, dont use dropout\n",
    "\n",
    "                # ReLU activation\n",
    "                if activation == \"LeakyReLU\":\n",
    "                    layers.append(nn.LeakyReLU(negative_slope=0.3))\n",
    "                elif activation == \"PReLU\":\n",
    "                    layers.append(nn.PReLU())\n",
    "                elif activation == \"ReLU6\":\n",
    "                    layers.append(nn.ReLU6())\n",
    "                elif activation == \"ELU\":\n",
    "                    layers.append(nn.ELU())\n",
    "                elif activation == \"SELU\":\n",
    "                    layers.append(nn.SELU())\n",
    "                elif activation == \"CELU\":\n",
    "                    layers.append(nn.CELU())\n",
    "\n",
    "            else:\n",
    "                # if this is not the first layer (we dont have layers)\n",
    "                layer = nn.Linear(hidden_size, hidden_size)\n",
    "                torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                layers.append(layer)\n",
    "                # layers.append(nn.Dropout(dropout_2))\n",
    "                # layers.append(nn.BatchNorm1d(hidden_size))\n",
    "\n",
    "                if activation == \"LeakyReLU\":\n",
    "                    layers.append(nn.LeakyReLU(negative_slope=0.3))\n",
    "                elif activation == \"PReLU\":\n",
    "                    layers.append(nn.PReLU())\n",
    "\n",
    "        # output layer:\n",
    "        output_layer = nn.Linear(hidden_size, ntargets)\n",
    "        torch.nn.init.xavier_uniform_(output_layer.weight)\n",
    "        layers.append(output_layer)\n",
    "\n",
    "        # only for classification add sigmoid\n",
    "        # layers.append(nn.Sigmoid()) or softmax\n",
    "        # we have defined sequential model using the layers in our list\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "def load_untrained_model(PARAMS):\n",
    "    \"\"\"Load an untrained model (with weights initiatted) according to model paramateters in the \n",
    "    PARAMS dictionary\n",
    "\n",
    "    Args:\n",
    "        PARAMS (dict): dictionary of model/training parameters: i.e. hyperparameters and training parameters.\n",
    "\n",
    "    Returns:\n",
    "        utils.RegularizedRegressionModel object\n",
    "    \"\"\"\n",
    "    model = RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    # model.apply(initialize_weights)\n",
    "    print(model)\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SaveModelCheckpoint:\n",
    "    \"\"\"Continuous model-checkpointing class. Updates the latest checkpoint of an object based o validation loss each time its called. \n",
    "    \"\"\"\n",
    "    def __init__(self, best_valid_loss=np.inf):\n",
    "        \"\"\"Initiate an instance of the class based on filename and best_valid_loss/\n",
    "\n",
    "        Args:\n",
    "            best_valid_loss (float, optional): Best possible validation loss of a checkpoint object. Defaults to np.inf.\n",
    "        \"\"\"\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        self.filename_model=filename_model\n",
    "\n",
    "    def __call__(self, model, current_valid_loss, filename_model):\n",
    "        \"\"\"When an object of the calss is called, its validation loss gets updated and the model based \n",
    "        on the latest validation loss is saved.\n",
    "\n",
    "        Args:\n",
    "            model: utils.RegularizedRegressionModel object.\n",
    "            current_valid_loss (float): current (latest) validation loss of this model during the training process.\n",
    "            filename_model (str): filename in which the latest model will be saved. Can be a relative or local path. \n",
    "        \"\"\"\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            # update the best loss\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            # filename_model='Trained_IQNx4_%s_%sK_iter.dict' % (target, str(int(n_iterations/1000)) )\n",
    "            # filename_model = \"Trained_IQNx4_%s_TUNED_2lin_with_noise.dict\" % target\n",
    "\n",
    "            # note that n_iterations is the total n_iterations, we dont want to save a million files for each iteration\n",
    "            trained_models_dir = \"trained_models\"\n",
    "            mkdir(trained_models_dir)\n",
    "            # on cluster, Im using another TRAIN directory\n",
    "            PATH_model = os.path.join(\n",
    "                IQN_BASE,\n",
    "                \"JupyterBook\",\n",
    "                \"Cluster\",\n",
    "                \"TRAIN\",\n",
    "                trained_models_dir,\n",
    "                filename_model,\n",
    "            )\n",
    "            torch.save(model.state_dict(), PATH_model)\n",
    "            print(\n",
    "                f\"\\nCurrent valid loss: {current_valid_loss};  saved better model at {PATH_model}\"\n",
    "            )\n",
    "            # save using .pth object which if a dictionary of dicionaries, so that I can have PARAMS saved in the same file\n",
    "\n",
    "\n",
    "def train(\n",
    "    target,\n",
    "    model,\n",
    "    avloss,\n",
    "    getbatch,\n",
    "    train_x,\n",
    "    train_t,\n",
    "    valid_x,\n",
    "    valid_t,\n",
    "    PARAMS,\n",
    "    traces,\n",
    "    step,\n",
    "    window,\n",
    "):\n",
    "    \"\"\"Training Function. \n",
    "\n",
    "    Args:\n",
    "        target (str): hoice of \"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\",\"RecoDatam\" as target.\n",
    "        model a torch NN model, e.g utils.RegularizedRegressionModel.\n",
    "        avloss (float): average training losss\n",
    "        getbatch (function): a get_batch function\n",
    "        train_x (numpy.DataFrame): 2-D numpy array of training features\n",
    "        train_t (numpy.DataFrame:  1-D numpy array of training targets\n",
    "        valid_x (numpy.DataFrame): 2-D numpy array of validation features\n",
    "        valid_t (numpy.DataFrame: 1-D numpy array of validation targets\n",
    "        PARAMS (dict): dictionary of model/training parameters \n",
    "        traces (tuple): tuple of  \n",
    "        (iteration, training accuracy, validation accuracy, running average of validation accuracy) \n",
    "        = (xx, yy_t, yy_v, yy_v_avg) \n",
    "        step (int): number of iterations to take a printout step of the traces\n",
    "        window (int): window of running average of validation loss  \n",
    "\n",
    "    Returns:\n",
    "        tuple: traces\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: obviously, for reference, the \"traces\" should be saved as a 2D numpy array\n",
    "    # with the same naming format as the \"model_filename\", so that it can be opened later and \n",
    "    # plot loss curves for different models.\n",
    "    model=model.to(device)\n",
    "    \n",
    "    # TODO: decay the stepsize, such that steps (and hence checkpointing) are large in the beginnig to the learning\n",
    "    # process (which corresponds to high learning rates), and decrease as time steps increase.\n",
    "    batch_size = PARAMS['batch_size']\n",
    "    n_iterations = PARAMS['n_iterations']\n",
    "    # to keep track of average losses\n",
    "    xx, yy_t, yy_v, yy_v_avg = traces\n",
    "    model_checkpoint = SaveModelCheckpoint()\n",
    "    n = len(valid_x)\n",
    "    \n",
    "    print(\"Iteration vs average loss\")\n",
    "    print(\"%10s\\t%10s\\t%10s\" % (\"iteration\", \"train-set\", \"test-set\"))\n",
    "    \n",
    "    fifth_n_iterations=int(n_iterations//5)\n",
    "    starting_learning_rate = PARAMS['starting_learning_rate']\n",
    "    for ii in range(n_iterations):\n",
    "        #experiment with annealing LR from beginning\n",
    "\n",
    "#         # starting learning rate (first fifth)\n",
    "        Cutoff_from_initial_LR = 15000\n",
    "        if ii< Cutoff_from_initial_LR:\n",
    "            learning_rate= starting_learning_rate\n",
    "        \n",
    "#         #second fifth\n",
    "\n",
    "#         if 2* fifth_n_iterations < ii < 3*fifth_n_iterations:\n",
    "#             learning_rate=starting_learning_rate/10 #1e-2\n",
    "        if ii > Cutoff_from_initial_LR:\n",
    "            LR_sched=LR_Cooler(starting_lr=starting_learning_rate, total_iterations=n_iterations, iter_=ii)\n",
    "            learning_rate=LR_sched.exponential_decay()\n",
    "#         #third fifth\n",
    "#         if 3*fifth_n_iterations < ii < 4*fifth_n_iterations:\n",
    "#             learning_rate=starting_learning_rate/100 #1e-3\n",
    "#         #frouth fifth: stary decay LR\n",
    "#         if ii > 4*fifth_n_iterations:\n",
    "#             learning_rate = decay_LR(ii)\n",
    "            \n",
    "        \n",
    "        # add weight decay (important regularization to reduce overfitting)\n",
    "        L2 = 1\n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2)\n",
    "        #SGD allows for: momentum=0, dampening=0, weight_decay=0, nesterov=boolean, differentiable=boolean\n",
    "\n",
    "        optimizer = getattr(torch.optim, optimizer_name)(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "         # amsgrad=True, \n",
    "\n",
    "        #  weight_decay=L2,#\n",
    "        # differentiable=True,\n",
    "        #For SGD nesterov, it requires momentum and zero dampening\n",
    "        # dampening=0,\n",
    "        # momentum=momentum,\n",
    "        # nesterov=True\n",
    "        # BUT no one should ever use SGD in 2022! Adam converges much better and faster.\n",
    "        )\n",
    "        \n",
    "        #if ii > 1e4: learning_rate=1e-4\n",
    "        # set mode to training so that training specific\n",
    "        # operations such as dropout are enabled.\n",
    "        # time_p_start = time.perf_counter()\n",
    "        model.train()\n",
    "\n",
    "        # get a random sample (a batch) of data (as numpy arrays)\n",
    "        batch_x, batch_t = getbatch(train_x, train_t, batch_size)\n",
    "        # Take df/ dtau\n",
    "        # x = torch.from_numpy(batch_x).float()\n",
    "        # # print('x is leaf: ', x.is_leaf)\n",
    "        # x.requires_grad_(True)\n",
    "        # # x.retain_grad()\n",
    "        # # print('x is leaf after retain: ', x.is_leaf)\n",
    "        # # x.requires_grad_(True)\n",
    "        # # x.retain_grad()\n",
    "        # f = model(x)\n",
    "        # f = f.view(-1)\n",
    "        # #multiply the model by its ransverse, remember we can only take gradients of scalars\n",
    "        # #and f will be a vector before this\n",
    "        # f = f @ f.t()\n",
    "        # f.retain_grad()\n",
    "        # f.backward(gradient=torch.ones_like(f), retain_graph=True)\n",
    "        # df_dx = x.grad\n",
    "        # df_dtau = df_dx[:,-1]\n",
    "        # x.grad.zero_()\n",
    "        \n",
    "        #add noise to training data\n",
    "        batch_x = add_noise(batch_x)\n",
    "        # batch_t = add_noise(batch_t)\n",
    "        \n",
    "        # Try torch scheduler\n",
    "        # scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "        # scheduler.step()\n",
    "\n",
    "        with torch.no_grad():  # no need to compute gradients\n",
    "            # wrt. x and t\n",
    "            x = torch.from_numpy(batch_x).float().to(device)\n",
    "            t = torch.from_numpy(batch_t).float().to(device)\n",
    "\n",
    "        outputs = model(x).reshape(t.shape)\n",
    "        outputs=outputs.to(device)\n",
    "\n",
    "        empirical_risk = avloss(outputs, t, x)\n",
    "\n",
    "        optimizer.zero_grad()  # clear previous gradients\n",
    "        empirical_risk.backward()  # compute gradients\n",
    "\n",
    "        optimizer.step()  # move one step towards the minimum of the loss function using an SGD-like algorithm.\n",
    "        \n",
    "        \n",
    "\n",
    "        if ii % step == 0:\n",
    "\n",
    "            print(f\"\\t\\tCURRENT LEARNING RATE: {learning_rate}\")\n",
    "            acc_t = validate(model, avloss, train_x[:n], train_t[:n])\n",
    "            #acc_t: list of training losses\n",
    "            acc_v = validate(model, avloss, valid_x[:n], valid_t[:n])\n",
    "            #acc_v: list of validation losses\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "            previous_iter_valid_loss = yy_v[-1]\n",
    "            print(f'previous_iter_valid_loss : {previous_iter_valid_loss}\\n')\n",
    "            # save better models based on valid loss\n",
    "            # filename_model=\"Trained_IQNx4_%s_TUNED_0lin_with_high_noise3.dict\" % target\n",
    "            filename_model=get_model_filename(target, PARAMS)\n",
    "             \n",
    "            model_checkpoint(model=model, filename_model =filename_model ,current_valid_loss=acc_v)\n",
    "            # compute running average for validation data\n",
    "            len_yy_v = len(yy_v)\n",
    "            if len_yy_v < window:\n",
    "                yy_v_avg.append(yy_v[-1])\n",
    "            elif len_yy_v == window:\n",
    "                yy_v_avg.append(sum(yy_v) / window)\n",
    "            else:\n",
    "                acc_v_avg = yy_v_avg[-1] * window\n",
    "                acc_v_avg += yy_v[-1] - yy_v[-window - 1]\n",
    "                yy_v_avg.append(acc_v_avg / window)\n",
    "\n",
    "            if len(xx) < 1:\n",
    "                xx.append(0)\n",
    "                print(\"%10d\\t%10.6f\\t%10.6f\" % (xx[-1], yy_t[-1], yy_v[-1]))\n",
    "            else:\n",
    "                xx.append(xx[-1] + step)\n",
    "\n",
    "                print(\n",
    "                    \"\\r%10d\\t%10.6f\\t%10.6f\\t%10.6f\"\n",
    "                    % (xx[-1], yy_t[-1], yy_v[-1], yy_v_avg[-1]),\n",
    "                    end=\"\",\n",
    "                )\n",
    "        # time_p_end = time.perf_counter()\n",
    "        # time_for_this_iter = time_p_end-time_p_start\n",
    "        # time_per_example = time_for_this_iter/batch_size\n",
    "        # print(f'training time for one example: {time_per_example}')\n",
    "\n",
    "    print()\n",
    "    return (xx, yy_t, yy_v, yy_v_avg)\n",
    "\n",
    "\n",
    "@utils.time_type_of_func(tuning_or_training=\"training\")\n",
    "def run(\n",
    "    target,\n",
    "    model,\n",
    "    train_x,\n",
    "    train_t,\n",
    "    valid_x,\n",
    "    valid_t,\n",
    "    traces,\n",
    "    PARAMS,\n",
    "    traces_step,\n",
    "    traces_window,\n",
    "    save_model,\n",
    "):\n",
    "    model=model.to(device)\n",
    "\n",
    "    traces = train(\n",
    "        target,\n",
    "        model,\n",
    "        average_quantile_loss,\n",
    "        get_batch,\n",
    "        train_x,\n",
    "        train_t,\n",
    "        valid_x,\n",
    "        valid_t,\n",
    "        PARAMS,\n",
    "        traces,\n",
    "        step=traces_step,\n",
    "        window=traces_window,\n",
    "    )\n",
    "\n",
    "    if save_model:\n",
    "        filename = \"Trained_IQNx4_%s_%sK_iter.dict\" % (\n",
    "            target,\n",
    "            str(int(n_iterations / 1000)),\n",
    "        )\n",
    "        PATH = os.path.join(IQN_BASE, \"trained_models\", filename)\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_train_scale_dict(USE_BRADEN_SCALING):\n",
    "    if USE_BRADEN_SCALING==True:\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(scaled_train_data)\n",
    "        print(\"BRADEN SCALING DICTIONARY\")\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    else:\n",
    "        print(\"NORMAL UNSCALED DICTIONARY\")\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(raw_train_data)\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    return TRAIN_SCALE_DICT\n",
    "\n",
    "\n",
    "@utils.debug\n",
    "def save_model(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "@utils.debug\n",
    "def save_model_params(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "@utils.debug\n",
    "def load_model(PATH):\n",
    "    # n_layers = int(BEST_PARAMS[\"n_layers\"])\n",
    "    # hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "    # dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "    # optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "    # learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "    # batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "    model = RegularizedRegressionModel(\n",
    "        nfeatures=train_x.shape[1],\n",
    "        ntargets=1,\n",
    "        nlayers=n_layers,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    # OR\n",
    "    # model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED!\n",
    "    model=model.to(device)\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_trained_model(PATH, PARAMS):\n",
    "    model = RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    print(model)\n",
    "    model=model.to(device)\n",
    "    model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWp3moJRgyM_"
   },
   "source": [
    "# 2.3 Load Data, split, scale, and Train Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WnW_FM8ggwZL"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "USE_BRADEN_SCALING=False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(1e5)  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCeI2oU9g0qp",
    "outputId": "65cc8345-c2ba-425f-9867-3f8aceff58a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--content-<ipython-input-3b01933686c0>.load_raw_data...\n",
      "load_raw_data()\n",
      "SUBSAMPLE = None\n",
      "\n",
      " RAW TRAIN DATA SHAPE\n",
      "\n",
      "(8000000, 9)\n",
      "\n",
      " RAW TRAIN DATA\n",
      "\n",
      "\n",
      " RAW TEST DATA\\ SHAPEn\n",
      "(1000000, 9)\n",
      "\n",
      " RAW TEST DATA\n",
      "\n",
      "___________________________________________________load_raw_data - 17.5s, 0.3min\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# Load data only once, and with caching!\n",
    "raw_train_data, raw_test_data, raw_valid_data =load_raw_data()\n",
    "# Load scaled data\n",
    "# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJoGckG7hH9q"
   },
   "source": [
    "## Train Mass\n",
    "\n",
    "### The model that needs the longest time in training is mass. Click here to scroll down to train $p_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SIlatO5Ig13D",
    "outputId": "a6793c8f-e108-4715-914f-b36f02d26f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n",
      "spliting data for RecoDatam\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 5)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 5)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 5)\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[ 1.81700902e-03  1.12510099e-03 -2.82526482e-04 -6.57626105e-04\n",
      "  5.00485136e-01] [1.01748627 0.9999745  0.99989115 0.99987283 0.28852734]\n",
      "[ 1.61650249e-15 -1.10134124e-18 -3.12905257e-17  5.01036723e-15\n",
      "  4.99915289e-01] [1.         1.         1.         1.         0.28867295]\n",
      "-0.0015599314696879494 0.9999191874249058\n",
      "6.996216939114674e-16 1.0000000000000002\n",
      "<class 'str'>\n",
      "training for 1000000 iteration, which is  64.0 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.3)\n",
      "    (4): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.3)\n",
      "    (6): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.3)\n",
      "    (10): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "training IQN \n",
      "Iteration vs average loss\n",
      " iteration\t train-set\t  test-set\n",
      "\t\tCURRENT LEARNING RATE: 0.5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-328a49577cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0mtraces_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mtraces_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraces_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m IQN = run(\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muntrained_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mwrapper_timer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"timing this arbitrary function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2d69a21ca30b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(target, model, train_x, train_t, valid_x, valid_t, traces, PARAMS, traces_step, traces_window, save_model)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m     traces = train(\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2d69a21ca30b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(target, model, avloss, getbatch, train_x, train_t, valid_x, valid_t, PARAMS, traces, step, window)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\t\\tCURRENT LEARNING RATE: {learning_rate}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0macc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m             \u001b[0;31m#acc_t: list of training losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0macc_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, avloss, inputs, targets)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;31m# remember to reshape!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mavloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2d69a21ca30b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;31m###################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING==True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = split_t_x(df=scaled_test_data, target=target, input_features=features)\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "    df=raw_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "    df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = normal_split_t_x(df=raw_test_data, target=target, input_features=features)\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "\n",
    "# print(f\"spliting data for {target}\")\n",
    "# train_t, train_x = normal_split_t_x(\n",
    "# df=raw_train_data, target=target, input_features=features\n",
    "# )\n",
    "# print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "# print(\"\\n Training features:\\n\")\n",
    "# print(train_x)\n",
    "# valid_t, valid_x = normal_split_t_x(\n",
    "# df=raw_valid_data, target=target, input_features=features\n",
    "# )\n",
    "# print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "# test_t, test_x = normal_split_t_x(df=raw_test_data, target=target, input_features=features)\n",
    "# print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "\n",
    "# print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "# print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "# print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "# print(valid_t.mean(), valid_t.std())\n",
    "# print(train_t.mean(), train_t.std())\n",
    "NFEATURES = train_x.shape[1]\n",
    "######################################################\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "TRAIN_SCALE_DICT=get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(\n",
    "    train_t, test_t, valid_t\n",
    ")\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################### Define Mass parameters ###################\n",
    "###########################################################\n",
    "# Decide on parameters for this model and training\n",
    "PARAMS_m = {\n",
    "\"n_layers\": int(5),\n",
    "\"hidden_size\": int(5),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'NAdam',\n",
    "    'starting_learning_rate':float(0.5),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(512),\n",
    "    'n_iterations': int(1e6),\n",
    "}\n",
    "###########################################################\n",
    "################################################## Train ###########################################################\n",
    "optimizer_name=PARAMS_m['optimizer_name']\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS=PARAMS_m['n_iterations']\n",
    "BATCHSIZE=PARAMS_m['batch_size']\n",
    "comment=''\n",
    "\n",
    "\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(f\"training for {NITERATIONS} iteration, which is  {N_epochs} epochs\")\n",
    "\n",
    "\n",
    "filename_model = get_model_filename(target, PARAMS_m)\n",
    "trained_models_dir = \"trained_models\"\n",
    "mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE, #the loaction of the repo\n",
    "    \"JupyterBook\", #up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\", \n",
    "    \"TRAIN\",\n",
    "    trained_models_dir, #/trained_models \n",
    "    filename_model # utils.get_model_filename has the saved file format \n",
    ")\n",
    "\n",
    "#LOAD EITHER TRAINED OR UNTRAINED MODEL\n",
    "# to load untrained model (start training from scratch), uncomment the next line\n",
    "untrained_model = load_untrained_model(PARAMS_m)\n",
    "# to continune training of model (pickup where the previous training left off), uncomment below\n",
    "# trained_model =load_trained_model(PATH=PATH_model, PARAMS=PARAMS_m)\n",
    "\n",
    "IQN_trace = ([], [], [], [])\n",
    "traces_step = int(20)\n",
    "traces_window = traces_step\n",
    "IQN = run(\n",
    "    target=target,\n",
    "    model=untrained_model,\n",
    "    train_x=train_x_z_scaled,\n",
    "    train_t=train_t_z_scaled,\n",
    "    valid_x=test_x_z_scaled,\n",
    "    valid_t=test_t_z_scaled,\n",
    "    traces=IQN_trace,\n",
    "    PARAMS=PARAMS_m,\n",
    "    traces_step=traces_step,\n",
    "    traces_window=traces_window,\n",
    "    save_model=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "SAVE_LAST_MODEL=False\n",
    "if SAVE_LAST_MODEL:\n",
    "    # ## Save last iteration of trained model \n",
    "    #dont save the last model, it might be worse than previous iterations, which were automatically savedby model checkpoints\n",
    "\n",
    "    final_path = get_model_filename(target, PARAMS_m).split('.dict')[0]+'_FINAL.dict'\n",
    "\n",
    "    trained_models_dir = \"trained_models\"\n",
    "    mkdir(trained_models_dir)\n",
    "    # on cluster, Im using another TRAIN directory\n",
    "    PATH_final_model = os.path.join(\n",
    "    IQN_BASE, \"JupyterBook\", \"Cluster\", \"TRAIN\", trained_models_dir, final_path\n",
    "    )\n",
    "\n",
    "    save_model(IQN, PATH_final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMlhxNW38J57"
   },
   "source": [
    "# Evaluate Mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFqvfw06_JUa"
   },
   "source": [
    "## Needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "XQulDxBF_I2l"
   },
   "outputs": [],
   "source": [
    "################################## Load unscaled dataframes ###################################\n",
    "@memory.cache\n",
    "def load_raw_data():\n",
    "    \"\"\"Dont use AUTOREGRESSIVE_DIST_NAME for training of any variable. \n",
    "    For mass evaluation: dont use AUTOREGRESSIVE_DIST_NAME. For pT evaluation use AUTOREGRESSIVE_DIST_NAME \n",
    "    as the distribution predicted by mass, etc.  \"\"\"\n",
    "    print(f\"\\nSUBSAMPLE = {SUBSAMPLE}\\n\")\n",
    "    raw_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    raw_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"validation_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    raw_test_data = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"test_data_10M_2.csv\"), \n",
    "    usecols=all_cols, \n",
    "    nrows=SUBSAMPLE\n",
    "    )\n",
    "\n",
    "    print(\"\\n RAW TRAIN DATA\\n\")\n",
    "    print(raw_train_data.shape)\n",
    "    raw_train_data.describe()  # unscaled\n",
    "    print(\"\\n RAW TEST DATA\\n\")\n",
    "    print(raw_test_data.shape)\n",
    "    raw_test_data.describe()  # unscaled\n",
    "\n",
    "    return raw_train_data, raw_test_data, raw_valid_data\n",
    "\n",
    "\n",
    "########## Generate scaled data###############\n",
    "# scaled_train_data = L_scale_df(raw_train_data, title='scaled_train_data_10M_2.csv',\n",
    "#                              save=True)\n",
    "# print('\\n\\n')\n",
    "# scaled_test_data = L_scale_df(raw_test_data,  title='scaled_test_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "# print('\\n\\n')\n",
    "\n",
    "# scaled_valid_data = L_scale_df(raw_valid_data,  title='scaled_valid_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "\n",
    "# explore_data(df=scaled_train_data, title='Braden Kronheim-L-scaled Dataframe', scaled=True)\n",
    "\n",
    "################ Load scaled data##############\n",
    "@utils.time_type_of_func(tuning_or_training=\"loading\")\n",
    "# @memory.cache\n",
    "def load_scaled_dataframes():\n",
    "    print(\"SCALED TRAIN DATA\")\n",
    "    scaled_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    print(\"TRAINING FEATURES\\n\", scaled_train_data.head())\n",
    "\n",
    "    scaled_test_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_test_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    scaled_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_valid_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    return scaled_train_data, scaled_test_data, scaled_valid_data\n",
    "\n",
    "\n",
    "#######################################\n",
    "#\n",
    "# # print('\\nTESTING FEATURES\\n', scaled_test_data.head())\n",
    "\n",
    "# print('\\ntrain set shape:',  scaled_train_data.shape)\n",
    "# print('\\ntest set shape:  ', scaled_test_data.shape)\n",
    "# # print('validation set shape:', valid_data.shape)\n",
    "# @memory.cache\n",
    "def get_train_scale_dict(USE_BRADEN_SCALING):\n",
    "    if USE_BRADEN_SCALING==True:\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(scaled_train_data)\n",
    "        print(\"BRADEN SCALING DICTIONARY\")\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    else:\n",
    "        print(\"NORMAL UNSCALED DICTIONARY\")\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(raw_train_data)\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    return TRAIN_SCALE_DICT\n",
    "\n",
    "\n",
    "################################ SPLIT###########\n",
    "# @memory.cache\n",
    "def T(variable, scaled_df):\n",
    "    if variable == \"pT\":\n",
    "        L_pT_gen = scaled_df[\"genDatapT\"]\n",
    "        L_pT_reco = scaled_df[\"RecoDatapT\"]\n",
    "        target = (L_pT_reco + 10) / (L_pT_gen + 10)\n",
    "    if variable == \"eta\":\n",
    "        L_eta_gen = scaled_df[\"genDataeta\"]\n",
    "        L_eta_reco = scaled_df[\"RecoDataeta\"]\n",
    "        target = (L_eta_reco + 10) / (L_eta_gen + 10)\n",
    "    if variable == \"phi\":\n",
    "        L_phi_gen = scaled_df[\"genDataphi\"]\n",
    "        L_phi_reco = scaled_df[\"RecoDataphi\"]\n",
    "        target = (L_phi_reco + 10) / (L_phi_gen + 10)\n",
    "    if variable == \"m\":\n",
    "        L_m_gen = scaled_df[\"genDatam\"]\n",
    "        L_m_reco = scaled_df[\"RecoDatam\"]\n",
    "        target = (L_m_reco + 10) / (L_m_gen + 10)\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def split_t_x(df, target, input_features):\n",
    "    \"\"\"Get teh target as the ratio, according to the T equation\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_train_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def split_t_x_test(df, target, input_features):\n",
    "    \"\"\"Get teh target as the ratio, according to the T equation\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_test_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# @memory.cache\n",
    "def normal_split_t_x(df, target, input_features):\n",
    "    # change from pandas dataframe format to a numpy\n",
    "    # array of the specified types\n",
    "    # t = np.array(df[target])\n",
    "    t = np.array(df[target])\n",
    "    x = np.array(df[input_features])\n",
    "    return t, x\n",
    "\n",
    "\n",
    "################ Apply Z scaling############\n",
    "def z(x):\n",
    "    eps = 1e-20\n",
    "    return (x - np.mean(x)) / (np.std(x) + eps)\n",
    "\n",
    "\n",
    "def z_inverse(xprime, x):\n",
    "    return xprime * np.std(x) + np.mean(x)\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z2(x, mean, std):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x ([type]): [description]\n",
    "        mean ([type]): [description]\n",
    "        std ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    scaled = (x - mean) / (std + eps)\n",
    "    return np.array(scaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "def z_inverse(xprime, x):\n",
    "    unscaled = xprime * np.std(x) + np.mean(x)\n",
    "    return np.array(unscaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z_inverse2(xprime, train_mean, train_std):\n",
    "    \"\"\"mean original train mean, std: original. Probably not needed\"\"\"\n",
    "    return xprime * train_std + train_mean\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x):\n",
    "    \"\"\"TO ensure this z scaling is only applied once to the training features, we use a generator.\n",
    "    This doesn't change the shapes of anything, just applies z to all the feature columns other than tau\"\"\"\n",
    "    NFEATURES = train_x.shape[1]\n",
    "    for i in range(NFEATURES - 1):\n",
    "        variable = list(TRAIN_SCALE_DICT)[i]\n",
    "        train_mean = float(TRAIN_SCALE_DICT[variable][\"mean\"])\n",
    "        train_std = float(TRAIN_SCALE_DICT[variable][\"std\"])\n",
    "        train_x[:, i] = z2(train_x[:, i], mean=train_mean, std=train_std)\n",
    "        test_x[:, i] = z2(test_x[:, i], mean=train_mean, std=train_std)\n",
    "        valid_x[:, i] = z2(valid_x[:, i], mean=train_mean, std=train_std)\n",
    "    yield train_x\n",
    "    yield test_x\n",
    "    yield valid_x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_targets(train_t, test_t, valid_t):\n",
    "    train_mean = np.mean(train_t)\n",
    "    train_std = np.std(train_t)\n",
    "    train_t_ = z2(train_t, mean=train_mean, std=train_std)\n",
    "    test_t_ = z2(test_t, mean=train_mean, std=train_std)\n",
    "    valid_t_ = z2(valid_t, mean=train_mean, std=train_std)\n",
    "\n",
    "    yield train_t_\n",
    "    yield test_t_\n",
    "    yield valid_t_\n",
    "\n",
    "\n",
    "# @utils.debug\n",
    "def save_model(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "# @utils.debug\n",
    "def load_model(PATH, PARAMS):\n",
    "    # n_layers = int(BEST_PARAMS[\"n_layers\"])\n",
    "    # hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "    # dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "    # optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "    # learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "    # batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "    model = RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    # OR\n",
    "    # model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED! Also, use dictionary \".pth\" which has both the model state dict and the PARAMS dict\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def simple_eval(model, test_x_z_scaled):\n",
    "    model.eval()\n",
    "    # evaluate on the scaled features\n",
    "    valid_x_tensor = torch.from_numpy(test_x_z_scaled).float()\n",
    "    # valid_x_tensor=torch.from_numpy(train_x).float()\n",
    "    pred = model(valid_x_tensor)\n",
    "    p = pred.detach().numpy()\n",
    "    # if USE_BRADEN_SCALING:\n",
    "    #     fig, ax = plt.subplots(1,1)\n",
    "    #     label=FIELDS[target]['ylabel']\n",
    "    #     ax.hist(p, label=f'Predicted post-z ratio for {label}', alpha=0.4, density=True)\n",
    "    #     # orig_ratio = z(T('m', scaled_df=scaled_train_data))\n",
    "    #     orig_ratio = z(T('m', scaled_df=scaled_test_data))\n",
    "    #     print(orig_ratio[:5])\n",
    "    #     ax.hist(orig_ratio, label = f'original post-z ratio for {label}', alpha=0.4,density=True)\n",
    "    #     ax.grid()\n",
    "    #     set_axes(ax, xlabel='predicted $T$')\n",
    "    # print('predicted ratio shape: ', p.shape)\n",
    "    return p\n",
    "\n",
    "def get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME):\n",
    "        \n",
    "    print(f'Test (evaluation) Data is Autoregressive, loading {AUTOREGRESSIVE_DIST_NAME}')\n",
    "    eval_data = pd.read_csv(\n",
    "        os.path.join(\n",
    "            IQN_BASE,\n",
    "            \"JupyterBook\",\n",
    "            \"Cluster\",\n",
    "            \"EVALUATE\",\n",
    "            AUTOREGRESSIVE_DIST_NAME,\n",
    "        )\n",
    "    )\n",
    "    return eval_data\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def get_hist(label):\n",
    "    \"\"\"label could be \"pT\", \"eta\", \"phi\", \"m\" \"\"\"\n",
    "    predicted_label_counts, label_edges = np.histogram(\n",
    "        JETS_DICT[\"Predicted_RecoData\" + label][\"dist\"],\n",
    "        range=JETS_DICT[\"Predicted_RecoData\" + label][\"range\"],\n",
    "        bins=bins,\n",
    "    )\n",
    "    real_label_counts, _ = np.histogram(\n",
    "        JETS_DICT[\"Real_RecoData\" + label][\"dist\"],\n",
    "        range=JETS_DICT[\"Real_RecoData\" + label][\"range\"],\n",
    "        bins=bins,\n",
    "    )\n",
    "    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2\n",
    "\n",
    "    return real_label_counts, predicted_label_counts, label_edges\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def get_hist_simple(predicted_dist, target):\n",
    "    \n",
    "    range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "    bins=50\n",
    "    predicted_label_counts, label_edges = np.histogram(\n",
    "        predicted_dist, range=range_, bins=bins\n",
    "    )\n",
    "    \n",
    "    \n",
    "    real_label_counts, _ = np.histogram(REAL_DIST, range=range_, bins=bins)\n",
    "    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2\n",
    "    return real_label_counts, predicted_label_counts, label_edges\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def plot_one(\n",
    "    target, real_edges, real_counts, predicted_counts, save_plot=False, PARAMS=None, JUPYTER=True\n",
    "):\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2, 1, figsize=(3.5 * 3 / 2.5, 3.8), gridspec_kw={\"height_ratios\": [2, 0.5]}\n",
    "    )\n",
    "    ax1.step(\n",
    "        real_edges, real_counts / norm_data, where=\"mid\", color=\"k\", linewidth=0.5\n",
    "    )  # step real_count_pt\n",
    "    ax1.step(\n",
    "        real_edges,\n",
    "        predicted_counts / norm_IQN,\n",
    "        where=\"mid\",\n",
    "        color=\"#D7301F\",\n",
    "        linewidth=0.5,\n",
    "    )  # step predicted_count_pt\n",
    "    ax1.scatter(\n",
    "        real_edges,\n",
    "        real_counts / norm_data,\n",
    "        label=\"reco\",\n",
    "        color=\"k\",\n",
    "        facecolors=\"none\",\n",
    "        marker=\"o\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax1.scatter(\n",
    "        real_edges,\n",
    "        predicted_counts / norm_IQN,\n",
    "        label=\"predicted\",\n",
    "        color=\"#D7301F\",\n",
    "        marker=\"x\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax1.set_xlim(range_)\n",
    "    ax1.set_ylim(0, max(predicted_counts / norm_IQN) * 1.1)\n",
    "    ax1.set_ylabel(\"counts\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    ratio = (predicted_counts / norm_IQN) / (real_counts / norm_data)\n",
    "    ax2.scatter(\n",
    "        real_edges, ratio, color=\"r\", marker=\"x\", s=5, linewidth=0.5\n",
    "    )  # PREDICTED (IQN)/Reco (Data)\n",
    "    ax2.scatter(\n",
    "        real_edges,\n",
    "        ratio / ratio,\n",
    "        color=\"k\",\n",
    "        marker=\"o\",\n",
    "        facecolors=\"none\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax2.set_xlim(range_)\n",
    "    ax2.set_xlabel(FIELDS[target][\"xlabel\"])\n",
    "    ax2.set_ylabel(\n",
    "        r\"$\\frac{\\textnormal{predicted}}{\\textnormal{reco}}$\"\n",
    "        #    , fontsize=10\n",
    "    )\n",
    "    ax2.set_ylim((YLIM))\n",
    "    ax2.set_xlim(range_)\n",
    "    ax2.set_yticklabels([0.8, 1.0, 1.2])\n",
    "    if JUPYTER==True:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(wspace=0.5, hspace=0.2)\n",
    "        fig.subplots_adjust(wspace=0.0, hspace=0.1)\n",
    "        plt.axis('off')\n",
    "\n",
    "    # plt.gca().set_position([0, 0, 1, 1])\n",
    "    if save_plot:\n",
    "        plot_filename = utils.get_model_filename(target, PARAMS).split(\".dict\")[0] + \".png\"\n",
    "        plt.savefig(\n",
    "            os.path.join(IQN_BASE, \n",
    "                         \"JupyterBook\", \n",
    "                         \"Cluster\", \n",
    "                        #  \"EVALUATE\", \n",
    "                         plot_filename)\n",
    "        )\n",
    "\n",
    "    \n",
    "    # fig.show()\n",
    "    # plt.show();\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.gca().set_position([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mE_bP72FhcYs",
    "outputId": "40bc8504-295c-4f69-8a43-54ac4fbf6fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n",
      "spliting data for RecoDatam\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 5)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[ 3.38253091  0.828187    2.90213     1.57969597  0.36130954]\n",
      " [ 3.19127027 -1.16351     0.636469    2.05883697  0.12689925]\n",
      " [ 3.19127027 -1.16351     0.636469    2.05883697  0.96230681]\n",
      " ...\n",
      " [ 3.72374454 -2.23358    -2.81921     2.21849454  0.08421659]\n",
      " [ 3.56850964 -1.12318     0.356494    2.08765398  0.05535172]\n",
      " [ 3.27935361 -1.09427    -1.49334     1.83323565  0.07489863]]\n",
      "valid_t shape =  (8000000,) valid_x shape =  (1000000, 5)\n",
      "test_t shape =  (8000000,) test_x shape =  (1000000, 5)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 3.42122253e+00  6.98189368e-04 -8.95543973e-04  2.15029052e+00\n",
      "  5.00485136e-01] [0.33618462 2.20425356 1.81362773 0.28827428 0.28852734]\n",
      "[ 3.42124491e+00 -1.78188172e-03 -3.83090331e-04  2.15056416e+00\n",
      "  4.99915289e-01] [0.33480629 2.20430976 1.81382516 0.28800584 0.28867295]\n",
      "0.9850983334720613 0.02124600582704668\n",
      "0.9850983334720613 0.02124600582704668\n",
      "BRADEN SCALING DICTIONARY\n",
      "{'genDatapT': {'mean': 3.4212449149089768, 'std': 0.33480629257351374}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 2.1505641641311892, 'std': 0.288005836783413}, 'RecoDatapT': {'mean': 3.4120016292119386, 'std': 0.3804797298312238}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 1.967801807330749, 'std': 0.3271234791069229}}\n",
      "\n",
      "\n",
      "\n",
      "[-6.68528705e-05  1.12510099e-03 -2.82526482e-04 -9.50128735e-04\n",
      "  5.00485136e-01] [1.0041168  0.9999745  0.99989115 1.00093207 0.28852734]\n",
      "[ 3.98275191e-15 -1.10134124e-18 -3.12905257e-17 -1.14646888e-14\n",
      "  4.99915289e-01] [1.         1.         1.         1.         0.28867295]\n",
      "5.849882889208402e-14 1.0000000000000004\n",
      "5.849882889208402e-14 1.0000000000000004\n",
      "<class 'str'>\n",
      "This model was trained for 1000000 iteration, which is  64.0 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.3)\n",
      "    (4): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.3)\n",
      "    (6): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.3)\n",
      "    (10): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "EVALUATION DATA NEW INDEX\n",
      "    RecoDatam  genDatapT  genDataeta  genDataphi  genDatam       tau\n",
      "0   2.097075    43.6113    0.824891    -1.26949   5.93310  0.250046\n",
      "1   2.538913    43.6113    0.824891    -1.26949   5.93310  0.847493\n",
      "2   2.165164    26.0153    3.529970     1.55495   7.41270  0.851995\n",
      "3   1.392167    28.4944   -1.159650     1.82602   7.84157  0.052378\n",
      "4   2.036560    21.9840    2.747660     2.03085   5.18315  0.542549\n",
      "norm_data 1000000 \n",
      "norm IQN 1000000 \n",
      "norm_autoregressive 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-2ec815f7e068>:382: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ratio = (predicted_counts / norm_IQN) / (real_counts / norm_data)\n",
      "<ipython-input-41-2ec815f7e068>:388: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ratio / ratio,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 302.4x273.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/_build/jupyter_execute/alphaDavidson_IQNx4_14_2.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: plot the loss curves (train and valid) of all 4 networks on the same plot,\n",
    "# and with the learning rate plotted on the same plot but on a different y axis \n",
    "# (x axis being iteration, with marks indicating epochs)\n",
    "\n",
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = True\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "########################################################################################\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING==True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = split_t_x(\n",
    "        df=scaled_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "        df=raw_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "        df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = normal_split_t_x(\n",
    "        df=raw_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "######################################################\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "\n",
    "NFEATURES = train_x.shape[1]\n",
    "TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "# to features\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "###########################################################\n",
    "# @utils.debug\n",
    "def load_model(PATH, PARAMS):\n",
    "    # n_layers = int(BEST_PARAMS[\"n_layers\"])\n",
    "    # hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "    # dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "    # optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "    # learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "    # batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "    model = RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    # OR\n",
    "    # model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED! Also, use dictionary \".pth\" which has both the model state dict and the PARAMS dict\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model.to(device)\n",
    "############################################################\n",
    "# Get the  parameters for this model and training\n",
    "PARAMS_m = {\n",
    "\"n_layers\": int(5),\n",
    "\"hidden_size\": int(5),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'NAdam',\n",
    "    'starting_learning_rate':float(0.5),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(512),\n",
    "    'n_iterations': int(1e6),\n",
    "}\n",
    "\n",
    "optimizer_name = PARAMS_m[\"optimizer_name\"]\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS = PARAMS_m[\"n_iterations\"]\n",
    "BATCHSIZE = PARAMS_m[\"batch_size\"]\n",
    "comment = \"\"\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(\n",
    "    f\"This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs\"\n",
    ")\n",
    "\n",
    "# 'Trained_IQNx4_%s_TUNED.dict' % target\n",
    "filename_model = utils.get_model_filename(target, PARAMS_m)\n",
    "# OR, if you know a model filename directly, you can also specify it, \n",
    "# BUT, if you pull a trained model explicitly, you have to make sure its parameters in the PARAMS dictionary above match\n",
    "# Nominal one is 'Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict', also in backup\n",
    "# filename_model='Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict'\n",
    "# filename_model='Trained_IQNx4_RecoDatapT_10_layer6_hiddenLeakyReLU_activation512_batchsize300_Kiteration.dict'\n",
    "\n",
    "trained_models_dir = \"trained_models\"\n",
    "utils.mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE,  # the loaction of the repo\n",
    "    \"JupyterBook\",  # up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\",\n",
    "    \"TRAIN\",\n",
    "    trained_models_dir,  # /trained_models\n",
    "    filename_model,  # utils.get_model_filename has the saved file format\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "# Get predicted distribution\n",
    "p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "\n",
    "range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "bins = 50\n",
    "if USE_BRADEN_SCALING==True:\n",
    "  REAL_RAW_DATA = scaled_test_data\n",
    "else:\n",
    "  REAL_RAW_DATA = raw_test_data\n",
    "\n",
    "YLIM = (0.8, 1.2)\n",
    "###########GET REAL DIST###########\n",
    "REAL_RAW_DATA = REAL_RAW_DATA[\n",
    "    [\"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\", \"RecoDatam\"]\n",
    "]\n",
    "REAL_RAW_DATA.columns = [\"realpT\", \"realeta\", \"realphi\", \"realm\"]\n",
    "REAL_DIST = REAL_RAW_DATA[\"realm\"]\n",
    "norm_data = REAL_RAW_DATA.shape[0]\n",
    "#############GET EVALUATION DIST#############\n",
    "if USE_BRADEN_SCALING==True:\n",
    "  scaled_test_data.describe()\n",
    "  m_reco = scaled_test_data[\"RecoDatam\"]\n",
    "  m_gen = scaled_test_data[\"genDatam\"]\n",
    "else:  \n",
    "  raw_test_data.describe()\n",
    "  m_reco = raw_test_data[\"RecoDatam\"]\n",
    "  m_gen = raw_test_data[\"genDatam\"]\n",
    "# plt.hist(m_reco,label=r'$m_{gen}^{test \\ data}$');plt.legend();plt.show()\n",
    "\n",
    "\n",
    "def descale_Braden_scaled_prediction(label, p):\n",
    "    \"\"\"Label could be m. p is the outcome of the model evaluation, e.g. \n",
    "    IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "    p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "    \n",
    "    \"\"\"\n",
    "    # make sure you've set braden scaling global variable to use this function.\n",
    "    assert USE_BRADEN_SCALING==True\n",
    "    orig_ratio = T(label, scaled_df=scaled_train_data)\n",
    "    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))\n",
    "    L_obs = L(orig_observable=m_gen, label=label)\n",
    "    z_inv_f = z_inv_f.flatten()\n",
    "    print(z_inv_f.shape)\n",
    "\n",
    "    factor = (z_inv_f * (L_obs + 10)) - 10\n",
    "    label_pred = L_inverse(L_observable=factor, label=label)\n",
    "    return label_pred\n",
    "    \n",
    "    \n",
    "m_pred = z_inverse2(\n",
    "    xprime=p,\n",
    "    train_mean=TRAIN_SCALE_DICT[target][\"mean\"],\n",
    "    train_std=TRAIN_SCALE_DICT[target][\"std\"],\n",
    ")\n",
    "m_pred = m_pred.flatten()\n",
    "\n",
    "# Get histogram of predicted distribution\n",
    "real_label_counts_m, predicted_label_counts_m, label_edges_m = get_hist_simple(\n",
    "    predicted_dist=m_pred, target=target\n",
    ")\n",
    "eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "# Get evaluation data\n",
    "# eval_data = pd.read_csv(DATA_DIR + \"/test_data_10M_2.csv\")\n",
    "ev_features = features\n",
    "eval_data = eval_data[ev_features]\n",
    "# save new distribution (m) in the eval data as autoregressive eval for next IQN\n",
    "eval_data[target] = m_pred\n",
    "\n",
    "new_cols = [target] + features\n",
    "eval_data = eval_data.reindex(columns=new_cols)\n",
    "print(\"EVALUATION DATA NEW INDEX\\n\", eval_data.head())\n",
    "\n",
    "eval_data.to_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \n",
    "        \"JupyterBook\", \n",
    "        \"Cluster\", \n",
    "        # \"EVALUATE\", \n",
    "        AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load this saved predited autoregressive distribution\n",
    "AUTOREGRESSIVE_DIST = pd.read_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \n",
    "        \"JupyterBook\", \n",
    "        \"Cluster\", \n",
    "        # \"EVALUATE\", \n",
    "        AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]\n",
    "# get normalization values\n",
    "norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]\n",
    "norm_IQN = norm_autoregressive\n",
    "print(\n",
    "    \"norm_data\",\n",
    "    norm_data,\n",
    "    \"\\nnorm IQN\",\n",
    "    norm_IQN,\n",
    "    \"\\nnorm_autoregressive\",\n",
    "    norm_autoregressive,\n",
    ")\n",
    "\n",
    "# Finally, plot predicted distribution\n",
    "\n",
    "plot_one(\n",
    "    target=target,\n",
    "    real_edges=label_edges_m,\n",
    "    real_counts=real_label_counts_m,\n",
    "    predicted_counts=predicted_label_counts_m,\n",
    "    save_plot=True,\n",
    "    PARAMS=PARAMS_m\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgAIlCNj4QVR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}