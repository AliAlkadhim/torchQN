
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>IQNx4: 1. Setup and Preprocess &#8212; torchIQNx4</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">torchIQNx4</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the torchIQNx4
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_Setup_and_Preprocess.html">
   IQNx4: Chapter 1. Setup and Preprocess
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2_Train_all_IQNs.html">
   IQNx4 Chapter 2: Train All Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_Autoregressive_Evaluation.html">
   IQNx4: Chapter 3: Autoregressive Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_SupplementaryMaterials_OptunaTuning_Theory_ML_Background.html">
   Chapter 4: Optional Supplementary Material
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/AliAlkadhim/torchQN/HEAD?labpath=JupyterBook/v2/gh/AliAlkadhim/torchQN/master?urlpath=tree/JupyterBook/Old_1.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/Old_1.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   IQNx4: 1. Setup and Preprocess
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#do-source-setup-sh-before-trying-to-run-this-notebook">
   Do
   <code class="docutils literal notranslate">
    <span class="pre">
     source
    </span>
    <span class="pre">
     setup.sh
    </span>
   </code>
   before trying to run this notebook!
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#external-imports">
     External Imports
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-you-want-to-run-this-interactively-on-binder-set-binder-true-this-will-download-our-datasets-using-the-kaggle-api-our-datasets-are-simply-the-following">
       If you want to run this interactively on binder, set
       <code class="docutils literal notranslate">
        <span class="pre">
         BINDER=True
        </span>
       </code>
       . This will download our datasets using the Kaggle API. Our datasets are simply the following:
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-utils-and-set-environemnt-variables">
     Import utils, and set environemnt variables
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-user-is-competent-enought-to-do-source-setup-sh-on-a-setup-sh-script-that-comes-in-the-repo-such-as-the-next-cell-uncommented">
       A user is competent enought to do
       <code class="docutils literal notranslate">
        <span class="pre">
         source
        </span>
        <span class="pre">
         setup.sh
        </span>
       </code>
       on a
       <code class="docutils literal notranslate">
        <span class="pre">
         setup.sh
        </span>
       </code>
       script that comes in the repo, such as the next cell uncommented
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utils-py">
   2. utils.py
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#set-arguments-optional-and-configurations">
     Set arguments (optional) and configurations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#import-the-numpy-data-convert-to-dataframe-and-save-if-you-haven-t-saved-the-dataframes">
       Import the numpy data, convert to dataframe and save (if you haven’t saved the dataframes)
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#explore-the-dataframe-and-preprocess">
   Explore the Dataframe and preprocess
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-and-explore-raw-unscaled-dataframes">
   3. Load and explore raw (unscaled) dataframes
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scaling">
   Scaling
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basically-a-standard-scaling-procedure-is-the-following-background">
     Basically a “standard scaling procedure” is the following (background):
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#braden-scaling">
   Braden scaling
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-scale-the-data-accoding-to-the-braden-kronheim-scaling">
   4. (Optional) Scale the data accoding to the “Braden Kronheim scaling” :
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#if-you-want-to-generate-the-scaled-data-frames-run-the-cell-below">
     If you want to generate the Scaled data frames, run the cell below
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#if-you-want-to-load-the-previously-generated-scaled-dataframe-run-the-cell-below">
     If you want to load the previously generated scaled dataframe, run the cell below
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-mass">
   Train Mass
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apply-z-to-targets-before-training">
     Apply
     <span class="math notranslate nohighlight">
      \(z\)
     </span>
     to targets before training
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-and-running-of-training-functions">
     Training and running-of-training functions
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-basic-nn-model">
     Define basic NN model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameter-training-workflow">
     Hyperparameter Training Workflow
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#run-training">
       Run training
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#see-if-trainig-works-on-t-ratio">
     See if trainig works on T ratio
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-trained-model-if-its-good-and-if-you-haven-t-saved-above-and-load-trained-model-if-you-saved-it">
     Save trained model (if its good, and if you haven’t saved above) and load trained model (if you saved it)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#paper-plotting">
       Paper plotting
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plot-predicted-vs-real-reco-in-our-paper-s-format">
   Plot predicted vs real reco (in our paper’s format)
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-p-t-using-saved-variables-above">
   Train
   <span class="math notranslate nohighlight">
    \(p_T\)
   </span>
   using saved variables above
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>IQNx4: 1. Setup and Preprocess</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   IQNx4: 1. Setup and Preprocess
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#do-source-setup-sh-before-trying-to-run-this-notebook">
   Do
   <code class="docutils literal notranslate">
    <span class="pre">
     source
    </span>
    <span class="pre">
     setup.sh
    </span>
   </code>
   before trying to run this notebook!
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#external-imports">
     External Imports
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#if-you-want-to-run-this-interactively-on-binder-set-binder-true-this-will-download-our-datasets-using-the-kaggle-api-our-datasets-are-simply-the-following">
       If you want to run this interactively on binder, set
       <code class="docutils literal notranslate">
        <span class="pre">
         BINDER=True
        </span>
       </code>
       . This will download our datasets using the Kaggle API. Our datasets are simply the following:
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-utils-and-set-environemnt-variables">
     Import utils, and set environemnt variables
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-user-is-competent-enought-to-do-source-setup-sh-on-a-setup-sh-script-that-comes-in-the-repo-such-as-the-next-cell-uncommented">
       A user is competent enought to do
       <code class="docutils literal notranslate">
        <span class="pre">
         source
        </span>
        <span class="pre">
         setup.sh
        </span>
       </code>
       on a
       <code class="docutils literal notranslate">
        <span class="pre">
         setup.sh
        </span>
       </code>
       script that comes in the repo, such as the next cell uncommented
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utils-py">
   2. utils.py
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#set-arguments-optional-and-configurations">
     Set arguments (optional) and configurations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#import-the-numpy-data-convert-to-dataframe-and-save-if-you-haven-t-saved-the-dataframes">
       Import the numpy data, convert to dataframe and save (if you haven’t saved the dataframes)
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#explore-the-dataframe-and-preprocess">
   Explore the Dataframe and preprocess
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-and-explore-raw-unscaled-dataframes">
   3. Load and explore raw (unscaled) dataframes
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scaling">
   Scaling
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basically-a-standard-scaling-procedure-is-the-following-background">
     Basically a “standard scaling procedure” is the following (background):
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#braden-scaling">
   Braden scaling
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-scale-the-data-accoding-to-the-braden-kronheim-scaling">
   4. (Optional) Scale the data accoding to the “Braden Kronheim scaling” :
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#if-you-want-to-generate-the-scaled-data-frames-run-the-cell-below">
     If you want to generate the Scaled data frames, run the cell below
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#if-you-want-to-load-the-previously-generated-scaled-dataframe-run-the-cell-below">
     If you want to load the previously generated scaled dataframe, run the cell below
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-mass">
   Train Mass
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#apply-z-to-targets-before-training">
     Apply
     <span class="math notranslate nohighlight">
      \(z\)
     </span>
     to targets before training
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-and-running-of-training-functions">
     Training and running-of-training functions
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-basic-nn-model">
     Define basic NN model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameter-training-workflow">
     Hyperparameter Training Workflow
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#run-training">
       Run training
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#see-if-trainig-works-on-t-ratio">
     See if trainig works on T ratio
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-trained-model-if-its-good-and-if-you-haven-t-saved-above-and-load-trained-model-if-you-saved-it">
     Save trained model (if its good, and if you haven’t saved above) and load trained model (if you saved it)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#paper-plotting">
       Paper plotting
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plot-predicted-vs-real-reco-in-our-paper-s-format">
   Plot predicted vs real reco (in our paper’s format)
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-p-t-using-saved-variables-above">
   Train
   <span class="math notranslate nohighlight">
    \(p_T\)
   </span>
   using saved variables above
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="iqnx4-1-setup-and-preprocess">
<h1>IQNx4: 1. Setup and Preprocess<a class="headerlink" href="#iqnx4-1-setup-and-preprocess" title="Permalink to this heading">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="do-source-setup-sh-before-trying-to-run-this-notebook">
<h1>Do <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">setup.sh</span></code> before trying to run this notebook!<a class="headerlink" href="#do-source-setup-sh-before-trying-to-run-this-notebook" title="Permalink to this heading">#</a></h1>
<section id="external-imports">
<h2>External Imports<a class="headerlink" href="#external-imports" title="Permalink to this heading">#</a></h2>
<p>If you don’t have some of these packages installed, you can also use the conda environment that has all of the packages by doing <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">env</span> <span class="pre">create</span> <span class="pre">-f</span> <span class="pre">IQN_env.yml</span> <span class="pre">&amp;&amp;</span> <span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">IQN_env</span></code></p>
<p>There is a live executable version of this notebook on binder, just click this link : <a class="reference external" href="https://mybinder.org/v2/gh/AliAlkadhim/torchQN/HEAD?labpath=JupyterBook"><img alt="Binder" src="https://mybinder.org/badge_logo.svg" /></a></p>
<p>Note that the binder will take a while to build, and you’d have to navigate through the files yourself (not as easy or nice as just going to <a class="reference external" href="https://alialkadhim.github.io/torchQN/">https://alialkadhim.github.io/torchQN/</a>)</p>
<section id="if-you-want-to-run-this-interactively-on-binder-set-binder-true-this-will-download-our-datasets-using-the-kaggle-api-our-datasets-are-simply-the-following">
<h3>If you want to run this interactively on binder, set <code class="docutils literal notranslate"><span class="pre">BINDER=True</span></code>. This will download our datasets using the Kaggle API. Our datasets are simply the following:<a class="headerlink" href="#if-you-want-to-run-this-interactively-on-binder-set-binder-true-this-will-download-our-datasets-using-the-kaggle-api-our-datasets-are-simply-the-following" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>BINDER=False
if BINDER:
    import opendatasets
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np; import pandas as pd
# import scipy as sp; import scipy.stats as st
import torch; import torch.nn as nn; print(f&quot;using torch version {torch.__version__}&quot;)
#use numba&#39;s just-in-time compiler to speed things up
# from numba import njit
from sklearn.preprocessing import StandardScaler; from sklearn.model_selection import train_test_split
import matplotlib as mp; print(&#39;matplotlib version= &#39;, mp.__version__)

import matplotlib.pyplot as plt; 
#reset matplotlib stle/parameters
import matplotlib as mpl
#reset matplotlib parameters to their defaults
mpl.rcParams.update(mpl.rcParamsDefault)
plt.style.use(&#39;seaborn-deep&#39;)
mp.rcParams[&#39;agg.path.chunksize&#39;] = 10000
font_legend = 15; font_axes=15
# %matplotlib inline
import sys; import os
from IPython.display import Image, display
# from importlib import import_module
import plotly
try:
    import optuna
    print(f&quot;using (optional) optuna version {optuna.__version__}&quot;)
except Exception:
    print(&#39;optuna is only used for hyperparameter tuning, not critical!&#39;)
    pass
import argparse
import time
# import sympy as sy
import ipywidgets as wid; 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>using torch version 1.9.0
matplotlib version=  3.5.1
using (optional) optuna version 2.8.0
</pre></div>
</div>
</div>
</div>
<p>print the versions of the packages</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%load_ext watermark

%watermark -v -m -p numpy,pandas,torch,scikit-learn,matplotlib,os,IPython,kaggle,plotly,optuna,argparse,time,ipywidgets

print(&quot; &quot;)
%watermark -u -n -t -z
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Python implementation: CPython
Python version       : 3.7.10
IPython version      : 7.31.1

numpy       : 1.21.5
pandas      : 1.3.5
torch       : 1.9.0
scikit-learn: 1.0.2
matplotlib  : 3.5.1
os          : unknown
IPython     : None
opendatasets: not installed
plotly      : 5.11.0
optuna      : 2.8.0
argparse    : 1.1
time        : unknown
ipywidgets  : 7.6.5

Compiler    : GCC 9.3.0
OS          : Linux
Release     : 5.4.0-135-generic
Machine     : x86_64
Processor   : x86_64
CPU cores   : 8
Architecture: 64bit

 
Last updated: Fri Dec 30 2022 19:40:01EST
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="import-utils-and-set-environemnt-variables">
<h2>Import utils, and set environemnt variables<a class="headerlink" href="#import-utils-and-set-environemnt-variables" title="Permalink to this heading">#</a></h2>
<p>need to tune latest braden scaling hyperparameters on cluster.</p>
<p>see if i can find/write a decorator to add the current cell to a file which will be run on cluster. I think %writefile <a class="reference external" href="http://file.py">file.py</a> could work, by adding the cell to another file…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># &#39;IQN&#39; in 
# some_environment={}
# some_environment.update(os.environ())
# some_environment
# &#39;DATA&#39; in list(os.environ)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># os.environ[&#39;IQN_BASE&#39;]=&#39;/home/ali/Desktop/Pulled_Github_Repositories/torchQN&#39;
# os.environ[&#39;DATA_DIR&#39;]=&#39;/home/DAVIDSON/alalkadhim.visitor/IQN/DAVIDSON_NEW/data&#39;
</pre></div>
</div>
</div>
</div>
<section id="a-user-is-competent-enought-to-do-source-setup-sh-on-a-setup-sh-script-that-comes-in-the-repo-such-as-the-next-cell-uncommented">
<h3>A user is competent enought to do <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">setup.sh</span></code> on a <code class="docutils literal notranslate"><span class="pre">setup.sh</span></code> script that comes in the repo, such as the next cell uncommented<a class="headerlink" href="#a-user-is-competent-enought-to-do-source-setup-sh-on-a-setup-sh-script-that-comes-in-the-repo-such-as-the-next-cell-uncommented" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># #!/bin/bash
# export IQN_BASE= $pwd #/home/ali/Desktop/Pulled_Github_Repositories/torchQN

# #DAVIDSON
# export DATA_DIR=&#39;/home/DAVIDSON/alalkadhim.visitor/IQN/DAVIDSON_NEW/data&#39;
# #LOCAL
# export DATA_DIR=&#39;/home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data&#39;
# echo DATA DIR
# ls -l $DATA_DIR
# #ln -s $DATA_DIR $IQN_BASE, if you want
# #conda create env -n torch_env -f torch_env.yml
# conda activate torch_env
# mkdir -p ${IQN_BASE}/images/loss_plots ${IQN_BASE}/trained_models  ${IQN_BASE}/hyperparameters ${IQN_BASE}/predicted_data
# tree $IQN_BASE
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># env = {}
# env.update(os.environ)
# env.update(source(os.environ[&quot;IQN_BASE&quot;])) 

try:
    IQN_BASE = os.environ[&#39;IQN_BASE&#39;]
    print(&#39;BASE directoy properly set = &#39;, IQN_BASE)
    utils_dir = os.path.join(IQN_BASE, &#39;utils/&#39;)
    sys.path.append(utils_dir)
    import utils
    #usually its not recommended to import everything from a module, but we know
    #whats in it so its fine
    from utils import *
    print(&#39;DATA directory also properly set, in %s&#39; % os.environ[&#39;DATA_DIR&#39;])
except Exception:
    # IQN_BASE=os.getcwd()
    print(&quot;&quot;&quot;\nBASE directory not properly set. Read repo README.\
    If you need a function from utils, use the decorator below, or add utils to sys.path\n
    You can also do 
    os.environ[&#39;IQN_BASE&#39;]=&lt;ABSOLUTE PATH FOR THE IQN REPO&gt;
    or
    os.environ[&#39;IQN_BASE&#39;]=os.getcwd()&quot;&quot;&quot;)
    pass
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN
using torch version 1.9.0
matplotlib version=  3.5.1
using (optional) optuna version 2.8.0
BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN
DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data
DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># update fonts
FONTSIZE = 14
font = {&#39;family&#39; : &#39;serif&#39;,
        &#39;weight&#39; : &#39;normal&#39;,
        &#39;size&#39;   : FONTSIZE}
mp.rc(&#39;font&#39;, **font)

# set usetex = False if LaTex is not 
# available on your system or if the 
# rendering is too slow
mp.rc(&#39;text&#39;, usetex=True)

# set a seed to ensure reproducibility
seed = 128
rnd  = np.random.RandomState(seed)
#sometimes jupyter doesnt initialize MathJax automatically for latex, so do this:
wid.HTMLMath(&#39;$\LaTeX$&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "f36c04486c6249f694fcab2542d2c61f", "version_major": 2, "version_minor": 0}
</script></div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="utils-py">
<h1>2. <a class="reference external" href="http://utils.py">utils.py</a><a class="headerlink" href="#utils-py" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np; import pandas as pd
# import scipy as sp; import scipy.stats as st
import torch; import torch.nn as nn; print(f&quot;using torch version {torch.__version__}&quot;)
#use numba&#39;s just-in-time compiler to speed things up
# from numba import njit
from sklearn.preprocessing import StandardScaler; from sklearn.model_selection import train_test_split
import matplotlib as mp; print(&#39;matplotlib version= &#39;, mp.__version__)

import matplotlib.pyplot as plt; 
#reset matplotlib stle/parameters
import matplotlib as mpl
#reset matplotlib parameters to their defaults
mpl.rcParams.update(mpl.rcParamsDefault)
plt.style.use(&#39;seaborn-deep&#39;)
mp.rcParams[&#39;agg.path.chunksize&#39;] = 10000
font_legend = 15; font_axes=15
# %matplotlib inline
import sys; import os
# from IPython.display import Image, display
# from importlib import import_module
#import plotly
try:
    import optuna
    print(f&quot;using (optional) optuna version {optuna.__version__}&quot;)
except Exception:
    print(&#39;optuna is only used for hyperparameter tuning, not critical!&#39;)
    pass
import argparse
import time
# import sympy as sy
#import ipywidgets as wid; 


try:
    IQN_BASE = os.environ[&#39;IQN_BASE&#39;]
    print(&#39;BASE directoy properly set = &#39;, IQN_BASE)
    utils_dir = os.path.join(IQN_BASE, &#39;utils&#39;)
    sys.path.append(utils_dir)
    import utils
    #usually its not recommended to import everything from a module, but we know
    #whats in it so its fine
    from utils import *
    print(&#39;DATA directory also properly set, in %s&#39; % os.environ[&#39;DATA_DIR&#39;])
except Exception:
    # IQN_BASE=os.getcwd()
    print(&quot;&quot;&quot;\nBASE directory not properly set. Read repo README.    If you need a function from utils, use the decorator below, or add utils to sys.path\n
    You can also do 
    os.environ[&#39;IQN_BASE&#39;]=&lt;ABSOLUTE PATH FOR THE IQN REPO&gt;
    or
    os.environ[&#39;IQN_BASE&#39;]=os.getcwd()&quot;&quot;&quot;)
    pass


# device = torch.device(&quot;cuda:0&quot;)

# update fonts
FONTSIZE = 18
font = {&#39;family&#39; : &#39;serif&#39;,
        &#39;weight&#39; : &#39;normal&#39;,
        &#39;size&#39;   : FONTSIZE}
mp.rc(&#39;font&#39;, **font)

####################################################################



# class CustomDataset(Dataset):
#     &quot;&quot;&quot;This takes the index for the data and target and gives dictionary of tensors of data and targets.
#     For example we could do train_dataset = CustomDataset(train_data, train_targets); test_dataset = CustomDataset(test_data, test_targets)
#  where train and test_dataset are np arrays that are reshaped to (-1,1).
#  Then train_dataset[0] gives a dictionary of samples &quot;X&quot; and targets&quot;&quot;&quot;
#     def __init__(self, data, targets):
#         self.data = data
#         self.targets=targets
#     def __len__(self):
#         return self.data.shape[0]
    
#     def __getitem__(self, idx):
        
#         current_sample = self.data[idx, :]
#         current_target = self.targets[idx]
#         return {&quot;x&quot;: torch.tensor(current_sample, dtype = torch.float),
#                &quot;y&quot;: torch.tensor(current_target, dtype= torch.float),
#                }#this already makes the targets made of one tensor (of one value) each


class RegressionModel(nn.Module):
    #inherit from the super class
    def __init__(self, nfeatures, ntargets, nlayers, hidden_size):
        super().__init__()
        layers = []
        for _ in range(nlayers):
            if len(layers) ==0:
                #inital layer has to have size of input features as its input layer
                #its output layer can have any size but it must match the size of the input layer of the next linear layer
                #here we choose its output layer as the hidden size (fully connected)
                layers.append(nn.Linear(nfeatures, hidden_size))
                #batch normalization
                # layers.append(nn.BatchNorm1d(hidden_size))
                #Dropout seems to worsen model performance
                # layers.append(nn.Dropout(dropout))
                #ReLU activation 
                # layers.append(nn.ReLU())
                layers.append(nn.LeakyReLU())
            else:
                #if this is not the first layer (we dont have layers)
                layers.append(nn.Linear(hidden_size, hidden_size))
                # layers.append(nn.BatchNorm1d(hidden_size))
                #Dropout seems to worsen model performance
                # layers.append(nn.Dropout(dropout))
                # layers.append(nn.ReLU())
                layers.append(nn.LeakyReLU())
                #output layer:
        layers.append(nn.Linear(hidden_size, ntargets)) 
        
        # ONLY IF ITS A CLASSIFICATION, ADD SIGMOID
        #layers.append(nn.Sigmoid())
            #we have defined sequential model using the layers in oulist 
        self.model = nn.Sequential(*layers)
            
    
    def forward(self, x):
        return self.model(x)




class RegressionEngine:
    &quot;&quot;&quot;loss, training and evaluation&quot;&quot;&quot;
    def __init__(self, model, optimizer):
                 #, device):
        self.model = model
        #self.device= device
        self.optimizer = optimizer
        
    #the loss function returns the loss function. It is a static method so it doesn&#39;t need self
    @staticmethod
    def quadratic_loss(targets, outputs):
         return nn.MSELoss()(outputs, targets)

    @staticmethod
    def average_quadratic_loss(targets, outputs):
    # f and t must be of the same shape
        return  torch.mean((outputs - targets)**2)
    
    @staticmethod
    def average_absolute_error(targets, outputs):
    # f and t must be of the same shape
        return  torch.mean(abs(outputs - targets))
    
    
    @staticmethod
    def average_cross_entropy_loss(targets, outputs):
        # f and t must be of the same shape
        loss = torch.where(targets &gt; 0.5, torch.log(outputs), torch.log(1 - outputs))
        # the above means loss = log outputs, if target&gt;0.5, and log(1-output) otherwise
        return -torch.mean(loss)
    
    @staticmethod
    def average_quantile_loss(targets, outputs):
        # f and t must be of the same shape
        tau = torch.rand(outputs.shape)
        #L= tau * (target - output), if target&gt;output
        #L= (1-tau)*(output-target), otherwise
        return torch.mean(torch.where(targets &gt; outputs, 
                                      tau * (targets - outputs), 
                                      (1 - tau)*(outputs - targets)))


    def train(self, data_loader):
        &quot;&quot;&quot;the training function: takes the training dataloader&quot;&quot;&quot;
        self.model.train()
        final_loss = 0
        for data in data_loader:
            self.optimizer.zero_grad()#only optimize weights for the current batch, otherwise it&#39;s meaningless!
            inputs = data[&quot;x&quot;]
            targets = data[&quot;y&quot;]
            outputs = self.model(inputs)
            loss = self.average_quantile_loss(targets, outputs)
            loss.backward()
            self.optimizer.step()
            final_loss += loss.item()
        return final_loss / len(data_loader)

    
    def evaluate(self, data_loader):
        &quot;&quot;&quot;the training function: takes the training dataloader&quot;&quot;&quot;
        self.model.eval()
        final_loss = 0
        for data in data_loader:
            inputs = data[&quot;x&quot;]#.to(self.device)
            targets = data[&quot;y&quot;]#.to(self.device)
            outputs = self.model(inputs)
            loss = self.average_quantile_loss(targets, outputs)
            final_loss += loss.item()
        return final_loss / len(data_loader)



class ModelHandler:
    def __init__(self, model, scalers):
        self.model  = model
        self.scaler_t, self.scaler_x = scalers
        
        self.scale  = self.scaler_t.scale_[0] # for output
        self.mean   = self.scaler_t.mean_[0]  # for output
        self.fields = self.scaler_x.feature_names_in_
        
    def __call__(self, df):
        
        # scale input data
        x  = np.array(self.scaler_x.transform(df[self.fields]))
        x  = torch.Tensor(x)

        # go to evaluation mode
        self.model.eval()
    
        # compute,reshape to a 1d array, and convert to a numpy array
        Y  = self.model(x).view(-1, ).detach().numpy()
        
        # rescale output
        Y  = self.mean + self.scale * Y
        
        if len(Y) == 1:
            return Y[0]
        else:
            return Y
        
    def show(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                print(name, param.data)
                print()


N_CNN_KERNEL=2
NFEATURES=1#train_x.shape[1]
N_MULT_FACTOR=2
N_HIDDEN=NFEATURES * N_MULT_FACTOR

class CNN_MODEL(nn.Module):    
    def __init__(self, n_feature, n_hidden, n_output, n_cnn_kernel, n_mult_factor=N_MULT_FACTOR):
        super(CNN_MODEL, self).__init__()
        self.n_feature=n_feature
        self.n_hidden=n_hidden
        self.n_output= n_output 
        self.n_cnn_kernel=n_cnn_kernel
        self.n_mult_factor=n_mult_factor
        self.n_l2_hidden=self.n_hidden * (self.n_mult_factor - self.n_cnn_kernel + 3)
        self.n_out_hidden=int (self.n_l2_hidden/2)
                        
        self.l1 = nn.Sequential(
            torch.nn.Linear(self.n_feature, self.n_hidden),
            torch.nn.Dropout(p=1 -.85),            
            torch.nn.LeakyReLU (0.1),            
            torch.nn.BatchNorm1d(self.n_hidden, eps=1e-05, momentum=0.1, affine=True)            
        )                
        self.c1= nn.Sequential(            
            torch.nn.Conv1d(self.n_feature, self.n_hidden, 
                            kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(1,)),
            torch.nn.Dropout(p=1 -.75),            
            torch.nn.LeakyReLU (0.1),
            torch.nn.BatchNorm1d(self.n_hidden, eps=1e-05, momentum=0.1, affine=True)        
        )                        
        self.out = nn.Sequential(
            torch.nn.Linear(self.n_l2_hidden,
                            self.n_output),  
        )                

        
    def forward(self, x):
        varSize=x.shape[0] # must be calculated here in forward() since its is a dynamic size        
        x=self.l1(x)                
        # for CNN        
        x = x.view(varSize,self.n_feature,self.n_mult_factor)
        x=self.c1(x)
        # for Linear layer
        x = x.view(varSize, self.n_hidden * (self.n_mult_factor -self.n_cnn_kernel + 3))
#         x=self.l2(x)                    
        x=self.out(x)   
        return x

# model = CNN_MODEL(n_feature=NFEATURES, n_hidden=N_HIDDEN, n_output=1, n_cnn_kernel=N_CNN_KERNEL)   # define the network    


#####CONVERT env.yml to requirementes.txt

# import ruamel.yaml
# yaml = ruamel.yaml.YAML()
# data = yaml.load(open(&#39;IQN_env.yml&#39;))
# requirements = []
# for dep in data[&#39;dependencies&#39;]:
#     if isinstance(dep, str):
#         package, package_version, python_version = dep.split(&#39;=&#39;)
#         if python_version == &#39;0&#39;:
#             continue
#         requirements.append(package + &#39;==&#39; + package_version)
#     elif isinstance(dep, dict):
#         for preq in dep.get(&#39;pip&#39;, []):
#             requirements.append(preq)

# with open(&#39;requirements.txt&#39;, &#39;w&#39;) as fp:
#     for requirement in requirements:
#        print(requirement, file=fp)

def show_jupyter_image(image_filename, width = 1300, height = 300):
    &quot;&quot;&quot;Show a saved image directly in jupyter. Make sure image_filename is in your IQN_BASE !&quot;&quot;&quot;
    display(Image(os.path.join(IQN_BASE,image_filename), width = width, height = height  ))
    
    
def use_svg_display():
    &quot;&quot;&quot;Use the svg format to display a plot in Jupyter (better quality)&quot;&quot;&quot;
    from matplotlib_inline import backend_inline
    backend_inline.set_matplotlib_formats(&#39;svg&#39;)

def reset_plt_params():
    &quot;&quot;&quot;reset matplotlib parameters - often useful&quot;&quot;&quot;
    use_svg_display()
    mpl.rcParams.update(mpl.rcParamsDefault)

def show_plot(legend=False):
    use_svg_display()
    plt.tight_layout();
    plt.show()
    if legend:
        plt.legend(loc=&#39;best&#39;)
        
def set_figsize(get_axes=False,figsize=(7, 7)):
    use_svg_display()
    plt.rcParams[&#39;figure.figsize&#39;] = figsize
    if get_axes:
        fig, ax = plt.subplots(1,1, figsize=figsize)
        return fig, ax
    
def set_axes(ax, xlabel, ylabel=None, xmin=None, xmax=None, ymin=None, ymax=None, title=None):
    &quot;&quot;&quot;saves a lot of time in explicitly difining each axis, its title and labels: do them all in one go&quot;&quot;&quot;
    use_svg_display()
    ax.set_xlabel(xlabel,fontsize=font_axes)
    if ylabel:
        ax.set_ylabel(ylabel,fontsize=font_axes)
    if xmin and xmax:
        ax.set_xlim(xmin, xmax)
    
    if ax.get_title()  != &#39;&#39;:
        #if the axes (plot) does have a title (which is non-empty string), display it 
        ax.set_title(title)
    if ax.legend():
        #if an axis has a legned label, display it
        ax.legend(loc=&#39;best&#39;,fontsize=font_legend)
    if ymin and ymax:
        #sometimes we dont have ylimits since we do a lot of histograms, but if an axis has ylimits, set them
        ax.set_ylim(ymin, ymax)
    
    try:
        fig.show()
    except Exception:
        pass
    plt.tight_layout()
    plt.show()
    
def explore_data(df, title, scaled=False):
    fig, ax = plt.subplots(1,5, figsize=(15,10) )
    # df = df[[&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;,&#39;RecoDatapT&#39;, &#39;RecoDataeta&#39;, &#39;RecoDataphi&#39;, &#39;RecoDatam&#39;]]
    levels = [&#39;RecoData&#39;, &#39;genData&#39;]
    kinematics=[&#39;pT&#39;,&#39;eta&#39;,&#39;phi&#39;,&#39;m&#39;]
    columns = [level+k for level in levels for k in kinematics]
    print(columns)
    columns = columns + [&#39;tau&#39;]
    print(columns)
    df = df[columns]
    
    for k_i, k in enumerate(kinematics):
        Reco_var = levels[0]+k
        gen_var = levels[1]+k
        print(&#39;Reco_var: &#39;, Reco_var, &#39;, \t gen_var: &#39;, gen_var)
        ax[k_i].hist(df[Reco_var], bins=100, label=Reco_var, alpha=0.35)
        ax[k_i].hist(df[gen_var], bins=100, label=gen_var, alpha=0.35)
        xmin, xmax = FIELDS[Reco_var][&#39;xmin&#39;], FIELDS[Reco_var][&#39;xmax&#39;]
        xlabel=FIELDS[Reco_var][&#39;xlabel&#39;]
        ax[k_i].set_xlim( (xmin, xmax) )
        # set_axes(ax[k_i], xlabel=xlabel, ylabel=&#39;&#39;, xmin=xmin, xmax=xmax)
        ax[k_i].set_xlabel(xlabel,fontsize=26)
        
        
                  
        if scaled:
            ax[k_i].set_xlim(df[gen_var].min(),df[gen_var].max() )
        
        ax[k_i].legend(loc=&#39;best&#39;, fontsize=13)
    ax[4].hist(df[&#39;tau&#39;],bins=100, label=r&#39;$\tau$&#39;)
    ax[4].legend(loc=&#39;best&#39;, fontsize=13)
    fig.suptitle(title, fontsize=30)
    show_plot()




def show_jupyter_image(image_filename, width = 1300, height = 300):
    &quot;&quot;&quot;Show a saved image directly in jupyter. Make sure image_filename is in your IQN_BASE !&quot;&quot;&quot;
    display(Image(os.path.join(IQN_BASE,image_filename), width = width, height = height  ))
    
    
def use_svg_display():
    &quot;&quot;&quot;Use the svg format to display a plot in Jupyter (better quality)&quot;&quot;&quot;
    from matplotlib_inline import backend_inline
    backend_inline.set_matplotlib_formats(&#39;svg&#39;)

def reset_plt_params():
    &quot;&quot;&quot;reset matplotlib parameters - often useful&quot;&quot;&quot;
    use_svg_display()
    mpl.rcParams.update(mpl.rcParamsDefault)
    


def show_plot(legend=False):
    use_svg_display()
    plt.tight_layout();
    plt.show()
    if legend:
        plt.legend(loc=&#39;best&#39;)
        
def set_figsize(get_axes=False,figsize=(7, 7)):
    use_svg_display()
    plt.rcParams[&#39;figure.figsize&#39;] = figsize
    if get_axes:
        fig, ax = plt.subplots(1,1, figsize=figsize)
        return fig, ax
    
def set_axes(ax, xlabel, ylabel=None, xmin=None, xmax=None, ymin=None, ymax=None, title=None):
    &quot;&quot;&quot;saves a lot of time in explicitly difining each axis, its title and labels: do them all in one go&quot;&quot;&quot;
    use_svg_display()
    ax.set_xlabel(xlabel,fontsize=font_axes)
    if ylabel:
        ax.set_ylabel(ylabel,fontsize=font_axes)
    if xmin and xmax:
        ax.set_xlim(xmin, xmax)
    
    if ax.get_title()  != &#39;&#39;:
        #if the axes (plot) does have a title (which is non-empty string), display it 
        ax.set_title(title)
    if ax.legend():
        #if an axis has a legned label, display it
        ax.legend(loc=&#39;best&#39;,fontsize=font_legend)
    if ymin and ymax:
        #sometimes we dont have ylimits since we do a lot of histograms, but if an axis has ylimits, set them
        ax.set_ylim(ymin, ymax)
    
    try:
        fig.show()
    except Exception:
        pass
    plt.tight_layout()
    # plt.show()

def get_finite(values):
    return values[np.isfinite(values)]

def mkdir(dir_):
    &quot;&quot;&quot;make a directory without overwriting what&#39;s in it if it exists&quot;&quot;&quot;
    # assert isinstance(dir_, str)
    try:
        os.system(&#39;mkdir -p %s&#39; % str(dir_) )
    except Exception:
        pass
    
############################ Some decorators ############################ 
def SourceIQN(func):
    def _func(*args):
        import os
        from common.utility.source import source
        env = {}
        env.update(os.environ)
        env.update(source(os.environ[&quot;IQN_BASE&quot;]))        
        func(*args, env=env)
    return _func


def time_type_of_func(tuning_or_training, _func=None):
    def timer(func):
        &quot;&quot;&quot;Print the runtime of the decorated function&quot;&quot;&quot;
        import functools
        import time
        @functools.wraps(func)
        def wrapper_timer(*args, **kwargs):
            if tuning_or_training==&#39;training&#39;:
                print(f&#39;training IQN &#39;)#to estimate {target}
            elif tuning_or_training==&#39;tuning&#39;:
                print(f&#39;tuning IQN hyperparameters &#39;)#to estimate {target}
            else:
                print(f&#39;timing this arbitrary function&#39;)
            start_time = time.perf_counter()    
            value = func(*args, **kwargs)
            end_time = time.perf_counter()      
            run_time = end_time - start_time    
            if tuning_or_training==&#39;training&#39;:
                print(f&quot;training target {target} using {func.__name__!r} in {run_time:.4f} secs&quot;)
            elif tuning_or_training==&#39;tuning&#39;:
                print(f&quot;tuning IQN hyperparameters for {target} using {func.__name__!r} in {run_time:.4f} secs&quot;)
            else:
                print(f&quot;this arbirary function took {run_time:.4f} secs&quot;)
            return value
        return wrapper_timer
    if _func is None:
        return timer
    else:
        return timer(_func)

def debug(func):
    &quot;&quot;&quot;Print the function signature and return value&quot;&quot;&quot;
    import functools
    @functools.wraps(func)
    def wrapper_debug(*args, **kwargs):
        args_repr = [repr(a) for a in args]                      
        kwargs_repr = [f&quot;{k}={v!r}&quot; for k, v in kwargs.items()]  
        signature = &quot;, &quot;.join(args_repr + kwargs_repr)           
        print(f&quot;Calling {func.__name__}({signature})&quot;)
        values = func(*args, **kwargs)
        print(f&quot;{func.__name__!r} returned {values!r}&quot;)           
        return values
    return wrapper_debug


def make_interactive(func):
    &quot;&quot;&quot; make the plot interactive&quot;&quot;&quot;
    import functools
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        plt.ion()
        output=func(*args, **kwargs)
        plt.ioff()
        return output
    return wrapper
        
# from IPython.core.magic import register_cell_magic

# @register_cell_magic
def write_and_run(line, cell):
    &quot;&quot;&quot;write the current cell to a file (or append it with -a argument) as well as execute it
    use with %%write_and_run at the top of a given cell&quot;&quot;&quot;
    argz = line.split()
    file = argz[-1]
    mode = &#39;w&#39;
    if len(argz) == 2 and argz[0] == &#39;-a&#39;:
        mode = &#39;a&#39;
    with open(file, mode) as f:
        f.write(cell)
    # get_ipython().run_cell(cell)
    
    
@debug
def get_model_params_simple():
    dropout=0.2
    n_layers = 2
    n_hidden=32
    starting_learning_rate=1e-3
    print(&#39;n_iterations, n_layers, n_hidden, starting_learning_rate, dropout&#39;)
    return n_iterations, n_layers, n_hidden, starting_learning_rate, dropout




# update fonts
FONTSIZE = 14
font = {&#39;family&#39; : &#39;serif&#39;,
        &#39;weight&#39; : &#39;normal&#39;,
        &#39;size&#39;   : FONTSIZE}
mp.rc(&#39;font&#39;, **font)

DATA_DIR=os.environ[&#39;DATA_DIR&#39;]
X       = [&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;, &#39;tau&#39;]

ORDER=&#39;m_First&#39;

if ORDER==&#39;m_First&#39;:
    FIELDS  = {&#39;RecoDatam&#39; : {&#39;inputs&#39;: X, 
                               &#39;xlabel&#39;:  r&#39;$m$ (GeV)&#39;, 
                              &#39;ylabel&#39;:&#39;$m^{reco}$&#39;,
                               &#39;xmin&#39;: 0, 
                               &#39;xmax&#39;: 25},
                           

               &#39;RecoDatapT&#39;: {&#39;inputs&#39;: [&#39;RecoDatam&#39;]+X, 
                               &#39;xlabel&#39;:  r&#39;$p_T$ (GeV)&#39; , 
                              &#39;ylabel&#39;: &#39;$p_T^{reco}$&#39;,
                               &#39;xmin&#39;  : 20, 
                               &#39;xmax&#39;  :  80},

               &#39;RecoDataeta&#39;: {&#39;inputs&#39;: [&#39;RecoDatam&#39;,&#39;RecoDatapT&#39;] + X, 
                               &#39;xlabel&#39;: r&#39;$\eta$&#39;,
                               &#39;ylabel&#39;:&#39;$\eta^{reco}$&#39;,
                               &#39;xmin&#39;  : -5,
                               &#39;xmax&#39;  :  5},

               &#39;RecoDataphi&#39;  : {&#39;inputs&#39;: [&#39;RecoDatam&#39;, &#39;RecodatapT&#39;, &#39;RecoDataeta&#39;]+X,
                               &#39;xlabel&#39;: r&#39;$\phi$&#39; ,
                                &#39;ylabel&#39; :&#39;$\phi^{reco}$&#39;,
                               &#39;xmin&#39;  : -3.2, 
                               &#39;xmax&#39;  :3.2}
              }


# Load and explore raw (unscaled) dataframes

# In[20]:


all_variable_cols=[&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;,&#39;RecoDatapT&#39;, &#39;RecoDataeta&#39;, &#39;RecoDataphi&#39;, &#39;RecoDatam&#39;]
all_cols=[&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;,&#39;RecoDatapT&#39;, &#39;RecoDataeta&#39;, &#39;RecoDataphi&#39;, &#39;RecoDatam&#39;, &#39;tau&#39;]


    
def get_model_filename(target, PARAMS):
    filename_model=f&quot;&quot;&quot;Trained_IQNx4_{target}_\
        {PARAMS[&#39;n_layers&#39;]}_layer\
        {PARAMS[&#39;hidden_size&#39;]}_hidden\
            {PARAMS[&#39;activation&#39;]}_activation\
                {PARAMS[&#39;batch_size&#39;]}_batchsize\
                    {PARAMS[&#39;n_iterations&#39;]}_iteration.dict&quot;&quot;&quot; 
                    
    return filename_model

def explore_data(df, title, scaled=False):
    fig, ax = plt.subplots(1,5, figsize=(15,10) )
    # df = df[[&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;,&#39;RecoDatapT&#39;, &#39;RecoDataeta&#39;, &#39;RecoDataphi&#39;, &#39;RecoDatam&#39;]]
    levels = [&#39;RecoData&#39;, &#39;genData&#39;]
    kinematics=[&#39;pT&#39;,&#39;eta&#39;,&#39;phi&#39;,&#39;m&#39;]
    columns = [level+k for level in levels for k in kinematics]
    print(columns)
    columns = columns + [&#39;tau&#39;]
    print(columns)
    df = df[columns]
    
    for k_i, k in enumerate(kinematics):
        Reco_var = levels[0]+k
        gen_var = levels[1]+k
        print(&#39;Reco_var: &#39;, Reco_var, &#39;, \t gen_var: &#39;, gen_var)
        ax[k_i].hist(df[Reco_var], bins=100, label=Reco_var, alpha=0.35)
        ax[k_i].hist(df[gen_var], bins=100, label=gen_var, alpha=0.35)
        xmin, xmax = FIELDS[Reco_var][&#39;xmin&#39;], FIELDS[Reco_var][&#39;xmax&#39;]
        xlabel=FIELDS[Reco_var][&#39;xlabel&#39;]
        ax[k_i].set_xlim( (xmin, xmax) )
        # set_axes(ax[k_i], xlabel=xlabel, ylabel=&#39;&#39;, xmin=xmin, xmax=xmax)
        ax[k_i].set_xlabel(xlabel,fontsize=26)
        
        
                  
        if scaled:
            ax[k_i].set_xlim(df[gen_var].min(),df[gen_var].max() )
        
        ax[k_i].legend(loc=&#39;best&#39;, fontsize=13)
    ax[4].hist(df[&#39;tau&#39;],bins=100, label=r&#39;$\tau$&#39;)
    ax[4].legend(loc=&#39;best&#39;, fontsize=13)
    fig.suptitle(title, fontsize=30)
    show_plot()
    
# @memory.cache
def get_scaling_info(df):
    &quot;&quot;&quot;args: df is train or eval df.
    returns: dictionary with mean of std of each feature (column) in the df&quot;&quot;&quot;
    features=[&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;,
              &#39;RecoDatapT&#39;, &#39;RecoDataeta&#39;, &#39;RecoDataphi&#39;, &#39;RecoDatam&#39;, 
            #   &#39;tau&#39;
              ]
    
    SCALE_DICT = dict.fromkeys(features)
    for i in range(8):
        feature = features[i]
        feature_values = np.array(df[feature])
        SCALE_DICT[feature]={}
        SCALE_DICT[feature][&#39;mean&#39;] = np.mean(feature_values, dtype=np.float64)
        SCALE_DICT[feature][&#39;std&#39;] = np.std(feature_values, dtype=np.float64)
    return SCALE_DICT





def L(orig_observable, label):
    eps=1e-20
    orig_observable=orig_observable+eps
    if label==&#39;pT&#39;:
        const=0
        log_pT_=np.log(orig_observable) 
        L_observable = log_pT_
    if label==&#39;eta&#39;:
        const=0
        L_observable=orig_observable
    if label==&#39;m&#39;:
        const=2
        L_observable=np.log(orig_observable + const)
    if label==&#39;phi&#39;:
        L_observable=orig_observable
    if label==&#39;tau&#39;:
        L_observable=orig_observable
#         L_observable = (6*orig_observable) - 3
    
    return L_observable.to_numpy()



def L_inverse(L_observable, label):
    eps=1e-20
    L_observable=L_observable+eps
    if label==&#39;pT&#39;:
        const=0
        L_inverse_observable = np.exp(L_observable)
    if label==&#39;eta&#39;:
        L_inverse_observable = L_observable
    if label==&#39;m&#39;:
        const=2
        L_inverse_observable = np.exp(L_observable) - const
    if label==&#39;tau&#39;:
        L_inverse_observable=L_observable
        # L_inverse_observable = (L_observable+3)/6
        
    if not isinstance(L_inverse_observable, np.ndarray):
        L_inverse_observable = L_inverse_observable.to_numpy()
    return L_inverse_observable




def T(variable, scaled_df):
    if variable==&#39;pT&#39;:
        L_pT_gen=scaled_df[&#39;genDatapT&#39;]
        L_pT_reco = scaled_df[&#39;RecoDatapT&#39;]
        target = (L_pT_reco+10)/(L_pT_gen+10) 
    if variable==&#39;eta&#39;:
        L_eta_gen=scaled_df[&#39;genDataeta&#39;]
        L_eta_reco = scaled_df[&#39;RecoDataeta&#39;]
        target =  (L_eta_reco+10)/(L_eta_gen+10) 
    if variable==&#39;phi&#39;:
        L_phi_gen=scaled_df[&#39;genDataphi&#39;]
        L_phi_reco = scaled_df[&#39;RecoDataphi&#39;]
        target =  (L_phi_reco+10)/(L_phi_gen+10) 
    if variable==&#39;m&#39;:
        L_m_gen=scaled_df[&#39;genDatam&#39;]
        L_m_reco = scaled_df[&#39;RecoDatam&#39;]
        target =  (L_m_reco+10)/(L_m_gen+10) 
    
    return target



def L_scale_df(df, title, save=False):
    #scale
    df = df[all_cols]
    # print(df.head())
    scaled_df = pd.DataFrame()
    #select the columns by index: 
    # 0:genDatapT, 1:genDataeta, 2:genDataphi, 3:genDatam, 
    # 4:RecoDatapT, 5:RecoDataeta, 6:RecoDataphi, 7: Recodatam
    scaled_df[&#39;genDatapT&#39;] = L(df.iloc[:,0], label=&#39;pT&#39;)
    scaled_df[&#39;RecoDatapT&#39;] = L(df.iloc[:,4], label=&#39;pT&#39;)
    
    scaled_df[&#39;genDataeta&#39;] = L(df.iloc[:,1], label=&#39;eta&#39;)
    scaled_df[&#39;RecoDataeta&#39;] = L(df.iloc[:,5],label=&#39;eta&#39;)
    
    
    scaled_df[&#39;genDataphi&#39;] = L(df.iloc[:,2],label=&#39;phi&#39;)
    scaled_df[&#39;RecoDataphi&#39;] = L(df.iloc[:,6],label=&#39;phi&#39;)

    scaled_df[&#39;genDatam&#39;] = L(df.iloc[:,3],label=&#39;m&#39;)
    scaled_df[&#39;RecoDatam&#39;] = L(df.iloc[:,7],label=&#39;m&#39;)
    #why scale tau?
    # scaled_df[&#39;tau&#39;] = 6 * df.iloc[:,8] - 3
    scaled_df[&#39;tau&#39;] = L(df.iloc[:,8],label=&#39;tau&#39;)
    
    print(scaled_df.describe())
    
    if save:
        scaled_df.to_csv(os.path.join(DATA_DIR, title) )
    return scaled_df


def decay_LR(iter):
    starting_LR = 1e-3
    return starting_LR * np.exp(- iter/ (1e4))

# @register_cell_magic
def write_and_run(line, cell):
    &quot;&quot;&quot;write the current cell to a file (or append it with -a argument) as well as execute it
    use with %%write_and_run at the top of a given cell&quot;&quot;&quot;
    argz = line.split()
    file = argz[-1]
    mode = &quot;w&quot;
    if len(argz) == 2 and argz[0] == &quot;-a&quot;:
        mode = &quot;a&quot;
    with open(file, mode) as f:
        f.write(cell)
    # get_ipython().run_cell(cell)


def get_batch(x, t, batch_size):
    # the numpy function choice(length, number)
    # selects at random &quot;batch_size&quot; integers from 
    # the range [0, length-1] corresponding to the
    # row indices.
    rows    = np.random.choice(len(x), batch_size)
    batch_x = x[rows]
    batch_t = t[rows]
    # batch_x.T[-1] = np.random.uniform(0, 1, batch_size)
    return (batch_x, batch_t)

def add_noise(x):
    noise = np.random.normal(loc=0, scale=0.6)
    if x.ndim==1:
        x = x + noise
    else:
        shape_x = x.shape
        x[:,:-1] = x[:, :-1] + noise 
    return x 
# Note: there are several average loss functions available 
# in pytorch, but it&#39;s useful to know how to create your own.
def average_quadratic_loss(f, t, x):
    # f and t must be of the same shape
    return  torch.mean((f - t)**2)

def average_cross_entropy_loss(f, t, x):
    # f and t must be of the same shape
    loss = torch.where(t &gt; 0.5, torch.log(f), torch.log(1 - f))
    return -torch.mean(loss)

def average_quantile_loss(f, t, x):
    
    # f and t must be of the same shape
    tau = x.T[-1] # last column is tau.
    #L= tau * (target - output), if target&gt;output
    #L= (1-tau)*(output-target), otherwise
    return torch.mean(torch.where(t &gt; f, 
                                  tau * (t - f), 
                                  (1 - tau)*(f - t)))
    
def average_huber_quantile_loss(f, t, x):
    
    # f and t must be of the same shape
    tau = x.T[-1] # last column is tau.
    #u = target-output
    u=t-f
    abs_u = abs(u)
    #threshold kappa
    kappa=0.5
    #L = (tau - I[u &lt;=0])/(2*kappa) * u**2 , if |u| &lt;= kappa
    #L = (1-tau)*kappa* (|u| - kappa/2), otherwise
    #call I[u &lt;= 0] = z
    z= (u &lt;= 0).float()
    return torch.mean(torch.where(abs_u &lt;= kappa, 
                                  (tau - z)/(2*kappa) * (u**2), 
                                 abs(tau - z) * (abs_u - (kappa/2) )                               
                                                                 )  
                                  )
                      
def RMS(v):
    return (torch.mean(v**2))**0.5


def average_quantile_loss_with_df_dtau(f, t, x, df_dtau):
    # f and t must be of the same shape
    tau = x.T[-1] # last column is tau.
    #Eq (2)
    return torch.mean(torch.where(t &gt;= f, 
                                  tau * (t - f) + (-df_dtau) * RMS(df_dtau), 
                                  (1 - tau)*(f - t) +(-df_dtau)* RMS(df_dtau)
                                  ))
    
# function to validate model during training.
def validate(model, avloss, inputs, targets):
    # make sure we set evaluation mode so that any training specific
    # operations are disabled.
    model.eval() # evaluation mode
    
    with torch.no_grad(): # no need to compute gradients wrt. x and t
        x = torch.from_numpy(inputs).float()
        t = torch.from_numpy(targets).float()
        # remember to reshape!
        o = model(x).reshape(t.shape)
    return avloss(o, t, x)


def plot_average_loss(traces, n_iterations, target, ftsize=18,save_loss_plots=True, show_loss_plots=True):
    
    xx, yy_t, yy_v, yy_v_avg = traces
    
    # create an empty figure
    fig = plt.figure(figsize=(6, 4.5))
    fig.tight_layout()
    
    # add a subplot to it
    nrows, ncols, index = 1,1,1
    ax  = fig.add_subplot(nrows,ncols,index)

    ax.set_title(&quot;Average loss&quot;)
    
    ax.plot(xx, yy_t, &#39;b&#39;, lw=2, label=&#39;Training&#39;)
    ax.plot(xx, yy_v, &#39;r&#39;, lw=2, label=&#39;Validation&#39;)
    #ax.plot(xx, yy_v_avg, &#39;g&#39;, lw=2, label=&#39;Running average&#39;)

    ax.set_xlabel(&#39;Iterations&#39;, fontsize=ftsize)
    ax.set_ylabel(&#39;average loss&#39;, fontsize=ftsize)
    ax.set_xscale(&#39;log&#39;)
    ax.set_yscale(&#39;log&#39;)
    ax.grid(True, which=&quot;both&quot;, linestyle=&#39;-&#39;)
    ax.legend(loc=&#39;upper right&#39;)
    if save_loss_plots:
        filename=&#39;IQNx4_Loss_%s_%sK_iteration.png&#39; % (target, sr(int(n_iterations)) )
        mkdir(&#39;images/loss_plots&#39;)
        PATH = os.path.join(IQN_BASE, &#39;images&#39;, &#39;loss_plots&#39;, filename)
        plt.savefig(PATH)
        print(&#39;\nloss curve saved in %s&#39; % PATH)
    if show_loss_plots:
        show_plot()
        
        

# def split_t_x(df, target, input_features):
#     &quot;&quot;&quot; Get teh target as the ratio, according to the T equation&quot;&quot;&quot;
    
#     if target==&#39;RecoDatam&#39;:
#         t = T(&#39;m&#39;, scaled_df=train_data_m)
#     if target==&#39;RecoDatapT&#39;:
#         t = T(&#39;pT&#39;, scaled_df=train_data_m)
#     if target==&#39;RecoDataeta&#39;:
#         t = T(&#39;eta&#39;, scaled_df=train_data_m)
#     if target==&#39;RecoDataphi&#39;:
#         t = T(&#39;phi&#39;, scaled_df=train_data_m)
#     x = np.array(df[input_features])
#     return np.array(t), x



# def apply_z_to_features():
#     &quot;&quot;&quot;TO ensure this z scaling is only applied once to the training features, we use a generator &quot;&quot;&quot;
#     for i in range(NFEATURES-1):
#         train_x[:,i] = z(train_x[:,i])
#         test_x[:,i] = z(test_x[:,i])
#         valid_x[:,i] = z(valid_x[:,i])
#     yield train_x 
#     yield test_x 
#     yield valid_x





# ### Apply $z$ to targets before training

# def apply_z_to_targets():
#     train_t_ratio_ = z(train_t_ratio) 
#     test_t_ratio_ = z(test_t_ratio) 
#     valid_t_ratio_ = z(valid_t_ratio)
    
#     yield train_t_ratio_
#     yield test_t_ratio_
#     yield valid_t_ratio_
    

class RegularizedRegressionModel(nn.Module):
    &quot;&quot;&quot;Used for hyperparameter tuning &quot;&quot;&quot;
    #inherit from the super class
    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout_1, dropout_2,activation):
        super().__init__()
        layers = []
        for _ in range(nlayers):
            if len(layers) ==0:
                #nlayers is number of hidden layers+1, since there is always an input layer and an output layer
                #INPUT LAYER
                #inital layer has to have size of (input features, output_nodes),
                #its output layer can have any size but it must match the size of the input layer of the next linear layer
                #here we choose its output layer as the hidden size (fully connected)
                #ALPHA DROPOUT
                # layers.append(nn.AlphaDropout(dropout_1))
                
                layer= nn.Linear(nfeatures, hidden_size)
                torch.nn.init.xavier_uniform_(layer.weight)
                layers.append(layer)
                #batch normalization
                # layers.append(nn.BatchNorm1d(hidden_size))
                #dropout should have higher values in deeper layers
                # layers.append(nn.Dropout(dropout_1))#Use small dropout for 1st layers &amp; larger dropout for later layers. In both cases, the larger he model the larger the dropout.
                #When model is in training, apply dropout. When using model for inference, dont use dropout

                
                #ReLU activation 
                if activation==&#39;LeakyReLU&#39;:
                    layers.append(nn.LeakyReLU(negative_slope=0.3))
                elif activation==&#39;PReLU&#39;:
                    layers.append(nn.PReLU())
                elif activation==&#39;ReLU6&#39;:
                    layers.append(nn.ReLU6())
                elif activation==&#39;ELU&#39;:
                    layers.append(nn.ELU())
                elif activation==&#39;SELU&#39;:
                    layers.append(nn.SELU())
                elif activation==&#39;CELU&#39;:
                    layers.append(nn.CELU())
                    
                    
            else:
                #if this is not the first layer (we dont have layers)
                layer=nn.Linear(hidden_size, hidden_size)
                torch.nn.init.xavier_uniform_(layer.weight)
                layers.append(layer)
                # layers.append(nn.Dropout(dropout_2))
                layers.append(nn.BatchNorm1d(hidden_size))
                
                if activation==&#39;LeakyReLU&#39;:
                    layers.append(nn.LeakyReLU(negative_slope=0.3))
                elif activation==&#39;PReLU&#39;:
                    layers.append(nn.PReLU())
                
        #output layer:
        output_layer=nn.Linear(hidden_size, ntargets)
        torch.nn.init.xavier_uniform_(output_layer.weight)
        layers.append(output_layer) 

        # only for classification add sigmoid
        # layers.append(nn.Sigmoid()) or softmax
            #we have defined sequential model using the layers in oulist 
        self.model = nn.Sequential(*layers)

    
    def forward(self, x):
        return self.model(x)
    
    
    
    
    
    
def initialize_weights_alone(m):
    &quot;&quot;&quot;use a different weight initialization &quot;&quot;&quot;
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight.data)
        nn.init.constant_(m.bias.data, 0)
    
# class TrainingRegularizedRegressionModel(nn.Module):
#     &quot;&quot;&quot;Used for training, and adds more regularization to prevent overfitting &quot;&quot;&quot;
#     #inherit from the super class
#     def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):
#         super().__init__()
#         layers = []
#         for _ in range(nlayers):
#             if len(layers) ==0:
#                 #inital layer has to have size of input features as its input layer
#                 #its output layer can have any size but it must match the size of the input layer of the next linear layer
#                 #here we choose its output layer as the hidden size (fully connected)
#                 layers.append(nn.Linear(nfeatures, hidden_size))
#                 #batch normalization
#                 layers.append(nn.BatchNorm1d(hidden_size))
#                 #dropout only in the first layer
#                 #Dropout seems to worsen model performance
#                 layers.append(nn.Dropout(dropout))
#                 #ReLU activation 
#                 layers.append(nn.LeakyReLU())
#             else:
#                 #if this is not the first layer (we dont have layers)
#                 layers.append(nn.Linear(hidden_size, hidden_size))
#                 layers.append(nn.BatchNorm1d(hidden_size))
#                 #Dropout seems to worsen model performance
#                 layers.append(nn.Dropout(dropout))
#                 layers.append(nn.LeakyReLU())
#                 #output layer:
#         layers.append(nn.Linear(hidden_size, ntargets)) 

#         # only for classification add sigmoid
#         # layers.append(nn.Sigmoid())
#             #we have defined sequential model using the layers in oulist 
#         self.model = nn.Sequential(*layers)

    
#     def forward(self, x):
#         return self.model(x)




# ## Hyperparameter Training Workflow


def get_tuning_sample():
    sample=int(200000)
    # train_x_sample, train_t_ratio_sample, valid_x_sample, valid_t_ratio_sample
    get_whole=True
    if get_whole:
        train_x_sample, train_t_ratio_sample, valid_x_sample, valid_t_ratio_sample = train_x, train_t_ratio, valid_x, valid_t_ratio
    else:
        train_x_sample, train_t_ratio_sample, valid_x_sample, valid_t_ratio_sample=train_x[:sample], train_t_ratio[:sample], valid_x[:sample], valid_t_ratio[:sample]
    return train_x_sample, train_t_ratio_sample, valid_x_sample, valid_t_ratio_sample



class HyperTrainer():
    &quot;&quot;&quot;loss, training and evaluation&quot;&quot;&quot;
    def __init__(self, model, optimizer, batch_size):
                 #, device):
        self.model = model
        #self.device= device
        self.optimizer = optimizer
        self.batch_size=batch_size
        self.n_iterations_tune=int(50)

        #the loss function returns the loss function. It is a static method so it doesn&#39;t need self
        # @staticmethod
        # def loss_fun(targets, outputs):
        #   tau = torch.rand(outputs.shape)
        #   return torch.mean(torch.where(targets &gt;= outputs, 
        #                                   tau * (targets - outputs), 
        #                                   (1 - tau)*(outputs - targets)))

        #     This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, 
        #     by combining the operations into one layer

    def train(self, x, t):

        self.model.train()
        final_loss = 0
        for iteration in range(self.n_iterations_tune):
            self.optimizer.zero_grad()
            batch_x, batch_t = get_batch(x, t,  self.batch_size)#x and t are train_x and train_t

            # with torch.no_grad():
            inputs=torch.from_numpy(batch_x).float()
            targets=torch.from_numpy(batch_t).float()
            outputs = self.model(inputs)
            loss = average_quantile_loss(outputs, targets, inputs)
            loss.backward()
            self.optimizer.step()
            final_loss += loss.item()

        return final_loss / self.batch_size

    def evaluate(self, x, t):

        self.model.eval()
        final_loss = 0
        for iteration in range(self.n_iterations_tune):
            batch_x, batch_t = get_batch(x, t,  self.batch_size)#x and t are train_x and train_t

            # with torch.no_grad():            
            inputs=torch.from_numpy(batch_x).float()
            targets=torch.from_numpy(batch_t).float()
            outputs = self.model(inputs)
            loss =average_quantile_loss(outputs, targets, inputs)
            final_loss += loss.item()
        return final_loss / self.batch_size

    
EPOCHS=1
def run_train(params, save_model=False):
    &quot;&quot;&quot;For tuning the parameters&quot;&quot;&quot;

    model =  RegularizedRegressionModel(
              nfeatures=train_x_sample.shape[1], 
                ntargets=1,
                nlayers=params[&quot;nlayers&quot;], 
                hidden_size=params[&quot;hidden_size&quot;],
                dropout=params[&quot;dropout&quot;]
                )
    # print(model)
    

    learning_rate= params[&quot;learning_rate&quot;]
    optimizer_name = params[&quot;optimizer_name&quot;]
    
    # optimizer = torch.optim.Adam(model.parameters(), lr=params[&quot;learning_rate&quot;]) 
    
    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), 
                            lr=learning_rate, momentum = params[&quot;momentum&quot;])
    
    trainer=HyperTrainer(model, optimizer, batch_size=params[&quot;batch_size&quot;])
    best_loss = np.inf
    early_stopping_iter=10#stop after 10 iteractions of not improving loss
    early_stopping_coutner=0

    # for epoch in range(EPOCHS):
    # train_loss = trainer.train(train_x_sample, train_t_ratio_sample)
        #test loss
    valid_loss=trainer.evaluate(valid_x_sample, valid_t_ratio_sample)

        # print(f&quot;{epoch} \t {train_loss} \t {valid_loss}&quot;)
        
        # if valid_loss&lt;best_loss:
        #     best_loss=valid_loss
        # else:
        #     early_stopping_coutner+=1
        # if early_stopping_coutner &gt; early_stopping_iter:
            # break
            
    # return best_loss
    return valid_loss




def objective(trial):
    CLUSTER=False
    #cluster has greater memory than my laptop, which allows higher max values in hyperparam. search space
    if CLUSTER:
        nlayers_max,n_hidden_max, batch_size_max=int(24),int(350), int(1e5)
        n_trials=1000
    else:
        nlayers_max,n_hidden_max, batch_size_max=int(6),int(256), int(3e4)
        n_trials=2
    #hyperparameter search space:
    params = {
          &quot;nlayers&quot;: trial.suggest_int(&quot;nlayers&quot;,1,nlayers_max),      
          &quot;hidden_size&quot;: trial.suggest_int(&quot;hidden_size&quot;, 1, n_hidden_max),
          &quot;dropout&quot;: trial.suggest_float(&quot;dropout&quot;, 0.0,0.5),
          &quot;optimizer_name&quot; : trial.suggest_categorical(&quot;optimizer_name&quot;, [&quot;RMSprop&quot;, &quot;SGD&quot;]),
          &quot;momentum&quot;: trial.suggest_float(&quot;momentum&quot;, 0.0,0.99),
          &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 1e-6, 1e-2),
          &quot;batch_size&quot;: trial.suggest_int(&quot;batch_size&quot;, 500, batch_size_max)

        }
    
    for step in range(10):

        temp_loss = run_train(params,save_model=False)
        trial.report(temp_loss, step)
        #activate pruning (early stopping if the current step in the trial has unpromising results)
        #instead of doing lots of iterations, do less iterations and more steps in each trial,  
        #such that a trial is terminated if a step yields an unpromising loss.
        
        if trial.should_prune():
            raise optuna.TrialPruned()
    
    return temp_loss

@time_type_of_func(tuning_or_training=&#39;tuning&#39;)
def tune_hyperparameters(save_best_params):
    

    sampler=False#use different sampling technique than the defualt one if sampler=True.
    if sampler:
        #choose a different sampling strategy (https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.CmaEsSampler.html#optuna.samplers.CmaEsSampler)
        # sampler=optuna.samplers.RandomSampler()
        study=optuna.create_study(direction=&#39;minimize&#39;,
                                  pruner=optuna.pruners.MedianPruner(), sampler=sampler)
    else:
        #but the default sampler is usually better - no need to change it!
        study=optuna.create_study(direction=&#39;minimize&#39;,
                                  pruner=optuna.pruners.HyperbandPruner())
    print(f&#39;using {n_trials} trials for tuning&#39;)
    study.optimize(objective, n_trials=n_trials)
    best_trial = study.best_trial
    print(&#39;best model parameters&#39;, best_trial.params)

    best_params=best_trial.params#this is a dictionary
    #save best hyperapameters in a pandas dataframe as a .csv
    if save_best_params:
        tuned_dir = os.path.join(IQN_BASE,&#39;best_params&#39;)
        mkdir(&#39;tuned_dir&#39;)
        filename=os.path.join(tuned_dir,&#39;best_params_mass_%s_trials.csv&#39; % str(int(n_trials)))
        param_df=pd.DataFrame({
                                &#39;n_layers&#39;:best_params[&quot;nlayers&quot;], 
                                &#39;hidden_size&#39;:best_params[&quot;hidden_size&quot;], 
                                &#39;dropout&#39;:best_params[&quot;dropout&quot;],
                                &#39;optimizer_name&#39;:best_params[&quot;optimizer_name&quot;],
                                &#39;learning_rate&#39;: best_params[&quot;learning_rate&quot;], 
                                &#39;batch_size&#39;:best_params[&quot;batch_size&quot;],
                                &#39;momentum&#39;:best_params[&quot;momentum&quot;]},
                                        index=[0]
        )

        param_df.to_csv(filename)   
    return study



# def load_untrained_model():
#     model=TrainingRegularizedRegressionModel(nfeatures=NFEATURES, ntargets=1,
#                                nlayers=n_layers, hidden_size=hidden_size, dropout=dropout)
    
    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>using torch version 1.9.0
matplotlib version=  3.5.1
using (optional) optuna version 2.8.0
BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN
DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data
</pre></div>
</div>
</div>
</div>
<section id="set-arguments-optional-and-configurations">
<h2>Set arguments (optional) and configurations<a class="headerlink" href="#set-arguments-optional-and-configurations" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># add_to_cluster()
################################### ARGUMENTS ###################################
parser=argparse.ArgumentParser(description=&#39;train for different targets&#39;)
parser.add_argument(&#39;--N&#39;, type=str, help=&#39;&#39;&#39;size of the dataset you want to use. 
                    Options are 10M and 100K and 10M_2, the default is 10M_2&#39;&#39;&#39;, required=False,default=&#39;10M_2&#39;)
#N_epochs X N_train_examples = N_iterations X batch_size
# N_iterations = (N_epochs * train_data.shape[0])/batch_size
#N_iterations = (N_epochs * train_data.shape[0])/64 = 125000 for 1 epoch
parser.add_argument(&#39;--n_iterations&#39;, type=int, help=&#39;&#39;&#39;The number of iterations for training, 
                    the default is&#39;&#39;&#39;, required=False,default=50)
#default=5000000 )
parser.add_argument(&#39;--n_layers&#39;, type=int, help=&#39;&#39;&#39;The number of layers in your NN, 
                    the default is 5&#39;&#39;&#39;, required=False,default=6)
parser.add_argument(&#39;--n_hidden&#39;, type=int, help=&#39;&#39;&#39;The number of hidden layers in your NN, 
                    the default is 5&#39;&#39;&#39;, required=False,default=6)
parser.add_argument(&#39;--starting_learning_rate&#39;, type=float, help=&#39;&#39;&#39;Starting learning rate, 
                    the defulat is 10^-3&#39;&#39;&#39;, required=False,default=1.e-2)
parser.add_argument(&#39;--show_loss_plots&#39;, type=bool, help=&#39;&#39;&#39;Boolean to show the loss plots, 
                    default is False&#39;&#39;&#39;, required=False,default=False)
parser.add_argument(&#39;--save_model&#39;, type=bool, help=&#39;&#39;&#39;Boolean to save the trained model dictionary&#39;&#39;&#39;, 
                    required=False,default=False)
parser.add_argument(&#39;--save_loss_plots&#39;, type=bool, help=&#39;&#39;&#39;Boolean to save the loss plots&#39;&#39;&#39;, 
                    required=False,default=False)


################################### CONFIGURATIONS ###################################
DATA_DIR=os.environ[&#39;DATA_DIR&#39;]
JUPYTER=True
use_subsample=False
if use_subsample:
    SUBSAMPLE=int(1e5)#subsample use for development - in production use whole dataset
else:
    SUBSAMPLE=None
    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>if JUPYTER:
    # print(plt.rcsetup.interactive_bk )
    # matplotlib interactive mode: ion or ioff
    plt.ioff()
    print(&#39;interactive? &#39;, mpl.is_interactive())
    args = parser.parse_args(args=[])
    N = &#39;10M_2&#39;
    n_iterations = int(1e4)
    n_layers, n_hidden = int(1), int(10)
    starting_learning_rate = float(1.e-2)
    show_loss_plots = False
    save_model=False
    save_loss_plots = False
else:
    args = parser.parse_args()
    N = args.N
    n_iterations = args.n_iterations
    n_layers = args.n_layers
    n_hidden = args.n_hidden
    starting_learning_rate=args.starting_learning_rate
    show_loss_plots=args.show_loss_plots
    save_model=args.save_model
    save_loss_plots=args.save_loss_plots
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>interactive?  False
</pre></div>
</div>
</div>
</div>
<section id="import-the-numpy-data-convert-to-dataframe-and-save-if-you-haven-t-saved-the-dataframes">
<h3>Import the numpy data, convert to dataframe and save (if you haven’t saved the dataframes)<a class="headerlink" href="#import-the-numpy-data-convert-to-dataframe-and-save-if-you-haven-t-saved-the-dataframes" title="Permalink to this heading">#</a></h3>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="explore-the-dataframe-and-preprocess">
<h1>Explore the Dataframe and preprocess<a class="headerlink" href="#explore-the-dataframe-and-preprocess" title="Permalink to this heading">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="data">
<h1>Data<a class="headerlink" href="#data" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>use_svg_display()
show_jupyter_image(&#39;images/pythia_ppt_diagram.png&#39;, width=2000,height=500)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Old_1_24_0.png" src="_images/Old_1_24_0.png" />
</div>
</div>
<p>Decide on an evaluation order</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>################################### SET DATA CONFIGURATIONS ###################################
X       = [&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;, &#39;tau&#39;]

#set order of training:
#pT_first: pT-&gt;&gt;m-&gt;eta-&gt;phi
#m_first: m-&gt;pT-&gt;eta-&gt;phi



#we&#39;ll just go with m first since that&#39;s the order we discuss in the paper.
ORDER=&#39;m_First&#39;

if ORDER==&#39;m_First&#39;:
    FIELDS  = {&#39;RecoDatam&#39; : {&#39;inputs&#39;: X, 
                               &#39;xlabel&#39;:  r&#39;$m$ (GeV)&#39;, 
                              &#39;ylabel&#39;:&#39;$m^{reco}$&#39;,
                               &#39;xmin&#39;: 0, 
                               &#39;xmax&#39;: 25},
                           

               &#39;RecoDatapT&#39;: {&#39;inputs&#39;: [&#39;RecoDatam&#39;]+X, 
                               &#39;xlabel&#39;:  r&#39;$p_T$ (GeV)&#39; , 
                              &#39;ylabel&#39;: &#39;$p_T^{reco}$&#39;,
                               &#39;xmin&#39;  : 20, 
                               &#39;xmax&#39;  :  80},

               &#39;RecoDataeta&#39;: {&#39;inputs&#39;: [&#39;RecoDatam&#39;,&#39;RecoDatapT&#39;] + X, 
                               &#39;xlabel&#39;: r&#39;$\eta$&#39;,
                               &#39;ylabel&#39;:&#39;$\eta^{reco}$&#39;,
                               &#39;xmin&#39;  : -5,
                               &#39;xmax&#39;  :  5},

               &#39;RecoDataphi&#39;  : {&#39;inputs&#39;: [&#39;RecoDatam&#39;, &#39;RecodatapT&#39;, &#39;RecoDataeta&#39;]+X,
                               &#39;xlabel&#39;: r&#39;$\phi$&#39; ,
                                &#39;ylabel&#39; :&#39;$\phi^{reco}$&#39;,
                               &#39;xmin&#39;  : -3.2, 
                               &#39;xmax&#39;  :3.2}
              }
    

all_variable_cols = [
    &quot;genDatapT&quot;,
    &quot;genDataeta&quot;,
    &quot;genDataphi&quot;,
    &quot;genDatam&quot;,
    &quot;RecoDatapT&quot;,
    &quot;RecoDataeta&quot;,
    &quot;RecoDataphi&quot;,
    &quot;RecoDatam&quot;,
]
all_cols = [
    &quot;genDatapT&quot;,
    &quot;genDataeta&quot;,
    &quot;genDataphi&quot;,
    &quot;genDatam&quot;,
    &quot;RecoDatapT&quot;,
    &quot;RecoDataeta&quot;,
    &quot;RecoDataphi&quot;,
    &quot;RecoDatam&quot;,
    &quot;tau&quot;,
]
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="load-and-explore-raw-unscaled-dataframes">
<h1>3. Load and explore raw (unscaled) dataframes<a class="headerlink" href="#load-and-explore-raw-unscaled-dataframes" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from joblib import  Memory
DATA_DIR = os.environ[&quot;DATA_DIR&quot;]
print(f&quot;using DATA_DIR={DATA_DIR}&quot;)

memory = Memory(DATA_DIR)

print(&quot;USING NEW DATASET\n&quot;)
######################################
USE_BRADEN_SCALING=False
#####################################
################################### CONFIGURATIONS ###################################
@memory.cache
def load_raw_data():
    print(f&#39;SUBSAMPLE = {SUBSAMPLE}&#39;)
    raw_train_data=pd.read_csv(os.path.join(DATA_DIR,&#39;train_data_10M_2.csv&#39;),
                        usecols=all_cols,
                        nrows=SUBSAMPLE
                        )

    raw_test_data=pd.read_csv(os.path.join(DATA_DIR,&#39;test_data_10M_2.csv&#39;),
                        usecols=all_cols,
                        nrows=SUBSAMPLE
                        )

    raw_valid_data=pd.read_csv(os.path.join(DATA_DIR,&#39;validation_data_10M_2.csv&#39;),
                        usecols=all_cols,
                        nrows=SUBSAMPLE
                        )


    print(&#39;\n RAW TRAIN DATA SHAPE\n&#39;)
    print(raw_train_data.shape)
    print(&#39;\n RAW TRAIN DATA\n&#39;)
    raw_train_data.describe()#unscaled
    print(&#39;\n RAW TEST DATA\ SHAPEn&#39;)
    print(raw_test_data.shape)
    print(&#39;\n RAW TEST DATA\n&#39;)
    raw_test_data.describe()#unscaled

    return raw_train_data, raw_test_data, raw_valid_data

    
JUPYTER = False
use_subsample = False
# use_subsample = True
if use_subsample:
    SUBSAMPLE = int(
        1e5
    )  # subsample use for development - in production use whole dataset
else:
    SUBSAMPLE = None




########################################################################################
raw_train_data, raw_test_data, raw_valid_data =load_raw_data()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>using DATA_DIR=/home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data
USING NEW DATASET

________________________________________________________________________________
[Memory] Calling __main__--tmp-ipykernel-3950853832.load_raw_data...
load_raw_data()
SUBSAMPLE = None

 RAW TRAIN DATA SHAPE

(8000000, 9)

 RAW TRAIN DATA


 RAW TEST DATA\ SHAPEn
(1000000, 9)

 RAW TEST DATA

___________________________________________________load_raw_data - 13.3s, 0.2min
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def explore_data(df, title, scaled=False):
    fig, ax = plt.subplots(1,5, figsize=(15,10) )
    # df = df[[&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;,&#39;RecoDatapT&#39;, &#39;RecoDataeta&#39;, &#39;RecoDataphi&#39;, &#39;RecoDatam&#39;]]
    levels = [&#39;RecoData&#39;, &#39;genData&#39;]
    kinematics=[&#39;pT&#39;,&#39;eta&#39;,&#39;phi&#39;,&#39;m&#39;]
    columns = [level+k for level in levels for k in kinematics]
    print(columns)
    columns = columns + [&#39;tau&#39;]
    print(columns)
    df = df[columns]
    
    for k_i, k in enumerate(kinematics):
        Reco_var = levels[0]+k
        gen_var = levels[1]+k
        print(&#39;Reco_var: &#39;, Reco_var, &#39;, \t gen_var: &#39;, gen_var)
        ax[k_i].hist(df[Reco_var], bins=100, label=Reco_var, alpha=0.35)
        ax[k_i].hist(df[gen_var], bins=100, label=gen_var, alpha=0.35)
        xmin, xmax = FIELDS[Reco_var][&#39;xmin&#39;], FIELDS[Reco_var][&#39;xmax&#39;]
        xlabel=FIELDS[Reco_var][&#39;xlabel&#39;]
        ax[k_i].set_xlim( (xmin, xmax) )
        # set_axes(ax[k_i], xlabel=xlabel, ylabel=&#39;&#39;, xmin=xmin, xmax=xmax)
        ax[k_i].set_xlabel(xlabel,fontsize=26)
        
        
                  
        if scaled:
            ax[k_i].set_xlim(df[gen_var].min(),df[gen_var].max() )
        
        ax[k_i].legend(loc=&#39;best&#39;, fontsize=13)
    ax[4].hist(df[&#39;tau&#39;],bins=100, label=r&#39;$\tau$&#39;)
    ax[4].legend(loc=&#39;best&#39;, fontsize=13)
    fig.suptitle(title, fontsize=30)
    show_plot()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>explore_data(df=raw_train_data, title=&#39;Unscaled Dataframe&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;RecoDatapT&#39;, &#39;RecoDataeta&#39;, &#39;RecoDataphi&#39;, &#39;RecoDatam&#39;, &#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;]
[&#39;RecoDatapT&#39;, &#39;RecoDataeta&#39;, &#39;RecoDataphi&#39;, &#39;RecoDatam&#39;, &#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;, &#39;tau&#39;]
Reco_var:  RecoDatapT , 	 gen_var:  genDatapT
Reco_var:  RecoDataeta , 	 gen_var:  genDataeta
Reco_var:  RecoDataphi , 	 gen_var:  genDataphi
Reco_var:  RecoDatam , 	 gen_var:  genDatam
</pre></div>
</div>
<img alt="_images/Old_1_30_1.svg" src="_images/Old_1_30_1.svg" /></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(raw_train_data.shape)
raw_train_data.describe()#unscaled
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(8000000, 9)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>genDatapT</th>
      <th>genDataeta</th>
      <th>genDataphi</th>
      <th>genDatam</th>
      <th>RecoDatapT</th>
      <th>RecoDataeta</th>
      <th>RecoDataphi</th>
      <th>RecoDatam</th>
      <th>tau</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>8.000000e+06</td>
      <td>8.000000e+06</td>
      <td>8.000000e+06</td>
      <td>8.000000e+06</td>
      <td>8.000000e+06</td>
      <td>8.000000e+06</td>
      <td>8.000000e+06</td>
      <td>8.000000e+06</td>
      <td>8.000000e+06</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.269523e+01</td>
      <td>-1.781882e-03</td>
      <td>-3.830903e-04</td>
      <td>6.962994e+00</td>
      <td>3.286720e+01</td>
      <td>-1.789886e-03</td>
      <td>-4.719170e-04</td>
      <td>5.555567e+00</td>
      <td>4.999153e-01</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.493793e+01</td>
      <td>2.204310e+00</td>
      <td>1.813825e+00</td>
      <td>2.781332e+00</td>
      <td>1.582936e+01</td>
      <td>2.197969e+00</td>
      <td>1.814474e+00</td>
      <td>2.664340e+00</td>
      <td>2.886730e-01</td>
    </tr>
    <tr>
      <th>min</th>
      <td>2.000000e+01</td>
      <td>-5.227320e+00</td>
      <td>-3.141590e+00</td>
      <td>-7.042220e-04</td>
      <td>1.144390e+01</td>
      <td>-5.006930e+00</td>
      <td>-3.480195e+00</td>
      <td>-8.631670e-05</td>
      <td>5.075658e-08</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2.376670e+01</td>
      <td>-1.654600e+00</td>
      <td>-1.571320e+00</td>
      <td>5.116180e+00</td>
      <td>2.348440e+01</td>
      <td>-1.651130e+00</td>
      <td>-1.571500e+00</td>
      <td>3.805560e+00</td>
      <td>2.498142e-01</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2.840240e+01</td>
      <td>-2.726765e-03</td>
      <td>6.159285e-05</td>
      <td>6.537620e+00</td>
      <td>2.898930e+01</td>
      <td>-3.001240e-03</td>
      <td>-4.192835e-05</td>
      <td>5.120820e+00</td>
      <td>4.999874e-01</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>3.627690e+01</td>
      <td>1.651250e+00</td>
      <td>1.570300e+00</td>
      <td>8.282190e+00</td>
      <td>3.749822e+01</td>
      <td>1.647990e+00</td>
      <td>1.570142e+00</td>
      <td>6.774150e+00</td>
      <td>7.500011e-01</td>
    </tr>
    <tr>
      <th>max</th>
      <td>8.397820e+02</td>
      <td>5.188200e+00</td>
      <td>3.141590e+00</td>
      <td>1.112440e+02</td>
      <td>8.148800e+02</td>
      <td>5.005230e+00</td>
      <td>3.482885e+00</td>
      <td>1.139730e+02</td>
      <td>9.999999e-01</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(raw_test_data.shape)
raw_test_data.describe()#unscaled
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1000000, 9)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>genDatapT</th>
      <th>genDataeta</th>
      <th>genDataphi</th>
      <th>genDatam</th>
      <th>RecoDatapT</th>
      <th>RecoDataeta</th>
      <th>RecoDataphi</th>
      <th>RecoDatam</th>
      <th>tau</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1.000000e+06</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>32.665481</td>
      <td>0.002822</td>
      <td>-0.000512</td>
      <td>6.957271</td>
      <td>32.849094</td>
      <td>0.002815</td>
      <td>-0.000550</td>
      <td>5.554067</td>
      <td>4.997935e-01</td>
    </tr>
    <tr>
      <th>std</th>
      <td>14.746671</td>
      <td>2.202514</td>
      <td>1.814159</td>
      <td>2.765754</td>
      <td>15.617181</td>
      <td>2.196167</td>
      <td>1.814699</td>
      <td>2.644894</td>
      <td>2.888304e-01</td>
    </tr>
    <tr>
      <th>min</th>
      <td>20.000000</td>
      <td>-5.159990</td>
      <td>-3.141590</td>
      <td>-0.000566</td>
      <td>11.475300</td>
      <td>-4.999270</td>
      <td>-3.477685</td>
      <td>-0.000061</td>
      <td>2.595965e-07</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>23.759175</td>
      <td>-1.648685</td>
      <td>-1.571470</td>
      <td>5.111978</td>
      <td>23.489400</td>
      <td>-1.644865</td>
      <td>-1.571400</td>
      <td>3.806208</td>
      <td>2.496069e-01</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>28.395500</td>
      <td>0.000701</td>
      <td>-0.001251</td>
      <td>6.532905</td>
      <td>28.992800</td>
      <td>0.001701</td>
      <td>-0.000872</td>
      <td>5.119875</td>
      <td>4.996784e-01</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>36.294825</td>
      <td>1.654850</td>
      <td>1.571060</td>
      <td>8.277790</td>
      <td>37.563675</td>
      <td>1.651652</td>
      <td>1.570125</td>
      <td>6.775580</td>
      <td>7.503565e-01</td>
    </tr>
    <tr>
      <th>max</th>
      <td>553.745000</td>
      <td>5.143080</td>
      <td>3.141590</td>
      <td>77.079700</td>
      <td>516.104000</td>
      <td>5.005660</td>
      <td>3.478095</td>
      <td>75.951300</td>
      <td>9.999998e-01</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># np.array(train_data[&#39;genDatapT&#39;])
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="scaling">
<h1>Scaling<a class="headerlink" href="#scaling" title="Permalink to this heading">#</a></h1>
<p>scaling (or standarization, normalization) is someimes done in the following way:
$<span class="math notranslate nohighlight">\( X' = \frac{X-X_{min}}{X_{max}-X_{min}} \qquad \rightarrow \qquad X= X' (X_{max}-X_{min}) + X_{min}\)</span>$</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># def standarize(values):
#     expected_min, expected_max = values.min(), values.max()
#     scale_factor = expected_max - expected_min
#     offset = expected_min
#     standarized_values = (values - offset)/scale_factor 
#     return standarized_values
</pre></div>
</div>
</div>
</div>
<p>Or by taking z-score:</p>
<div class="math notranslate nohighlight">
\[ X'=z(X)=\frac{X-E[X]}{\sigma_{X}}  \qquad \rightarrow \qquad X = z^{-1}(X')= X' \sigma_{X} + E[X].\]</div>
<hr class="docutils" />
<section id="basically-a-standard-scaling-procedure-is-the-following-background">
<h2>Basically a “standard scaling procedure” is the following (background):<a class="headerlink" href="#basically-a-standard-scaling-procedure-is-the-following-background" title="Permalink to this heading">#</a></h2>
<ol class="simple">
<li><p>Split the data into train and test dataframes</p></li>
<li><p>Fit on the train set, and transform everything according to the train set, that is, get the mean and std, ( optionally and min and max or other quantities) of each feature (column) of each of the train set, and standarize everything according to that.</p></li>
<li><p>transform each of the train and test sets independently. That is, use the means and stds of each column to transform a column <span class="math notranslate nohighlight">\(X\)</span> into a column <span class="math notranslate nohighlight">\(X'\)</span> e.g. according to
$<span class="math notranslate nohighlight">\( X'=z(X)= \frac{X-E[X]}{\sigma_{X}}\)</span>$</p></li>
<li><p>Train NN on transformed features <span class="math notranslate nohighlight">\(X_{train}'\)</span> (and target <span class="math notranslate nohighlight">\(y_{train}'\)</span>) (in train df, but validate on test set, which will not influence the weights of NN ( just used for observation that it doesnt overfit) )</p></li>
<li><p>Once the training is done, <em>evaluate the NN on transformed features of the test set</em> <span class="math notranslate nohighlight">\(X_{test}'\)</span>, i.e. do <span class="math notranslate nohighlight">\(NN(X_{test}')\)</span>, which will result in a scaled prediction of the target <span class="math notranslate nohighlight">\(y_{pred}'\)</span></p></li>
<li><p>Unscale the <span class="math notranslate nohighlight">\(y_{pred}'\)</span>, i.e. apply the inverse of the scaling operation, e.g.
$<span class="math notranslate nohighlight">\( y_{pred}=z^{-1}(y_{pred}')= y_{pred}' \sigma_{y} + E[y]\)</span>$,
where</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\sigma_y\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[E[y]\]</div>
<p>are attained from the test set <em>prior to training and scaling</em>.</p>
<ol class="simple">
<li><p>Compare to <span class="math notranslate nohighlight">\(y\)</span> (the actual distribution you’re trying to estimate) one-to-one</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>use_svg_display()
show_jupyter_image(&#39;images/scaling_forNN.jpg&#39;, width=2000,height=500)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Old_1_38_0.jpg" src="_images/Old_1_38_0.jpg" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="braden-scaling">
<h1>Braden scaling<a class="headerlink" href="#braden-scaling" title="Permalink to this heading">#</a></h1>
<p>In the IQN-scipost overleaf, we say the scaling is the following:</p>
<div class="math notranslate nohighlight">
\[\mathbb{T}(p_T) = z(\log p_T), \qquad \mathbb{T}(\eta) = z(\eta), \qquad \mathbb{T}(\phi) = z(\phi), \qquad \mathbb{T}(m) = z(\log (m + 2))\]</div>
<div class="math notranslate nohighlight">
\[ \mathbb{T}(\tau) = 6\tau - 3 \]</div>
<p>Which means, for a jet observable <span class="math notranslate nohighlight">\(\mathcal{O}\)</span> (or quantile <span class="math notranslate nohighlight">\(\tau\)</span>), the Braden-scaling perscribes that the data is first scaled according to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \mathbb{L} (\mathcal{O} \mid \mathcal{O} \in X ) &amp;=
    \begin{cases}
        z \left( \log{\mathcal{O}} \right), \qquad &amp; \text{if } \mathcal{O}= p_T \\
        z \left(\mathcal{O} \right), \qquad &amp; \text{if } \mathcal{O}=\eta \\
        z \left( \log (\mathcal{O} + 2) \right), \qquad &amp; \text{if } \mathcal{O}=m \\
        z \left( \mathcal{O} \right), \qquad &amp; \text{if } \mathcal{O}=\phi \\
        z \left( 6 \mathcal{O} -3 \right), \qquad &amp; \text{if } \mathcal{O}=\tau
    \end{cases}
\end{align}
\end{split}\]</div>
<p>Note that the equation above describes the scaling for the training features <span class="math notranslate nohighlight">\(X\)</span>. The point of taking the log is</p>
<p>The targets, as we say in the paper, are chosen to be the following:</p>
<div class="math notranslate nohighlight">
\[
z\left(\frac{y_n + 10}{x_n + 10}\right), \qquad n = 1,\cdots,4,
\label{eq:normalization}
\]</div>
<p>We mean that the predicted target for a desired reco observable <span class="math notranslate nohighlight">\(\mathcal{O}\)</span> is chosen to be the following function</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{T}(\mathcal{O} \mid \mathcal{O} \in y) = z \left( \frac{\mathbb{L} (\mathcal{O}^{\text{reco}}) +10 }{\mathbb{L}(\mathcal{O}^{\text{gen}}) +10} \right),
\]</div>
<p>where for a random variable <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(z\)</span> is the standardization function (z-score):</p>
<div class="math notranslate nohighlight">
\[
   x'= z (x) \equiv \frac{x-\bar{x}}{\sigma_{x}} \ .
\]</div>
<p>Such that its inverse is</p>
<div class="math notranslate nohighlight">
\[ z^{-1}(x') = x' \ \sigma_x + \bar{x} \]</div>
<p>If we do this on the data, after training, the NN <span class="math notranslate nohighlight">\(f_{\text{IQN}}\)</span> will not estimate the observable,</p>
<div class="math notranslate nohighlight">
\[\mathcal{O}^{\text{predicted}} \ne \mathcal{O}^{\text{reco}}\]</div>
<p>but will instead estimate</p>
<div class="math notranslate nohighlight">
\[
        f_{\text{IQN}} (\mathcal{O}) \approx  z \left( \frac{\mathbb{L} (\mathcal{O}^{\text{reco}}) +10 }{\mathbb{L}(\mathcal{O}^{\text{gen}}) +10} \right),
\]</div>
<p>which needs to be de-scaled (when evaluated on the data that which has been scaled according to</p>
<div class="math notranslate nohighlight">
\[\mathbb{T}(\text{evaluation data}) = z \left( \frac{\mathbb{L} (\text{data}^{\text{reco}}) +10 }{\mathbb{L}(\text{data}^{\text{gen}}) +10} \right) \]</div>
<p>in order to copare with <span class="math notranslate nohighlight">\(\mathcal{O}\)</span> directly.) The descaling for <span class="math notranslate nohighlight">\(\mathcal{O}=p_T\)</span> (as an example) would be:</p>
<div class="math notranslate nohighlight">
\[
    p_T^{\text{predicted}} = \mathbb{L}^{-1} \left[ z^{-1} (f_{\text{IQN}} ) \left[ \mathbb{L} (p_T^\text{gen})+10 \right] -10 \right]
\]</div>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="optional-scale-the-data-accoding-to-the-braden-kronheim-scaling">
<h1>4. (Optional) Scale the data accoding to the “Braden Kronheim scaling” :<a class="headerlink" href="#optional-scale-the-data-accoding-to-the-braden-kronheim-scaling" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>TRAIN_SCALE_DICT = get_scaling_info(raw_train_data);print(TRAIN_SCALE_DICT)
print(&#39;\n\n&#39;)
TEST_SCALE_DICT = get_scaling_info(raw_test_data);print(TEST_SCALE_DICT)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;genDatapT&#39;: {&#39;mean&#39;: 32.695234084987476, &#39;std&#39;: 14.937932540562551}, &#39;genDataeta&#39;: {&#39;mean&#39;: -0.0017818817154031672, &#39;std&#39;: 2.204309760627079}, &#39;genDataphi&#39;: {&#39;mean&#39;: -0.0003830903308450233, &#39;std&#39;: 1.8138251604791067}, &#39;genDatam&#39;: {&#39;mean&#39;: 6.962994352358474, &#39;std&#39;: 2.781332025286383}, &#39;RecoDatapT&#39;: {&#39;mean&#39;: 32.86720151648752, &#39;std&#39;: 15.829355769531851}, &#39;RecoDataeta&#39;: {&#39;mean&#39;: -0.0017898858568513964, &#39;std&#39;: 2.197968491495457}, &#39;RecoDataphi&#39;: {&#39;mean&#39;: -0.0004719170328962474, &#39;std&#39;: 1.8144739820043825}, &#39;RecoDatam&#39;: {&#39;mean&#39;: 5.555567451922438, &#39;std&#39;: 2.664339857066051}}



{&#39;genDatapT&#39;: {&#39;mean&#39;: 32.6654809616, &#39;std&#39;: 14.746663776411731}, &#39;genDataeta&#39;: {&#39;mean&#39;: 0.002821766734118711, &#39;std&#39;: 2.2025132218012735}, &#39;genDataphi&#39;: {&#39;mean&#39;: -0.0005116820324618385, &#39;std&#39;: 1.8141583811877282}, &#39;genDatam&#39;: {&#39;mean&#39;: 6.957271011526192, &#39;std&#39;: 2.765752894623606}, &#39;RecoDatapT&#39;: {&#39;mean&#39;: 32.8490938466, &#39;std&#39;: 15.617172793936655}, &#39;RecoDataeta&#39;: {&#39;mean&#39;: 0.0028149588128446, &#39;std&#39;: 2.196165457846061}, &#39;RecoDataphi&#39;: {&#39;mean&#39;: -0.0005495613684869987, &#39;std&#39;: 1.8146982567744574}, &#39;RecoDatam&#39;: {&#39;mean&#39;: 5.554066938043617, &#39;std&#39;: 2.644892802850232}}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def L(orig_observable, label):
    eps=1e-20
    orig_observable=orig_observable+eps
    if label==&#39;pT&#39;:
        const=0
        log_pT_=np.log(orig_observable) 
        L_observable = log_pT_
    if label==&#39;eta&#39;:
        const=0
        L_observable=orig_observable
    if label==&#39;m&#39;:
        const=2
        L_observable=np.log(orig_observable + const)
    if label==&#39;phi&#39;:
        L_observable=orig_observable
    if label==&#39;tau&#39;:
        L_observable=orig_observable
#         L_observable = (6*orig_observable) - 3
    
    return L_observable.to_numpy()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def L_inverse(L_observable, label):
    eps=1e-20
    L_observable=L_observable+eps
    if label==&#39;pT&#39;:
        const=0
        L_inverse_observable = np.exp(L_observable)
    if label==&#39;eta&#39;:
        L_inverse_observable = L_observable
    if label==&#39;m&#39;:
        const=2
        L_inverse_observable = np.exp(L_observable) - const
    if label==&#39;tau&#39;:
        L_inverse_observable=L_observable
        # L_inverse_observable = (L_observable+3)/6
        
    if not isinstance(L_inverse_observable, np.ndarray):
        L_inverse_observable = L_inverse_observable.to_numpy()
    return L_inverse_observable
</pre></div>
</div>
</div>
</div>
<div class="math notranslate nohighlight">
\[
    \mathbb{T}(\mathcal{O}) = z \left( \frac{\mathbb{L} (\mathcal{O}^{\text{reco}}) +10 }{\mathbb{L}(\mathcal{O}^{\text{gen}}) +10} \right),
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def T(variable, scaled_df):
    if variable==&#39;pT&#39;:
        L_pT_gen=scaled_df[&#39;genDatapT&#39;]
        L_pT_reco = scaled_df[&#39;RecoDatapT&#39;]
        target = (L_pT_reco+10)/(L_pT_gen+10) 
    if variable==&#39;eta&#39;:
        L_eta_gen=scaled_df[&#39;genDataeta&#39;]
        L_eta_reco = scaled_df[&#39;RecoDataeta&#39;]
        target =  (L_eta_reco+10)/(L_eta_gen+10) 
    if variable==&#39;phi&#39;:
        L_phi_gen=scaled_df[&#39;genDataphi&#39;]
        L_phi_reco = scaled_df[&#39;RecoDataphi&#39;]
        target =  (L_phi_reco+10)/(L_phi_gen+10) 
    if variable==&#39;m&#39;:
        L_m_gen=scaled_df[&#39;genDatam&#39;]
        L_m_reco = scaled_df[&#39;RecoDatam&#39;]
        target =  (L_m_reco+10)/(L_m_gen+10) 
    
    return target
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def L_scale_df(df, title, save=False):
    #scale
    # SUBSAMPLE=int(1e6)
    df = df[all_cols]#[:SUBSAMPLE]
    # print(df.head())
    scaled_df = pd.DataFrame()
    #select the columns by index: 
    # 0:genDatapT, 1:genDataeta, 2:genDataphi, 3:genDatam, 
    # 4:RecoDatapT, 5:RecoDataeta, 6:RecoDataphi, 7: Recodatam
    scaled_df[&#39;genDatapT&#39;] = L(df.iloc[:,0], label=&#39;pT&#39;)
    scaled_df[&#39;RecoDatapT&#39;] = L(df.iloc[:,4], label=&#39;pT&#39;)
    
    scaled_df[&#39;genDataeta&#39;] = L(df.iloc[:,1], label=&#39;eta&#39;)
    scaled_df[&#39;RecoDataeta&#39;] = L(df.iloc[:,5],label=&#39;eta&#39;)
    
    
    scaled_df[&#39;genDataphi&#39;] = L(df.iloc[:,2],label=&#39;phi&#39;)
    scaled_df[&#39;RecoDataphi&#39;] = L(df.iloc[:,6],label=&#39;phi&#39;)

    scaled_df[&#39;genDatam&#39;] = L(df.iloc[:,3],label=&#39;m&#39;)
    scaled_df[&#39;RecoDatam&#39;] = L(df.iloc[:,7],label=&#39;m&#39;)
    #why scale tau?
    # scaled_df[&#39;tau&#39;] = 6 * df.iloc[:,8] - 3
    scaled_df[&#39;tau&#39;] = L(df.iloc[:,8],label=&#39;tau&#39;)
    
    print(scaled_df.describe())
    
    if save:
        scaled_df.to_csv(os.path.join(DATA_DIR, title) )
    return scaled_df
</pre></div>
</div>
</div>
</div>
<section id="if-you-want-to-generate-the-scaled-data-frames-run-the-cell-below">
<h2>If you want to generate the Scaled data frames, run the cell below<a class="headerlink" href="#if-you-want-to-generate-the-scaled-data-frames-run-the-cell-below" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>scaled_train_data = L_scale_df(raw_train_data, title=&#39;scaled_train_data_10M_2.csv&#39;,
                             save=True)
print(&#39;\n\n&#39;)
scaled_test_data = L_scale_df(raw_test_data,  title=&#39;scaled_test_data_10M_2.csv&#39;,
                            save=True)
print(&#39;\n\n&#39;)

scaled_valid_data = L_scale_df(raw_valid_data,  title=&#39;scaled_valid_data_10M_2.csv&#39;,
                            save=True)

explore_data(df=scaled_train_data, title=&#39;Braden Kronheim-L-scaled Dataframe&#39;, scaled=True)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>          genDatapT    RecoDatapT    genDataeta   RecoDataeta    genDataphi  \
count  8.000000e+06  8.000000e+06  8.000000e+06  8.000000e+06  8.000000e+06   
mean   3.421245e+00  3.412002e+00 -1.781882e-03 -1.789886e-03 -3.830903e-04   
std    3.348063e-01  3.804798e-01  2.204310e+00  2.197969e+00  1.813825e+00   
min    2.995732e+00  2.437457e+00 -5.227320e+00 -5.006930e+00 -3.141590e+00   
25%    3.168285e+00  3.156336e+00 -1.654600e+00 -1.651130e+00 -1.571320e+00   
50%    3.346474e+00  3.366927e+00 -2.726765e-03 -3.001240e-03  6.159285e-05   
75%    3.591181e+00  3.624294e+00  1.651250e+00  1.647990e+00  1.570300e+00   
max    6.733142e+00  6.703041e+00  5.188200e+00  5.005230e+00  3.141590e+00   

        RecoDataphi      genDatam     RecoDatam           tau  
count  8.000000e+06  8.000000e+06  8.000000e+06  8.000000e+06  
mean  -4.719170e-04  2.150564e+00  1.967802e+00  4.999153e-01  
std    1.814474e+00  2.880059e-01  3.271235e-01  2.886730e-01  
min   -3.480195e+00  6.927950e-01  6.931040e-01  5.075658e-08  
25%   -1.571500e+00  1.962371e+00  1.758816e+00  2.498142e-01  
50%   -4.192835e-05  2.144482e+00  1.963023e+00  4.999874e-01  
75%    1.570142e+00  2.330413e+00  2.171810e+00  7.500011e-01  
max    3.482885e+00  4.729545e+00  4.753357e+00  9.999999e-01  



            genDatapT      RecoDatapT      genDataeta     RecoDataeta  \
count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   
mean         3.420882        3.412147        0.002822        0.002815   
std          0.334051        0.379574        2.202514        2.196167   
min          2.995732        2.440197       -5.159990       -4.999270   
25%          3.167969        3.156549       -1.648685       -1.644865   
50%          3.346231        3.367048        0.000701        0.001701   
75%          3.591675        3.626037        1.654850        1.651652   
max          6.316704        6.246308        5.143080        5.005660   

           genDataphi     RecoDataphi        genDatam       RecoDatam  \
count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   
mean        -0.000512       -0.000550        2.150071        1.967908   
std          1.814159        1.814699        0.287706        0.326501   
min         -3.141590       -3.477685        0.692864        0.693117   
25%         -1.571470       -1.571400        1.961780        1.758928   
50%         -0.001251       -0.000872        2.143930        1.962890   
75%          1.571060        1.570125        2.329985        2.171973   
max          3.141590        3.478095        4.370456        4.356084   

                tau  
count  1.000000e+06  
mean   4.997935e-01  
std    2.888304e-01  
min    2.595965e-07  
25%    2.496069e-01  
50%    4.996784e-01  
75%    7.503565e-01  
max    9.999998e-01  



            genDatapT      RecoDatapT      genDataeta     RecoDataeta  \
count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   
mean         3.421223        3.411684        0.000698        0.000735   
std          0.336185        0.381671        2.204255        2.197915   
min          2.995732        2.438330       -5.184830       -5.000600   
25%          3.167410        3.155224       -1.652290       -1.647702   
50%          3.345898        3.366547        0.001781        0.000949   
75%          3.591019        3.623741        1.652180        1.648922   
max          6.518423        6.374464        5.139480        4.999840   

           genDataphi     RecoDataphi        genDatam       RecoDatam  \
count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   
mean        -0.000896       -0.000819        2.150291        1.967158   
std          1.813629        1.814279        0.288274        0.327415   
min         -3.141580       -3.480465        0.693135        0.693121   
25%         -1.569685       -1.570240        1.962370        1.758022   
50%         -0.000836       -0.001184        2.144196        1.961933   
75%          1.568172        1.568535        2.329907        2.171459   
max          3.141590        3.483175        4.110044        4.258457   

                tau  
count  1.000000e+06  
mean   5.004851e-01  
std    2.885275e-01  
min    5.130061e-07  
25%    2.507914e-01  
50%    5.007886e-01  
75%    7.504558e-01  
max    9.999994e-01  
[&#39;RecoDatapT&#39;, &#39;RecoDataeta&#39;, &#39;RecoDataphi&#39;, &#39;RecoDatam&#39;, &#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;]
[&#39;RecoDatapT&#39;, &#39;RecoDataeta&#39;, &#39;RecoDataphi&#39;, &#39;RecoDatam&#39;, &#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;, &#39;tau&#39;]
Reco_var:  RecoDatapT , 	 gen_var:  genDatapT
Reco_var:  RecoDataeta , 	 gen_var:  genDataeta
Reco_var:  RecoDataphi , 	 gen_var:  genDataphi
Reco_var:  RecoDatam , 	 gen_var:  genDatam
</pre></div>
</div>
<img alt="_images/Old_1_48_1.svg" src="_images/Old_1_48_1.svg" /><img alt="_images/Old_1_48_2.svg" src="_images/Old_1_48_2.svg" /></div>
</div>
</section>
<section id="if-you-want-to-load-the-previously-generated-scaled-dataframe-run-the-cell-below">
<h2>If you want to load the previously generated scaled dataframe, run the cell below<a class="headerlink" href="#if-you-want-to-load-the-previously-generated-scaled-dataframe-run-the-cell-below" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>target = &#39;RecoDatam&#39;
source  = FIELDS[target]
features= source[&#39;inputs&#39;]
########

print(&#39;USING NEW DATASET\n&#39;)
#UNSCALED
# train_data_m=pd.read_csv(os.path.join(DATA_DIR,&#39;train_data_10M_2.csv&#39;),
#                        usecols=features,
#                        nrows=SUBSAMPLE)

# print(&#39;TRAINING FEATURES\n&#39;, train_data.head())

# test_data_m= pd.read_csv(os.path.join(DATA_DIR,&#39;test_data_10M_2.csv&#39;),
#                        usecols=features,
#                        nrows=SUBSAMPLE)
# print(&#39;\nTESTING FEATURES\n&#39;, test_data.head())
# valid_data= pd.read_csv(os.path.join(DATA_DIR,&#39;valid_data_10M_2.csv&#39;),
#                        usecols=features,
#                        nrows=SUBSAMPLE)


# SCALED
train_data_m=pd.read_csv(os.path.join(DATA_DIR,&#39;scaled_train_data_10M_2.csv&#39;),
                       usecols=all_cols,
                       nrows=SUBSAMPLE)

print(&#39;TRAINING FEATURES\n&#39;, train_data_m.head())

test_data_m= pd.read_csv(os.path.join(DATA_DIR,&#39;scaled_test_data_10M_2.csv&#39;),
                       usecols=all_cols,
                       nrows=SUBSAMPLE)

valid_data_m= pd.read_csv(os.path.join(DATA_DIR,&#39;scaled_valid_data_10M_2.csv&#39;),
                       usecols=all_cols,
                       nrows=SUBSAMPLE)
print(&#39;\nTESTING FEATURES\n&#39;, test_data_m.head())

print(&#39;\ntrain set shape:&#39;,  train_data_m.shape)
print(&#39;\ntest set shape:  &#39;, test_data_m.shape)
# print(&#39;validation set shape:&#39;, valid_data.shape)

scaled_train_data = train_data_m
scaled_test_data = test_data_m
scaled_valid_data = valid_data_m
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>USING NEW DATASET

TRAINING FEATURES
    genDatapT  RecoDatapT  genDataeta  RecoDataeta  genDataphi  RecoDataphi  \
0   3.382531    3.463020    0.828187     0.817082    2.902130     2.919510   
1   3.191270    3.308764   -1.163510    -1.151020    0.636469     0.652153   
2   3.191270    3.308764   -1.163510    -1.151020    0.636469     0.652153   
3   3.191270    3.308764   -1.163510    -1.151020    0.636469     0.652153   
4   3.004211    3.187005    1.844410     1.837910   -0.186685    -0.160621   

   genDatam  RecoDatam       tau  
0  1.579696   1.525158  0.361310  
1  2.058837   1.995432  0.126899  
2  2.058837   1.995432  0.962307  
3  2.058837   1.995432  0.457282  
4  2.040038   1.886115  0.840862  

TESTING FEATURES
    genDatapT  RecoDatapT  genDataeta  RecoDataeta  genDataphi  RecoDataphi  \
0   3.775316    3.791603    0.824891     0.824645    -1.26949     -1.26117   
1   3.775316    3.791603    0.824891     0.824645    -1.26949     -1.26117   
2   3.258685    3.313277    3.529970     3.590390     1.55495      1.52062   
3   3.349708    3.522816   -1.159650    -1.139940     1.82602      1.76254   
4   3.090315    3.149058    2.747660     2.775790     2.03085      2.10209   

   genDatam  RecoDatam       tau  
0  2.071044   2.054470  0.250046  
1  2.071044   2.054470  0.847493  
2  2.242060   1.918984  0.851995  
3  2.286615   2.204338  0.052378  
4  1.971738   1.805105  0.542549  

train set shape: (8000000, 9)

test set shape:   (1000000, 9)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
labels = [&#39;pT&#39;, &#39;eta&#39;,&#39;phi&#39;,&#39;m&#39;]
fig, ax=plt.subplots(1,1)
for label in labels:
    target_ = T(label, scaled_df=scaled_train_data)
    
    ax.hist(target_, label = &#39;$T($&#39; +label+ &#39;$)$&#39;, alpha=0.4 )
set_axes(ax=ax, xlabel=&#39;pre-z ratio targets T&#39;, )
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Old_1_51_1.svg" src="_images/Old_1_51_1.svg" /></div>
</div>
<p>Our risk functional is minimized for</p>
<div class="math notranslate nohighlight">
\[\frac{\delta R_{\text{IQN}x4} }{\delta f_m}=0\tag{5}\]</div>
<p>(which is basically what’s done in the training process to get <span class="math notranslate nohighlight">\(f_m^{*}\)</span> whose weights/parameters minimize the loss). Suppose we factorize the risk as</p>
<div class="math notranslate nohighlight">
\[ R_{\text{IQN}x4}  = R_{\text{IQN}}^m \ R_{\text{IQN}}^{p_T}  \ R_{\text{IQN}}^\eta \ R_{\text{IQN}}^\phi \tag{6},\]</div>
<p>then, by Eq (4),</p>
<div class="math notranslate nohighlight">
\[R_{\text{IQN}}^m \equiv \int L_\text{IQN} \left( f_m (\mathbf{x_m},\tau), \mathbf{y_m} \right) p(\mathbf{x_m, y_m,\tau})  d \mathbf{x_m} d \mathbf{y_m} d \mathbf{\tau},
\]</div>
<p>and by Eq (5)</p>
<div class="math notranslate nohighlight">
\[\int d \mathbf{x_m} d \mathbf{y_m} d \mathbf{\tau} \ p(\mathbf{x_m, y_m,\tau})   \ \frac{ \delta L_\text{IQN} \left( f_m (\mathbf{x_m},\tau), \mathbf{y_m} \right) }{\delta f_m} = 0\]</div>
<p>and by Eq (2)</p>
<div class="math notranslate nohighlight">
\[
\int d \mathbf{x_m} d \mathbf{y_m} d \mathbf{\tau} \ p(\mathbf{x_m, y_m,\tau})   \ \frac{ \delta L_\text{IQN} \left( f_m (\mathbf{x_m},\tau), \mathbf{y_m} \right) }{\delta f_m} = 0 \tag{7}
\]</div>
<blockquote>
<div><blockquote>
<div><p>…
<br></p>
</div></blockquote>
</div></blockquote>
<p>Expand Eq (2) in Eq (7) and integrate wrt y over the appropriate limits to see that  <span class="math notranslate nohighlight">\(f(\mathbf{x},\mathbf{\tau})\)</span> is the quantile function for <span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})\)</span>, i.e. (I believe) that IQNx4 should work basically exactly.</p>
<div class="math notranslate nohighlight">
\[R_{\text{IQN}x4} = [ L \left( f_m( \{ p_T^{\text{gen}}, \eta^{\text{gen}}, \phi^{\text{gen}}, m^{\text{gen}} , \tau \}, m^\text{reco} ) \]</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="train-mass">
<h1>Train Mass<a class="headerlink" href="#train-mass" title="Permalink to this heading">#</a></h1>
<p>for mass,</p>
<div class="math notranslate nohighlight">
\[\mathbf{y_m}=m_{\text{reco}}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\mathbf{x_m}=\{p_T^{\text{gen}}, \eta^{\text{gen}}, \phi^{\text{gen}}, m^{\text{gen}} , \tau \}.\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_jupyter_image(&#39;images/IQN_training_flowchart.png&#39;,width=3000,height=1000)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Old_1_54_0.png" src="_images/Old_1_54_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>apply_z_generator = apply_z_to_features()

train_x = next(apply_z_generator)
test_x = next(apply_z_generator)
valid_x = next(apply_z_generator)

print(valid_x.mean(axis=0), valid_x.std(axis=0))

print(train_x.mean(axis=0), train_x.std(axis=0))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-2.55532484e-16  2.34408049e-17 -2.47215581e-17 -2.22109975e-15
  5.00485136e-01] [1.         1.         1.         1.         0.28852734]
[ 3.98275191e-15 -1.10134124e-18 -3.12905257e-17 -1.14646888e-14
  4.99915289e-01] [1.         1.         1.         1.         0.28867295]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train_x[:3,:], test_x[:3,:], valid_x[:3,:]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[-0.11563106,  0.37652098,  1.60021658, -1.98214105,  0.36130954],
        [-0.68688865, -0.5270258 ,  0.35110997, -0.31849075,  0.12689925],
        [-0.68688865, -0.5270258 ,  0.35110997, -0.31849075,  0.96230681]]),
 array([[ 1.06101772,  0.37324145, -0.69948596, -0.27467897,  0.25004557],
        [ 1.06101772,  0.37324145, -0.69948596, -0.27467897,  0.84749256],
        [-0.48554751,  1.60141978,  0.85740126,  0.31973299,  0.85199529]]),
 array([[-0.68215953,  0.56054886, -0.34477387, -1.23054084,  0.93582274],
        [-0.2284882 ,  1.43668218, -0.12155883, -0.6725882 ,  0.9272482 ],
        [-0.2284882 ,  1.43668218, -0.12155883, -0.6725882 ,  0.37568739]]))
</pre></div>
</div>
</div>
</div>
<section id="apply-z-to-targets-before-training">
<h2>Apply <span class="math notranslate nohighlight">\(z\)</span> to targets before training<a class="headerlink" href="#apply-z-to-targets-before-training" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def apply_z_to_targets():
    train_t_ratio_ = z(train_t_ratio) 
    test_t_ratio_ = z(test_t_ratio) 
    valid_t_ratio_ = z(valid_t_ratio)
    
    yield train_t_ratio_
    yield test_t_ratio_
    yield valid_t_ratio_
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>apply_z_to_targets_generator = apply_z_to_targets()
train_t_ratio = next(apply_z_to_targets_generator)
test_t_ratio = next(apply_z_to_targets_generator)
valid_t_ratio = next(apply_z_to_targets_generator)

print(valid_t_ratio.mean(), valid_t_ratio.std())
print(train_t_ratio.mean(), train_t_ratio.std())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5.849882889208402e-14 1.0000000000000004
5.849882889208402e-14 1.0000000000000004
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#check that it looks correct
fig = plt.figure(figsize=(10, 4))
ax = fig.add_subplot(autoscale_on=True)
ax.grid()
for i in range(NFEATURES):
    ax.hist(train_x[:,i], alpha=0.35, label=f&#39;feature {i}&#39; )
    # set_axes(ax=ax, xlabel=&quot;Transformed features X&#39; &quot;,title=&quot;training features post-z score: X&#39;=z(L(X))&quot;)
ax.legend()
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_45271</span><span class="o">/</span><span class="mf">3949263507.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1">#check that it looks correct</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">autoscale_on</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NFEATURES</span><span class="p">):</span>

<span class="ne">NameError</span>: name &#39;plt&#39; is not defined
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The history saving thread hit an unexpected error (OperationalError(&#39;database is locked&#39;)).History will not be written to the database.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train_x[:,-1].max()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9999999358588744
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-and-running-of-training-functions">
<h2>Training and running-of-training functions<a class="headerlink" href="#training-and-running-of-training-functions" title="Permalink to this heading">#</a></h2>
</section>
<section id="define-basic-nn-model">
<h2>Define basic NN model<a class="headerlink" href="#define-basic-nn-model" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class RegularizedRegressionModel(nn.Module):
    &quot;&quot;&quot;Used for hyperparameter tuning &quot;&quot;&quot;
    #inherit from the super class
    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):
        super().__init__()
        layers = []
        for _ in range(nlayers):
            if len(layers) ==0:
                #inital layer has to have size of input features as its input layer
                #its output layer can have any size but it must match the size of the input layer of the next linear layer
                #here we choose its output layer as the hidden size (fully connected)
                layers.append(nn.Linear(nfeatures, hidden_size))
                #batch normalization
                # layers.append(nn.BatchNorm1d(hidden_size))
                #dropout only in the first layer
                #Dropout seems to worsen model performance
                layers.append(nn.Dropout(dropout))
                #ReLU activation 
                layers.append(nn.LeakyReLU())
            else:
                #if this is not the first layer (we dont have layers)
                layers.append(nn.Linear(hidden_size, hidden_size))
                # layers.append(nn.BatchNorm1d(hidden_size))
                #Dropout seems to worsen model performance
                # layers.append(nn.Dropout(dropout))
                layers.append(nn.LeakyReLU())
                #output layer:
        layers.append(nn.Linear(hidden_size, ntargets)) 

        # only for classification add sigmoid
        # layers.append(nn.Sigmoid())
            #we have defined sequential model using the layers in oulist 
        self.model = nn.Sequential(*layers)

    
    def forward(self, x):
        return self.model(x)
    
    
class TrainingRegularizedRegressionModel(nn.Module):
    &quot;&quot;&quot;Used for training, and adds more regularization to prevent overfitting &quot;&quot;&quot;
    #inherit from the super class
    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):
        super().__init__()
        layers = []
        for _ in range(nlayers):
            if len(layers) ==0:
                #inital layer has to have size of input features as its input layer
                #its output layer can have any size but it must match the size of the input layer of the next linear layer
                #here we choose its output layer as the hidden size (fully connected)
                layers.append(nn.Linear(nfeatures, hidden_size))
                #batch normalization
                layers.append(nn.BatchNorm1d(hidden_size))
                #dropout only in the first layer
                #Dropout seems to worsen model performance
                layers.append(nn.Dropout(dropout))
                #ReLU activation 
                layers.append(nn.LeakyReLU())
            else:
                #if this is not the first layer (we dont have layers)
                layers.append(nn.Linear(hidden_size, hidden_size))
                layers.append(nn.BatchNorm1d(hidden_size))
                #Dropout seems to worsen model performance
                layers.append(nn.Dropout(dropout))
                layers.append(nn.LeakyReLU())
                #output layer:
        layers.append(nn.Linear(hidden_size, ntargets)) 

        # only for classification add sigmoid
        # layers.append(nn.Sigmoid())
            #we have defined sequential model using the layers in oulist 
        self.model = nn.Sequential(*layers)

    
    def forward(self, x):
        return self.model(x)
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
</section>
<hr class="docutils" />
<section id="hyperparameter-training-workflow">
<h2>Hyperparameter Training Workflow<a class="headerlink" href="#hyperparameter-training-workflow" title="Permalink to this heading">#</a></h2>
<p>We should not touch our test set until we have chosen our hyperparameters (have done model selection) using the valid set (also, we can’t tune the model using the train set since it obviously will just overfit the train set). Then in the training process we evaluate on test set to keep ourselves honest and to see if we are overfitting the train set.</p>
<p>It seems that one of the big challenges in this project is generalization (or overfitting). ways to reduce overfitting and improve generalization (that is, make the generalization error <span class="math notranslate nohighlight">\(R[f]\)</span> closer to the training error <span class="math notranslate nohighlight">\(R_{\text{emp}}\)</span> is:</p>
<ol class="simple">
<li><p>Reducing model complexity (e.g. using a linear model instead of a non-linear model, and user fewer model parameters). 2. using more training data. 3. feature selecting (i.e. using fewer features, since some of the features might be irrelevant or redundant). 4. Data augmentation, i.e. generating additional training examples through e.g. horizontal flipping/random cropping for images, or adding noise to training examples, all of which increase the size and diversity of the training set. 5. Regularization techniques, such as adding a penalty to the weights of the model during training (weight decay). This penalty could be e.g. L1 or L2 norm of the weights. 5. Cross valudation: dividing the training set into several smaller sets, training the model on one set, and evaluating it on the others. (can use e.g. <code class="docutils literal notranslate"><span class="pre">sklearn.model_selection.KFold</span></code> instead of doing it from scratch).</p></li>
</ol>
<p>Other than overfitting, the name of the game for training of course is to minimize the loss with respect to the weights. INPUT (features) is forward propagated (meaning each layer is basically a matrix of the size of the weights is a matrix of <span class="math notranslate nohighlight">\(( N_inputs + 1) X (N_outputs)\)</span> (and the inputs) is <span class="math notranslate nohighlight">\(w[N_inputs]+b\)</span> which is the rows, and the outputs (<span class="math notranslate nohighlight">\(y=\mathbf{w}+b\)</span>) will have shape <span class="math notranslate nohighlight">\([N_outputs]\)</span>  through each layer into the OUTPUT (target). More explicitly the shape of output <span class="math notranslate nohighlight">\(y\)</span> of each layer will be</p>
<div class="math notranslate nohighlight">
\[[y] = [1 \times (N_{input}+1) ] \cdot [(N_{input} +1) \times N_{output}] = [1\times N_{output}]  \]</div>
<p>Where <span class="math notranslate nohighlight">\(N_{input}\)</span> and <span class="math notranslate nohighlight">\(N_{output}\)</span> are the numbers of input and output features, taken to be rows and columns of the matrices, respectively. The <span class="math notranslate nohighlight">\(+1\)</span> in <span class="math notranslate nohighlight">\( (N_{input}+1) \)</span> is for the bias column.</p>
<p>Forward Prop: pass.
INPUT <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> -&gt; … -&gt; a bunch of layers … -&gt; OUTPUT <span class="math notranslate nohighlight">\(y\)</span></p>
<p>Backprop.: Going backward from the output layer to the input layer. Basically, for one layer, <span class="math notranslate nohighlight">\(Backprop(dLoss/dy) = dLoss/dx\)</span>, where <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are the inputs and outputs for one layer. This will use, in the simplest case, <span class="math notranslate nohighlight">\(dLoss/dx = dLoss/dy \ dy/dx\)</span> where <span class="math notranslate nohighlight">\( dy/dx\)</span> will be in terms of the weights of the current layer. <span class="math notranslate nohighlight">\(dLoss/dw = dLoss/dy \ dy/dw\)</span>, and then the weights are updated as to minimize the loss, e.g. for SGD: <span class="math notranslate nohighlight">\(w \leftarrow w - (dLoss/dw \ \eta\)</span> where <span class="math notranslate nohighlight">\(\eta\)</span> is the learning rate.</p>
<p>the dLoss/dx becomes the dLoss/dy of the next layer:</p>
<p>X layer1 - &gt; layer_2 … -&gt; layer_n -&gt; y
&lt;-…  dL/dx  &lt;- BP dL/dy  &lt;- dL/dx   &lt;-BP  dL/dy</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def get_tuning_sample():
    sample=int(200000)
    # train_x_sample, train_t_ratio_sample, valid_x_sample, valid_t_ratio_sample
    get_whole=True
    if get_whole:
        train_x_sample, train_t_ratio_sample, valid_x_sample, valid_t_ratio_sample = train_x, train_t_ratio, valid_x, valid_t_ratio
    else:
        train_x_sample, train_t_ratio_sample, valid_x_sample, valid_t_ratio_sample=train_x[:sample], train_t_ratio[:sample], valid_x[:sample], valid_t_ratio[:sample]
    return train_x_sample, train_t_ratio_sample, valid_x_sample, valid_t_ratio_sample

train_x_sample, train_t_ratio_sample, valid_x_sample, valid_t_ratio_sample = get_tuning_sample()
print(train_x_sample.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(8000000, 5)
</pre></div>
</div>
</div>
</div>
<p>Need to use test set <em>and</em> validation set for tuning.</p>
<p>Note that hyperparameters are usually not directly transferrable across architectures and datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class HyperTrainer():
    &quot;&quot;&quot;loss, training and evaluation&quot;&quot;&quot;
    def __init__(self, model, optimizer, batch_size):
                 #, device):
        self.model = model
        #self.device= device
        self.optimizer = optimizer
        self.batch_size=batch_size
        self.n_iterations_tune=int(50)

        #the loss function returns the loss function. It is a static method so it doesn&#39;t need self
        # @staticmethod
        # def loss_fun(targets, outputs):
        #   tau = torch.rand(outputs.shape)
        #   return torch.mean(torch.where(targets &gt;= outputs, 
        #                                   tau * (targets - outputs), 
        #                                   (1 - tau)*(outputs - targets)))

        #     This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, 
        #     by combining the operations into one layer

    def train(self, x, t):

        self.model.train()
        final_loss = 0
        for iteration in range(self.n_iterations_tune):
            self.optimizer.zero_grad()
            batch_x, batch_t = get_batch(x, t,  self.batch_size)#x and t are train_x and train_t

            # with torch.no_grad():
            inputs=torch.from_numpy(batch_x).float()
            targets=torch.from_numpy(batch_t).float()
            outputs = self.model(inputs)
            loss = average_quantile_loss(outputs, targets, inputs)
            loss.backward()
            self.optimizer.step()
            final_loss += loss.item()

        return final_loss / self.batch_size

    def evaluate(self, x, t):

        self.model.eval()
        final_loss = 0
        for iteration in range(self.n_iterations_tune):
            batch_x, batch_t = get_batch(x, t,  self.batch_size)#x and t are train_x and train_t

            # with torch.no_grad():            
            inputs=torch.from_numpy(batch_x).float()
            targets=torch.from_numpy(batch_t).float()
            outputs = self.model(inputs)
            loss =average_quantile_loss(outputs, targets, inputs)
            final_loss += loss.item()
        return final_loss / self.batch_size

    
EPOCHS=1
def run_train(params, save_model=False):
    &quot;&quot;&quot;For tuning the parameters&quot;&quot;&quot;

    model =  RegularizedRegressionModel(
              nfeatures=train_x_sample.shape[1], 
                ntargets=1,
                nlayers=params[&quot;nlayers&quot;], 
                hidden_size=params[&quot;hidden_size&quot;],
                dropout=params[&quot;dropout&quot;]
                )
    # print(model)
    

    learning_rate= params[&quot;learning_rate&quot;]
    optimizer_name = params[&quot;optimizer_name&quot;]
    
    # optimizer = torch.optim.Adam(model.parameters(), lr=params[&quot;learning_rate&quot;]) 
    
    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), 
                            lr=learning_rate, momentum = params[&quot;momentum&quot;])
    
    trainer=HyperTrainer(model, optimizer, batch_size=params[&quot;batch_size&quot;])
    best_loss = np.inf
    early_stopping_iter=10#stop after 10 iteractions of not improving loss
    early_stopping_coutner=0

    # for epoch in range(EPOCHS):
    # train_loss = trainer.train(train_x_sample, train_t_ratio_sample)
        #test loss
    valid_loss=trainer.evaluate(valid_x_sample, valid_t_ratio_sample)

        # print(f&quot;{epoch} \t {train_loss} \t {valid_loss}&quot;)
        
        # if valid_loss&lt;best_loss:
        #     best_loss=valid_loss
        # else:
        #     early_stopping_coutner+=1
        # if early_stopping_coutner &gt; early_stopping_iter:
            # break
            
    # return best_loss
    return valid_loss


def objective(trial):
    CLUSTER=False
    #cluster has greater memory than my laptop, which allows higher max values in hyperparam. search space
    if CLUSTER:
        nlayers_max,n_hidden_max, batch_size_max=int(24),int(350), int(2e5)
    else:
        nlayers_max,n_hidden_max, batch_size_max=int(6),int(256), int(3e4)

    #hyperparameter search space:
    params = {
          &quot;nlayers&quot;: trial.suggest_int(&quot;nlayers&quot;,1,nlayers_max),      
          &quot;hidden_size&quot;: trial.suggest_int(&quot;hidden_size&quot;, 1, n_hidden_max),
          &quot;dropout&quot;: trial.suggest_float(&quot;dropout&quot;, 0.0,0.5),
          &quot;optimizer_name&quot; : trial.suggest_categorical(&quot;optimizer_name&quot;, [&quot;RMSprop&quot;, &quot;SGD&quot;]),
          &quot;momentum&quot;: trial.suggest_float(&quot;momentum&quot;, 0.0,0.99),
          &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 1e-6, 1e-2),
          &quot;batch_size&quot;: trial.suggest_int(&quot;batch_size&quot;, 500, batch_size_max)

        }
    
    for step in range(10):

        temp_loss = run_train(params,save_model=False)
        trial.report(temp_loss, step)
        #activate pruning (early stopping if the current step in the trial has unpromising results)
        #instead of doing lots of iterations, do less iterations and more steps in each trial,  
        #such that a trial is terminated if a step yields an unpromising loss.
        
        if trial.should_prune():
            raise optuna.TrialPruned()
    
    return temp_loss

@time_type_of_func(tuning_or_training=&#39;tuning&#39;)
def tune_hyperparameters(save_best_params):
    

    sampler=False#use different sampling technique than the defualt one if sampler=True.
    if sampler:
        #choose a different sampling strategy (https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.CmaEsSampler.html#optuna.samplers.CmaEsSampler)
        # sampler=optuna.samplers.RandomSampler()
        study=optuna.create_study(direction=&#39;minimize&#39;,
                                  pruner=optuna.pruners.MedianPruner(), sampler=sampler)
    else:
        #but the default sampler is usually better - no need to change it!
        study=optuna.create_study(direction=&#39;minimize&#39;,
                                  pruner=optuna.pruners.HyperbandPruner())
    n_trials=100
    study.optimize(objective, n_trials=n_trials)
    best_trial = study.best_trial
    print(&#39;best model parameters&#39;, best_trial.params)

    best_params=best_trial.params#this is a dictionary
    #save best hyperapameters in a pandas dataframe as a .csv
    if save_best_params:
        tuned_dir = os.path.join(IQN_BASE,&#39;best_params&#39;)
        mkdir(tuned_dir)
        filename=os.path.join(tuned_dir,&#39;best_params_mass_%s_trials.csv&#39; % str(int(n_trials)))
        param_df=pd.DataFrame({
                                &#39;n_layers&#39;:best_params[&quot;nlayers&quot;], 
                                &#39;hidden_size&#39;:best_params[&quot;hidden_size&quot;], 
                                &#39;dropout&#39;:best_params[&quot;dropout&quot;],
                                &#39;optimizer_name&#39;:best_params[&quot;optimizer_name&quot;],
                                &#39;learning_rate&#39;: best_params[&quot;learning_rate&quot;], 
                                &#39;batch_size&#39;:best_params[&quot;batch_size&quot;],
                                &#39;momentum&#39;:best_params[&quot;momentum&quot;]},
                                        index=[0]
        )

        param_df.to_csv(filename)   
    return study
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>study= tune_hyperparameters(save_best_params=True)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tuning IQN hyperparameters to estimate RecoDatam
Getting best hyperparameters for target RecoDatam
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span> <span class="p">[</span><span class="mi">56</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">study</span><span class="o">=</span> <span class="n">tune_hyperparameters</span><span class="p">(</span><span class="n">save_best_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nn">Cell In [6], line 93,</span> in <span class="ni">time_type_of_func.&lt;locals&gt;.timer.&lt;locals&gt;.wrapper_timer</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">91</span>     <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;tuning IQN hyperparameters to estimate </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">92</span> <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>    
<span class="ne">---&gt; </span><span class="mi">93</span> <span class="n">value</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">94</span> <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>      
<span class="g g-Whitespace">     </span><span class="mi">95</span> <span class="n">run_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>    

<span class="nn">Cell In [55], line 138,</span> in <span class="ni">tune_hyperparameters</span><span class="nt">(save_best_params)</span>
<span class="g g-Whitespace">    </span><span class="mi">135</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">136</span>     <span class="n">study</span><span class="o">=</span><span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s1">&#39;minimize&#39;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">137</span>                               <span class="n">pruner</span><span class="o">=</span><span class="n">optuna</span><span class="o">.</span><span class="n">pruners</span><span class="o">.</span><span class="n">HyperbandPruner</span><span class="p">())</span>
<span class="ne">--&gt; </span><span class="mi">138</span> <span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">139</span> <span class="n">best_trial</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best model parameters&#39;</span><span class="p">,</span> <span class="n">best_trial</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/envs/new_torch/lib/python3.9/site-packages/optuna/study/study.py:419,</span> in <span class="ni">Study.optimize</span><span class="nt">(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)</span>
<span class="g g-Whitespace">    </span><span class="mi">315</span> <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">316</span>     <span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">317</span>     <span class="n">func</span><span class="p">:</span> <span class="n">ObjectiveFuncType</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">324</span>     <span class="n">show_progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">325</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">326</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Optimize an objective function.</span>
<span class="g g-Whitespace">    </span><span class="mi">327</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">328</span><span class="sd">     Optimization is done by choosing a suitable set of hyperparameter values from a given</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">    </span><span class="mi">416</span><span class="sd">             If nested invocation of this method occurs.</span>
<span class="g g-Whitespace">    </span><span class="mi">417</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">419</span>     <span class="n">_optimize</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">420</span>         <span class="n">study</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">421</span>         <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">422</span>         <span class="n">n_trials</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">423</span>         <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">424</span>         <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">425</span>         <span class="n">catch</span><span class="o">=</span><span class="n">catch</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">426</span>         <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">427</span>         <span class="n">gc_after_trial</span><span class="o">=</span><span class="n">gc_after_trial</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">428</span>         <span class="n">show_progress_bar</span><span class="o">=</span><span class="n">show_progress_bar</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">429</span>     <span class="p">)</span>

<span class="nn">File ~/anaconda3/envs/new_torch/lib/python3.9/site-packages/optuna/study/_optimize.py:66,</span> in <span class="ni">_optimize</span><span class="nt">(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)</span>
<span class="g g-Whitespace">     </span><span class="mi">64</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">65</span>     <span class="k">if</span> <span class="n">n_jobs</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">66</span>         <span class="n">_optimize_sequential</span><span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">67</span>             <span class="n">study</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">68</span>             <span class="n">func</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">69</span>             <span class="n">n_trials</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">70</span>             <span class="n">timeout</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">71</span>             <span class="n">catch</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">72</span>             <span class="n">callbacks</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">73</span>             <span class="n">gc_after_trial</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">74</span>             <span class="n">reseed_sampler_rng</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">75</span>             <span class="n">time_start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">76</span>             <span class="n">progress_bar</span><span class="o">=</span><span class="n">progress_bar</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">77</span>         <span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">78</span>     <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">79</span>         <span class="k">if</span> <span class="n">n_jobs</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/envs/new_torch/lib/python3.9/site-packages/optuna/study/_optimize.py:160,</span> in <span class="ni">_optimize_sequential</span><span class="nt">(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)</span>
<span class="g g-Whitespace">    </span><span class="mi">157</span>         <span class="k">break</span>
<span class="g g-Whitespace">    </span><span class="mi">159</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">160</span>     <span class="n">frozen_trial</span> <span class="o">=</span> <span class="n">_run_trial</span><span class="p">(</span><span class="n">study</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">catch</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">161</span> <span class="k">finally</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">162</span>     <span class="c1"># The following line mitigates memory problems that can be occurred in some</span>
<span class="g g-Whitespace">    </span><span class="mi">163</span>     <span class="c1"># environments (e.g., services that use computing containers such as CircleCI).</span>
<span class="g g-Whitespace">    </span><span class="mi">164</span>     <span class="c1"># Please refer to the following PR for further details:</span>
<span class="g g-Whitespace">    </span><span class="mi">165</span>     <span class="c1"># https://github.com/optuna/optuna/pull/325.</span>
<span class="g g-Whitespace">    </span><span class="mi">166</span>     <span class="k">if</span> <span class="n">gc_after_trial</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/envs/new_torch/lib/python3.9/site-packages/optuna/study/_optimize.py:234,</span> in <span class="ni">_run_trial</span><span class="nt">(study, func, catch)</span>
<span class="g g-Whitespace">    </span><span class="mi">227</span>         <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;Should not reach.&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">229</span> <span class="k">if</span> <span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">230</span>     <span class="n">frozen_trial</span><span class="o">.</span><span class="n">state</span> <span class="o">==</span> <span class="n">TrialState</span><span class="o">.</span><span class="n">FAIL</span>
<span class="g g-Whitespace">    </span><span class="mi">231</span>     <span class="ow">and</span> <span class="n">func_err</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">232</span>     <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func_err</span><span class="p">,</span> <span class="n">catch</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">233</span> <span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">234</span>     <span class="k">raise</span> <span class="n">func_err</span>
<span class="g g-Whitespace">    </span><span class="mi">235</span> <span class="k">return</span> <span class="n">frozen_trial</span>

<span class="nn">File ~/anaconda3/envs/new_torch/lib/python3.9/site-packages/optuna/study/_optimize.py:196,</span> in <span class="ni">_run_trial</span><span class="nt">(study, func, catch)</span>
<span class="g g-Whitespace">    </span><span class="mi">194</span> <span class="k">with</span> <span class="n">get_heartbeat_thread</span><span class="p">(</span><span class="n">trial</span><span class="o">.</span><span class="n">_trial_id</span><span class="p">,</span> <span class="n">study</span><span class="o">.</span><span class="n">_storage</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">195</span>     <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">196</span>         <span class="n">value_or_values</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">197</span>     <span class="k">except</span> <span class="n">exceptions</span><span class="o">.</span><span class="n">TrialPruned</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">198</span>         <span class="c1"># TODO(mamu): Handle multi-objective cases.</span>
<span class="g g-Whitespace">    </span><span class="mi">199</span>         <span class="n">state</span> <span class="o">=</span> <span class="n">TrialState</span><span class="o">.</span><span class="n">PRUNED</span>

<span class="nn">Cell In [55], line 117,</span> in <span class="ni">objective</span><span class="nt">(trial)</span>
<span class="g g-Whitespace">    </span><span class="mi">104</span> <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
<span class="g g-Whitespace">    </span><span class="mi">105</span>       <span class="s2">&quot;nlayers&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;nlayers&quot;</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span>      
<span class="g g-Whitespace">    </span><span class="mi">106</span>       <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">112</span> 
<span class="g g-Whitespace">    </span><span class="mi">113</span>     <span class="p">}</span>
<span class="g g-Whitespace">    </span><span class="mi">115</span> <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">117</span>     <span class="n">temp_loss</span> <span class="o">=</span> <span class="n">run_train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">save_model</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">118</span>     <span class="n">trial</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">temp_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">119</span>     <span class="c1">#activate pruning (early stopping)</span>

<span class="nn">Cell In [55], line 87,</span> in <span class="ni">run_train</span><span class="nt">(params, save_model)</span>
<span class="g g-Whitespace">     </span><span class="mi">82</span> <span class="n">early_stopping_coutner</span><span class="o">=</span><span class="mi">0</span>
<span class="g g-Whitespace">     </span><span class="mi">84</span> <span class="c1"># for epoch in range(EPOCHS):</span>
<span class="g g-Whitespace">     </span><span class="mi">85</span> <span class="c1"># train_loss = trainer.train(train_x_sample, train_t_ratio_sample)</span>
<span class="g g-Whitespace">     </span><span class="mi">86</span>     <span class="c1">#test loss</span>
<span class="ne">---&gt; </span><span class="mi">87</span> <span class="n">valid_loss</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">valid_x_sample</span><span class="p">,</span> <span class="n">valid_t_ratio_sample</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">89</span>     <span class="c1"># print(f&quot;{epoch} \t {train_loss} \t {valid_loss}&quot;)</span>
<span class="g g-Whitespace">     </span><span class="mi">90</span>     
<span class="g g-Whitespace">     </span><span class="mi">91</span>     <span class="c1"># if valid_loss&lt;best_loss:</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">97</span>         
<span class="g g-Whitespace">     </span><span class="mi">98</span> <span class="c1"># return best_loss</span>
<span class="g g-Whitespace">     </span><span class="mi">99</span> <span class="k">return</span> <span class="n">valid_loss</span>

<span class="nn">Cell In [55], line 52,</span> in <span class="ni">HyperTrainer.evaluate</span><span class="nt">(self, x, t)</span>
<span class="g g-Whitespace">     </span><span class="mi">50</span>     <span class="n">targets</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_t</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">51</span>     <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">52</span>     <span class="n">loss</span> <span class="o">=</span><span class="n">average_quantile_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">53</span>     <span class="n">final_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">54</span> <span class="k">return</span> <span class="n">final_loss</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

<span class="nn">Cell In [38], line 28,</span> in <span class="ni">average_quantile_loss</span><span class="nt">(f, t, x)</span>
<span class="g g-Whitespace">     </span><span class="mi">25</span> <span class="n">tau</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># last column is tau.</span>
<span class="g g-Whitespace">     </span><span class="mi">26</span> <span class="c1">#Eq (2)</span>
<span class="g g-Whitespace">     </span><span class="mi">27</span> <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">t</span> <span class="o">&gt;=</span> <span class="n">f</span><span class="p">,</span> 
<span class="ne">---&gt; </span><span class="mi">28</span>                               <span class="n">tau</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="n">f</span><span class="p">),</span> 
<span class="g g-Whitespace">     </span><span class="mi">29</span>                               <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">f</span> <span class="o">-</span> <span class="n">t</span><span class="p">)))</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#make sure you have plotly installed with jupyterlab support. For jupyterlab3.x:
# conda install &quot;jupyterlab&gt;=3&quot; &quot;ipywidgets&gt;=7.6&quot;

# for JupyterLab 2.x renderer support:
# jupyter labextension install jupyterlab-plotly@5.11.0 @jupyter-widgets/jupyterlab-manager
#conda install -c plotly plotly=5.11.0
from optuna import visualization

visualization.plot_param_importances(study)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">~/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.7</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">optuna</span><span class="o">/</span><span class="n">visualization</span><span class="o">/</span><span class="n">_plotly_imports</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="k">with</span> <span class="n">try_import</span><span class="p">()</span> <span class="k">as</span> <span class="n">_imports</span><span class="p">:</span>  <span class="c1"># NOQA</span>
<span class="ne">----&gt; </span><span class="mi">7</span>     <span class="kn">import</span> <span class="nn">plotly</span>  <span class="c1"># NOQA</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span>     <span class="kn">from</span> <span class="nn">plotly</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">plotly_version</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;plotly&#39;

<span class="n">The</span> <span class="n">above</span> <span class="n">exception</span> <span class="n">was</span> <span class="n">the</span> <span class="n">direct</span> <span class="n">cause</span> <span class="n">of</span> <span class="n">the</span> <span class="n">following</span> <span class="n">exception</span><span class="p">:</span>

<span class="ne">ImportError</span><span class="g g-Whitespace">                               </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_797645</span><span class="o">/</span><span class="mf">859914537.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="kn">from</span> <span class="nn">optuna</span> <span class="kn">import</span> <span class="n">visualization</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> 
<span class="ne">----&gt; </span><span class="mi">9</span> <span class="n">visualization</span><span class="o">.</span><span class="n">plot_param_importances</span><span class="p">(</span><span class="n">study</span><span class="p">)</span>

<span class="nn">~/anaconda3/lib/python3.7/site-packages/optuna/visualization/_param_importances.py</span> in <span class="ni">plot_param_importances</span><span class="nt">(study, evaluator, params, target, target_name)</span>
<span class="g g-Whitespace">     </span><span class="mi">93</span>     <span class="s2">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">     </span><span class="mi">94</span><span class="s2"> </span>
<span class="ne">---&gt; </span><span class="mi">95</span><span class="s2">     _imports.check()</span>
<span class="g g-Whitespace">     </span><span class="mi">96</span><span class="s2">     _check_plot_args(study, target, target_name)</span>
<span class="g g-Whitespace">     </span><span class="mi">97</span><span class="s2"> </span>

<span class="nn">~/anaconda3/lib/python3.7/site-packages/optuna/_imports.py</span> in <span class="ni">check</span><span class="nt">(self)</span>
<span class="g g-Whitespace">     </span><span class="mi">84</span><span class="s2">         if self._deferred is not None:</span>
<span class="g g-Whitespace">     </span><span class="mi">85</span><span class="s2">             exc_value, message = self._deferred</span>
<span class="ne">---&gt; </span><span class="mi">86</span><span class="s2">             raise ImportError(message) from exc_value</span>
<span class="g g-Whitespace">     </span><span class="mi">87</span><span class="s2"> </span>
<span class="g g-Whitespace">     </span><span class="mi">88</span><span class="s2"> </span>

<span class="ne">ImportError</span>: Tried to import &#39;plotly&#39; but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named &#39;plotly&#39;.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>optuna.visualization.plot_parallel_coordinate(study)
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<hr class="docutils" />
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># BEST_PARAMS = {&#39;nlayers&#39;: 6, &#39;hidden_size&#39;: 2, &#39;dropout&#39;: 0.40716885971031636, &#39;optimizer_name&#39;: &#39;Adam&#39;, &#39;learning_rate&#39;: 0.005215585403055171, &#39;batch_size&#39;: 1983}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># def get_model_params_tuned()

tuned_dir = os.path.join(IQN_BASE,&#39;best_params&#39;)
tuned_filename=os.path.join(tuned_dir,&#39;best_params_mass.csv&#39;)
# BEST_PARAMS = pd.read_csv(os.path.join(IQN_BASE, &#39;best_params&#39;,&#39;best_params_Test_Trials.csv&#39;))
BEST_PARAMS=pd.read_csv(tuned_filename)
print(BEST_PARAMS)

n_layers = int(BEST_PARAMS[&quot;n_layers&quot;]) 
hidden_size = int(BEST_PARAMS[&quot;hidden_size&quot;])
dropout = float(BEST_PARAMS[&quot;dropout&quot;])

optimizer_name = BEST_PARAMS[&quot;optimizer_name&quot;]
print(type(optimizer_name))
optimizer_name = BEST_PARAMS[&quot;optimizer_name&quot;].to_string().split()[1]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   Unnamed: 0  n_layers  hidden_size   dropout optimizer_name  learning_rate  \
0           0         1            5  0.141347            SGD       0.003165   

   batch_size  momentum  
0        2948  0.369386  
&lt;class &#39;pandas.core.series.Series&#39;&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>NFEATURES=train_x.shape[1]

def load_untrained_model():
    model=TrainingRegularizedRegressionModel(nfeatures=NFEATURES, ntargets=1,
                               nlayers=n_layers, hidden_size=hidden_size, dropout=dropout)
    print(model)
    return model

model=load_untrained_model()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=20, bias=True)
    (1): Dropout(p=0.141346858975224, inplace=False)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Linear(in_features=20, out_features=20, bias=True)
    (4): LeakyReLU(negative_slope=0.01)
    (5): Linear(in_features=20, out_features=20, bias=True)
    (6): LeakyReLU(negative_slope=0.01)
    (7): Linear(in_features=20, out_features=1, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># optimizer_name =  &#39;Adam&#39;
best_learning_rate =  float(BEST_PARAMS[&quot;learning_rate&quot;])
momentum=float(BEST_PARAMS[&quot;momentum&quot;]) 
best_optimizer_temp = getattr(torch.optim, optimizer_name)(model.parameters(), lr=best_learning_rate,momentum=momentum)
batch_size = int(BEST_PARAMS[&quot;batch_size&quot;])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>best_optimizer_temp
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SGD (
Parameter Group 0
    dampening: 0
    lr: 0.0031645926758082
    momentum: 0.7387727733031814
    nesterov: False
    weight_decay: 0
)
</pre></div>
</div>
</div>
</div>
<section id="run-training">
<h3>Run training<a class="headerlink" href="#run-training" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># BATCHSIZE=10000
BATCHSIZE=batch_size * 2
def train(model, optimizer, avloss, getbatch,
          train_x, train_t, 
          valid_x, valid_t,
          batch_size, 
          n_iterations, traces, 
          step=10, window=10):
    
    # to keep track of average losses
    xx, yy_t, yy_v, yy_v_avg = traces
    
    n = len(valid_x)
    
    print(&#39;Iteration vs average loss&#39;)
    print(&quot;%10s\t%10s\t%10s&quot; % \
          (&#39;iteration&#39;, &#39;train-set&#39;, &#39;test-set&#39;))
    
    for ii in range(n_iterations):

        # set mode to training so that training specific 
        # operations such as dropout are enabled.
        model.train()
        
        # get a random sample (a batch) of data (as numpy arrays)
        batch_x, batch_t = getbatch(train_x, train_t, batch_size)
        # batch_x[:,-1]=batch_x[:,-1] 
        # convert the numpy arrays batch_x and batch_t to tensor 
        # types. The PyTorch tensor type is the magic that permits 
        # automatic differentiation with respect to parameters. 
        # However, since we do not need to take the derivatives
        # with respect to x and t, we disable this feature
        with torch.no_grad(): # no need to compute gradients 
            # wrt. x and t
            x = torch.from_numpy(batch_x).float()
            t = torch.from_numpy(batch_t).float()      

        # compute the output of the model for the batch of data x
        # Note: outputs is 
        #   of shape (-1, 1), but the tensor targets, t, is
        #   of shape (-1,)
        # In order for the tensor operations with outputs and t
        # to work correctly, it is necessary that they have the
        # same shape. We can do this with the reshape method.
        outputs = model(x).reshape(t.shape)
   
        # compute a noisy approximation to the average loss
        empirical_risk = avloss(outputs, t, x)
        
        # use automatic differentiation to compute a 
        # noisy approximation of the local gradient
        optimizer.zero_grad()       # clear previous gradients
        empirical_risk.backward()   # compute gradients
        
        # finally, advance one step in the direction of steepest 
        # descent, using the noisy local gradient. 
        optimizer.step()            # move one step
        
        #add early stopping when the model starts to overfit, as determined by performance on
        #valid set (when valid loss plateaus or starts increasing when train loss keeps decreasing)
        if ii % step == 0:
            
            acc_t = validate(model, avloss, train_x[:n], train_t[:n]) 
            acc_v = validate(model, avloss, valid_x[:n], valid_t[:n])
            yy_t.append(acc_t)
            yy_v.append(acc_v)
            
            # compute running average for validation data
            len_yy_v = len(yy_v)
            if   len_yy_v &lt; window:
                yy_v_avg.append( yy_v[-1] )
            elif len_yy_v == window:
                yy_v_avg.append( sum(yy_v) / window )
            else:
                acc_v_avg  = yy_v_avg[-1] * window
                acc_v_avg += yy_v[-1] - yy_v[-window-1]
                yy_v_avg.append(acc_v_avg / window)
                        
            if len(xx) &lt; 1:
                xx.append(0)
                print(&quot;%10d\t%10.6f\t%10.6f&quot; % \
                      (xx[-1], yy_t[-1], yy_v[-1]))
            else:
                xx.append(xx[-1] + step)
                    
                print(&quot;\r%10d\t%10.6f\t%10.6f\t%10.6f&quot; % \
                          (xx[-1], yy_t[-1], yy_v[-1], yy_v_avg[-1]), 
                      end=&#39;&#39;)
            
    print()      
    return (xx, yy_t, yy_v, yy_v_avg)


@time_type_of_func(tuning_or_training=&#39;training&#39;)
def run(model, 
        train_x, train_t, 
        valid_x, valid_t, traces,
        n_batch=BATCHSIZE, 
        n_iterations=n_iterations, 
        traces_step=200, 
        traces_window=200,
        save_model=False):

    learning_rate= best_learning_rate
    #add weight decay (important regularization to reduce overfitting)
    L2=1e-3
    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2)
    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=best_learning_rate, momentum=momentum, weight_decay=L2)
    
    #starting at 10^-3	    
    traces = train(model, optimizer, 
                      average_quantile_loss,
                      get_batch,
                      train_x, train_t, 
                      valid_x, valid_t,
                      n_batch, 
                  n_iterations,
                  traces,
                  step=traces_step, 
                  window=traces_window)
    
    learning_rate=learning_rate/100
    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=best_learning_rate, momentum=momentum)
    #10^-4
    traces = train(model, optimizer, 
                      average_quantile_loss,
                      get_batch,
                      train_x, train_t, 
                      valid_x, valid_t,
                      n_batch, 
                  n_iterations,
                  traces,
                  step=traces_step, 
                  window=traces_window)


#     learning_rate=learning_rate/100
#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) 
#     #10^-6
#     traces = train(model, optimizer, 
#                       average_quantile_loss,
#                       get_batch,
#                       train_x, train_t, 
#                       valid_x, valid_t,
#                       n_batch, 
#                   n_iterations,
#                   traces,
#                   step=traces_step, 
#                   window=traces_window)

    # plot_average_loss(traces)

    if save_model:
        filename=&#39;Trained_IQNx4_%s_%sK_iter.dict&#39; % (target, str(int(n_iterations/1000)) )
        PATH = os.path.join(IQN_BASE, &#39;trained_models&#39;, filename)
        torch.save(model.state_dict(), PATH)
        print(&#39;\ntrained model dictionary saved in %s&#39; % PATH)
    return  model
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="see-if-trainig-works-on-t-ratio">
<h2>See if trainig works on T ratio<a class="headerlink" href="#see-if-trainig-works-on-t-ratio" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
IQN_trace=([], [], [], [])
traces_step = 800
traces_window=traces_step
n_iterations=100000
IQN = run(model=model,train_x=train_x, train_t=train_t_ratio, 
        valid_x=test_x, valid_t=test_t_ratio, traces=IQN_trace, n_batch=BATCHSIZE, 
        n_iterations=n_iterations, traces_step=traces_step, traces_window=traces_window,
        save_model=False)

</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>training IQN to estimate RecoDatam
Iteration vs average loss
 iteration	 train-set	  test-set
         0	  0.383932	  0.383932
     99200	  0.273795	  0.273795	  0.273795
Iteration vs average loss
 iteration	 train-set	  test-set
    199200	  0.273961	  0.273961	  0.273961
training target RecoDatam using &#39;run&#39; in 614.7817 secs
</pre></div>
</div>
</div>
</div>
</section>
<section id="save-trained-model-if-its-good-and-if-you-haven-t-saved-above-and-load-trained-model-if-you-saved-it">
<h2>Save trained model (if its good, and if you haven’t saved above) and load trained model (if you saved it)<a class="headerlink" href="#save-trained-model-if-its-good-and-if-you-haven-t-saved-above-and-load-trained-model-if-you-saved-it" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>filename=&#39;Trained_IQNx4_%s_%sK_iter.dict&#39; % (target, str(int(n_iterations/1000)) )
trained_models_dir=&#39;trained_models&#39;
mkdir(trained_models_dir)
PATH = os.path.join(IQN_BASE,trained_models_dir , filename)

@debug
def save_model(model):
    print(model)
    torch.save(model.state_dict(), PATH)
    print(&#39;\ntrained model dictionary saved in %s&#39; % PATH)

@debug
def load_model(PATH):
    # n_layers = int(BEST_PARAMS[&quot;n_layers&quot;]) 
    # hidden_size = int(BEST_PARAMS[&quot;hidden_size&quot;])
    # dropout = float(BEST_PARAMS[&quot;dropout&quot;])
    # optimizer_name = BEST_PARAMS[&quot;optimizer_name&quot;].to_string().split()[1]
    # learning_rate =  float(BEST_PARAMS[&quot;learning_rate&quot;])
    # batch_size = int(BEST_PARAMS[&quot;batch_size&quot;])
    model =  RegularizedRegressionModel(
        nfeatures=train_x.shape[1], 
        ntargets=1,
        nlayers=n_layers, 
        hidden_size=hidden_size, 
        dropout=dropout
        )
    model.load_state_dict(torch.load(PATH) )
    #OR
    #model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED!
    model.eval()
    print(model)
    return model
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>save_model(IQN)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Calling save_model(RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=20, bias=True)
    (1): Dropout(p=0.141346858975224, inplace=False)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Linear(in_features=20, out_features=20, bias=True)
    (4): LeakyReLU(negative_slope=0.01)
    (5): Linear(in_features=20, out_features=20, bias=True)
    (6): LeakyReLU(negative_slope=0.01)
    (7): Linear(in_features=20, out_features=1, bias=True)
  )
))
RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=20, bias=True)
    (1): Dropout(p=0.141346858975224, inplace=False)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Linear(in_features=20, out_features=20, bias=True)
    (4): LeakyReLU(negative_slope=0.01)
    (5): Linear(in_features=20, out_features=20, bias=True)
    (6): LeakyReLU(negative_slope=0.01)
    (7): Linear(in_features=20, out_features=1, bias=True)
  )
)

trained model dictionary saved in /home/ali/Desktop/Pulled_Github_Repositories/torchQN/trained_models/Trained_IQNx4_RecoDatam_100K_iter.dict
&#39;save_model&#39; returned None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>IQN
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=20, bias=True)
    (1): Dropout(p=0.141346858975224, inplace=False)
    (2): LeakyReLU(negative_slope=0.01)
    (3): Linear(in_features=20, out_features=20, bias=True)
    (4): LeakyReLU(negative_slope=0.01)
    (5): Linear(in_features=20, out_features=20, bias=True)
    (6): LeakyReLU(negative_slope=0.01)
    (7): Linear(in_features=20, out_features=1, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.hist(valid_t_ratio, label=&#39;post-z ratio target&#39;);
for i in range(NFEATURES):
    plt.hist(valid_x[:,i], label =f&quot;feature {i}&quot;, alpha=0.35)
plt.legend();plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Old_1_88_0.svg" src="_images/Old_1_88_0.svg" /></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def simple_eval(model):
    model.eval()
    valid_x_tensor=torch.from_numpy(valid_x).float()
    pred = IQN(valid_x_tensor)
    p = pred.detach().numpy()
    fig, ax = plt.subplots(1,1)
    label=FIELDS[target][&#39;ylabel&#39;]
    ax.hist(p, label=f&#39;Predicted post-z ratio for {label}&#39;, alpha=0.4, density=True)
    orig_ratio = z(T(&#39;m&#39;, scaled_df=train_data_m))
    print(orig_ratio[:5])
    ax.hist(orig_ratio, label = f&#39;original post-z ratio for {label}&#39;, alpha=0.4,density=True)
    ax.grid()
    set_axes(ax, xlabel=&#39;predicted $T$&#39;)
    print(&#39;predicted ratio shape: &#39;, p.shape)
    return p
    
p = simple_eval(IQN)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    0.479708
1    0.453907
2    0.453907
3    0.453907
4    0.099661
dtype: float64
predicted ratio shape:  (1000000, 1)
</pre></div>
</div>
<img alt="_images/Old_1_89_2.svg" src="_images/Old_1_89_2.svg" /></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># IQN.eval()
# valid_x_tensor=torch.from_numpy(valid_x).float()
# pred = IQN(valid_x_tensor)
# p = pred.detach().numpy()
# plt.hist(p, label=&#39;predicted $T$ ratio&#39;);plt.legend();plt.show()
</pre></div>
</div>
</div>
</div>
<div class="math notranslate nohighlight">
\[
        f_{\text{IQN}} (\mathcal{O}) =  z \left( \frac{\mathbb{L} (\mathcal{O}^{\text{reco}}) +10 }{\mathbb{L}(\mathcal{O}^{\text{gen}}) +10} \right),
\]</div>
<p>So, to de-scale, (for our observable <span class="math notranslate nohighlight">\(\mathcal{O}=m\)</span> ),</p>
<div class="math notranslate nohighlight">
\[
    m^{\text{predicted}} = \mathbb{L}^{-1} \left[ z^{-1} (f_{\text{IQN}} ) \left[ \mathbb{L} (m^\text{gen})+10 \right] -10 \right]
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(z^{-1} (f_{\text{IQN}} )\)</span> should use the mean and std of the ratio thing for the target</p>
<div class="math notranslate nohighlight">
\[z^{-1} (f_{\text{IQN}} ) = z^{-1}\left( y_{pred}, \text{mean}=\text{mean}(\mathbb{T}(\text{target_variable})), std=std (\mathbb{T}(\text{target_variable} ) \right)\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def z_inverse(xprime, mean, std):
    return xprime * std + mean
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>recom_unsc_mean=TEST_SCALE_DICT[target][&#39;mean&#39;]
recom_unsc_std=TEST_SCALE_DICT[target][&#39;std&#39;]
print(recom_unsc_mean,recom_unsc_std)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5.554066938043617 2.644892802850232
</pre></div>
</div>
</div>
</div>
<p>Get unscaled dataframe again, just to verify</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>raw_train_data=pd.read_csv(os.path.join(DATA_DIR,&#39;train_data_10M_2.csv&#39;),
                      usecols=all_cols,
                      nrows=SUBSAMPLE
                      )

raw_test_data=pd.read_csv(os.path.join(DATA_DIR,&#39;test_data_10M_2.csv&#39;),
                      usecols=all_cols,
                     nrows=SUBSAMPLE
                     )
raw_test_data.describe()
m_reco = raw_test_data[&#39;RecoDatam&#39;]
m_gen = raw_test_data[&#39;genDatam&#39;]
plt.hist(m_reco,label=r&#39;$m_{gen}^{test \ data}$&#39;);plt.legend();plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Old_1_95_0.svg" src="_images/Old_1_95_0.svg" /></div>
</div>
<p>Apply the descaling formula for our observable</p>
<div class="math notranslate nohighlight">
\[
    m^{\text{predicted}} = \mathbb{L}^{-1} \left[ z^{-1} (f_{\text{IQN}} ) \left[ \mathbb{L} (m^\text{gen})+10 \right] -10 \right]
\]</div>
<ul class="simple">
<li><p>First, calculate <span class="math notranslate nohighlight">\(z^{-1} (f_{\text{IQN}} )\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(valid_t_ratio.shape, valid_t_ratio[:5])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(8000000,) [0.47970804 0.45390654 0.45390654 0.45390654 0.09966065]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>orig_ratio = T(&#39;m&#39;, scaled_df=train_data_m)
orig_ratio[:5]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    0.995290
1    0.994742
2    0.994742
3    0.994742
4    0.987216
dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>z_inv_f =z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))
z_inv_f[:5]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1.010673  ],
       [1.0099984 ],
       [0.98119617],
       [0.97980946],
       [0.98448396]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Now</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathbb{L}(\mathcal{O^{\text{gen}}}) = \mathbb{L} (m^{\text{gen}})\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>L_obs = L(orig_observable=m_gen, label=&#39;m&#39;)
L_obs[:5]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([2.07104388, 2.07104388, 2.24205984, 2.28661525, 1.97173801])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(L_obs.shape, z_inv_f.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1000000,) (1000000, 1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>z_inv_f = z_inv_f.flatten();print(z_inv_f.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1000000,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>“factor” <span class="math notranslate nohighlight">\( = z^{-1} (f_{\text{IQN}} ) \left[ \mathbb{L} (m^\text{gen})+10 \right] -10 \)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>factor = (z_inv_f * (L_obs  + 10) )-10
factor[:5]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([2.19987869, 2.1917355 , 2.01186217, 2.03854189, 1.78598401])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>m_pred = L_inverse(L_observable=factor, label=&#39;m&#39;)
# pT_pred=get_finite(pT_pred)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>m_pred
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([7.02391872, 6.95073361, 5.47722825, ..., 2.41978714, 3.13010808,
       5.48240938])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.hist(m_pred.flatten(),label=&#39;predicted&#39;,alpha=0.3);
plt.hist(m_reco,label=r&#39;$m_{reco}^{test \ data}$&#39;,alpha=0.3);

plt.legend();plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Old_1_108_0.svg" src="_images/Old_1_108_0.svg" /></div>
</div>
<hr class="docutils" />
<section id="paper-plotting">
<h3>Paper plotting<a class="headerlink" href="#paper-plotting" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>range_=[0,25]
bins=50
data=raw_train_data
YLIM=(0.8,1.2)
data = data[[&#39;RecoDatapT&#39;,&#39;RecoDataeta&#39;,&#39;RecoDataphi&#39;,&#39;RecoDatam&#39;]]
data.columns = [&#39;realpT&#39;,&#39;realeta&#39;,&#39;realphi&#39;,&#39;realm&#39;]
REAL_DIST=data[&#39;realm&#39;]
norm_data=data.shape[0]
AUTOREGRESSIVE_DIST = m_pred
norm_IQN=AUTOREGRESSIVE_DIST.shape[0]
norm_autoregressive=AUTOREGRESSIVE_DIST.shape[0]
norm_IQN=norm_autoregressive
print(&#39;norm_data&#39;,norm_data,&#39;\nnorm IQN&#39;,norm_IQN,&#39;\nnorm_autoregressive&#39;, norm_autoregressive)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>norm_data 8000000 
norm IQN 1000000 
norm_autoregressive 1000000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def get_hist(label):
    &quot;&quot;&quot;label could be &quot;pT&quot;, &quot;eta&quot;, &quot;phi&quot;, &quot;m&quot;
    &quot;&quot;&quot;
    predicted_label_counts, label_edges = np.histogram(JETS_DICT[&#39;Predicted_RecoData&#39;+label][&#39;dist&#39;], 
    range=JETS_DICT[&#39;Predicted_RecoData&#39;+label][&#39;range&#39;], bins=bins)
    real_label_counts, _ = np.histogram(JETS_DICT[&#39;Real_RecoData&#39;+label][&#39;dist&#39;], 
    range=JETS_DICT[&#39;Real_RecoData&#39;+label][&#39;range&#39;], bins=bins)
    label_edges = label_edges[1:]/2+label_edges[:-1]/2

    return real_label_counts, predicted_label_counts, label_edges

def get_hist_simple(label):
    predicted_label_counts, label_edges = np.histogram(m_pred , range=range_, bins=bins)
    real_label_counts, _ = np.histogram(REAL_DIST, range=range_, bins=bins)
    label_edges = label_edges[1:]/2+label_edges[:-1]/2
    return real_label_counts, predicted_label_counts, label_edges
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>real_label_counts_m, predicted_label_counts_m, label_edges_m = get_hist_simple(&#39;m&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_one_m():
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(3.5*3/2.5,3.8), gridspec_kw={&#39;height_ratios&#39;: [2,0.5]})
    ax1.step(label_edges_m, real_label_counts_m/norm_data, where=&quot;mid&quot;, color=&quot;k&quot;, linewidth=0.5)# step real_count_pt
    ax1.step(label_edges_m, predicted_label_counts_m/norm_IQN, where=&quot;mid&quot;, color=&quot;#D7301F&quot;, linewidth=0.5)# step predicted_count_pt
    ax1.scatter(label_edges_m, real_label_counts_m/norm_data, label=&quot;reco&quot;,  color=&quot;k&quot;,facecolors=&#39;none&#39;, marker=&quot;o&quot;, s=5, linewidth=0.5)
    ax1.scatter(label_edges_m,predicted_label_counts_m/norm_IQN, label=&quot;predicted sbatch 1&quot;, color=&quot;#D7301F&quot;, marker=&quot;x&quot;, s=5, linewidth=0.5)
    ax1.set_xlim(range_)
    ax1.set_ylim(0, max(predicted_label_counts_m/norm_IQN)*1.1)
    ax1.set_ylabel(&quot;counts&quot;)
    ax1.set_xticklabels([])
    ax1.legend(loc=&#39;upper right&#39;)

    ratio=(predicted_label_counts_m/norm_IQN)/(real_label_counts_m/norm_data)
    ax2.scatter(label_edges_m, ratio, color=&quot;r&quot;, marker=&quot;x&quot;, s=5, linewidth=0.5)#PREDICTED (IQN)/Reco (Data)
    ax2.scatter(label_edges_m, ratio/ratio, color=&quot;k&quot;, marker=&quot;o&quot;,facecolors=&quot;none&quot;, s=5, linewidth=0.5)
    ax2.set_xlim(range_)
    # ax2.set_xlabel(labels[3])
    ax2.set_ylabel(r&quot;$\frac{\textnormal{predicted}}{\textnormal{reco}}$&quot;)
    ax2.set_ylim((YLIM))
    ax2.set_xlim(range_)
    plt.tight_layout()
    fig.subplots_adjust(wspace=0.5, hspace=0.2)
    fig.subplots_adjust(wspace=0.0, hspace=0.1)
    # plt.savefig(DIR+&#39;AUTOREGRESSIVE_m_TUNEND_MLP_OCT_18.pdf&#39;)
    #   plt.savefig(&#39;images/all_m_g2r.pdf&#39;)
    plt.show(); 
    # fig.show()

    plt.axis(&#39;off&#39;)
    plt.gca().set_position([0, 0, 1, 1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_one_m()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Old_1_114_0.svg" src="_images/Old_1_114_0.svg" /></div>
</div>
<hr class="docutils" />
<hr class="docutils" />
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>if target== &#39;RecoDatapT&#39;:
    label= &#39;$p_T$ [GeV]&#39;
    x_min, x_max = 20, 60
elif target== &#39;RecoDataeta&#39;:
    label = &#39;$\eta$&#39;
    x_min, x_max = -5.4, 5.4
elif target ==&#39;RecoDataphi&#39;:
    label=&#39;$\phi$&#39;
    x_min, x_max = -3.4, 3.4
elif target == &#39;RecoDatam&#39;:
    label = &#39; $m$ [GeV]&#39;
    x_min, x_max = 0, 18


    
def evaluate_model(dnn, target, src,
               fgsize=(6, 6), 
               ftsize=20,save_image=False, save_pred=False,
               show_plot=True):
    eval_data=pd.read_csv(os.path.join(DATA_DIR,&#39;test_data_10M_2.csv&#39;))
    ev_features=X
    #[&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;,&#39;tau&#39;]
    
    eval_data=eval_data[ev_features]
    
    print(&#39;EVALUATION DATA OLD INDEX\n&#39;, eval_data.head())

    

                            
    dnn.eval()
    y = dnn(eval_data)
    eval_data[&#39;RecoDatam&#39;]=y
    new_cols= [&#39;RecoDatam&#39;] + X
    eval_data=eval_data.reindex(columns=new_cols)
    print(&#39;EVALUATION DATA NEW INDEX\n&#39;, eval_data.head())

    eval_data.to_csv(&#39;AUTOREGRESSIVE_m_Prime.csv&#39;)


    if save_pred:
        pred_df = pd.DataFrame({T+&#39;_predicted&#39;:y})
        pred_df.to_csv(&#39;predicted_data/dataset2/&#39;+T+&#39;_predicted_MLP_iter_5000000.csv&#39;)
        
    if save_image or show_plot:
        gfile =&#39;fig_model_%s.png&#39; % target
        xbins = 100
        xmin  = src[&#39;xmin&#39;]
        xmax  = src[&#39;xmax&#39;]
        xlabel= src[&#39;xlabel&#39;]
        xstep = (xmax - xmin)/xbins

        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=fgsize)
        
        ax.set_xlim(xmin, xmax)
        ax.set_xlabel(xlabel, fontsize=ftsize)
        ax.set_xlabel(&#39;reco jet &#39;+label, fontsize=ftsize)
        ax.set_ylabel(y_label_dict[target], fontsize=ftsize)

        ax.hist(train_data[&#39;RecoDatam&#39;], 
                bins=xbins, 
                range=(xmin, xmax), 
                alpha=0.3, 
                color=&#39;blue&#39;, 
                density=True, 
                label=&#39;simulation&#39;)
        ax.hist(y, 
                bins=xbins, 
                range=(xmin, xmax), 
                alpha=0.3, 
                color=&#39;red&#39;, 
                density=True, 
                label=&#39;$y^\prime$&#39;)
        ax.grid()
        ax.legend()
        
        
        if save_image:
            plt.savefig(&#39;images/&#39;+T+&#39;IQN_Consecutive_&#39;+N+&#39;.png&#39;)
            print(&#39;images/&#39;+T+&#39;IQN_Consecutive_&#39;+N+&#39;.png&#39;)
        if show_plot:
            plt.tight_layout()
            plt.show()
##########
################################################CNN







def main():
    start=time.time()
    print(&#39;estimating mass\n&#39;)
    model =  utils.RegularizedRegressionModel(nfeatures=train_x.shape[1], ntargets=1,nlayers=n_layers, hidden_size=n_hidden)
    traces = ([], [], [], [])
    dnn = run(model, scalers, target, train_x, train_t, valid_x, valid_t, traces)
    evaluate_model( dnn, target, source)



if __name__ == &quot;__main__&quot;:
    main()

</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>estimating mass
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_282404</span><span class="o">/</span><span class="mf">1611009616.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">    </span><span class="mi">102</span> 
<span class="g g-Whitespace">    </span><span class="mi">103</span> <span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">104</span>     <span class="n">main</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">105</span> 

<span class="nn">/tmp/ipykernel_282404/1611009616.py</span> in <span class="ni">main</span><span class="nt">()</span>
<span class="g g-Whitespace">     </span><span class="mi">96</span>     <span class="n">model</span> <span class="o">=</span>  <span class="n">utils</span><span class="o">.</span><span class="n">RegularizedRegressionModel</span><span class="p">(</span><span class="n">nfeatures</span><span class="o">=</span><span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ntargets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">nlayers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">97</span>     <span class="n">traces</span> <span class="o">=</span> <span class="p">([],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[])</span>
<span class="ne">---&gt; </span><span class="mi">98</span>     <span class="n">dnn</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">scalers</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_t</span><span class="p">,</span> <span class="n">valid_x</span><span class="p">,</span> <span class="n">valid_t</span><span class="p">,</span> <span class="n">traces</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">99</span>     <span class="n">evaluate_model</span><span class="p">(</span> <span class="n">dnn</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">100</span> 

<span class="ne">NameError</span>: name &#39;scalers&#39; is not defined
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="plot-predicted-vs-real-reco-in-our-paper-s-format">
<h1>Plot predicted vs real reco (in our paper’s format)<a class="headerlink" href="#plot-predicted-vs-real-reco-in-our-paper-s-format" title="Permalink to this heading">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="train-p-t-using-saved-variables-above">
<h1>Train <span class="math notranslate nohighlight">\(p_T\)</span> using saved variables above<a class="headerlink" href="#train-p-t-using-saved-variables-above" title="Permalink to this heading">#</a></h1>
<p>Evaluate <span class="math notranslate nohighlight">\(p_T\)</span> and save predicted distribution</p>
<p>Plot reco <span class="math notranslate nohighlight">\(p_T\)</span> and  predicted reco <span class="math notranslate nohighlight">\(p_T\)</span> marginal densities</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># show_jupyter_image(&#39;screenshot.png&#39;)
</pre></div>
</div>
</div>
</div>
<!-- > I guess it works now --><p>commented new ideas below</p>
<!-- ### Ideas for a future paper

me and Harrison would like to use this method for on-the-fly stochastic folding of events in MC generators (potentially even including CMSSW formats like [nanoaod](https://github.com/cms-nanoAOD/nanoAOD-tools), such as in Madminer (but using IQN as opposed to Delphes for detector simulation) for any observable. This also beings the possibility of using LFI methods for much better inference on models (such as SMEFT) using any observable post-detector simulation. If you're interested in helping out on this, me and Harrison would like to do most of the code/ideas, but your occasional ideas/input would be incredibly valuable! --></section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ali Al Kadhim<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>