{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d6920b-1452-45a8-8751-0b7d6e8e8fd0",
   "metadata": {},
   "source": [
    "# IQNx4: Chapter 3: Autoregressive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c623666-defa-4463-adfe-f7667c7d385d",
   "metadata": {},
   "source": [
    "## 3.1: Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31994c2b-a804-4348-a5a9-8bd8c89ff64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using torch version 1.9.0\n",
      "matplotlib version=  3.5.1\n",
      "using (optional) optuna version 2.8.0\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "using torch version 1.9.0\n",
      "matplotlib version=  3.5.1\n",
      "using (optional) optuna version 2.8.0\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n",
      "DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "using DATA_DIR=/home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: MatplotlibDeprecationWarning: Support for setting an rcParam that expects a str value to a non-str value is deprecated since 3.5 and support will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import scipy as sp; import scipy.stats as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"using torch version {torch.__version__}\")\n",
    "# use numba's just-in-time compiler to speed things up\n",
    "# from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "\n",
    "print(\"matplotlib version= \", mp.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reset matplotlib stle/parameters\n",
    "# reset matplotlib parameters to their defaults\n",
    "# plt.style.use('seaborn-deep')\n",
    "# mp.rcParams['agg.path.chunksize'] = 10000\n",
    "font_legend = 15\n",
    "font_axes = 15\n",
    "# %matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from IPython.display import Image, display\n",
    "# from importlib import import_module\n",
    "# import plotly\n",
    "try:\n",
    "    import optuna\n",
    "\n",
    "    print(f\"using (optional) optuna version {optuna.__version__}\")\n",
    "except Exception:\n",
    "    print(\"optuna is only used for hyperparameter tuning, not critical!\")\n",
    "    pass\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# import sympy as sy\n",
    "# import ipywidgets as wid;\n",
    "\n",
    "# update fonts\n",
    "font = {\"family\": \"serif\", \"size\": 10}\n",
    "mp.rc(\"font\", **font)\n",
    "\n",
    "# set usetex = False if LaTex is not\n",
    "# available on your system or if the\n",
    "# rendering is too slow\n",
    "mp.rcParams.update({\"text.usetex\": True})\n",
    "# plt.rcParams['text.usetex'] = True\n",
    "mp.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]  # for \\text command\n",
    "\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "# seed = 128\n",
    "# rnd  = np.random.RandomState(seed)\n",
    "# sometimes jupyter doesnt initialize MathJax automatically for latex, so do this:\n",
    "\n",
    "try:\n",
    "    IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "    print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "    utils_dir = os.path.join(IQN_BASE, 'utils/')\n",
    "    sys.path.append(utils_dir)\n",
    "    import utils\n",
    "\n",
    "    # usually its not recommended to import everything from a module, but we know\n",
    "    # whats in it so its fine\n",
    "    # from utils import *\n",
    "    print(\"DATA directory also properly set, in %s\" % os.environ[\"DATA_DIR\"])\n",
    "except Exception:\n",
    "    # IQN_BASE=os.getcwd()\n",
    "    print(\n",
    "        \"\"\"\\nBASE directory not properly set. Read repo README.    If you need a function from utils, use the decorator below, or add utils to sys.path\\n\n",
    "    You can also do \n",
    "    os.environ['IQN_BASE']=<ABSOLUTE PATH FOR THE IQN REPO>\n",
    "    or\n",
    "    os.environ['IQN_BASE']=os.getcwd()\"\"\"\n",
    "    )\n",
    "    pass\n",
    "\n",
    "# from IQNx4_utils import *\n",
    "IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "utils_dir = os.path.join(IQN_BASE, \"utils/\")\n",
    "sys.path.append(utils_dir)\n",
    "# usually its not recommended to import everything from a module, but we know\n",
    "# whats in it so its fine\n",
    "\n",
    "# or use joblib for caching on disk\n",
    "from joblib import Memory\n",
    "\n",
    "\n",
    "################################### CONFIGURATIONS ###################################\n",
    "DATA_DIR = os.environ[\"DATA_DIR\"]\n",
    "print(f\"using DATA_DIR={DATA_DIR}\")\n",
    "JUPYTER = False\n",
    "use_subsample = False\n",
    "# use_subsample=True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "# memory = Memory(DATA_DIR)\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p(p_T)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "    \"RecoDataeta\": \"$p(\\eta)$\",\n",
    "    \"RecoDataphi\": \"$p(\\phi)$\",\n",
    "    \"RecoDatam\": \"$p(m)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "}\n",
    "\n",
    "loss_y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p_T^{reco}$\",\n",
    "    \"RecoDataeta\": \"$\\eta^{reco}$\",\n",
    "    \"RecoDataphi\": \"$\\phi^{reco}$\",\n",
    "    \"RecoDatam\": \"$m^{reco}$\",\n",
    "}\n",
    "\n",
    "\n",
    "################################### SET DATA CONFIGURATIONS ###################################\n",
    "X = [\"genDatapT\", \"genDataeta\", \"genDataphi\", \"genDatam\", \"tau\"]\n",
    "\n",
    "# set order of training:\n",
    "# pT_first: pT->>m->eta->phi\n",
    "# m_first: m->pT->eta->phi\n",
    "\n",
    "\n",
    "ORDER = \"m_First\"\n",
    "\n",
    "if ORDER == \"m_First\":\n",
    "    FIELDS = {\n",
    "        \"RecoDatam\": {\n",
    "            \"inputs\": X,\n",
    "            \"xlabel\": r\"$m$ (GeV)\",\n",
    "            \"ylabel\": \"$m^{reco}$\",\n",
    "            \"xmin\": 0,\n",
    "            \"xmax\": 25,\n",
    "        },\n",
    "        \"RecoDatapT\": {\n",
    "            \"inputs\": [\"RecoDatam\"] + X,\n",
    "            \"xlabel\": r\"$p_T$ (GeV)\",\n",
    "            \"ylabel\": \"$p_T^{reco}$\",\n",
    "            \"xmin\": 20,\n",
    "            \"xmax\": 80,\n",
    "        },\n",
    "        \"RecoDataeta\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\"] + X,\n",
    "            \"xlabel\": r\"$\\eta$\",\n",
    "            \"ylabel\": \"$\\eta^{reco}$\",\n",
    "            \"xmin\": -5,\n",
    "            \"xmax\": 5,\n",
    "        },\n",
    "        \"RecoDataphi\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\", \"RecoDataeta\"] + X,\n",
    "            \"xlabel\": r\"$\\phi$\",\n",
    "            \"ylabel\": \"$\\phi^{reco}$\",\n",
    "            \"xmin\": -3.2,\n",
    "            \"xmax\": 3.2,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Load and explore raw (unscaled) dataframes\n",
    "\n",
    "\n",
    "all_variable_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "]\n",
    "all_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "    \"tau\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb2b41-f128-45d0-993a-cfdd421b768d",
   "metadata": {},
   "source": [
    "## 3.2: Needed Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29814968-1591-409f-9655-42a962b5eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3c5cc4-9f36-4642-b4c2-9fc9e3150955",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Load unscaled dataframes ###################################\n",
    "# @memory.cache\n",
    "def load_raw_data():\n",
    "    \"\"\"Dont use AUTOREGRESSIVE_DIST_NAME for training of any variable. \n",
    "    For mass evaluation: dont use AUTOREGRESSIVE_DIST_NAME. For pT evaluation use AUTOREGRESSIVE_DIST_NAME \n",
    "    as the distribution predicted by mass, etc.  \"\"\"\n",
    "    print(f\"\\nSUBSAMPLE = {SUBSAMPLE}\\n\")\n",
    "    raw_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    raw_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"validation_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    raw_test_data = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"test_data_10M_2.csv\"), \n",
    "    usecols=all_cols, \n",
    "    nrows=SUBSAMPLE\n",
    "    )\n",
    "\n",
    "    print(\"\\n RAW TRAIN DATA\\n\")\n",
    "    print(raw_train_data.shape)\n",
    "    raw_train_data.describe()  # unscaled\n",
    "    print(\"\\n RAW TEST DATA\\n\")\n",
    "    print(raw_test_data.shape)\n",
    "    raw_test_data.describe()  # unscaled\n",
    "\n",
    "    return raw_train_data, raw_test_data, raw_valid_data\n",
    "\n",
    "\n",
    "########## Generate scaled data###############\n",
    "# scaled_train_data = L_scale_df(raw_train_data, title='scaled_train_data_10M_2.csv',\n",
    "#                              save=True)\n",
    "# print('\\n\\n')\n",
    "# scaled_test_data = L_scale_df(raw_test_data,  title='scaled_test_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "# print('\\n\\n')\n",
    "\n",
    "# scaled_valid_data = L_scale_df(raw_valid_data,  title='scaled_valid_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "\n",
    "# explore_data(df=scaled_train_data, title='Braden Kronheim-L-scaled Dataframe', scaled=True)\n",
    "\n",
    "################ Load scaled data##############\n",
    "@utils.time_type_of_func(tuning_or_training=\"loading\")\n",
    "# @memory.cache\n",
    "def load_scaled_dataframes():\n",
    "    print(\"SCALED TRAIN DATA\")\n",
    "    scaled_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    print(\"TRAINING FEATURES\\n\", scaled_train_data.head())\n",
    "\n",
    "    scaled_test_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_test_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    scaled_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_valid_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    return scaled_train_data, scaled_test_data, scaled_valid_data\n",
    "\n",
    "\n",
    "#######################################\n",
    "#\n",
    "# # print('\\nTESTING FEATURES\\n', scaled_test_data.head())\n",
    "\n",
    "# print('\\ntrain set shape:',  scaled_train_data.shape)\n",
    "# print('\\ntest set shape:  ', scaled_test_data.shape)\n",
    "# # print('validation set shape:', valid_data.shape)\n",
    "# @memory.cache\n",
    "def get_train_scale_dict(USE_BRADEN_SCALING):\n",
    "    if USE_BRADEN_SCALING==True:\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(scaled_train_data)\n",
    "        print(\"BRADEN SCALING DICTIONARY\")\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    else:\n",
    "        print(\"NORMAL UNSCALED DICTIONARY\")\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(raw_train_data)\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    return TRAIN_SCALE_DICT\n",
    "\n",
    "\n",
    "################################ SPLIT###########\n",
    "# @memory.cache\n",
    "def T(variable, scaled_df):\n",
    "    if variable == \"pT\":\n",
    "        L_pT_gen = scaled_df[\"genDatapT\"]\n",
    "        L_pT_reco = scaled_df[\"RecoDatapT\"]\n",
    "        target = (L_pT_reco + 10) / (L_pT_gen + 10)\n",
    "    if variable == \"eta\":\n",
    "        L_eta_gen = scaled_df[\"genDataeta\"]\n",
    "        L_eta_reco = scaled_df[\"RecoDataeta\"]\n",
    "        target = (L_eta_reco + 10) / (L_eta_gen + 10)\n",
    "    if variable == \"phi\":\n",
    "        L_phi_gen = scaled_df[\"genDataphi\"]\n",
    "        L_phi_reco = scaled_df[\"RecoDataphi\"]\n",
    "        target = (L_phi_reco + 10) / (L_phi_gen + 10)\n",
    "    if variable == \"m\":\n",
    "        L_m_gen = scaled_df[\"genDatam\"]\n",
    "        L_m_reco = scaled_df[\"RecoDatam\"]\n",
    "        target = (L_m_reco + 10) / (L_m_gen + 10)\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def split_t_x(df, target, input_features):\n",
    "    \"\"\"Get teh target as the ratio, according to the T equation\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_train_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def split_t_x_test(df, target, input_features):\n",
    "    \"\"\"Get teh target as the ratio, according to the T equation\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_test_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# @memory.cache\n",
    "def normal_split_t_x(df, target, input_features):\n",
    "    # change from pandas dataframe format to a numpy\n",
    "    # array of the specified types\n",
    "    # t = np.array(df[target])\n",
    "    t = np.array(df[target])\n",
    "    x = np.array(df[input_features])\n",
    "    return t, x\n",
    "\n",
    "\n",
    "################ Apply Z scaling############\n",
    "def z(x):\n",
    "    eps = 1e-20\n",
    "    return (x - np.mean(x)) / (np.std(x) + eps)\n",
    "\n",
    "\n",
    "def z_inverse(xprime, x):\n",
    "    return xprime * np.std(x) + np.mean(x)\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z2(x, mean, std):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x ([type]): [description]\n",
    "        mean ([type]): [description]\n",
    "        std ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    scaled = (x - mean) / (std + eps)\n",
    "    return np.array(scaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "def z_inverse(xprime, x):\n",
    "    unscaled = xprime * np.std(x) + np.mean(x)\n",
    "    return np.array(unscaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z_inverse2(xprime, train_mean, train_std):\n",
    "    \"\"\"mean original train mean, std: original. Probably not needed\"\"\"\n",
    "    return xprime * train_std + train_mean\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x):\n",
    "    \"\"\"TO ensure this z scaling is only applied once to the training features, we use a generator.\n",
    "    This doesn't change the shapes of anything, just applies z to all the feature columns other than tau\"\"\"\n",
    "    NFEATURES = train_x.shape[1]\n",
    "    for i in range(NFEATURES - 1):\n",
    "        variable = list(TRAIN_SCALE_DICT)[i]\n",
    "        train_mean = float(TRAIN_SCALE_DICT[variable][\"mean\"])\n",
    "        train_std = float(TRAIN_SCALE_DICT[variable][\"std\"])\n",
    "        train_x[:, i] = z2(train_x[:, i], mean=train_mean, std=train_std)\n",
    "        test_x[:, i] = z2(test_x[:, i], mean=train_mean, std=train_std)\n",
    "        valid_x[:, i] = z2(valid_x[:, i], mean=train_mean, std=train_std)\n",
    "    yield train_x\n",
    "    yield test_x\n",
    "    yield valid_x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_targets(train_t, test_t, valid_t):\n",
    "    train_mean = np.mean(train_t)\n",
    "    train_std = np.std(train_t)\n",
    "    train_t_ = z2(train_t, mean=train_mean, std=train_std)\n",
    "    test_t_ = z2(test_t, mean=train_mean, std=train_std)\n",
    "    valid_t_ = z2(valid_t, mean=train_mean, std=train_std)\n",
    "\n",
    "    yield train_t_\n",
    "    yield test_t_\n",
    "    yield valid_t_\n",
    "\n",
    "\n",
    "# @utils.debug\n",
    "def save_model(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "# @utils.debug\n",
    "def load_model(PATH, PARAMS):\n",
    "    # n_layers = int(BEST_PARAMS[\"n_layers\"])\n",
    "    # hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "    # dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "    # optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "    # learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "    # batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "    model = utils.RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    # OR\n",
    "    # model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED! Also, use dictionary \".pth\" which has both the model state dict and the PARAMS dict\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def simple_eval(model, test_x_z_scaled):\n",
    "    model.eval()\n",
    "    # evaluate on the scaled features\n",
    "    valid_x_tensor = torch.from_numpy(test_x_z_scaled).float()\n",
    "    # valid_x_tensor=torch.from_numpy(train_x).float()\n",
    "    pred = model(valid_x_tensor)\n",
    "    p = pred.detach().numpy()\n",
    "    # if USE_BRADEN_SCALING:\n",
    "    #     fig, ax = plt.subplots(1,1)\n",
    "    #     label=FIELDS[target]['ylabel']\n",
    "    #     ax.hist(p, label=f'Predicted post-z ratio for {label}', alpha=0.4, density=True)\n",
    "    #     # orig_ratio = z(T('m', scaled_df=scaled_train_data))\n",
    "    #     orig_ratio = z(T('m', scaled_df=scaled_test_data))\n",
    "    #     print(orig_ratio[:5])\n",
    "    #     ax.hist(orig_ratio, label = f'original post-z ratio for {label}', alpha=0.4,density=True)\n",
    "    #     ax.grid()\n",
    "    #     set_axes(ax, xlabel='predicted $T$')\n",
    "    # print('predicted ratio shape: ', p.shape)\n",
    "    return p\n",
    "\n",
    "def get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME):\n",
    "        \n",
    "    print(f'Test (evaluation) Data is Autoregressive, loading {AUTOREGRESSIVE_DIST_NAME}')\n",
    "    eval_data = pd.read_csv(\n",
    "        os.path.join(\n",
    "            IQN_BASE,\n",
    "            \"JupyterBook\",\n",
    "            \"Cluster\",\n",
    "            \"EVALUATE\",\n",
    "            AUTOREGRESSIVE_DIST_NAME,\n",
    "        )\n",
    "    )\n",
    "    return eval_data\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def get_hist(label):\n",
    "    \"\"\"label could be \"pT\", \"eta\", \"phi\", \"m\" \"\"\"\n",
    "    predicted_label_counts, label_edges = np.histogram(\n",
    "        JETS_DICT[\"Predicted_RecoData\" + label][\"dist\"],\n",
    "        range=JETS_DICT[\"Predicted_RecoData\" + label][\"range\"],\n",
    "        bins=bins,\n",
    "    )\n",
    "    real_label_counts, _ = np.histogram(\n",
    "        JETS_DICT[\"Real_RecoData\" + label][\"dist\"],\n",
    "        range=JETS_DICT[\"Real_RecoData\" + label][\"range\"],\n",
    "        bins=bins,\n",
    "    )\n",
    "    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2\n",
    "\n",
    "    return real_label_counts, predicted_label_counts, label_edges\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def get_hist_simple(predicted_dist, target):\n",
    "    \n",
    "    range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "    bins=50\n",
    "    predicted_label_counts, label_edges = np.histogram(\n",
    "        predicted_dist, range=range_, bins=bins\n",
    "    )\n",
    "    \n",
    "    \n",
    "    real_label_counts, _ = np.histogram(REAL_DIST, range=range_, bins=bins)\n",
    "    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2\n",
    "    return real_label_counts, predicted_label_counts, label_edges\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def plot_one(\n",
    "    target, real_edges, real_counts, predicted_counts, save_plot=False, PARAMS=None, JUPYTER=True\n",
    "):\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2, 1, figsize=(3.5 * 3 / 2.5, 3.8), gridspec_kw={\"height_ratios\": [2, 0.5]}\n",
    "    )\n",
    "    ax1.step(\n",
    "        real_edges, real_counts / norm_data, where=\"mid\", color=\"k\", linewidth=0.5\n",
    "    )  # step real_count_pt\n",
    "    ax1.step(\n",
    "        real_edges,\n",
    "        predicted_counts / norm_IQN,\n",
    "        where=\"mid\",\n",
    "        color=\"#D7301F\",\n",
    "        linewidth=0.5,\n",
    "    )  # step predicted_count_pt\n",
    "    ax1.scatter(\n",
    "        real_edges,\n",
    "        real_counts / norm_data,\n",
    "        label=\"reco\",\n",
    "        color=\"k\",\n",
    "        facecolors=\"none\",\n",
    "        marker=\"o\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax1.scatter(\n",
    "        real_edges,\n",
    "        predicted_counts / norm_IQN,\n",
    "        label=\"predicted\",\n",
    "        color=\"#D7301F\",\n",
    "        marker=\"x\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax1.set_xlim(range_)\n",
    "    ax1.set_ylim(0, max(predicted_counts / norm_IQN) * 1.1)\n",
    "    ax1.set_ylabel(\"counts\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    ratio = (predicted_counts / norm_IQN) / (real_counts / norm_data)\n",
    "    ax2.scatter(\n",
    "        real_edges, ratio, color=\"r\", marker=\"x\", s=5, linewidth=0.5\n",
    "    )  # PREDICTED (IQN)/Reco (Data)\n",
    "    ax2.scatter(\n",
    "        real_edges,\n",
    "        ratio / ratio,\n",
    "        color=\"k\",\n",
    "        marker=\"o\",\n",
    "        facecolors=\"none\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax2.set_xlim(range_)\n",
    "    ax2.set_xlabel(FIELDS[target][\"xlabel\"])\n",
    "    ax2.set_ylabel(\n",
    "        r\"$\\frac{\\textnormal{predicted}}{\\textnormal{reco}}$\"\n",
    "        #    , fontsize=10\n",
    "    )\n",
    "    ax2.set_ylim((YLIM))\n",
    "    ax2.set_xlim(range_)\n",
    "    ax2.set_yticklabels([])\n",
    "    if JUPYTER==True:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(wspace=0.5, hspace=0.2)\n",
    "        fig.subplots_adjust(wspace=0.0, hspace=0.1)\n",
    "        plt.axis('off')\n",
    "\n",
    "    # plt.gca().set_position([0, 0, 1, 1])\n",
    "    if save_plot:\n",
    "        plot_filename = utils.get_model_filename(target, PARAMS).split(\".dict\")[0] + \".png\"\n",
    "        plt.savefig(\n",
    "            os.path.join(IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", plot_filename)\n",
    "        )\n",
    "\n",
    "    \n",
    "    # fig.show()\n",
    "    # plt.show();\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.gca().set_position([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3336b47e-6e8a-43ba-a93a-5067d6b44dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUBSAMPLE = None\n",
      "\n",
      "\n",
      " RAW TRAIN DATA\n",
      "\n",
      "(8000000, 9)\n",
      "\n",
      " RAW TEST DATA\n",
      "\n",
      "(1000000, 9)\n"
     ]
    }
   ],
   "source": [
    "raw_train_data, raw_test_data, raw_valid_data = load_raw_data()\n",
    "\n",
    "# Load scaled data\n",
    "# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d40d613-52be-4b51-ab10-822df7976083",
   "metadata": {},
   "source": [
    "## 3.3: Evaluate Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42c5cef-a246-45ea-9ee4-d4d22452a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n",
      "spliting data for RecoDatam\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 5)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 5)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 5)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 3.27223764e+01  6.98189368e-04 -8.95543973e-04  6.96116528e+00\n",
      "  5.00485136e-01] [15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]\n",
      "[ 3.26952341e+01 -1.78188172e-03 -3.83090331e-04  6.96299435e+00\n",
      "  4.99915289e-01] [14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]\n",
      "5.5514112643334546 2.664124544901276\n",
      "5.555567451922438 2.664339857066051\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[ 1.81700902e-03  1.12510099e-03 -2.82526482e-04 -6.57626105e-04\n",
      "  5.00485136e-01] [1.01748627 0.9999745  0.99989115 0.99987283 0.28852734]\n",
      "[ 1.61650249e-15 -1.10134124e-18 -3.12905257e-17  5.01036723e-15\n",
      "  4.99915289e-01] [1.         1.         1.         1.         0.28867295]\n",
      "-0.0015599314696879494 0.9999191874249058\n",
      "6.996216939114674e-16 1.0000000000000002\n",
      "<class 'str'>\n",
      "This model was trained for 5000000 iteration, which is  640.0 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (3): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.3)\n",
      "    (5): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (6): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (9): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.3)\n",
      "    (11): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (12): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.3)\n",
      "    (14): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (15): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.3)\n",
      "    (17): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (18): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.3)\n",
      "    (20): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (21): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.3)\n",
      "    (23): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (24): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): LeakyReLU(negative_slope=0.3)\n",
      "    (26): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "EVALUATION DATA NEW INDEX\n",
      "    RecoDatam  genDatapT  genDataeta  genDataphi  genDatam       tau\n",
      "0  30.548403    43.6113    0.824891    -1.26949   5.93310  0.250046\n",
      "1  42.843403    43.6113    0.824891    -1.26949   5.93310  0.847493\n",
      "2  26.045616    26.0153    3.529970     1.55495   7.41270  0.851995\n",
      "3  25.753584    28.4944   -1.159650     1.82602   7.84157  0.052378\n",
      "4  25.384985    21.9840    2.747660     2.03085   5.18315  0.542549\n",
      "norm_data 1000000 \n",
      "norm IQN 1000000 \n",
      "norm_autoregressive 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:388: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 302.4x273.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: plot the loss curves (train and valid) of all 4 networks on the same plot,\n",
    "# and with the learning rate plotted on the same plot but on a different y axis \n",
    "# (x axis being iteration, with marks indicating epochs)\n",
    "\n",
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "########################################################################################\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING==True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = split_t_x(\n",
    "        df=scaled_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "        df=raw_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "        df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = normal_split_t_x(\n",
    "        df=raw_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "######################################################\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "\n",
    "NFEATURES = train_x.shape[1]\n",
    "TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "# to features\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "###########################################################\n",
    "# Get the  parameters for this model and training\n",
    "PARAMS_m = {\n",
    "\"n_layers\": int(9),\n",
    "\"hidden_size\": int(5),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(1e-1),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(1024),\n",
    "    'n_iterations': int(5e6),\n",
    "}\n",
    "optimizer_name = PARAMS_m[\"optimizer_name\"]\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS = PARAMS_m[\"n_iterations\"]\n",
    "BATCHSIZE = PARAMS_m[\"batch_size\"]\n",
    "comment = \"\"\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(\n",
    "    f\"This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs\"\n",
    ")\n",
    "\n",
    "# 'Trained_IQNx4_%s_TUNED.dict' % target\n",
    "filename_model = utils.get_model_filename(target, PARAMS_m)\n",
    "# OR, if you know a model filename directly, you can also specify it, but you have to make sure its parameters are the same\n",
    "# Nominal one is 'Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict', also in backup\n",
    "# filename_model='Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict'\n",
    "trained_models_dir = \"trained_models\"\n",
    "utils.mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE,  # the loaction of the repo\n",
    "    \"JupyterBook\",  # up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\",\n",
    "    \"TRAIN\",\n",
    "    trained_models_dir,  # /trained_models\n",
    "    filename_model,  # utils.get_model_filename has the saved file format\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "# Get predicted distribution\n",
    "p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "\n",
    "range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "bins = 50\n",
    "REAL_RAW_DATA = raw_test_data\n",
    "\n",
    "YLIM = (0.8, 1.2)\n",
    "###########GET REAL DIST###########\n",
    "REAL_RAW_DATA = REAL_RAW_DATA[\n",
    "    [\"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\", \"RecoDatam\"]\n",
    "]\n",
    "REAL_RAW_DATA.columns = [\"realpT\", \"realeta\", \"realphi\", \"realm\"]\n",
    "REAL_DIST = REAL_RAW_DATA[\"realm\"]\n",
    "norm_data = REAL_RAW_DATA.shape[0]\n",
    "#############GET EVALUATION DIST#############\n",
    "raw_test_data.describe()\n",
    "m_reco = raw_test_data[\"RecoDatam\"]\n",
    "m_gen = raw_test_data[\"genDatam\"]\n",
    "# plt.hist(m_reco,label=r'$m_{gen}^{test \\ data}$');plt.legend();plt.show()\n",
    "\n",
    "\n",
    "def descale_Braden_scaled_prediction(label, p):\n",
    "    \"\"\"Label could be m. p is the outcome of the model evaluation, e.g. \n",
    "    IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "    p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "    \n",
    "    \"\"\"\n",
    "    # make sure you've set braden scaling global variable to use this function.\n",
    "    assert USE_BRADEN_SCALING==True\n",
    "    orig_ratio = T(label, scaled_df=scaled_train_data)\n",
    "    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))\n",
    "    L_obs = L(orig_observable=m_gen, label=label)\n",
    "    z_inv_f = z_inv_f.flatten()\n",
    "    print(z_inv_f.shape)\n",
    "\n",
    "    factor = (z_inv_f * (L_obs + 10)) - 10\n",
    "    label_pred = L_inverse(L_observable=factor, label=label)\n",
    "    return label_pred\n",
    "    \n",
    "    \n",
    "m_pred = z_inverse2(\n",
    "    xprime=p,\n",
    "    train_mean=TRAIN_SCALE_DICT[target][\"mean\"],\n",
    "    train_std=TRAIN_SCALE_DICT[target][\"std\"],\n",
    ")\n",
    "m_pred = m_pred.flatten()\n",
    "\n",
    "# Get histogram of predicted distribution\n",
    "real_label_counts_m, predicted_label_counts_m, label_edges_m = get_hist_simple(\n",
    "    predicted_dist=m_pred, target=target\n",
    ")\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "# Get evaluation data\n",
    "eval_data = pd.read_csv(DATA_DIR + \"/test_data_10M_2.csv\")\n",
    "ev_features = features\n",
    "eval_data = eval_data[ev_features]\n",
    "# save new distribution (m) in the eval data as autoregressive eval for next IQN\n",
    "eval_data[target] = m_pred\n",
    "\n",
    "new_cols = [target] + features\n",
    "eval_data = eval_data.reindex(columns=new_cols)\n",
    "print(\"EVALUATION DATA NEW INDEX\\n\", eval_data.head())\n",
    "\n",
    "eval_data.to_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load this saved predited autoregressive distribution\n",
    "AUTOREGRESSIVE_DIST = pd.read_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]\n",
    "# get normalization values\n",
    "norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]\n",
    "norm_IQN = norm_autoregressive\n",
    "print(\n",
    "    \"norm_data\",\n",
    "    norm_data,\n",
    "    \"\\nnorm IQN\",\n",
    "    norm_IQN,\n",
    "    \"\\nnorm_autoregressive\",\n",
    "    norm_autoregressive,\n",
    ")\n",
    "\n",
    "# Finally, plot predicted distribution\n",
    "\n",
    "plot_one(\n",
    "    target=target,\n",
    "    real_edges=label_edges_m,\n",
    "    real_counts=real_label_counts_m,\n",
    "    predicted_counts=predicted_label_counts_m,\n",
    "    save_plot=True,\n",
    "    PARAMS=PARAMS_m\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df205105-a24d-4694-a182-ef84c08bfd4c",
   "metadata": {},
   "source": [
    "## 3.4: Evaluate $p_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed548cae-19f9-44d7-b30f-1328fd8fb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['RecoDatam', 'genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatapT\n",
      "USING NEW DATASET\n",
      "\n",
      "\n",
      "SUBSAMPLE = None\n",
      "\n",
      "\n",
      " RAW TRAIN DATA\n",
      "\n",
      "(8000000, 9)\n",
      "\n",
      " RAW TEST DATA\n",
      "\n",
      "(1000000, 9)\n",
      "spliting autoregressive evaluation data for RecoDatapT\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 6)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[ 2.59587    29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [ 6.25659    41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [ 6.11213    35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [ 4.17483    26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 6)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 6)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 5.55141126e+00  3.27223764e+01  6.98189368e-04 -8.95543973e-04\n",
      "  6.96116528e+00  5.00485136e-01] [ 2.66412454 15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]\n",
      "[ 5.55556745e+00  3.26952341e+01 -1.78188172e-03 -3.83090331e-04\n",
      "  6.96299435e+00  4.99915289e-01] [ 2.66433986 14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]\n",
      "32.881453465999996 16.02400426348493\n",
      "32.86720151648752 15.829355769531851\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[-1.81710707e+00  1.48455353e+01  5.96132264e-04 -2.50379668e+00\n",
      " -1.63658184e+00  5.00485136e-01] [0.17834627 6.89519305 1.2152514  0.65207164 0.17568487 0.28852734]\n",
      "[-1.81682884e+00  1.48332220e+01 -7.71183141e-04 -2.50361243e+00\n",
      " -1.63646629e+00  4.99915289e-01] [0.17836068 6.77669391 1.21528238 0.65214262 0.17570722 0.28867295]\n",
      "0.0009003493079555966 1.0122966781963252\n",
      "-1.2048033681821834e-15 1.0000000000000002\n",
      "<class 'str'>\n",
      "This model was trained for 300000 iteration, which is  19.2 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (3): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.3)\n",
      "    (5): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (6): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (9): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.3)\n",
      "    (11): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (12): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.3)\n",
      "    (14): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (15): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.3)\n",
      "    (17): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (18): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.3)\n",
      "    (20): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (21): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.3)\n",
      "    (23): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (24): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): LeakyReLU(negative_slope=0.3)\n",
      "    (26): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (27): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (28): LeakyReLU(negative_slope=0.3)\n",
      "    (29): Linear(in_features=6, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "EVALUATION DATA NEW INDEX\n",
      "    RecoDatam  RecoDatapT  genDatapT  genDataeta  genDataphi  genDatam  \\\n",
      "0    5.80270     44.3274    43.6113    0.824891    -1.26949   5.93310   \n",
      "1    5.80270     44.3274    43.6113    0.824891    -1.26949   5.93310   \n",
      "2    4.81403     27.4750    26.0153    3.529970     1.55495   7.41270   \n",
      "3    7.06425     33.8797    28.4944   -1.159650     1.82602   7.84157   \n",
      "4    4.08061     23.3141    21.9840    2.747660     2.03085   5.18315   \n",
      "\n",
      "        tau  \n",
      "0  0.250046  \n",
      "1  0.847493  \n",
      "2  0.851995  \n",
      "3  0.052378  \n",
      "4  0.542549  \n",
      "norm_data 1000000 \n",
      "norm IQN 1000000 \n",
      "norm_autoregressive 1000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 302.4x273.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = \"RecoDatapT\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "PREVIOUS_AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime_pT_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "raw_train_data, raw_test_data, raw_valid_data = load_raw_data()\n",
    "\n",
    "\n",
    "# Load scaled data\n",
    "# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()\n",
    "\n",
    "\n",
    "################################################## Load Evaluation Data\n",
    "#eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "# Or test on actual test (evaluation) data for development\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING == True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "        \n",
    "    ##### WHAT MATTERS IS TEST (EVALUATION)\n",
    "\n",
    "    test_t, test_x = split_t_x(\n",
    "        df=scaled_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting autoregressive evaluation data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "            df=raw_train_data, target=target, input_features=features\n",
    "        )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "        df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    ##### WHAT MATTERS IS TEST (EVALUATION)\n",
    "    test_t, test_x = normal_split_t_x(\n",
    "        df=raw_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "######################################################\n",
    "# Replace test_x with eval_data\n",
    "\n",
    "# ev_features = features\n",
    "# eval_data_df = eval_data[ev_features]\n",
    "# eval_data = np.array(eval_data_df)\n",
    "# test_x = eval_data\n",
    "\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "    \n",
    "# eval_data=raw_train_data[:raw_test_data.shape[0]]\n",
    "# test_x = np.array\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "\n",
    "NFEATURES = train_x.shape[1]\n",
    "# GET EVALUATION DATASET\n",
    "# eval_data= get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME=PREVIOUS_AUTOREGRESSIVE_DIST_NAME)\n",
    "# test_x = np.array(eval_data[features])\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "# to features\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "###########################################################\n",
    "# Get the  parameters for this model and training\n",
    "PARAMS_pT =  {\n",
    "\"n_layers\": int(10),\n",
    "\"hidden_size\": int(6),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(1e-2),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(512),\n",
    "    'n_iterations': int(3e5),\n",
    "}\n",
    "\n",
    "optimizer_name = PARAMS_pT[\"optimizer_name\"]\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS = PARAMS_pT[\"n_iterations\"]\n",
    "BATCHSIZE = PARAMS_pT[\"batch_size\"]\n",
    "comment = \"\"\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(\n",
    "    f\"This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs\"\n",
    ")\n",
    "\n",
    "\n",
    "filename_model = utils.get_model_filename(target, PARAMS_pT)\n",
    "# filename_model = 'Trained_IQNx4_RecoDatapT_ 13_layer6_hiddenLeakyReLU_activation1024_batchsize200_Kiteration.dict'\n",
    "filename_model = 'Trained_IQNx4_RecoDatapT_10_layer6_hiddenLeakyReLU_activation512_batchsize300_Kiteration.dict'\n",
    "trained_models_dir = \"trained_models\"\n",
    "utils.mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE,  # the loaction of the repo\n",
    "    \"JupyterBook\",  # up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\",\n",
    "    \"TRAIN\",\n",
    "    trained_models_dir,  # /trained_models\n",
    "    filename_model,  # utils.get_model_filename has the saved file format\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "IQN_pT = load_model(PATH_model, PARAMS_pT)\n",
    "# Get predicted distribution\n",
    "p = simple_eval(IQN_pT, test_x_z_scaled)\n",
    "\n",
    "range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "bins = 50\n",
    "REAL_RAW_DATA = raw_test_data\n",
    "\n",
    "YLIM = (0.8, 1.2)\n",
    "###########GET REAL DIST###########\n",
    "REAL_RAW_DATA = REAL_RAW_DATA[\n",
    "    [\"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\", \"RecoDatam\"]\n",
    "]\n",
    "REAL_RAW_DATA.columns = [\"realpT\", \"realeta\", \"realphi\", \"realm\"]\n",
    "REAL_DIST = REAL_RAW_DATA[\"realpT\"]\n",
    "norm_data = REAL_RAW_DATA.shape[0]\n",
    "#############GET EVALUATION DIST#############\n",
    "raw_test_data.describe()\n",
    "pT_reco = raw_test_data[\"RecoDatapT\"]\n",
    "pT_gen = raw_test_data[\"genDatapT\"]\n",
    "# plt.hist(m_reco,label=r'$m_{gen}^{test \\ data}$');plt.legend();plt.show()\n",
    "\n",
    "def descale_Braden_scaled_prediction(label, p):\n",
    "    \"\"\"Label could be m. p is the outcome of the model evaluation, e.g. \n",
    "    IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "    p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "\n",
    "    \"\"\"\n",
    "    # make sure you've set braden scaling global variable to use this function.\n",
    "    assert USE_BRADEN_SCALING==True\n",
    "    orig_ratio = T(label, scaled_df=scaled_train_data)\n",
    "    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))\n",
    "    L_obs = L(orig_observable=pT_gen, label=label)\n",
    "    z_inv_f = z_inv_f.flatten()\n",
    "    print(z_inv_f.shape)\n",
    "\n",
    "    factor = (z_inv_f * (L_obs + 10)) - 10\n",
    "    label_pred = L_inverse(L_observable=factor, label=label)\n",
    "    return label_pred\n",
    "\n",
    "\n",
    "pT_pred = z_inverse2(\n",
    "    xprime=p,\n",
    "    train_mean=TRAIN_SCALE_DICT[target][\"mean\"],\n",
    "    train_std=TRAIN_SCALE_DICT[target][\"std\"],\n",
    ")\n",
    "pT_pred = pT_pred.flatten()\n",
    "\n",
    "# Get histogram of predicted distribution\n",
    "real_label_counts_pT, predicted_label_counts_pT, label_edges_pT = get_hist_simple(\n",
    "    predicted_dist=pT_pred, target=target\n",
    ")\n",
    "\n",
    "# Get evaluation data as test data for development\n",
    "\n",
    "eval_data_df=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')#[features]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_cols = [\"RecoDatam\", target] + X\n",
    "eval_data_df = eval_data_df.reindex(columns=new_cols)\n",
    "print(\"EVALUATION DATA NEW INDEX\\n\", eval_data_df.head())\n",
    "# save \n",
    "eval_data_df.to_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load this saved predited autoregressive distribution\n",
    "AUTOREGRESSIVE_DIST = pd.read_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]\n",
    "# get normalization values\n",
    "norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]\n",
    "norm_IQN = norm_autoregressive\n",
    "print(\n",
    "    \"norm_data\",\n",
    "    norm_data,\n",
    "    \"\\nnorm IQN\",\n",
    "    norm_IQN,\n",
    "    \"\\nnorm_autoregressive\",\n",
    "    norm_autoregressive,\n",
    ")\n",
    "\n",
    "# Finally, plot predicted distribution\n",
    "plot_one(\n",
    "    target=target,\n",
    "    real_edges=label_edges_pT,\n",
    "    real_counts=real_label_counts_pT,\n",
    "    predicted_counts=predicted_label_counts_pT,\n",
    "    save_plot=True,\n",
    "    PARAMS=PARAMS_pT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48801bd-6864-425d-b4cd-fbe5fdf89790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
