{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d6920b-1452-45a8-8751-0b7d6e8e8fd0",
   "metadata": {},
   "source": [
    "# IQNx4: Chapter 3: Autoregressive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c623666-defa-4463-adfe-f7667c7d385d",
   "metadata": {},
   "source": [
    "## 3.1: Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31994c2b-a804-4348-a5a9-8bd8c89ff64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using torch version 1.9.0\n",
      "matplotlib version=  3.5.1\n",
      "using (optional) optuna version 2.8.0\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "using DATA_DIR=/home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: MatplotlibDeprecationWarning: Support for setting an rcParam that expects a str value to a non-str value is deprecated since 3.5 and support will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import scipy as sp; import scipy.stats as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"using torch version {torch.__version__}\")\n",
    "# use numba's just-in-time compiler to speed things up\n",
    "# from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "\n",
    "print(\"matplotlib version= \", mp.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reset matplotlib stle/parameters\n",
    "# reset matplotlib parameters to their defaults\n",
    "# plt.style.use('seaborn-deep')\n",
    "# mp.rcParams['agg.path.chunksize'] = 10000\n",
    "font_legend = 15\n",
    "font_axes = 15\n",
    "# %matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from IPython.display import Image, display\n",
    "# from importlib import import_module\n",
    "# import plotly\n",
    "try:\n",
    "    import optuna\n",
    "\n",
    "    print(f\"using (optional) optuna version {optuna.__version__}\")\n",
    "except Exception:\n",
    "    print(\"optuna is only used for hyperparameter tuning, not critical!\")\n",
    "    pass\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# import sympy as sy\n",
    "# import ipywidgets as wid;\n",
    "\n",
    "# update fonts\n",
    "font = {\"family\": \"serif\", \"size\": 10}\n",
    "mp.rc(\"font\", **font)\n",
    "\n",
    "# set usetex = False if LaTex is not\n",
    "# available on your system or if the\n",
    "# rendering is too slow\n",
    "mp.rcParams.update({\"text.usetex\": True})\n",
    "# plt.rcParams['text.usetex'] = True\n",
    "mp.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]  # for \\text command\n",
    "\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "# seed = 128\n",
    "# rnd  = np.random.RandomState(seed)\n",
    "# sometimes jupyter doesnt initialize MathJax automatically for latex, so do this:\n",
    "\n",
    "try:\n",
    "    IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "    print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "    utils_dir = os.path.join(IQN_BASE, 'utils/')\n",
    "    sys.path.append(utils_dir)\n",
    "    import utils\n",
    "\n",
    "    # usually its not recommended to import everything from a module, but we know\n",
    "    # whats in it so its fine\n",
    "    # from utils import *\n",
    "    print(\"DATA directory also properly set, in %s\" % os.environ[\"DATA_DIR\"])\n",
    "except Exception:\n",
    "    # IQN_BASE=os.getcwd()\n",
    "    print(\n",
    "        \"\"\"\\nBASE directory not properly set. Read repo README.    If you need a function from utils, use the decorator below, or add utils to sys.path\\n\n",
    "    You can also do \n",
    "    os.environ['IQN_BASE']=<ABSOLUTE PATH FOR THE IQN REPO>\n",
    "    or\n",
    "    os.environ['IQN_BASE']=os.getcwd()\"\"\"\n",
    "    )\n",
    "    pass\n",
    "\n",
    "# from IQNx4_utils import *\n",
    "IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "utils_dir = os.path.join(IQN_BASE, \"utils/\")\n",
    "sys.path.append(utils_dir)\n",
    "# usually its not recommended to import everything from a module, but we know\n",
    "# whats in it so its fine\n",
    "\n",
    "# or use joblib for caching on disk\n",
    "from joblib import Memory\n",
    "\n",
    "\n",
    "################################### CONFIGURATIONS ###################################\n",
    "DATA_DIR = os.environ[\"DATA_DIR\"]\n",
    "print(f\"using DATA_DIR={DATA_DIR}\")\n",
    "JUPYTER = False\n",
    "use_subsample = False\n",
    "# use_subsample=True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "# memory = Memory(DATA_DIR)\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p(p_T)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "    \"RecoDataeta\": \"$p(\\eta)$\",\n",
    "    \"RecoDataphi\": \"$p(\\phi)$\",\n",
    "    \"RecoDatam\": \"$p(m)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "}\n",
    "\n",
    "loss_y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p_T^{reco}$\",\n",
    "    \"RecoDataeta\": \"$\\eta^{reco}$\",\n",
    "    \"RecoDataphi\": \"$\\phi^{reco}$\",\n",
    "    \"RecoDatam\": \"$m^{reco}$\",\n",
    "}\n",
    "\n",
    "\n",
    "################################### SET DATA CONFIGURATIONS ###################################\n",
    "X = [\"genDatapT\", \"genDataeta\", \"genDataphi\", \"genDatam\", \"tau\"]\n",
    "\n",
    "# set order of training:\n",
    "# pT_first: pT->>m->eta->phi\n",
    "# m_first: m->pT->eta->phi\n",
    "\n",
    "\n",
    "ORDER = \"m_First\"\n",
    "\n",
    "if ORDER == \"m_First\":\n",
    "    FIELDS = {\n",
    "        \"RecoDatam\": {\n",
    "            \"inputs\": X,\n",
    "            \"xlabel\": r\"$m$ (GeV)\",\n",
    "            \"ylabel\": \"$m^{reco}$\",\n",
    "            \"xmin\": 0,\n",
    "            \"xmax\": 25,\n",
    "        },\n",
    "        \"RecoDatapT\": {\n",
    "            \"inputs\": [\"RecoDatam\"] + X,\n",
    "            \"xlabel\": r\"$p_T$ (GeV)\",\n",
    "            \"ylabel\": \"$p_T^{reco}$\",\n",
    "            \"xmin\": 20,\n",
    "            \"xmax\": 80,\n",
    "        },\n",
    "        \"RecoDataeta\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\"] + X,\n",
    "            \"xlabel\": r\"$\\eta$\",\n",
    "            \"ylabel\": \"$\\eta^{reco}$\",\n",
    "            \"xmin\": -5,\n",
    "            \"xmax\": 5,\n",
    "        },\n",
    "        \"RecoDataphi\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\", \"RecoDataeta\"] + X,\n",
    "            \"xlabel\": r\"$\\phi$\",\n",
    "            \"ylabel\": \"$\\phi^{reco}$\",\n",
    "            \"xmin\": -3.2,\n",
    "            \"xmax\": 3.2,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Load and explore raw (unscaled) dataframes\n",
    "\n",
    "\n",
    "all_variable_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "]\n",
    "all_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "    \"tau\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb2b41-f128-45d0-993a-cfdd421b768d",
   "metadata": {},
   "source": [
    "## 3.2: Needed Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29814968-1591-409f-9655-42a962b5eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d3c5cc4-9f36-4642-b4c2-9fc9e3150955",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Load unscaled dataframes ###################################\n",
    "# @memory.cache\n",
    "def load_raw_data(AUTOREGRESSIVE_DIST_NAME=None):\n",
    "    \"\"\"Dont use AUTOREGRESSIVE_DIST_NAME for training of any variable. \n",
    "    For mass evaluation: dont use AUTOREGRESSIVE_DIST_NAME. For pT evaluation use AUTOREGRESSIVE_DIST_NAME \n",
    "    as the distribution predicted by mass, etc.  \"\"\"\n",
    "    print(f\"\\nSUBSAMPLE = {SUBSAMPLE}\\n\")\n",
    "    raw_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    raw_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"validation_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    \n",
    "    if AUTOREGRESSIVE_DIST_NAME:\n",
    "        print(f'Test (evaluation) Data is Autoregressive, loading {AUTOREGRESSIVE_DIST_NAME}')\n",
    "        eval_data = pd.read_csv(\n",
    "            os.path.join(\n",
    "                IQN_BASE,\n",
    "                \"JupyterBook\",\n",
    "                \"Cluster\",\n",
    "                \"EVALUATE\",\n",
    "                AUTOREGRESSIVE_DIST_NAME,\n",
    "            )\n",
    "        )\n",
    "        raw_test_data = eval_data\n",
    "    else:\n",
    "    \n",
    "        raw_test_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"test_data_10M_2.csv\"), \n",
    "        usecols=all_cols, \n",
    "        nrows=SUBSAMPLE\n",
    "        )\n",
    "\n",
    "    print(\"\\n RAW TRAIN DATA\\n\")\n",
    "    print(raw_train_data.shape)\n",
    "    raw_train_data.describe()  # unscaled\n",
    "    print(\"\\n RAW TEST DATA\\n\")\n",
    "    print(raw_test_data.shape)\n",
    "    raw_test_data.describe()  # unscaled\n",
    "\n",
    "    return raw_train_data, raw_test_data, raw_valid_data\n",
    "\n",
    "\n",
    "########## Generate scaled data###############\n",
    "# scaled_train_data = L_scale_df(raw_train_data, title='scaled_train_data_10M_2.csv',\n",
    "#                              save=True)\n",
    "# print('\\n\\n')\n",
    "# scaled_test_data = L_scale_df(raw_test_data,  title='scaled_test_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "# print('\\n\\n')\n",
    "\n",
    "# scaled_valid_data = L_scale_df(raw_valid_data,  title='scaled_valid_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "\n",
    "# explore_data(df=scaled_train_data, title='Braden Kronheim-L-scaled Dataframe', scaled=True)\n",
    "\n",
    "################ Load scaled data##############\n",
    "@utils.time_type_of_func(tuning_or_training=\"loading\")\n",
    "# @memory.cache\n",
    "def load_scaled_dataframes():\n",
    "    print(\"SCALED TRAIN DATA\")\n",
    "    scaled_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    print(\"TRAINING FEATURES\\n\", scaled_train_data.head())\n",
    "\n",
    "    scaled_test_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_test_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    scaled_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_valid_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    return scaled_train_data, scaled_test_data, scaled_valid_data\n",
    "\n",
    "\n",
    "#######################################\n",
    "#\n",
    "# # print('\\nTESTING FEATURES\\n', scaled_test_data.head())\n",
    "\n",
    "# print('\\ntrain set shape:',  scaled_train_data.shape)\n",
    "# print('\\ntest set shape:  ', scaled_test_data.shape)\n",
    "# # print('validation set shape:', valid_data.shape)\n",
    "# @memory.cache\n",
    "def get_train_scale_dict(USE_BRADEN_SCALING):\n",
    "    if USE_BRADEN_SCALING==True:\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(scaled_train_data)\n",
    "        print(\"BRADEN SCALING DICTIONARY\")\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    else:\n",
    "        print(\"NORMAL UNSCALED DICTIONARY\")\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(raw_train_data)\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    return TRAIN_SCALE_DICT\n",
    "\n",
    "\n",
    "################################ SPLIT###########\n",
    "# @memory.cache\n",
    "def T(variable, scaled_df):\n",
    "    if variable == \"pT\":\n",
    "        L_pT_gen = scaled_df[\"genDatapT\"]\n",
    "        L_pT_reco = scaled_df[\"RecoDatapT\"]\n",
    "        target = (L_pT_reco + 10) / (L_pT_gen + 10)\n",
    "    if variable == \"eta\":\n",
    "        L_eta_gen = scaled_df[\"genDataeta\"]\n",
    "        L_eta_reco = scaled_df[\"RecoDataeta\"]\n",
    "        target = (L_eta_reco + 10) / (L_eta_gen + 10)\n",
    "    if variable == \"phi\":\n",
    "        L_phi_gen = scaled_df[\"genDataphi\"]\n",
    "        L_phi_reco = scaled_df[\"RecoDataphi\"]\n",
    "        target = (L_phi_reco + 10) / (L_phi_gen + 10)\n",
    "    if variable == \"m\":\n",
    "        L_m_gen = scaled_df[\"genDatam\"]\n",
    "        L_m_reco = scaled_df[\"RecoDatam\"]\n",
    "        target = (L_m_reco + 10) / (L_m_gen + 10)\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def split_t_x(df, target, input_features):\n",
    "    \"\"\"Get teh target as the ratio, according to the T equation\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_train_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def split_t_x_test(df, target, input_features):\n",
    "    \"\"\"Get teh target as the ratio, according to the T equation\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_test_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# @memory.cache\n",
    "def normal_split_t_x(df, target, input_features):\n",
    "    # change from pandas dataframe format to a numpy\n",
    "    # array of the specified types\n",
    "    # t = np.array(df[target])\n",
    "    t = np.array(df[target])\n",
    "    x = np.array(df[input_features])\n",
    "    return t, x\n",
    "\n",
    "\n",
    "################ Apply Z scaling############\n",
    "def z(x):\n",
    "    eps = 1e-20\n",
    "    return (x - np.mean(x)) / (np.std(x) + eps)\n",
    "\n",
    "\n",
    "def z_inverse(xprime, x):\n",
    "    return xprime * np.std(x) + np.mean(x)\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z2(x, mean, std):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x ([type]): [description]\n",
    "        mean ([type]): [description]\n",
    "        std ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    scaled = (x - mean) / (std + eps)\n",
    "    return np.array(scaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "def z_inverse(xprime, x):\n",
    "    unscaled = xprime * np.std(x) + np.mean(x)\n",
    "    return np.array(unscaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z_inverse2(xprime, train_mean, train_std):\n",
    "    \"\"\"mean original train mean, std: original. Probably not needed\"\"\"\n",
    "    return xprime * train_std + train_mean\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x):\n",
    "    \"\"\"TO ensure this z scaling is only applied once to the training features, we use a generator.\n",
    "    This doesn't change the shapes of anything, just applies z to all the feature columns other than tau\"\"\"\n",
    "    NFEATURES = train_x.shape[1]\n",
    "    for i in range(NFEATURES - 1):\n",
    "        variable = list(TRAIN_SCALE_DICT)[i]\n",
    "        train_mean = float(TRAIN_SCALE_DICT[variable][\"mean\"])\n",
    "        train_std = float(TRAIN_SCALE_DICT[variable][\"std\"])\n",
    "        train_x[:, i] = z2(train_x[:, i], mean=train_mean, std=train_std)\n",
    "        test_x[:, i] = z2(test_x[:, i], mean=train_mean, std=train_std)\n",
    "        valid_x[:, i] = z2(valid_x[:, i], mean=train_mean, std=train_std)\n",
    "    yield train_x\n",
    "    yield test_x\n",
    "    yield valid_x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_targets(train_t, test_t, valid_t):\n",
    "    train_mean = np.mean(train_t)\n",
    "    train_std = np.std(train_t)\n",
    "    train_t_ = z2(train_t, mean=train_mean, std=train_std)\n",
    "    test_t_ = z2(test_t, mean=train_mean, std=train_std)\n",
    "    valid_t_ = z2(valid_t, mean=train_mean, std=train_std)\n",
    "\n",
    "    yield train_t_\n",
    "    yield test_t_\n",
    "    yield valid_t_\n",
    "\n",
    "\n",
    "# @utils.debug\n",
    "def save_model(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "# @utils.debug\n",
    "def load_model(PATH, PARAMS):\n",
    "    # n_layers = int(BEST_PARAMS[\"n_layers\"])\n",
    "    # hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "    # dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "    # optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "    # learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "    # batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "    model = utils.RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    # OR\n",
    "    # model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED! Also, use dictionary \".pth\" which has both the model state dict and the PARAMS dict\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def simple_eval(model, test_x_z_scaled):\n",
    "    model.eval()\n",
    "    # evaluate on the scaled features\n",
    "    valid_x_tensor = torch.from_numpy(test_x_z_scaled).float()\n",
    "    # valid_x_tensor=torch.from_numpy(train_x).float()\n",
    "    pred = model(valid_x_tensor)\n",
    "    p = pred.detach().numpy()\n",
    "    # if USE_BRADEN_SCALING:\n",
    "    #     fig, ax = plt.subplots(1,1)\n",
    "    #     label=FIELDS[target]['ylabel']\n",
    "    #     ax.hist(p, label=f'Predicted post-z ratio for {label}', alpha=0.4, density=True)\n",
    "    #     # orig_ratio = z(T('m', scaled_df=scaled_train_data))\n",
    "    #     orig_ratio = z(T('m', scaled_df=scaled_test_data))\n",
    "    #     print(orig_ratio[:5])\n",
    "    #     ax.hist(orig_ratio, label = f'original post-z ratio for {label}', alpha=0.4,density=True)\n",
    "    #     ax.grid()\n",
    "    #     set_axes(ax, xlabel='predicted $T$')\n",
    "    # print('predicted ratio shape: ', p.shape)\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def get_hist(label):\n",
    "    \"\"\"label could be \"pT\", \"eta\", \"phi\", \"m\" \"\"\"\n",
    "    predicted_label_counts, label_edges = np.histogram(\n",
    "        JETS_DICT[\"Predicted_RecoData\" + label][\"dist\"],\n",
    "        range=JETS_DICT[\"Predicted_RecoData\" + label][\"range\"],\n",
    "        bins=bins,\n",
    "    )\n",
    "    real_label_counts, _ = np.histogram(\n",
    "        JETS_DICT[\"Real_RecoData\" + label][\"dist\"],\n",
    "        range=JETS_DICT[\"Real_RecoData\" + label][\"range\"],\n",
    "        bins=bins,\n",
    "    )\n",
    "    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2\n",
    "\n",
    "    return real_label_counts, predicted_label_counts, label_edges\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def get_hist_simple(predicted_dist, target):\n",
    "    \n",
    "    range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "    bins=50\n",
    "    predicted_label_counts, label_edges = np.histogram(\n",
    "        predicted_dist, range=range_, bins=bins\n",
    "    )\n",
    "    \n",
    "    \n",
    "    real_label_counts, _ = np.histogram(REAL_DIST, range=range_, bins=bins)\n",
    "    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2\n",
    "    return real_label_counts, predicted_label_counts, label_edges\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def plot_one(\n",
    "    target, real_edges, real_counts, predicted_counts, save_plot=False, PARAMS=None, JUPYTER=True\n",
    "):\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2, 1, figsize=(3.5 * 3 / 2.5, 3.8), gridspec_kw={\"height_ratios\": [2, 0.5]}\n",
    "    )\n",
    "    ax1.step(\n",
    "        real_edges, real_counts / norm_data, where=\"mid\", color=\"k\", linewidth=0.5\n",
    "    )  # step real_count_pt\n",
    "    ax1.step(\n",
    "        real_edges,\n",
    "        predicted_label_counts_m / norm_IQN,\n",
    "        where=\"mid\",\n",
    "        color=\"#D7301F\",\n",
    "        linewidth=0.5,\n",
    "    )  # step predicted_count_pt\n",
    "    ax1.scatter(\n",
    "        real_edges,\n",
    "        real_counts / norm_data,\n",
    "        label=\"reco\",\n",
    "        color=\"k\",\n",
    "        facecolors=\"none\",\n",
    "        marker=\"o\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax1.scatter(\n",
    "        real_edges,\n",
    "        predicted_counts / norm_IQN,\n",
    "        label=\"predicted\",\n",
    "        color=\"#D7301F\",\n",
    "        marker=\"x\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax1.set_xlim(range_)\n",
    "    ax1.set_ylim(0, max(predicted_counts / norm_IQN) * 1.1)\n",
    "    ax1.set_ylabel(\"counts\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    ratio = (predicted_counts / norm_IQN) / (real_counts / norm_data)\n",
    "    ax2.scatter(\n",
    "        real_edges, ratio, color=\"r\", marker=\"x\", s=5, linewidth=0.5\n",
    "    )  # PREDICTED (IQN)/Reco (Data)\n",
    "    ax2.scatter(\n",
    "        real_edges,\n",
    "        ratio / ratio,\n",
    "        color=\"k\",\n",
    "        marker=\"o\",\n",
    "        facecolors=\"none\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax2.set_xlim(range_)\n",
    "    ax2.set_xlabel(FIELDS[target][\"xlabel\"])\n",
    "    ax2.set_ylabel(\n",
    "        r\"$\\frac{\\textnormal{predicted}}{\\textnormal{reco}}$\"\n",
    "        #    , fontsize=10\n",
    "    )\n",
    "    ax2.set_ylim((YLIM))\n",
    "    ax2.set_xlim(range_)\n",
    "    ax2.set_yticklabels([])\n",
    "    if JUPYTER==True:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(wspace=0.5, hspace=0.2)\n",
    "        fig.subplots_adjust(wspace=0.0, hspace=0.1)\n",
    "        plt.axis('off')\n",
    "\n",
    "    # plt.gca().set_position([0, 0, 1, 1])\n",
    "    if save_plot:\n",
    "        plot_filename = utils.get_model_filename(target, PARAMS).split(\".dict\")[0] + \".png\"\n",
    "        plt.savefig(\n",
    "            os.path.join(IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", plot_filename)\n",
    "        )\n",
    "\n",
    "    \n",
    "    # fig.show()\n",
    "    # plt.show();\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.gca().set_position([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3336b47e-6e8a-43ba-a93a-5067d6b44dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUBSAMPLE = None\n",
      "\n",
      "\n",
      " RAW TRAIN DATA\n",
      "\n",
      "(8000000, 9)\n",
      "\n",
      " RAW TEST DATA\n",
      "\n",
      "(1000000, 9)\n"
     ]
    }
   ],
   "source": [
    "raw_train_data, raw_test_data, raw_valid_data = load_raw_data()\n",
    "\n",
    "# Load scaled data\n",
    "scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d40d613-52be-4b51-ab10-822df7976083",
   "metadata": {},
   "source": [
    "## 3.3: Evaluate Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a42c5cef-a246-45ea-9ee4-d4d22452a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n",
      "spliting data for RecoDatam\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 5)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 5)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 5)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 3.27223764e+01  6.98189368e-04 -8.95543973e-04  6.96116528e+00\n",
      "  5.00485136e-01] [15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]\n",
      "[ 3.26952341e+01 -1.78188172e-03 -3.83090331e-04  6.96299435e+00\n",
      "  4.99915289e-01] [14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]\n",
      "5.5514112643334546 2.664124544901276\n",
      "5.555567451922438 2.664339857066051\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[ 1.81700902e-03  1.12510099e-03 -2.82526482e-04 -6.57626105e-04\n",
      "  5.00485136e-01] [1.01748627 0.9999745  0.99989115 0.99987283 0.28852734]\n",
      "[ 1.61650249e-15 -1.10134124e-18 -3.12905257e-17  5.01036723e-15\n",
      "  4.99915289e-01] [1.         1.         1.         1.         0.28867295]\n",
      "-0.0015599314696879494 0.9999191874249058\n",
      "6.996216939114674e-16 1.0000000000000002\n",
      "<class 'str'>\n",
      "This model was trained for 300000 iteration, which is  38.4 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (3): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.3)\n",
      "    (5): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (6): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (9): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.3)\n",
      "    (11): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (12): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.3)\n",
      "    (14): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (15): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.3)\n",
      "    (17): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (18): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.3)\n",
      "    (20): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (21): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.3)\n",
      "    (23): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "EVALUATION DATA NEW INDEX\n",
      "    RecoDatam  genDatapT  genDataeta  genDataphi  genDatam       tau\n",
      "0   4.786460    43.6113    0.824891    -1.26949   5.93310  0.250046\n",
      "1   6.979283    43.6113    0.824891    -1.26949   5.93310  0.847493\n",
      "2   5.452590    26.0153    3.529970     1.55495   7.41270  0.851995\n",
      "3   3.426970    28.4944   -1.159650     1.82602   7.84157  0.052378\n",
      "4   3.526737    21.9840    2.747660     2.03085   5.18315  0.542549\n",
      "norm_data 1000000 \n",
      "norm IQN 1000000 \n",
      "norm_autoregressive 1000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 302.4x273.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "########################################################################################\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING==True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = split_t_x(\n",
    "        df=scaled_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "        df=raw_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "        df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = normal_split_t_x(\n",
    "        df=raw_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "######################################################\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "\n",
    "NFEATURES = train_x.shape[1]\n",
    "TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "# to features\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "###########################################################\n",
    "# Get the  parameters for this model and training\n",
    "PARAMS_m = {\n",
    "\"n_layers\": int(8),\n",
    "\"hidden_size\": int(5),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(1e-3),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(1024),\n",
    "    'n_iterations': int(3e5),\n",
    "}\n",
    "\n",
    "optimizer_name = PARAMS_m[\"optimizer_name\"]\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS = PARAMS_m[\"n_iterations\"]\n",
    "BATCHSIZE = PARAMS_m[\"batch_size\"]\n",
    "comment = \"\"\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(\n",
    "    f\"This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs\"\n",
    ")\n",
    "\n",
    "# 'Trained_IQNx4_%s_TUNED.dict' % target\n",
    "filename_model = utils.get_model_filename(target, PARAMS_m)\n",
    "# OR, if you know a model filename directly, you can also specify it, but you have to make sure its parameters are the same\n",
    "# Nominal one is 'Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict', also in backup\n",
    "filename_model='Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict'\n",
    "trained_models_dir = \"trained_models\"\n",
    "utils.mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE,  # the loaction of the repo\n",
    "    \"JupyterBook\",  # up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\",\n",
    "    \"TRAIN\",\n",
    "    trained_models_dir,  # /trained_models\n",
    "    filename_model,  # utils.get_model_filename has the saved file format\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "# Get predicted distribution\n",
    "p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "\n",
    "range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "bins = 50\n",
    "REAL_RAW_DATA = raw_test_data\n",
    "\n",
    "YLIM = (0.8, 1.2)\n",
    "###########GET REAL DIST###########\n",
    "REAL_RAW_DATA = REAL_RAW_DATA[\n",
    "    [\"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\", \"RecoDatam\"]\n",
    "]\n",
    "REAL_RAW_DATA.columns = [\"realpT\", \"realeta\", \"realphi\", \"realm\"]\n",
    "REAL_DIST = REAL_RAW_DATA[\"realm\"]\n",
    "norm_data = REAL_RAW_DATA.shape[0]\n",
    "#############GET EVALUATION DIST#############\n",
    "raw_test_data.describe()\n",
    "m_reco = raw_test_data[\"RecoDatam\"]\n",
    "m_gen = raw_test_data[\"genDatam\"]\n",
    "# plt.hist(m_reco,label=r'$m_{gen}^{test \\ data}$');plt.legend();plt.show()\n",
    "\n",
    "\n",
    "def descale_Braden_scaled_prediction(label, p):\n",
    "    \"\"\"Label could be m. p is the outcome of the model evaluation, e.g. \n",
    "    IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "    p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "    \n",
    "    \"\"\"\n",
    "    # make sure you've set braden scaling global variable to use this function.\n",
    "    assert USE_BRADEN_SCALING==True\n",
    "    orig_ratio = T(label, scaled_df=scaled_train_data)\n",
    "    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))\n",
    "    L_obs = L(orig_observable=m_gen, label=label)\n",
    "    z_inv_f = z_inv_f.flatten()\n",
    "    print(z_inv_f.shape)\n",
    "\n",
    "    factor = (z_inv_f * (L_obs + 10)) - 10\n",
    "    label_pred = L_inverse(L_observable=factor, label=label)\n",
    "    return label_pred\n",
    "    \n",
    "    \n",
    "m_pred = z_inverse2(\n",
    "    xprime=p,\n",
    "    train_mean=TRAIN_SCALE_DICT[target][\"mean\"],\n",
    "    train_std=TRAIN_SCALE_DICT[target][\"std\"],\n",
    ")\n",
    "m_pred = m_pred.flatten()\n",
    "\n",
    "# Get histogram of predicted distribution\n",
    "real_label_counts_m, predicted_label_counts_m, label_edges_m = get_hist_simple(\n",
    "    predicted_dist=m_pred, target=target\n",
    ")\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "# Get evaluation data\n",
    "eval_data = pd.read_csv(DATA_DIR + \"/test_data_10M_2.csv\")\n",
    "ev_features = features\n",
    "eval_data = eval_data[ev_features]\n",
    "# save new distribution (m) in the eval data as autoregressive eval for next IQN\n",
    "eval_data[target] = m_pred\n",
    "\n",
    "new_cols = [target] + features\n",
    "eval_data = eval_data.reindex(columns=new_cols)\n",
    "print(\"EVALUATION DATA NEW INDEX\\n\", eval_data.head())\n",
    "\n",
    "eval_data.to_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load this saved predited autoregressive distribution\n",
    "AUTOREGRESSIVE_DIST = pd.read_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]\n",
    "# get normalization values\n",
    "norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]\n",
    "norm_IQN = norm_autoregressive\n",
    "print(\n",
    "    \"norm_data\",\n",
    "    norm_data,\n",
    "    \"\\nnorm IQN\",\n",
    "    norm_IQN,\n",
    "    \"\\nnorm_autoregressive\",\n",
    "    norm_autoregressive,\n",
    ")\n",
    "\n",
    "# Finally, plot predicted distribution\n",
    "\n",
    "plot_one(\n",
    "    target=target,\n",
    "    real_edges=label_edges_m,\n",
    "    real_counts=real_label_counts_m,\n",
    "    predicted_counts=predicted_label_counts_m,\n",
    "    save_plot=True,\n",
    "    PARAMS=PARAMS_m\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df205105-a24d-4694-a182-ef84c08bfd4c",
   "metadata": {},
   "source": [
    "## 3.4: Evaluate $p_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed548cae-19f9-44d7-b30f-1328fd8fb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['RecoDatam', 'genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatapT\n",
      "USING NEW DATASET\n",
      "\n",
      "\n",
      "SUBSAMPLE = None\n",
      "\n",
      "\n",
      " RAW TRAIN DATA\n",
      "\n",
      "(8000000, 9)\n",
      "\n",
      " RAW TEST DATA\n",
      "\n",
      "(1000000, 9)\n",
      "spliting autoregressive evaluation data for RecoDatapT\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 6)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[ 2.59587    29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [ 6.25659    41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [ 6.11213    35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [ 4.17483    26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 6)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 6)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 5.55141126e+00  3.27223764e+01  6.98189368e-04 -8.95543973e-04\n",
      "  6.96116528e+00  5.00485136e-01] [ 2.66412454 15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]\n",
      "[ 5.55556745e+00  3.26952341e+01 -1.78188172e-03 -3.83090331e-04\n",
      "  6.96299435e+00  4.99915289e-01] [ 2.66433986 14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]\n",
      "32.881453465999996 16.02400426348493\n",
      "32.86720151648752 15.829355769531851\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[-1.81710707e+00  1.48455353e+01  5.96132264e-04 -2.50379668e+00\n",
      " -1.63658184e+00  5.00485136e-01] [0.17834627 6.89519305 1.2152514  0.65207164 0.17568487 0.28852734]\n",
      "[-1.81682884e+00  1.48332220e+01 -7.71183141e-04 -2.50361243e+00\n",
      " -1.63646629e+00  4.99915289e-01] [0.17836068 6.77669391 1.21528238 0.65214262 0.17570722 0.28867295]\n",
      "0.0009003493079555966 1.0122966781963252\n",
      "-1.2048033681821834e-15 1.0000000000000002\n",
      "<class 'str'>\n",
      "This model was trained for 200000 iteration, which is  25.6 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=5, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (3): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.3)\n",
      "    (5): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (6): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (9): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.3)\n",
      "    (11): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (12): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.3)\n",
      "    (14): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (15): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.3)\n",
      "    (17): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (18): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.3)\n",
      "    (20): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (21): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.3)\n",
      "    (23): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (24): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): LeakyReLU(negative_slope=0.3)\n",
      "    (26): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (27): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (28): LeakyReLU(negative_slope=0.3)\n",
      "    (29): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (30): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): LeakyReLU(negative_slope=0.3)\n",
      "    (32): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (33): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (34): LeakyReLU(negative_slope=0.3)\n",
      "    (35): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (36): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (37): LeakyReLU(negative_slope=0.3)\n",
      "    (38): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (39): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (40): LeakyReLU(negative_slope=0.3)\n",
      "    (41): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (42): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (43): LeakyReLU(negative_slope=0.3)\n",
      "    (44): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "EVALUATION DATA NEW INDEX\n",
      "    RecoDatam  RecoDatapT  genDatapT  genDataeta  genDataphi  genDatam  \\\n",
      "0    5.80270         NaN    43.6113    0.824891    -1.26949   5.93310   \n",
      "1    5.80270         NaN    43.6113    0.824891    -1.26949   5.93310   \n",
      "2    4.81403         NaN    26.0153    3.529970     1.55495   7.41270   \n",
      "3    7.06425         NaN    28.4944   -1.159650     1.82602   7.84157   \n",
      "4    4.08061         NaN    21.9840    2.747660     2.03085   5.18315   \n",
      "\n",
      "        tau  \n",
      "0  0.250046  \n",
      "1  0.847493  \n",
      "2  0.851995  \n",
      "3  0.052378  \n",
      "4  0.542549  \n",
      "norm_data 1000000 \n",
      "norm IQN 1000000 \n",
      "norm_autoregressive 1000000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASMAAAD6CAYAAADqZiF+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwUUlEQVR4nO3de1yUVf4H8M9RQcTbKGqWNxhKu1gql7K1XC9j202tvFVbC9vm5be17WbGrVWHLAU0NTUvtGlt5a6BXTUtyDQt2wQ0L2ACAymiqeCAqCAw398f88zjMHKZGZ5hnhm+79eLl8xzmy+H8cs553nOOYKIwBhj7tbG3QEwxhjAyYgxphKcjBhjqsDJiDGmCpyMGGOq0M7dATijR48eFBgY2GLvV3P2N7TreV2LvZ9aY2DMEZmZmeeIqKe9x3tkMgoMDERGRkaLvd/ZtcvQc9aLLfZ+ao2BMUcIIX515HhupjHGVIGTEWNMFTgZMcZUwSP7jBhzterqahQVFaGystLdoaien58f+vbtCx8fn2Zdh5MRY/UoKipC586dERgYCCGEu8NRLSJCSUkJioqKEBQU1KxrcTONsXpUVlYiICCAE1EThBAICAhQpAbJyYixBnAiso9S5cTJiDGmCpyMGGOq4JIObCHEZABGAFoiSrZnvxAiBIAWAIgo1RVxMcbUS/GakZRoQETp0mudnftjpSTUXQihVTouxlzpt99+g16vx/bt2xW5Xnp6OsaNG4fU1FQkJSXBaDQiKSkJ6enpSE42/31PSkpCVlYW0tPTAQDJycnIysqS93saVzTTwgEYpO8NAEKa2i+EmAFgnxBCS0TJRGSAAoyfp+BK8QkYP09R4nKMNWjFihWIiorCzp07FbmzpNPpYDAYMHnyZERFRWHRokXQ6XTQ6XTIzMxEamoqtFotQkJCkJaWhqSkJISFhSEkJARardYjE5IrkpHG5nWAHfuDpX9LhRDrhBC2x0AIMUMIkSGEyDh79qxdgfiHDYfhoXvhHzbcruMZc1ZgYCDeeecdlJWVwdfXV5FrhoRc/TuelZWF0tJSZGVlYebMmUhLS4NWa25AJCYmIi0tDRqNBgCg1WqRlpamSAwtyRXJyAiguxP784nICCATwAzbnVKNKYyIwnr2tG9WgksZP0K7dTcuZfxo1/GMOWv69Ol45JFHsHz5crRpo/x/q3HjxgGAXPMJDg5GaWkpAMBoNCIkJAQGg7lBYTAYEB4erngMruaKDux9uFr70QKwTdH17dfgaoLSwJywmk0zYQoAwHdCP3lbRUUFKioq0Lt3b4euZfw8Bf5hw3Ep40f5uoxZ69evX9MH2Sk9PR1ZWVnIyspCSEgIoqKikJSUJO+PiopCdHQ0unfvjtLSUiQmJsr7s7KyEBUVpVgsLUXxZEREqUKIKKljWmPVUZ1GROMa2W/ZhvruwCmhrKwM7z3zFOiW2zHEV2DUvNftPtfS5NNu3e2K0BirQ6fTIT8/v8422wSTmJhY736drs49I4/hklv7RGRJ4elW28Y1sf+abUp77bXX8IvxIvr/azW2D7wdO016+Pn5ISYmpslzrZt81jUtxpgyWtVA2Y4dO+Kvdw3D0eF3Ye6Q23Dd1Kcwc+ZMvPDCCzCZTEhMTETHjh3rPbe+Jh9jTDmtKhkBwP0L38D9Vq8LCwsREhKCvLw8zJ49G0FBQXbVlBhjymr1w0Hee+89mEwmDBs2DGvWrOH5axhzk1ZXM7LVu3fvOh2Btxw7iLNrl9U5po1v+5YOi7FWx6uT0ZtvvokzZ86gT58++Otf/9rgcda37duaalW5Ckcb3/Z1kmQb3/YIeKbhn4kxT+O1zbSEhASkpqbCx8cHGzduhF5vvnNWH094Ujvgmb+i56wX5S/TlSp3h8RULjo6Wn72yGAwYMqUhp+PS021b2y69TWV5rXJqLKyEnPmzMGVK1cQFxcHvV7fYMe07ZPaX3zxBeLi4vDhhx+2ZMiMKWratGny91qtFikp9Y/RNBqNdg8fsb6m0rw2GQHAxIkTsXDhQjz44IONHqeZMAW+N/STb9/v3bsXCxcuxJEjR1oiTMaukZ6ejtDQUHmUvmVkvr2j+dPT0+VzAPNT2ZYhJZZjLCP+DQYDMjIy5OPtvabSvDoZOaNdu3bIzs7Gvffei/3790Ov1yMhIcHdYTEPoOQsETqdDt27d4dOp8OMGTMwc+ZMeXtjo/mTk5MREhIib7MICQmRB9Lajvi3jHezHG/vNZXGycjGiHvuwaeffordu3dj27Zt0Ov1fLuf2UXpvkdL8gDMzSzLQFh7R/M3xHbEvy1nrqkETkaMKUTpWSKMRqP8vcFgqDch2I7mDw8PR1ZWFgDIo/pt2Y74t2bdnHPkmkrgZMSYQmz7HpvLUjtJTk7GunXrANQdzQ+YB8da+n4yMjIQFRUFg8EgH5OWlgaj0Si/NhgMiIqKQlpaGrKyspCRkQHAXPNKT0+HVqu1+5qKIyKP+woNDaWmzJ8/v8lj6nNmzVLFruVK9cXJlJOdne3uEGjy5MnuDsFu9ZUXgAxy4P+119SMysvL8frrr9v9vIQjiAhvvPEGoqKicOzYMcWvz5gtSw3I0k/UGnhNMlq5ciVmzJiBwsJCFBUVKXrtiooK+Pr6YuHChXj//fcVvTZj9bHMZ9QSHcdqoaalis4DyACQRlfnNrJLQkICfv75Zzz55JM4f/48SktL0aVLl+b+GLKAgAD85z//wcaNGzFo0CD5aW4e3e/diIhXlbWDuUXWfIonI+uliKRJ9HUkzebYxP4p1sc5orKyEh999BFOnjyJrl27olOnTor8LBZxcXGIi4urs02v1yv6Hkxd/Pz8UFJSgoCAAE5IjSAilJSUNDjUyhGuqBmFA9gkfW9Zqijdjv0aaakipxvJffr0cfZUxuro27cvioqKYO9KNK2Zn58f+vbt2+zruCIZaWxe27NUEWCekL9UCLGOiGbaXlRaW20GAPTv37/5UTLWCB8fHwQFBbk7jFZFNUsVkXkpIiMAo6UpV89+h5YqYox5DlckI4eXKpL6jmxXnnUYryDLmOdSPBkRUSoAbX1LFTWy/yPpmMlWxzjME+YlYozVTxVLFUnNsyzpy+mnFltyOSEiwldffYWuXbti+HBOfow1l9c89AgoPzaoMUePHkVZWRm+++47HD58WN7OTUXGnONVyUhJTSUVX19ffL5iGXZ88TnWRM+R5zzipiJjzuFk1ICmksr777+P6f94EcsrirDs7bflOY+UnkaCsdbCq1cHaQ57+p+G+Ar42yQeXnmWMedwMmqAPUmlzjFZPF82Y83BychD2a6jZtnGa6kxT8XJSEEfffQR9u/fD41Gg+joaJe+V31JxzY5MeZJOBkpxM/PDytXrsTYsWOxZcsWXL58GX5+fph1a7C8Wm1LPHLAmKfiu2kKiYmJQXx8PKqqqhAZGSmvKsK3+hmzD9eMFDRmzBiMGTOmzraWfCqcMU/mscno5MYNqBl4G7oW/+ruUBrFt/oZs49HNtOqq6uxeuf3uDx9Kg5UmdwdDmNMAR6bjO7u2hGfPPA41ryxRJEpLxlj7uWRzTR/f38Yh4Sj/ZkzeP+77+Hr6+vukBhjzeSRyQgAnnrqKXeHwBhTkEuaaUKIyUIInTRvtUP7hRCJroiJMaZuiicj66WIpNc6e/dL33vVqnXLly9HVFQUMjMz3R0KY6rmippROMxLEAFXlyJqcr8QQmu1/RrSPNkZQogMT1k+5sqVK6ioqEBiYiI+/vhjd4fDmKq5IhlpbF7bu1RRo2umeeLqIJ07d8bXX3+NESNG4Pjx49Dr9fIkbIyxulzRgW2Eg0sV2a466y1iY2MRGxtbZxuvRMtY/exKRkKIMTA3oTQAdABSiaiwgcMdXqoI5sUbddJ2rRAihIiy7ImNMeYd7G2maaTkkwIgGdc2tWTOLFVERFnScd0buzZjzHvZ20wrk2pH+4moXOpsPtDQwY4uVWS1PRnmZMecwBOuMU9mbzIqBTANwHQhxCQAYQD49pATampqsGDBArRp0wZRUVHw8fFR7No84RrzZHYlIyLaD2A/AAghDLCp0XiykvWrYbpSJb9u49vepe939OhRxMbGorKyEl9++SUmTpzo0vdjzFPY3YFNRDsAc2KSmmw7XBpZCzFdqULPWS+22PsFBQXhn08/iXJfP4QN6IecnBzExMS02PszplaNJiOpSTYOQJgQIh+AkHblw0uSUUtbvHgxLv/tOfw6cRS0a7ZgYfI77g6JMVVoNBkR0WYhRDrMDyTut2wXQnRxeWRerOrAPl7okTEbTTbTiKhMCAEhxCJpkwAwDMAfXBqZF+P11hi7lr1303Soe8td19CBjDHmDHuTUSYRFVheWB5gZIwxpdj7BHaMECJXCLFJCPERrh3iwZqhoqICycnJOHKEm2ys9bI3GSUS0U1ENI2IpgKY6sqgWpukpCSMHTsWa9asQW1trbvDYcwt7H3o8RubTfkuiKVV8vPzQ1paGgwGA3JycvDqq6+iQ4cO/OwRa3XsfehxkfVLAGNhniSNNVNMTAxefvll7Nq1C4MHD0avXr2g1+th/DyFl8VmrYojY9NSpe+14JqRotq2bXvNSrSWZbG1W3e7KSrGWpa9zbTFVi8LhBAlLoqHSXhZbNba2NtM+xrAeZibaATzBGkHGjl+MswzOmqlaUGa3G81Mf84Ioq2+yfwEtbNMoCXxWatj73NtMR6OrHrZb36hzSJfp0pZevbD3MzMISIkoQQ0UKIRufD9kZ1mmUKPpVtO8cRz2/E1Mruu2lCiOkwP3m9j4iWNHJ4OIBN0veW1T/SG9svTbaWJYTQADC0tkQE1G2WAcDJkyeRkZGB+++/H+3bOz+tiW3i4fmNmFrZ9ZyRlIgMAGIA7BdCzGnkcI3Na3tXBwHMk7bV2znuiUsVOUIzYQp8b+gHzYQpICK89tpr6NWrFxYtWtT0yYx5AXubaRlWo/YLhBCNHWuEg6uDWEhNtylCiMnSXNnW++QpacPCwsjOuD2Sn58f9u3bByJCTk6OvI2fPWLezN4nsMOEEEOFEIHSxGrDGjnW4dVBhBCJVktdG9F4MvN6sbGxSE1NxciRI7Ft2zbo9XpUVla6OyzGXMrePqO3hRAvwzzRWiYRxTZybKoQIqq+1UGIaFx9+6WpbK1XDGn1k/IHBgYiMDDQ3WEw1mLsvbU/FkAAEd0nhOhqPQ1tfRxdHUTqsDbYnsMYaz3sbaaVEFEMYJ5sDeZb8ayFbdq0CRs2bIDJZHJ3KIwpzt5kNE4I8ZjUbzQG5mWLWAv69ddfYTKZ0L9/f3z8Ma8SxbyP3cNBpMn5ZwHIa6zPiLlGt27dsHr1avj6+mLAgAHIy8vju2vMq9h7ax9EtBnAZhfGwhqxdOlS5K9fi9pBt6FXyWks57mzmZext5nGVKDf/Q/B9PzT8A8b7u5QGFMcJyMPYjtkhDFvYnczjbkfL3HEvBnXjDzYqVOnMG/ePKSkpLg7FMaajZORB1u9ejViYmKQk5ODsrIyd4fDWLNwM81DWQbTPvzwwygvL0dNTQ38/f35dj/zWJyMPJQl6ZSWlqJz587w8fGBXq9v8jzbydYs23jCNeZunIw8XPfujk1wUF/S4QnXmBpwn5GX+fLLLxEXF4esrCx3h8KYQzgZeTjj5ym4UnwCxs/Nd9R27dqF119/HZs2bWriTMbUhZORh7NM5O8fNhx+fn7Yu3cvRo8ejUOHDkGv1yMhIcHdITJmF5f0GTm6VJE0Eb9W+gp31VJFJetXw3Slqs62Nr7OT3avBtZPZcfExCAmJgZVVVXyJP72dGozpgaKJyMnlyrSStuShRDhQogZrpjt0XSlCj1nvaj0Zd3Kdn01y/prRl4Wm3kYVzTTwnF11kbLUkWN7ieiZKvko7Xazxxk3WwDgKqqKjz//PP429/+huzsbDdHx1jDXJGMNDav7V6qSAihBVBqXZOy2ufVSxUpxXYw7fnz51Fx5CB8q69gfsRT3IfEVMsVycgIJ5cqAjCZiGbWt0OqPYURUVjPnj2bF6EXs15/DQBWrVqF28LvxPT/fYWV6zfwKiNMtVyRjBxeqggw9yVZJuoXQtg27ZiT2rVrh+n33AXt1t3wKzgmb7d9JIAxd1O8A9vJpYp0ABKFEJbpbF1yN621sp16JCcnB5u+2oHH57+MoC++Q+25szxEhLmdS27tO7FUUTqAYFfEwq61ceNGvKgbhS1aLdpu34KbnuUhIsz9eGxaK+Pn54eCggI8lLQcbdu2Rd6YMfBPSMCsW4PhHzYcl/iRAOYmnIxamfqmGNHr9fIjAdqtu90QFWM8HIRJyvbuRtmCFSjfu8fdobBWipMRAwAk7s3CGRJ4I+Ogu0NhrRQnIwY/Pz/s2bMHe/bswc6dO6HX6/H9Hq4hsZbFyYghJiYGH374IXr27IlNmzZBr9ejpqbG3WGxVoaTEQMABAcHY86cOejfv7+8bceOHXj77bdRXV3txshYa8HJiNWrsrISxf99D8ODg/Dpi8+5OxzWCvCtfVYvX9/2WJV1CHd8PhlnQn+HI3o9/Pz8ePUR5jKcjFi9xt5/P4b99AMqR98NTc0V+PXuis++3Ia///0UiAgJCQnw9/d3d5jMi3AyYvUKeOav14xNu/D++/C/zR+5ubmYPXs2AgMDuabEFMN9RsxujzzyKLQn8jFi4I1YdP8Yno6EKYqTEbNbp86dEJH0Bh7Y9l90vPNuAEBOTg7mzp2LjIwMN0fHPB0nI+YQ25kk33nnHej1emzcuNHNkTFPx31GzCHWcyP5Zedj165d+OOI4ThDAjFPTIFmSCj3IzGnuKRmJISYLITQCSFm2Ltf2mY7KyRTsZiYGOzYsQOLVqzE6itn8OobS1FZWYn8/Hxs374dJpPJ3SEyD6J4MrJeikh6rbNnPxGlKh0LU1Yb3/Y4u3aZ/FWyfjWEEOh2+oTcdKuqqsKKFSvQpk0brFq1yt0hMw/iimZaOADL2sqWpYrSHdjPVMr2Vr9lNkjrpptv5mEc2vIZUs6ewfncX1BaWsoPSzK7uCIZaWxe271UUWOkJt0MAHXGTzF1iY+Px8GR98B3znQM+GwnOvQ3P4sUFxeHDh06IDY2Fu3acVclu5balipqEC9V5Dn6XzRCu3U3qg7sAwAcOnQIz940AHffGITMpeZ124gIxcXFPDsAk7niT5RTSxUx72G75PbAgQPxpxVrMPRUITpMehLfJiTA19cX7dq1Q2FhIZYuXerOcJlKqGKpImm/DkCYtH5aszuzS9avhulKVZ1tbXzbN/eyzAnLli2D8fMUecL/tdn52Lx5M8aOHYu9e/dCLw3CfeaZZ3Ds2DH87ne/Q5s2/AhcayOIyN0xOCwsLIyaeuL37Npl6DnrxRaKqHVqKOHbs95abm4u9rw+HzdOeBS3twPe2HcQZ8+exQMPPID8/HzMnj3bVWGzFiKEyCSiMHuP555E5rT6ko69663ddNNNGLAwEYaH7oX/1t3w/fkoTuzZhW9qqnE8cx/Ky8vlju62bdtizpw58PHxUTR+pi6cjJjbWA8tmTt3LrLuDIN/zP/h+pSv0fXGgZg0aRJeeeUVXLx4Edu3b8f48eNx7tw5JCcnY/To0bj77rvd/SMwBXEyYm5j29GtraqAv2Xc240DERQUhH8+9QTKfP0QNqAfjhw5ggsXLiAqKgrz58/HnXfeibZt27rzR2AK4j4jpqjm9CPV59KvBTj+yGhot+7G0n9/iG+//RY9LpajsOIS7r15IDRDQ2EymVBeXo4RI0Zg/PjxSvwYTAHcZ8Tcqjn9SPW58nOG3JSLiYlBdHQ0Dn/7DXxeehbapUvxxnsf4MdNGzFENw5v/zMWmZmZ8PHxARGhuroa0dHRPCOlh+D7p0zVNBOmwPeGfnKTTgiBfhXn5QQVGxuLR578I/644xNE6eOh1+uRnZ2Ncb7A1NG/x874VwAAly5dwpo1a7Bv37461+eHLtWDa0bM5SwDbK1fO9tsA67ta3rslhvhv3U3ektzLA0YMACzN6Zg2NIlwIOP4mBCAqqrq/H49QH48N31GFD4C3pNeQrz5s0DEaFPnz6YNWtWM35CpgRORszlGhpgqxTb5LRw4UIY7xqGDqF34XLW/7A2Ox8//PADsn19EPC/77C8nQ98j+Rh/6ebMVR3H7auXonTp0/Dx8cHlZWVqKysxEsvvYTevXuDiLBnzx4MGDCAx0S6GHdgsxandCe3PYgImUsT0GfcA+hQmAvNhCn48r0N0C6Px8VFqxF6/4P485//jD8H98VNEyZhT/JbmLLqbSQnJyP4pAE7Tp3D82NG4vrH/4TMzEx8+umnmDZtGgYPHuyymD0dd2Az1VO6k9seQgiEvRRrfnHHUADA77p1uvooAYDAwED885PPcMeqlRAPPoojej1++OEHaHtfh3Zff4E32rZDj8JiaH7eh1eSluK96JcweKN55NL69etRVFSEf/zjH+jSpQtqa2uxadMm9O/fH/fcc49LfzZvwcmIqYJtv5JlmytrS7bNu/nz5+Pvw26Vx9BpJkxBdXU1voqbg17vvI+BtZVYm52PlIIiHA4fgl+Cb8UpvR4XLlzASKrCQ8/MwBdzXsAfk99FcnIywi8Z8d2hn9H72CHc+Mz/ISUlBeVfbIZ/2F14ILAvNBOmoLa2FlVVVXzHD9xMYyqm1t/h+U83wXjDAGiKj6PbI1Oh1+uR9uknGHb6V5TfMxbawbfj4MGDaHelCr0yvofvxKnoFaRFeXk59M//HwwP3Qvt1t2ggF5Y9+RkVPTXYkS3zvj9vNfwyy+/YOXKlSAiLFy4EF27dsXly5eRlpaG4cOHo1evXu7+8e3maDONkxFTLdu+JVfXlJrj3Ob/ALfdgXbHsuUaV8YbC9H17pHoee4U1mbn46effkJN/jG0ue563BygQfUN/fFLViYCcw7gF+0tGHHfH/DTTz9hoG9blLb1gW+5ETeOvQ8VFRV4tHsnpBz5BTETH0TX8ZPxwfQIGPw64aGgvgh7KQ4lJSVYtGgRhBCYN28eOnfujNOnTyM5ORmjRo3CyJEjAQAmkwkVFRXo0qWLHHtNTQ2ISPGxf5yMJJyMvI87Or5dbcvLf8ehWoEnhw3GgKefxblz57B2gR6Tv9uC/p9+ixX/2YQtW7ZgyI3BENs/Q7c//hk1vn7Y//0e3HjsELIDB2Hk/Q8gOzsb8WPuwYV+Qbi47weM1i9CXFwc/jHsVqz85jvMvl+Hbo9MxZrHH0VV8M3QVlZgwhsrceLECSxcuBBt27bF888/j5tvvhnHjh3D+vXrodVqMWOGec2MXbt2IS0tDU8//TQGDRoEANi8eTPOnj2LZ599Vh7UXFpaik6dOsHX17d1JiNv/JAy+3jj79567ifNhCkoLy/HnoXzceukx6E5dRxdx0/GhzMikd++Ix4K6oewl2LxyiuvYPf2bbjjpAH+k56Ef89e+PHHH+FPJtzw80/o9uSf0aZzF3y37UvcdjwX2QMGYuQDDyI3NxcdzhSj3fV9ccGQh9sfnogLFy5gTvgdeG/fATx+x63oPe1prH1yEiITl+LfsXMw44MU7N+/H8c/eAd9xz2I4m+2Y/ziFfjkk09wYevHOFwr8PJD96PXlD+6PxlJK4AYAWiJKNme/U2dY802GXEtiFmrL0HZ8vSEVR/bJEZE+CV5FfqMewC1h/dDM2EKvtXHYW/ZRUy+dSAGTn8ONTU1WB0/D/dt2Yg+H6fjrU2p+Pnnn3EqPw+Dj+dC8+Sf0a5LV+zcugWDT+Sh4OYhCB81GkajEfu/34PbT+Thwj1jETT4duzatQvht92Kbp9vwtOffYV+IWHuTUZWSxGlSpPoGyyzOTa0H9I0tA2dY4uTEWsuexKWvTw9sdkmMQAwfpYC//Cr285/ugmXgm5Cx8J8aCaajyl8721UBQ/CdaW/QTNhCkpKSrDl5RfQMfx30PXphW4Tp7o9GSUC2EREWdJUsiFElNTYfphXCGnwHFuWZGT8PAXVxUUe/2Fgnk3JxOYNfG4wP7aghoceNTav7VmqqKlz6ixVBKBKCHG4zgF/ec6BEF2qB4Bz7g6iARybczg25wxy5GBXJCMjHF+qqKlzIPUjWfqXMhzJuC2JY3MOx+YctcfmyPFqWapI08Q5jDEvp/h8RtIyQ9r6lipqaH9D5zDGWg+XjE2z6nxOt9o2ron912xrRKO3/t2MY3MOx+Ycr4nNIx96ZIx5H552ljGmCpyMGGOqwMmIMaYKnIwYY6rAyYgxpgqcjBhjqsDJiDGmCpyMGGOqwMmIMaYKnIwYY6rAyYgxpgotkoyEEF2svq9tifdkjHkWRUftCyHm1LcZgA7AH6xeO3NteabHjh07ht58881OxcgYaxmZmZnniKinvccrPYVIDwCbYE4+lqlAbGdwdGqaAOuZHu1ZN40x5l5CiF8dOV7RZEREMVIQ3Yhov1VQJUq+D2PM+7hkcjUAoUIIwLwMUQjMU8kecNF7Mca8gEs6sIloMYBgAEkwL8q4xBXvwxjzHi5JRkKI6QBCASwC8LYQYowr3qdFvfsuUFho/pcxpjhXNdPyiehtIcRQIiqTmmyebdQoICgIKChwdySMeSVXPWcUKoQYCqC7VCsKddH7tJydO82JaOdOd0fCmFdyyYT8QoiuAGJh7rz+2rrPSAhRS0Rtm3N9vrXPmPo5ury1qzqwy4gohojuA/CN9RPYrQ73NTFmF1d1YMsd1tLzRqpcfrdFWPqaRo1y7LyWTmKcNJmbKT0cZBKAcQDChBD5MA/9IJifN9qh5HvZ5d13zUlg504gMrLF3x5A3b4mR2Jo6Q5z7qBnbqZozYiINgOIBjCdiKYR0VTp31jbY4UQ04UQmxoYz6YMZ2slzWFbw4iMBAIDHU+GLd1hzh30nsULa7Ku6sAeC0BHRLFSZ3YoEe2Q9plgHvBaAHONSQtgmCMPRtrdge2OmlFh4dUaRmBgy7wna3084HOmig5sACWW2hARlQEotdmfQUTfEFEBEX0DYP81V1CCs7WS5uAaBnMF25qQF37OXFUzehlAPsw1n+4AxlmSk1XNKAOAEeaaUYhLakasYWroT2P284CakC1V1IyksWkCwCyYE01s3d30L5g7upNhTlQ8dq2luaM/jV3L3r4fL6wJ2XLZTI9EtJmIZhHREiFEoNUuId36D5CeQ0pwZuya0WjE6dOn62wrKipCRUWF/Lq2thb5+fkwmUzytgsXLuDkyZN1zisuLkZ5ebn82mQyIT8/HzU1NfK2ixcv4sSJE3XO++2333D+/HnrnxkGgwFXrlyRt12+fBm//lp3WpezZ8+ipKTurCqFhYWorKyUX1dVVaGgoADWNdfS0lKcOXOmznnHjx/HpUuX5NfV1dUwGAx1zqu3rD7+GBWHD8sfbk8qq4KCAlRVVcmvHSmry5cvy6+rq6uRn5/fdFkp+blKSkJNXp6cfC7eeSdO2PxROH36NIxGo/yaiGAYORLVffrItVh7yoqIri2rt99Gwe7doA0b5G0lJSU4e/ZsnWv9+uuv15SVPZ+rEydO4OLFi3CGos00IcQmIpomhPgagOWTJ2DuoL5JOsYEc23pgNV5Q61fN2XIkCE0smdP9Lj9doz09cXoxESkpqaisLAQ+fn5WLx4MTp16oRXXnkFWq0Wx48fR3x8PIxGI+IeewwD7rwTt1y+jAlvvomvvvoKGRkZKC4uRnx8PHr06IEFCxbg+uuvR25uLhITE3H58mW8+OKLGDRoEHr37o0nnngCP/zwA7Zv346ysjLMmTMH/fr1w5IlS9ClSxdkZ2dj+fLlqK2txXPPPYfBgwfDz88Pzz77LA4ePIgPP/wQtbW1mD59OgYNGoS33noLbdq0wcGDB7F69WoIIfD8889j8ODBqKqqwt///nfk5+dj1apV6NChAyZNmoTQ0FC8++67KC8vR3Z2NlauXAkfHx+89NJLuPnmm1FSUoKYmBicOnUKCxcuRI8ePfD73/8eo0aNQkpKCo4fP468vDwsWbIEHTt2RFxcHIKDg+uWVVwcBgwYgFtvvRXjx4/H9u3bkZWVhZMnT+LVV19FQEAAXn31Vdxwww2NltX333+Pr776CkajEVFRUejbty8WL16Mrl27NlpWP//8MzZu3IiamhrMmDGjTlkdOnQIb731Vp2yunLlCl544QXk5eVh9erVaN++PaZMmYKQkBBs2LABFRUVOHLkiFxWs2fPxi233ILS0lJER0ejuLgYixYtQo8ePTB69GiMHDnSXFabNyOvXTssufdedJw5E7GxsQgODkZRURH0ej3Onz+PVyZNQv/wcAyuqsLDy5dj27Zt2L9/P4qKirBgwQIEBAQgPj4efdq3R15sLBIKCnD5uuvw4oMPYuCIEbihpASPr1mDPXv2IC0tDefPn0d0dDT69OmDpKQkdOvWDdnZ2Vi2bJlcVrfddhv8/f3xl7/8BQcOHMB///tfVFdXY+bMmRg4cCBWrVqFtm3b4vDhw1i1ahWEEHju6adx+wcfoHr+fPxNr0dubi7WrFmD9u3bY+rUqRg2bFidslq1ahXatWvXYFkFBARgzJgxGDlyJD766COcOHECubm5WLp0KTp27OjWZlqM9G+0dEt/GhFNBTDV5jidEOIxIcRQqVY0rakLCyFmCCEyhBAZZ86cQfgDD+CJ5cuR06kTAOCXX35BREQEgoOD5b8qly9fRkREhPxXraSkBLeMHImIxETkdOgAADhy5AiefvppDB06VP5LWlZWhsjISPmvdkVFBfr374/IyEjk5uYCAI4ePYopPj4YcdNNKFq3DgBw5swZREZGwjIw+MqVK+jWrRsiIyPlv2J5eXl4+OGHcd9998FgMAAATp48iYiICPj7+8NkMoGI4OPjg8jISJw6dQqAueY0ZswYTJw4EceOHQNgriFERkaiV69e8l8xk8mEyMhI+S9kcXEx7rrrLjz++OPIycmpU1ZarVYuq8rKyjplde7cOdxaXo4/jR6N7H//u05ZDRkyRC6r8vJyREZGorq6GoC5htC/f39ERETUKaupU6dixIgRKCoqAmD+K25dVlVVVejevTsiIiLkssrNzcX48eMxbtw4FEjPP508eRKRkZHo0KGDXFa+vr6IjIxEcXGxXFZjx469pqwiIiLQq1cvuQZKRIiMjMS5c+fksho+fDimTZuG7Ozsq2UVFYWgDz9E2dChcqyRkZG4cOGCXFa3jRqFPyUlIdvPTy6rP/3pT7jjjjvkWkd5eTkie/ZE1TPPADt3ory8HIF/+AMiXnwRuTfcAADIycnB1KlTcffdd8tlde7cOURERMj/F6qqqhAQEIDIyEgUFhbKZTVhwgTodLprysrPzw9EBJPJhPZnziDy6FEU79snl5VOp8OECROu+Vz17NmzTllFRETIn6uTJ0/i7rvvxrRp03D06FEAwLFjxxAZGYmgoCCUlZXBYUTk8i8AXay+r5X+nQRgLYA5jl4vNDSUNvzlL5QUE0OVyclERFRWVkavvfYapaSkkMWBAwdo7ty5dPjwYXnbf2bNooUvv0wVa9cSEdGlS5coISGB3n//ffmYo0eP0ty5cykjI0Pe9sknn9CCBQvo/PnzRERUVVVFS+Li6G2ATAYDEREVFBTQvHnzaM+ePfJ527ZtI71eT7/99hsREdXU1NCbTz5Jq+Ljqfadd4iIqLi4mObPn0/p6enyed9++y3Nnz+fTpw4QUREJpOJ1qxZQ8uWLaPq6moiIjp37hzFx8fTli1b5PP27t1L8+bNo7y8PHlbQ2WVmpp6TVkdOXLkalmtWEELAaqQtl28eJESEhLogw8+kI/JycmhuXPnUmZm5tWy+tvfaMHs2XT+rbeultWSJfSvf/2LTCZTk2V15syZq2X15pv01ltvyeedPHmywbIqKipqsKzOnj1L8Y88QlvXryfasKFOWeXn58vXWr9+PS1evJgqKyuvltVjj1Hq6tXyefv377+mrDbOnEmLoqLkz9XFixdp0aJFdcoqOzv7mrL6+OOPacGCBWQ0GuWyWrx4Mb3zzjvyz2wwGGjevHn0/fffy+d9+eWXFB8fX6eslj/xBK1esIBM69fXKatvvvlGPm/Hjh3XlNXq1atp+fLlVFNTc7Ws4uNp69at8nk//PBDnbIymUzXlJXRaKQFjz5Km9esIdqwgWC+a25/nnDk4CYvBsyp5+tlAF9ZHWMC8KyUiIYC6ApgjCPvExoaSorasIGooED+sLn8vIICc9EXFLSe93P2Wkpythxsufpnaenfs5KsYnA0GSndTOsB4BuY+4m+kb6ycHVyfgsDEc2SamZO1OcU5uydpZZ+utrZOJ19PyV/Pntid/VTxbZx1fd+9sRg7+/B2Z9Hyd+zszE4e15z7vo5krns/YJNTQfAUKvvTTDXmIYCGCN9OdRUa7Bm1NI1nJamhjjri8GeuOw5RskanLPvZ08M9v4eWrpGqmQMCtSy4M5mmnxRc9NsDIBAAI9ZJxsAtQC6AEgA8LWjiYgaS0ZqqKY6S6kPoKsTlrP/ge1hb+y2xymZVJQsP7X+8XDleVZUkYzMcWA6gI9sk42lZtScayteM1IDpf5DuzohK/kf2Nkale3P6Oqk4u1c9PtTRTKSEtFaAMNg00EtJaPHbI4f6sj1Fe/AVgNPqRkpyZ7EqdbOcG/ibC2yifMcTUauegI7n8wd1ET1d1DPFELkSlOIfAQgxUVxeA5nO4uVmrLEHezp7KzvGE/6GT2BPb+H+jrVFR6i4sqBsmm4urR1KJnHq1mewNaRNKWItG0smUfv24UHylrxwAGUzAM5MbBaFQNlYR4A+zjMT2SHWBKRhKwTkbTB7kTEbLSCAZRMBVqgNuqqddOmE1FM04exZrN8OLjJwjycq2pGBusX0hpqnsULp/VkTM1cVTOaJYRIhPnpawHzXbWbXPRersET1DPWolxVM0okopvo6qj9WS56H9fhvhjGWpRL7qbJFxcikIgKbbbxirKMtQKquJsmhBgrhMgDkCw9T+TwTI6MsdbFVc00LRHdSET3kXmGx2AXvQ9jzEu47Alsm9f7AEAIEeSi92OMeThX3U1LEkKUwLwUUTcA3YQQBgChzl5QCDED5iWOAKBKCHG42VG6Rg8A59wdRAM4NudwbM4Z5MjBLltRtr6nqoUQOgBfE1GzamRCiAxHOsZaEsfmHI7NOd4Um0tqRg0N7yCidLhweSTGmOfixMAYUwVPTUbJ7g6gERybczg253hNbC596JExxuzlqTUjxpiX4WTEGFMFVz1npBghhAaAVvoKJ6JoaftkmJ9j0hKR29rN0uMKADBObbFZCCES1RabEOI8gAwAaUSUpLLYQmD+vIGIUtUSmxRXihQHAKQTUbQaYgPqLyNHYvOEmtFUAGFWH4oZ0g9oeVTAOiG0KOnDESLFESKE0KolNqsYdZD+Y6kstilENM4mEakltljp89ZdZb/T7kQUTEShMC96sU4tsUnva5DiMAghQhyNTfXJiIiSrTKqFuaJ28JxdQI3A4AQN8WWRURJUu3NQESqiQ0AhBCW8rJQTWwANFJ8FqqITXrSf58QQit99lTzO7X8p5Zo1RQbzLXcFEutkoiyHI1N9cnIQvrglkq/EI3N7oCWj6iOMFwdj6ex2efO2CwfWAuNzX53xtYdQKkQYp30WmOz312xBUvvXSqEWCf9odHYHOPWz5sQYoalpQCVxEZERgDrYG5GWoZ9aWwOazQ2j0lGACYT0UzpeyOurjzidlKCDLZqH7s9NiGEzuYvKaCS2AC5xmsEYFRTuUnypdgyYR4PaYR6YgOAcVbfG6GC2KQmWDoRBcPJ36lHJCMhxGSrvoUQmGcB0Ei7tTAvi+SOuBKlaj1wteBVERvMf9l10odCq7JymyHFY00VsUlxWGhg/r2qJTbLDR1raoktRGqaAcAiOPF/QfXJSMq4iUKITCFEJsydeKkw/wfTAdDUUwNoKetg7qyzxJGsltik/qx0mD8UGmmbKmKDedlz607rVLXEJsWhsXS2qul3KukOoNTyQkWxJUt/ZHQApjpTbvwENmNMFVRfM2KMtQ6cjBhjqsDJiDGmCpyMGGOqwMmIMaYKnIyY21hu7Uvfa4QQUUKIydLzUTohRFQD5+mEEOetnvGCdO46IUSg9XWZ5+BkxNxCevYky2pTCoBU6ZmjdJjHMtW73p6033YEeBYRzbSsYGwz7o15AE5GzCnSqGxLTSbEidrIOMu4OasHDOVxdNL366T9llqTzqo2tA7ATKvraazOTbXZxzyA6uczYqoWAPN4pCwhRKwQwghzUtBK+9JgfvI2tZ5zNVbfa3F1jh6Z1fCCWACbpPdZJ+0zCCHkAdSoOzuB5ZrMg3DNiDlFShRaq4ShgXkaFUviWSc1p7LqO99GBqyShzSHkGUIkAbmqSe6S+PZ1lmdZ6kd6azisCgF8yicjFizSU20dVbNrHDL9zZTmNRLSiQGSz+PdE4agAxp9Hya9XFW5yUD4M5qL8HNNOYUKXFYBpR2d2K6U6P1CyKaIvULWZKNFtIcUdIEdlFCCMvh1gMu021e13t9pn48UJY5RepItkwzars9FEC0VKtp6HzLNKVN1pyciM1l12auw8005jCpH2dKffukqSNmNpaIpOPS4YIpUi3z/XAi8jxcM2KMqQLXjBhjqsDJiDGmCpyMGGOqwMmIMaYKnIwYY6rAyYgxpgr/Dx/1pY39+izwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 302.4x273.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = \"RecoDatapT\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "PREVIOUS_AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime_pT_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "\n",
    "########################################################################################\n",
    "raw_train_data, raw_test_data, raw_valid_data = load_raw_data(AUTOREGRESSIVE_DIST_NAME=PREVIOUS_AUTOREGRESSIVE_DIST_NAME)\n",
    "# Load scaled data\n",
    "# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()\n",
    "\n",
    "\n",
    "################################################## Load Evaluation Data\n",
    "    \n",
    "# eval_data = pd.read_csv(\n",
    "#     os.path.join(\n",
    "#         IQN_BASE,\n",
    "#         \"JupyterBook\",\n",
    "#         \"Cluster\",\n",
    "#         \"EVALUATE\",\n",
    "#         PREVIOUS_AUTOREGRESSIVE_DIST_NAME,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# Or test on actual test (evaluation) data for development\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING == True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "        \n",
    "    ##### WHAT MATTERS IS TEST (EVALUATION)\n",
    "\n",
    "    test_t, test_x = split_t_x(\n",
    "        df=scaled_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting autoregressive evaluation data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "            df=raw_train_data, target=target, input_features=features\n",
    "        )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "        df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    ##### WHAT MATTERS IS TEST (EVALUATION)\n",
    "    test_t, test_x = normal_split_t_x(\n",
    "        df=raw_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "######################################################\n",
    "# Replace test_x with eval_data\n",
    "\n",
    "# ev_features = features\n",
    "# eval_data_df = eval_data[ev_features]\n",
    "# eval_data = np.array(eval_data_df)\n",
    "# test_x = eval_data\n",
    "\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "    \n",
    "# eval_data=raw_train_data[:raw_test_data.shape[0]]\n",
    "# test_x = np.array\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "\n",
    "NFEATURES = train_x.shape[1]\n",
    "TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "# to features\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "###########################################################\n",
    "# Get the  parameters for this model and training\n",
    "PARAMS_pT =  {\n",
    "\"n_layers\": int(15),\n",
    "\"hidden_size\": int(5),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(1e-3),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(1024),\n",
    "    'n_iterations': int(2e5),\n",
    "}\n",
    "\n",
    "optimizer_name = PARAMS_pT[\"optimizer_name\"]\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS = PARAMS_pT[\"n_iterations\"]\n",
    "BATCHSIZE = PARAMS_pT[\"batch_size\"]\n",
    "comment = \"\"\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(\n",
    "    f\"This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs\"\n",
    ")\n",
    "\n",
    "\n",
    "filename_model = utils.get_model_filename(target, PARAMS_pT)\n",
    "# filename_model = 'Trained_IQNx4_RecoDatapT_11_layer5_hiddenLeakyReLU_activation1024_batchsize200_Kiteration.dict'\n",
    "trained_models_dir = \"trained_models\"\n",
    "utils.mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE,  # the loaction of the repo\n",
    "    \"JupyterBook\",  # up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\",\n",
    "    \"TRAIN\",\n",
    "    trained_models_dir,  # /trained_models\n",
    "    filename_model,  # utils.get_model_filename has the saved file format\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "IQN_pT = load_model(PATH_model, PARAMS_pT)\n",
    "# Get predicted distribution\n",
    "p = simple_eval(IQN_pT, test_x_z_scaled)\n",
    "\n",
    "range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "bins = 50\n",
    "REAL_RAW_DATA = raw_test_data\n",
    "\n",
    "YLIM = (0.8, 1.2)\n",
    "###########GET REAL DIST###########\n",
    "REAL_RAW_DATA = REAL_RAW_DATA[\n",
    "    [\"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\", \"RecoDatam\"]\n",
    "]\n",
    "REAL_RAW_DATA.columns = [\"realpT\", \"realeta\", \"realphi\", \"realm\"]\n",
    "REAL_DIST = REAL_RAW_DATA[\"realpT\"]\n",
    "norm_data = REAL_RAW_DATA.shape[0]\n",
    "#############GET EVALUATION DIST#############\n",
    "raw_test_data.describe()\n",
    "pT_reco = raw_test_data[\"RecoDatapT\"]\n",
    "pT_gen = raw_test_data[\"genDatapT\"]\n",
    "# plt.hist(m_reco,label=r'$m_{gen}^{test \\ data}$');plt.legend();plt.show()\n",
    "\n",
    "def descale_Braden_scaled_prediction(label, p):\n",
    "    \"\"\"Label could be m. p is the outcome of the model evaluation, e.g. \n",
    "    IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "    p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "\n",
    "    \"\"\"\n",
    "    # make sure you've set braden scaling global variable to use this function.\n",
    "    assert USE_BRADEN_SCALING==True\n",
    "    orig_ratio = T(label, scaled_df=scaled_train_data)\n",
    "    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))\n",
    "    L_obs = L(orig_observable=pT_gen, label=label)\n",
    "    z_inv_f = z_inv_f.flatten()\n",
    "    print(z_inv_f.shape)\n",
    "\n",
    "    factor = (z_inv_f * (L_obs + 10)) - 10\n",
    "    label_pred = L_inverse(L_observable=factor, label=label)\n",
    "    return label_pred\n",
    "\n",
    "\n",
    "pT_pred = z_inverse2(\n",
    "    xprime=p,\n",
    "    train_mean=TRAIN_SCALE_DICT[target][\"mean\"],\n",
    "    train_std=TRAIN_SCALE_DICT[target][\"std\"],\n",
    ")\n",
    "pT_pred = pT_pred.flatten()\n",
    "\n",
    "# Get histogram of predicted distribution\n",
    "real_label_counts_pT, predicted_label_counts_pT, label_edges_pT = get_hist_simple(\n",
    "    predicted_dist=pT_pred, target=target\n",
    ")\n",
    "\n",
    "# Get evaluation data as test data for development\n",
    "\n",
    "eval_data_df=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')[features]\n",
    "\n",
    "\n",
    "# save new distribution (m) in the eval data as autoregressive eval for next IQN\n",
    "# eval_data_df[target] = pT_pred\n",
    "\n",
    "new_cols = [\"RecoDatam\", target] + X\n",
    "eval_data_df = eval_data_df.reindex(columns=new_cols)\n",
    "print(\"EVALUATION DATA NEW INDEX\\n\", eval_data_df.head())\n",
    "\n",
    "eval_data_df.to_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load this saved predited autoregressive distribution\n",
    "AUTOREGRESSIVE_DIST = pd.read_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]\n",
    "# get normalization values\n",
    "norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]\n",
    "norm_IQN = norm_autoregressive\n",
    "print(\n",
    "    \"norm_data\",\n",
    "    norm_data,\n",
    "    \"\\nnorm IQN\",\n",
    "    norm_IQN,\n",
    "    \"\\nnorm_autoregressive\",\n",
    "    norm_autoregressive,\n",
    ")\n",
    "\n",
    "# Finally, plot predicted distribution\n",
    "plot_one(\n",
    "    target=target,\n",
    "    real_edges=label_edges_pT,\n",
    "    real_counts=real_label_counts_pT,\n",
    "    predicted_counts=predicted_label_counts_pT,\n",
    "    save_plot=True,\n",
    "    PARAMS=PARAMS_pT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48801bd-6864-425d-b4cd-fbe5fdf89790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
