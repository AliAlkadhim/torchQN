{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d6920b-1452-45a8-8751-0b7d6e8e8fd0",
   "metadata": {},
   "source": [
    "# IQNx4: Chapter 3: Autoregressive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c623666-defa-4463-adfe-f7667c7d385d",
   "metadata": {},
   "source": [
    "## 3.1: Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31994c2b-a804-4348-a5a9-8bd8c89ff64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using torch version 1.9.0\n",
      "matplotlib version=  3.5.1\n",
      "using (optional) optuna version 2.8.0\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "using DATA_DIR=/home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: MatplotlibDeprecationWarning: Support for setting an rcParam that expects a str value to a non-str value is deprecated since 3.5 and support will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import scipy as sp; import scipy.stats as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"using torch version {torch.__version__}\")\n",
    "# use numba's just-in-time compiler to speed things up\n",
    "# from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "\n",
    "print(\"matplotlib version= \", mp.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reset matplotlib stle/parameters\n",
    "# reset matplotlib parameters to their defaults\n",
    "# plt.style.use('seaborn-deep')\n",
    "# mp.rcParams['agg.path.chunksize'] = 10000\n",
    "font_legend = 15\n",
    "font_axes = 15\n",
    "# %matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from IPython.display import Image, display\n",
    "# from importlib import import_module\n",
    "# import plotly\n",
    "try:\n",
    "    import optuna\n",
    "\n",
    "    print(f\"using (optional) optuna version {optuna.__version__}\")\n",
    "except Exception:\n",
    "    print(\"optuna is only used for hyperparameter tuning, not critical!\")\n",
    "    pass\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# import sympy as sy\n",
    "# import ipywidgets as wid;\n",
    "\n",
    "# update fonts\n",
    "font = {\"family\": \"serif\", \"size\": 10}\n",
    "mp.rc(\"font\", **font)\n",
    "\n",
    "# set usetex = False if LaTex is not\n",
    "# available on your system or if the\n",
    "# rendering is too slow\n",
    "mp.rcParams.update({\"text.usetex\": True})\n",
    "# plt.rcParams['text.usetex'] = True\n",
    "mp.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]  # for \\text command\n",
    "\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "# seed = 128\n",
    "# rnd  = np.random.RandomState(seed)\n",
    "# sometimes jupyter doesnt initialize MathJax automatically for latex, so do this:\n",
    "\n",
    "try:\n",
    "    IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "    print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "    utils_dir = os.path.join(IQN_BASE, 'utils/')\n",
    "    sys.path.append(utils_dir)\n",
    "    import utils\n",
    "\n",
    "    # usually its not recommended to import everything from a module, but we know\n",
    "    # whats in it so its fine\n",
    "    # from utils import *\n",
    "    print(\"DATA directory also properly set, in %s\" % os.environ[\"DATA_DIR\"])\n",
    "except Exception:\n",
    "    # IQN_BASE=os.getcwd()\n",
    "    print(\n",
    "        \"\"\"\\nBASE directory not properly set. Read repo README.    If you need a function from utils, use the decorator below, or add utils to sys.path\\n\n",
    "    You can also do \n",
    "    os.environ['IQN_BASE']=<ABSOLUTE PATH FOR THE IQN REPO>\n",
    "    or\n",
    "    os.environ['IQN_BASE']=os.getcwd()\"\"\"\n",
    "    )\n",
    "    pass\n",
    "\n",
    "\n",
    "IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "utils_dir = os.path.join(IQN_BASE, \"utils/\")\n",
    "sys.path.append(utils_dir)\n",
    "# usually its not recommended to import everything from a module, but we know\n",
    "# whats in it so its fine\n",
    "\n",
    "# or use joblib for caching on disk\n",
    "from joblib import Memory\n",
    "\n",
    "\n",
    "################################### CONFIGURATIONS ###################################\n",
    "DATA_DIR = os.environ[\"DATA_DIR\"]\n",
    "print(f\"using DATA_DIR={DATA_DIR}\")\n",
    "JUPYTER = False\n",
    "use_subsample = False\n",
    "# use_subsample=True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "memory = Memory(DATA_DIR)\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p(p_T)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "    \"RecoDataeta\": \"$p(\\eta)$\",\n",
    "    \"RecoDataphi\": \"$p(\\phi)$\",\n",
    "    \"RecoDatam\": \"$p(m)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "}\n",
    "\n",
    "loss_y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p_T^{reco}$\",\n",
    "    \"RecoDataeta\": \"$\\eta^{reco}$\",\n",
    "    \"RecoDataphi\": \"$\\phi^{reco}$\",\n",
    "    \"RecoDatam\": \"$m^{reco}$\",\n",
    "}\n",
    "\n",
    "\n",
    "################################### SET DATA CONFIGURATIONS ###################################\n",
    "X = [\"genDatapT\", \"genDataeta\", \"genDataphi\", \"genDatam\", \"tau\"]\n",
    "\n",
    "# set order of training:\n",
    "# pT_first: pT->>m->eta->phi\n",
    "# m_first: m->pT->eta->phi\n",
    "\n",
    "\n",
    "ORDER = \"m_First\"\n",
    "\n",
    "if ORDER == \"m_First\":\n",
    "    FIELDS = {\n",
    "        \"RecoDatam\": {\n",
    "            \"inputs\": X,\n",
    "            \"xlabel\": r\"$m$ (GeV)\",\n",
    "            \"ylabel\": \"$m^{reco}$\",\n",
    "            \"xmin\": 0,\n",
    "            \"xmax\": 25,\n",
    "        },\n",
    "        \"RecoDatapT\": {\n",
    "            \"inputs\": [\"RecoDatam\"] + X,\n",
    "            \"xlabel\": r\"$p_T$ (GeV)\",\n",
    "            \"ylabel\": \"$p_T^{reco}$\",\n",
    "            \"xmin\": 20,\n",
    "            \"xmax\": 80,\n",
    "        },\n",
    "        \"RecoDataeta\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\"] + X,\n",
    "            \"xlabel\": r\"$\\eta$\",\n",
    "            \"ylabel\": \"$\\eta^{reco}$\",\n",
    "            \"xmin\": -5,\n",
    "            \"xmax\": 5,\n",
    "        },\n",
    "        \"RecoDataphi\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\", \"RecoDataeta\"] + X,\n",
    "            \"xlabel\": r\"$\\phi$\",\n",
    "            \"ylabel\": \"$\\phi^{reco}$\",\n",
    "            \"xmin\": -3.2,\n",
    "            \"xmax\": 3.2,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Load and explore raw (unscaled) dataframes\n",
    "\n",
    "\n",
    "all_variable_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "]\n",
    "all_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "    \"tau\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb2b41-f128-45d0-993a-cfdd421b768d",
   "metadata": {},
   "source": [
    "## 3.2: Needed Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29814968-1591-409f-9655-42a962b5eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d3c5cc4-9f36-4642-b4c2-9fc9e3150955",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Load unscaled dataframes ###################################\n",
    "@memory.cache\n",
    "def load_raw_data():\n",
    "    \"\"\"Dont use AUTOREGRESSIVE_DIST_NAME for training of any variable. \n",
    "    For mass evaluation: dont use AUTOREGRESSIVE_DIST_NAME. For pT evaluation use AUTOREGRESSIVE_DIST_NAME \n",
    "    as the distribution predicted by mass, etc.  \"\"\"\n",
    "    print(f\"\\nSUBSAMPLE = {SUBSAMPLE}\\n\")\n",
    "    raw_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    raw_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"validation_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    raw_test_data = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"test_data_10M_2.csv\"), \n",
    "    usecols=all_cols, \n",
    "    nrows=SUBSAMPLE\n",
    "    )\n",
    "\n",
    "    print(\"\\n RAW TRAIN DATA\\n\")\n",
    "    print(raw_train_data.shape)\n",
    "    raw_train_data.describe()  # unscaled\n",
    "    print(\"\\n RAW TEST DATA\\n\")\n",
    "    print(raw_test_data.shape)\n",
    "    raw_test_data.describe()  # unscaled\n",
    "\n",
    "    return raw_train_data, raw_test_data, raw_valid_data\n",
    "\n",
    "\n",
    "########## Generate scaled data###############\n",
    "# scaled_train_data = L_scale_df(raw_train_data, title='scaled_train_data_10M_2.csv',\n",
    "#                              save=True)\n",
    "# print('\\n\\n')\n",
    "# scaled_test_data = L_scale_df(raw_test_data,  title='scaled_test_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "# print('\\n\\n')\n",
    "\n",
    "# scaled_valid_data = L_scale_df(raw_valid_data,  title='scaled_valid_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "\n",
    "# explore_data(df=scaled_train_data, title='Braden Kronheim-L-scaled Dataframe', scaled=True)\n",
    "\n",
    "################ Load scaled data##############\n",
    "@utils.time_type_of_func(tuning_or_training=\"loading\")\n",
    "# @memory.cache\n",
    "def load_scaled_dataframes():\n",
    "    print(\"SCALED TRAIN DATA\")\n",
    "    scaled_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    print(\"TRAINING FEATURES\\n\", scaled_train_data.head())\n",
    "\n",
    "    scaled_test_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_test_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    scaled_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_valid_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    return scaled_train_data, scaled_test_data, scaled_valid_data\n",
    "\n",
    "\n",
    "#######################################\n",
    "#\n",
    "# # print('\\nTESTING FEATURES\\n', scaled_test_data.head())\n",
    "\n",
    "# print('\\ntrain set shape:',  scaled_train_data.shape)\n",
    "# print('\\ntest set shape:  ', scaled_test_data.shape)\n",
    "# # print('validation set shape:', valid_data.shape)\n",
    "# @memory.cache\n",
    "def get_train_scale_dict(USE_BRADEN_SCALING):\n",
    "    if USE_BRADEN_SCALING==True:\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(scaled_train_data)\n",
    "        print(\"BRADEN SCALING DICTIONARY\")\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    else:\n",
    "        print(\"NORMAL UNSCALED DICTIONARY\")\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(raw_train_data)\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    return TRAIN_SCALE_DICT\n",
    "\n",
    "\n",
    "################################ SPLIT###########\n",
    "# @memory.cache\n",
    "def T(variable, scaled_df):\n",
    "    if variable == \"pT\":\n",
    "        L_pT_gen = scaled_df[\"genDatapT\"]\n",
    "        L_pT_reco = scaled_df[\"RecoDatapT\"]\n",
    "        target = (L_pT_reco + 10) / (L_pT_gen + 10)\n",
    "    if variable == \"eta\":\n",
    "        L_eta_gen = scaled_df[\"genDataeta\"]\n",
    "        L_eta_reco = scaled_df[\"RecoDataeta\"]\n",
    "        target = (L_eta_reco + 10) / (L_eta_gen + 10)\n",
    "    if variable == \"phi\":\n",
    "        L_phi_gen = scaled_df[\"genDataphi\"]\n",
    "        L_phi_reco = scaled_df[\"RecoDataphi\"]\n",
    "        target = (L_phi_reco + 10) / (L_phi_gen + 10)\n",
    "    if variable == \"m\":\n",
    "        L_m_gen = scaled_df[\"genDatam\"]\n",
    "        L_m_reco = scaled_df[\"RecoDatam\"]\n",
    "        target = (L_m_reco + 10) / (L_m_gen + 10)\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def split_t_x(df, target, input_features):\n",
    "    \"\"\"Get teh target as the ratio, according to the T equation\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_train_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def split_t_x_test(df, target, input_features):\n",
    "    \"\"\"Get teh target as the ratio, according to the T equation\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_test_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_test_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# @memory.cache\n",
    "def normal_split_t_x(df, target, input_features):\n",
    "    # change from pandas dataframe format to a numpy\n",
    "    # array of the specified types\n",
    "    # t = np.array(df[target])\n",
    "    t = np.array(df[target])\n",
    "    x = np.array(df[input_features])\n",
    "    return t, x\n",
    "\n",
    "\n",
    "################ Apply Z scaling############\n",
    "def z(x):\n",
    "    eps = 1e-20\n",
    "    return (x - np.mean(x)) / (np.std(x) + eps)\n",
    "\n",
    "\n",
    "def z_inverse(xprime, x):\n",
    "    return xprime * np.std(x) + np.mean(x)\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z2(x, mean, std):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x ([type]): [description]\n",
    "        mean ([type]): [description]\n",
    "        std ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    scaled = (x - mean) / (std + eps)\n",
    "    return np.array(scaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "def z_inverse(xprime, x):\n",
    "    unscaled = xprime * np.std(x) + np.mean(x)\n",
    "    return np.array(unscaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z_inverse2(xprime, train_mean, train_std):\n",
    "    \"\"\"mean original train mean, std: original. Probably not needed\"\"\"\n",
    "    return xprime * train_std + train_mean\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x):\n",
    "    \"\"\"TO ensure this z scaling is only applied once to the training features, we use a generator.\n",
    "    This doesn't change the shapes of anything, just applies z to all the feature columns other than tau\"\"\"\n",
    "    NFEATURES = train_x.shape[1]\n",
    "    for i in range(NFEATURES - 1):\n",
    "        variable = list(TRAIN_SCALE_DICT)[i]\n",
    "        train_mean = float(TRAIN_SCALE_DICT[variable][\"mean\"])\n",
    "        train_std = float(TRAIN_SCALE_DICT[variable][\"std\"])\n",
    "        train_x[:, i] = z2(train_x[:, i], mean=train_mean, std=train_std)\n",
    "        test_x[:, i] = z2(test_x[:, i], mean=train_mean, std=train_std)\n",
    "        valid_x[:, i] = z2(valid_x[:, i], mean=train_mean, std=train_std)\n",
    "    yield train_x\n",
    "    yield test_x\n",
    "    yield valid_x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_targets(train_t, test_t, valid_t):\n",
    "    train_mean = np.mean(train_t)\n",
    "    train_std = np.std(train_t)\n",
    "    train_t_ = z2(train_t, mean=train_mean, std=train_std)\n",
    "    test_t_ = z2(test_t, mean=train_mean, std=train_std)\n",
    "    valid_t_ = z2(valid_t, mean=train_mean, std=train_std)\n",
    "\n",
    "    yield train_t_\n",
    "    yield test_t_\n",
    "    yield valid_t_\n",
    "\n",
    "\n",
    "# @utils.debug\n",
    "def save_model(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "# @utils.debug\n",
    "def load_model(PATH, PARAMS):\n",
    "    # n_layers = int(BEST_PARAMS[\"n_layers\"])\n",
    "    # hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "    # dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "    # optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "    # learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "    # batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "    model = utils.RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    # OR\n",
    "    # model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED! Also, use dictionary \".pth\" which has both the model state dict and the PARAMS dict\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def simple_eval(model, test_x_z_scaled):\n",
    "    model.eval()\n",
    "    # evaluate on the scaled features\n",
    "    valid_x_tensor = torch.from_numpy(test_x_z_scaled).float()\n",
    "    # valid_x_tensor=torch.from_numpy(train_x).float()\n",
    "    pred = model(valid_x_tensor)\n",
    "    p = pred.detach().numpy()\n",
    "    # if USE_BRADEN_SCALING:\n",
    "    #     fig, ax = plt.subplots(1,1)\n",
    "    #     label=FIELDS[target]['ylabel']\n",
    "    #     ax.hist(p, label=f'Predicted post-z ratio for {label}', alpha=0.4, density=True)\n",
    "    #     # orig_ratio = z(T('m', scaled_df=scaled_train_data))\n",
    "    #     orig_ratio = z(T('m', scaled_df=scaled_test_data))\n",
    "    #     print(orig_ratio[:5])\n",
    "    #     ax.hist(orig_ratio, label = f'original post-z ratio for {label}', alpha=0.4,density=True)\n",
    "    #     ax.grid()\n",
    "    #     set_axes(ax, xlabel='predicted $T$')\n",
    "    # print('predicted ratio shape: ', p.shape)\n",
    "    return p\n",
    "\n",
    "def get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME):\n",
    "        \n",
    "    print(f'Test (evaluation) Data is Autoregressive, loading {AUTOREGRESSIVE_DIST_NAME}')\n",
    "    eval_data = pd.read_csv(\n",
    "        os.path.join(\n",
    "            IQN_BASE,\n",
    "            \"JupyterBook\",\n",
    "            \"Cluster\",\n",
    "            \"EVALUATE\",\n",
    "            AUTOREGRESSIVE_DIST_NAME,\n",
    "        )\n",
    "    )\n",
    "    return eval_data\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def get_hist(label):\n",
    "    \"\"\"label could be \"pT\", \"eta\", \"phi\", \"m\" \"\"\"\n",
    "    predicted_label_counts, label_edges = np.histogram(\n",
    "        JETS_DICT[\"Predicted_RecoData\" + label][\"dist\"],\n",
    "        range=JETS_DICT[\"Predicted_RecoData\" + label][\"range\"],\n",
    "        bins=bins,\n",
    "    )\n",
    "    real_label_counts, _ = np.histogram(\n",
    "        JETS_DICT[\"Real_RecoData\" + label][\"dist\"],\n",
    "        range=JETS_DICT[\"Real_RecoData\" + label][\"range\"],\n",
    "        bins=bins,\n",
    "    )\n",
    "    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2\n",
    "\n",
    "    return real_label_counts, predicted_label_counts, label_edges\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def get_hist_simple(predicted_dist, target):\n",
    "    \n",
    "    range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "    bins=50\n",
    "    predicted_label_counts, label_edges = np.histogram(\n",
    "        predicted_dist, range=range_, bins=bins\n",
    "    )\n",
    "    \n",
    "    \n",
    "    real_label_counts, _ = np.histogram(REAL_DIST, range=range_, bins=bins)\n",
    "    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2\n",
    "    return real_label_counts, predicted_label_counts, label_edges\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def plot_one(\n",
    "    target, real_edges, real_counts, predicted_counts, save_plot=False, PARAMS=None, JUPYTER=True\n",
    "):\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2, 1, figsize=(3.5 * 3 / 2.5, 3.8), gridspec_kw={\"height_ratios\": [2, 0.5]}\n",
    "    )\n",
    "    ax1.step(\n",
    "        real_edges, real_counts / norm_data, where=\"mid\", color=\"k\", linewidth=0.5\n",
    "    )  # step real_count_pt\n",
    "    ax1.step(\n",
    "        real_edges,\n",
    "        predicted_counts / norm_IQN,\n",
    "        where=\"mid\",\n",
    "        color=\"#D7301F\",\n",
    "        linewidth=0.5,\n",
    "    )  # step predicted_count_pt\n",
    "    ax1.scatter(\n",
    "        real_edges,\n",
    "        real_counts / norm_data,\n",
    "        label=\"reco\",\n",
    "        color=\"k\",\n",
    "        facecolors=\"none\",\n",
    "        marker=\"o\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax1.scatter(\n",
    "        real_edges,\n",
    "        predicted_counts / norm_IQN,\n",
    "        label=\"predicted\",\n",
    "        color=\"#D7301F\",\n",
    "        marker=\"x\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax1.set_xlim(range_)\n",
    "    ax1.set_ylim(0, max(predicted_counts / norm_IQN) * 1.1)\n",
    "    ax1.set_ylabel(\"counts\")\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    ratio = (predicted_counts / norm_IQN) / (real_counts / norm_data)\n",
    "    ax2.scatter(\n",
    "        real_edges, ratio, color=\"r\", marker=\"x\", s=5, linewidth=0.5\n",
    "    )  # PREDICTED (IQN)/Reco (Data)\n",
    "    ax2.scatter(\n",
    "        real_edges,\n",
    "        ratio / ratio,\n",
    "        color=\"k\",\n",
    "        marker=\"o\",\n",
    "        facecolors=\"none\",\n",
    "        s=5,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax2.set_xlim(range_)\n",
    "    ax2.set_xlabel(FIELDS[target][\"xlabel\"])\n",
    "    ax2.set_ylabel(\n",
    "        r\"$\\frac{\\textnormal{predicted}}{\\textnormal{reco}}$\"\n",
    "        #    , fontsize=10\n",
    "    )\n",
    "    ax2.set_ylim((YLIM))\n",
    "    ax2.set_xlim(range_)\n",
    "    ax2.set_yticklabels([])\n",
    "    if JUPYTER==True:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(wspace=0.5, hspace=0.2)\n",
    "        fig.subplots_adjust(wspace=0.0, hspace=0.1)\n",
    "        plt.axis('off')\n",
    "\n",
    "    # plt.gca().set_position([0, 0, 1, 1])\n",
    "    if save_plot:\n",
    "        plot_filename = utils.get_model_filename(target, PARAMS).split(\".dict\")[0] + \".png\"\n",
    "        plt.savefig(\n",
    "            os.path.join(IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", plot_filename)\n",
    "        )\n",
    "\n",
    "    \n",
    "    # fig.show()\n",
    "    # plt.show();\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.gca().set_position([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3336b47e-6e8a-43ba-a93a-5067d6b44dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-2930683716.load_raw_data...\n",
      "load_raw_data()\n",
      "\n",
      "SUBSAMPLE = None\n",
      "\n",
      "\n",
      " RAW TRAIN DATA\n",
      "\n",
      "(8000000, 9)\n",
      "\n",
      " RAW TEST DATA\n",
      "\n",
      "(1000000, 9)\n",
      "___________________________________________________load_raw_data - 23.2s, 0.4min\n"
     ]
    }
   ],
   "source": [
    "#load data only once, and with caching!\n",
    "aw_train_data, raw_test_data, raw_valid_data = load_raw_data()\n",
    "\n",
    "# Load scaled data\n",
    "# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d40d613-52be-4b51-ab10-822df7976083",
   "metadata": {},
   "source": [
    "## 3.3: Evaluate Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42c5cef-a246-45ea-9ee4-d4d22452a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n",
      "spliting data for RecoDatam\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 5)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 5)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 5)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 3.27223764e+01  6.98189368e-04 -8.95543973e-04  6.96116528e+00\n",
      "  5.00485136e-01] [15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]\n",
      "[ 3.26952341e+01 -1.78188172e-03 -3.83090331e-04  6.96299435e+00\n",
      "  4.99915289e-01] [14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]\n",
      "5.5514112643334546 2.664124544901276\n",
      "5.555567451922438 2.664339857066051\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[ 1.81700902e-03  1.12510099e-03 -2.82526482e-04 -6.57626105e-04\n",
      "  5.00485136e-01] [1.01748627 0.9999745  0.99989115 0.99987283 0.28852734]\n",
      "[ 1.61650249e-15 -1.10134124e-18 -3.12905257e-17  5.01036723e-15\n",
      "  4.99915289e-01] [1.         1.         1.         1.         0.28867295]\n",
      "-0.0015599314696879494 0.9999191874249058\n",
      "6.996216939114674e-16 1.0000000000000002\n",
      "<class 'str'>\n",
      "This model was trained for 5000000 iteration, which is  640.0 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (3): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.3)\n",
      "    (5): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (6): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (9): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.3)\n",
      "    (11): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (12): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.3)\n",
      "    (14): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (15): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.3)\n",
      "    (17): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (18): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.3)\n",
      "    (20): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (21): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.3)\n",
      "    (23): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (24): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): LeakyReLU(negative_slope=0.3)\n",
      "    (26): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "EVALUATION DATA NEW INDEX\n",
      "    RecoDatam  genDatapT  genDataeta  genDataphi  genDatam       tau\n",
      "0   4.784566    43.6113    0.824891    -1.26949   5.93310  0.250046\n",
      "1   6.989782    43.6113    0.824891    -1.26949   5.93310  0.847493\n",
      "2   5.444815    26.0153    3.529970     1.55495   7.41270  0.851995\n",
      "3   3.631184    28.4944   -1.159650     1.82602   7.84157  0.052378\n",
      "4   3.358185    21.9840    2.747660     2.03085   5.18315  0.542549\n",
      "norm_data 1000000 \n",
      "norm IQN 1000000 \n",
      "norm_autoregressive 1000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 302.4x273.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: plot the loss curves (train and valid) of all 4 networks on the same plot,\n",
    "# and with the learning rate plotted on the same plot but on a different y axis \n",
    "# (x axis being iteration, with marks indicating epochs)\n",
    "\n",
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "########################################################################################\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING==True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = split_t_x(\n",
    "        df=scaled_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "        df=raw_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "        df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = normal_split_t_x(\n",
    "        df=raw_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "######################################################\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "\n",
    "NFEATURES = train_x.shape[1]\n",
    "TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "# to features\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "###########################################################\n",
    "# Get the  parameters for this model and training\n",
    "PARAMS_m = {\n",
    "\"n_layers\": int(10),\n",
    "\"hidden_size\": int(6),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(1e-1),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(512),\n",
    "    'n_iterations': int(3e5),\n",
    "}\n",
    "optimizer_name = PARAMS_m[\"optimizer_name\"]\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS = PARAMS_m[\"n_iterations\"]\n",
    "BATCHSIZE = PARAMS_m[\"batch_size\"]\n",
    "comment = \"\"\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(\n",
    "    f\"This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs\"\n",
    ")\n",
    "\n",
    "# 'Trained_IQNx4_%s_TUNED.dict' % target\n",
    "filename_model = utils.get_model_filename(target, PARAMS_m)\n",
    "# OR, if you know a model filename directly, you can also specify it, \n",
    "# BUT, if you pull a trained model explicitly, you have to make sure its parameters in the PARAMS dictionary above match\n",
    "# Nominal one is 'Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict', also in backup\n",
    "# filename_model='Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict'\n",
    "filename_model='Trained_IQNx4_RecoDatapT_10_layer6_hiddenLeakyReLU_activation512_batchsize300_Kiteration.dict'\n",
    "trained_models_dir = \"trained_models\"\n",
    "utils.mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE,  # the loaction of the repo\n",
    "    \"JupyterBook\",  # up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\",\n",
    "    \"TRAIN\",\n",
    "    trained_models_dir,  # /trained_models\n",
    "    filename_model,  # utils.get_model_filename has the saved file format\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "# Get predicted distribution\n",
    "p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "\n",
    "range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "bins = 50\n",
    "REAL_RAW_DATA = raw_test_data\n",
    "\n",
    "YLIM = (0.8, 1.2)\n",
    "###########GET REAL DIST###########\n",
    "REAL_RAW_DATA = REAL_RAW_DATA[\n",
    "    [\"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\", \"RecoDatam\"]\n",
    "]\n",
    "REAL_RAW_DATA.columns = [\"realpT\", \"realeta\", \"realphi\", \"realm\"]\n",
    "REAL_DIST = REAL_RAW_DATA[\"realm\"]\n",
    "norm_data = REAL_RAW_DATA.shape[0]\n",
    "#############GET EVALUATION DIST#############\n",
    "raw_test_data.describe()\n",
    "m_reco = raw_test_data[\"RecoDatam\"]\n",
    "m_gen = raw_test_data[\"genDatam\"]\n",
    "# plt.hist(m_reco,label=r'$m_{gen}^{test \\ data}$');plt.legend();plt.show()\n",
    "\n",
    "\n",
    "def descale_Braden_scaled_prediction(label, p):\n",
    "    \"\"\"Label could be m. p is the outcome of the model evaluation, e.g. \n",
    "    IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "    p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "    \n",
    "    \"\"\"\n",
    "    # make sure you've set braden scaling global variable to use this function.\n",
    "    assert USE_BRADEN_SCALING==True\n",
    "    orig_ratio = T(label, scaled_df=scaled_train_data)\n",
    "    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))\n",
    "    L_obs = L(orig_observable=m_gen, label=label)\n",
    "    z_inv_f = z_inv_f.flatten()\n",
    "    print(z_inv_f.shape)\n",
    "\n",
    "    factor = (z_inv_f * (L_obs + 10)) - 10\n",
    "    label_pred = L_inverse(L_observable=factor, label=label)\n",
    "    return label_pred\n",
    "    \n",
    "    \n",
    "m_pred = z_inverse2(\n",
    "    xprime=p,\n",
    "    train_mean=TRAIN_SCALE_DICT[target][\"mean\"],\n",
    "    train_std=TRAIN_SCALE_DICT[target][\"std\"],\n",
    ")\n",
    "m_pred = m_pred.flatten()\n",
    "\n",
    "# Get histogram of predicted distribution\n",
    "real_label_counts_m, predicted_label_counts_m, label_edges_m = get_hist_simple(\n",
    "    predicted_dist=m_pred, target=target\n",
    ")\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "# Get evaluation data\n",
    "eval_data = pd.read_csv(DATA_DIR + \"/test_data_10M_2.csv\")\n",
    "ev_features = features\n",
    "eval_data = eval_data[ev_features]\n",
    "# save new distribution (m) in the eval data as autoregressive eval for next IQN\n",
    "eval_data[target] = m_pred\n",
    "\n",
    "new_cols = [target] + features\n",
    "eval_data = eval_data.reindex(columns=new_cols)\n",
    "print(\"EVALUATION DATA NEW INDEX\\n\", eval_data.head())\n",
    "\n",
    "eval_data.to_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load this saved predited autoregressive distribution\n",
    "AUTOREGRESSIVE_DIST = pd.read_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]\n",
    "# get normalization values\n",
    "norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]\n",
    "norm_IQN = norm_autoregressive\n",
    "print(\n",
    "    \"norm_data\",\n",
    "    norm_data,\n",
    "    \"\\nnorm IQN\",\n",
    "    norm_IQN,\n",
    "    \"\\nnorm_autoregressive\",\n",
    "    norm_autoregressive,\n",
    ")\n",
    "\n",
    "# Finally, plot predicted distribution\n",
    "\n",
    "plot_one(\n",
    "    target=target,\n",
    "    real_edges=label_edges_m,\n",
    "    real_counts=real_label_counts_m,\n",
    "    predicted_counts=predicted_label_counts_m,\n",
    "    save_plot=True,\n",
    "    PARAMS=PARAMS_m\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df205105-a24d-4694-a182-ef84c08bfd4c",
   "metadata": {},
   "source": [
    "## 3.4: Evaluate $p_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed548cae-19f9-44d7-b30f-1328fd8fb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['RecoDatam', 'genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatapT\n",
      "USING NEW DATASET\n",
      "\n",
      "\n",
      "SUBSAMPLE = None\n",
      "\n",
      "\n",
      " RAW TRAIN DATA\n",
      "\n",
      "(8000000, 9)\n",
      "\n",
      " RAW TEST DATA\n",
      "\n",
      "(1000000, 9)\n",
      "spliting autoregressive evaluation data for RecoDatapT\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 6)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[ 2.59587    29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [ 6.25659    41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [ 6.11213    35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [ 4.17483    26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 6)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 6)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 5.55141126e+00  3.27223764e+01  6.98189368e-04 -8.95543973e-04\n",
      "  6.96116528e+00  5.00485136e-01] [ 2.66412454 15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]\n",
      "[ 5.55556745e+00  3.26952341e+01 -1.78188172e-03 -3.83090331e-04\n",
      "  6.96299435e+00  4.99915289e-01] [ 2.66433986 14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]\n",
      "32.881453465999996 16.02400426348493\n",
      "32.86720151648752 15.829355769531851\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[-1.81710707e+00  1.48455353e+01  5.96132264e-04 -2.50379668e+00\n",
      " -1.63658184e+00  5.00485136e-01] [0.17834627 6.89519305 1.2152514  0.65207164 0.17568487 0.28852734]\n",
      "[-1.81682884e+00  1.48332220e+01 -7.71183141e-04 -2.50361243e+00\n",
      " -1.63646629e+00  4.99915289e-01] [0.17836068 6.77669391 1.21528238 0.65214262 0.17570722 0.28867295]\n",
      "0.0009003493079555966 1.0122966781963252\n",
      "-1.2048033681821834e-15 1.0000000000000002\n",
      "<class 'str'>\n",
      "This model was trained for 300000 iteration, which is  19.2 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (3): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.3)\n",
      "    (5): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (6): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (9): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.3)\n",
      "    (11): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (12): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.3)\n",
      "    (14): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (15): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.3)\n",
      "    (17): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (18): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.3)\n",
      "    (20): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (21): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.3)\n",
      "    (23): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (24): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): LeakyReLU(negative_slope=0.3)\n",
      "    (26): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (27): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (28): LeakyReLU(negative_slope=0.3)\n",
      "    (29): Linear(in_features=6, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "EVALUATION DATA NEW INDEX\n",
      "    RecoDatam  RecoDatapT  genDatapT  genDataeta  genDataphi  genDatam  \\\n",
      "0    5.80270     44.3274    43.6113    0.824891    -1.26949   5.93310   \n",
      "1    5.80270     44.3274    43.6113    0.824891    -1.26949   5.93310   \n",
      "2    4.81403     27.4750    26.0153    3.529970     1.55495   7.41270   \n",
      "3    7.06425     33.8797    28.4944   -1.159650     1.82602   7.84157   \n",
      "4    4.08061     23.3141    21.9840    2.747660     2.03085   5.18315   \n",
      "\n",
      "        tau  \n",
      "0  0.250046  \n",
      "1  0.847493  \n",
      "2  0.851995  \n",
      "3  0.052378  \n",
      "4  0.542549  \n",
      "norm_data 1000000 \n",
      "norm IQN 1000000 \n",
      "norm_autoregressive 1000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 302.4x273.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = \"RecoDatapT\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "PREVIOUS_AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime.csv\"\n",
    "AUTOREGRESSIVE_DIST_NAME = \"AUTOREGRESSIVE_m_Prime_pT_Prime.csv\"\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING = False\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "raw_train_data, raw_test_data, raw_valid_data = load_raw_data()\n",
    "\n",
    "\n",
    "# Load scaled data\n",
    "# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()\n",
    "\n",
    "\n",
    "################################################## Load Evaluation Data\n",
    "#eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "# Or test on actual test (evaluation) data for development\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING == True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "        \n",
    "    ##### WHAT MATTERS IS TEST (EVALUATION)\n",
    "\n",
    "    test_t, test_x = split_t_x(\n",
    "        df=scaled_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting autoregressive evaluation data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "            df=raw_train_data, target=target, input_features=features\n",
    "        )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "        df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    ##### WHAT MATTERS IS TEST (EVALUATION)\n",
    "    test_t, test_x = normal_split_t_x(\n",
    "        df=raw_test_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "######################################################\n",
    "# Replace test_x with eval_data\n",
    "\n",
    "# ev_features = features\n",
    "# eval_data_df = eval_data[ev_features]\n",
    "# eval_data = np.array(eval_data_df)\n",
    "# test_x = eval_data\n",
    "\n",
    "# eval_data=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')\n",
    "    \n",
    "# eval_data=raw_train_data[:raw_test_data.shape[0]]\n",
    "# test_x = np.array\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "\n",
    "NFEATURES = train_x.shape[1]\n",
    "# GET EVALUATION DATASET\n",
    "# eval_data= get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME=PREVIOUS_AUTOREGRESSIVE_DIST_NAME)\n",
    "# test_x = np.array(eval_data[features])\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "# to features\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "###########################################################\n",
    "# Get the  parameters for this model and training\n",
    "PARAMS_pT =  {\n",
    "\"n_layers\": int(10),\n",
    "\"hidden_size\": int(6),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(1e-2),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(512),\n",
    "    'n_iterations': int(3e5),\n",
    "}\n",
    "\n",
    "optimizer_name = PARAMS_pT[\"optimizer_name\"]\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS = PARAMS_pT[\"n_iterations\"]\n",
    "BATCHSIZE = PARAMS_pT[\"batch_size\"]\n",
    "comment = \"\"\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(\n",
    "    f\"This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs\"\n",
    ")\n",
    "\n",
    "\n",
    "filename_model = utils.get_model_filename(target, PARAMS_pT)\n",
    "# filename_model = 'Trained_IQNx4_RecoDatapT_ 13_layer6_hiddenLeakyReLU_activation1024_batchsize200_Kiteration.dict'\n",
    "filename_model = 'Trained_IQNx4_RecoDatapT_10_layer6_hiddenLeakyReLU_activation512_batchsize300_Kiteration.dict'\n",
    "trained_models_dir = \"trained_models\"\n",
    "utils.mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE,  # the loaction of the repo\n",
    "    \"JupyterBook\",  # up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\",\n",
    "    \"TRAIN\",\n",
    "    trained_models_dir,  # /trained_models\n",
    "    filename_model,  # utils.get_model_filename has the saved file format\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "IQN_pT = load_model(PATH_model, PARAMS_pT)\n",
    "# Get predicted distribution\n",
    "p = simple_eval(IQN_pT, test_x_z_scaled)\n",
    "\n",
    "range_ = (FIELDS[target][\"xmin\"], FIELDS[target][\"xmax\"])\n",
    "bins = 50\n",
    "REAL_RAW_DATA = raw_test_data\n",
    "\n",
    "YLIM = (0.8, 1.2)\n",
    "###########GET REAL DIST###########\n",
    "REAL_RAW_DATA = REAL_RAW_DATA[\n",
    "    [\"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\", \"RecoDatam\"]\n",
    "]\n",
    "REAL_RAW_DATA.columns = [\"realpT\", \"realeta\", \"realphi\", \"realm\"]\n",
    "REAL_DIST = REAL_RAW_DATA[\"realpT\"]\n",
    "norm_data = REAL_RAW_DATA.shape[0]\n",
    "#############GET EVALUATION DIST#############\n",
    "raw_test_data.describe()\n",
    "pT_reco = raw_test_data[\"RecoDatapT\"]\n",
    "pT_gen = raw_test_data[\"genDatapT\"]\n",
    "# plt.hist(m_reco,label=r'$m_{gen}^{test \\ data}$');plt.legend();plt.show()\n",
    "\n",
    "def descale_Braden_scaled_prediction(label, p):\n",
    "    \"\"\"Label could be m. p is the outcome of the model evaluation, e.g. \n",
    "    IQN_m = load_model(PATH_model, PARAMS_m)\n",
    "    p = simple_eval(IQN_m, test_x_z_scaled)\n",
    "\n",
    "    \"\"\"\n",
    "    # make sure you've set braden scaling global variable to use this function.\n",
    "    assert USE_BRADEN_SCALING==True\n",
    "    orig_ratio = T(label, scaled_df=scaled_train_data)\n",
    "    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))\n",
    "    L_obs = L(orig_observable=pT_gen, label=label)\n",
    "    z_inv_f = z_inv_f.flatten()\n",
    "    print(z_inv_f.shape)\n",
    "\n",
    "    factor = (z_inv_f * (L_obs + 10)) - 10\n",
    "    label_pred = L_inverse(L_observable=factor, label=label)\n",
    "    return label_pred\n",
    "\n",
    "\n",
    "pT_pred = z_inverse2(\n",
    "    xprime=p,\n",
    "    train_mean=TRAIN_SCALE_DICT[target][\"mean\"],\n",
    "    train_std=TRAIN_SCALE_DICT[target][\"std\"],\n",
    ")\n",
    "pT_pred = pT_pred.flatten()\n",
    "\n",
    "# Get histogram of predicted distribution\n",
    "real_label_counts_pT, predicted_label_counts_pT, label_edges_pT = get_hist_simple(\n",
    "    predicted_dist=pT_pred, target=target\n",
    ")\n",
    "\n",
    "# Get evaluation data as test data for development\n",
    "\n",
    "eval_data_df=pd.read_csv(DATA_DIR+'/test_data_10M_2.csv')#[features]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_cols = [\"RecoDatam\", target] + X\n",
    "eval_data_df = eval_data_df.reindex(columns=new_cols)\n",
    "print(\"EVALUATION DATA NEW INDEX\\n\", eval_data_df.head())\n",
    "# save \n",
    "eval_data_df.to_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load this saved predited autoregressive distribution\n",
    "AUTOREGRESSIVE_DIST = pd.read_csv(\n",
    "    os.path.join(\n",
    "        IQN_BASE, \"JupyterBook\", \"Cluster\", \"EVALUATE\", AUTOREGRESSIVE_DIST_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]\n",
    "# get normalization values\n",
    "norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]\n",
    "norm_IQN = norm_autoregressive\n",
    "print(\n",
    "    \"norm_data\",\n",
    "    norm_data,\n",
    "    \"\\nnorm IQN\",\n",
    "    norm_IQN,\n",
    "    \"\\nnorm_autoregressive\",\n",
    "    norm_autoregressive,\n",
    ")\n",
    "\n",
    "# Finally, plot predicted distribution\n",
    "plot_one(\n",
    "    target=target,\n",
    "    real_edges=label_edges_pT,\n",
    "    real_counts=real_label_counts_pT,\n",
    "    predicted_counts=predicted_label_counts_pT,\n",
    "    save_plot=True,\n",
    "    PARAMS=PARAMS_pT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48801bd-6864-425d-b4cd-fbe5fdf89790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
