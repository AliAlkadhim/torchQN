{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ddfd24-43c6-44ac-b171-7269c28f4f93",
   "metadata": {},
   "source": [
    "# 2.1 Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "faf410b4-9e3a-4b0d-9bac-fd65f3997d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using torch version 1.9.0\n",
      "matplotlib version=  3.5.1\n",
      "using (optional) optuna version 2.8.0\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "using DATA_DIR=/home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import scipy as sp; import scipy.stats as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"using torch version {torch.__version__}\")\n",
    "# use numba's just-in-time compiler to speed things up\n",
    "# from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "\n",
    "print(\"matplotlib version= \", mp.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reset matplotlib stle/parameters\n",
    "import matplotlib as mpl\n",
    "\n",
    "# reset matplotlib parameters to their defaults\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use(\"seaborn-deep\")\n",
    "mp.rcParams[\"agg.path.chunksize\"] = 10000\n",
    "font_legend = 15\n",
    "font_axes = 15\n",
    "# %matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "#or use joblib for caching on disk\n",
    "from joblib import  Memory\n",
    "# from IPython.display import Image, display\n",
    "# from importlib import import_module\n",
    "# import plotly\n",
    "try:\n",
    "    import optuna\n",
    "\n",
    "    print(f\"using (optional) optuna version {optuna.__version__}\")\n",
    "except Exception:\n",
    "    print(\"optuna is only used for hyperparameter tuning, not critical!\")\n",
    "    pass\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# import sympy as sy\n",
    "# import ipywidgets as wid;\n",
    "\n",
    "\n",
    "# try:\n",
    "#     IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "#     print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "#     utils_dir = os.path.join(IQN_BASE, \"utils/\")\n",
    "#     sys.path.append(utils_dir)\n",
    "#     import utils\n",
    "\n",
    "#     # usually its not recommended to import everything from a module, but we know\n",
    "#     # whats in it so its fine\n",
    "#     from utils import *\n",
    "\n",
    "#     print(\"DATA directory also properly set, in %s\" % os.environ[\"DATA_DIR\"])\n",
    "# except Exception:\n",
    "#     # IQN_BASE=os.getcwd()\n",
    "#     print(\n",
    "#         \"\"\"\\nBASE directory not properly set. Read repo README.    If you need a function from utils, use the decorator below, or add utils to sys.path\\n\n",
    "#     You can also do \n",
    "#     os.environ['IQN_BASE']=<ABSOLUTE PATH FOR THE IQN REPO>\n",
    "#     or\n",
    "#     os.environ['IQN_BASE']=os.getcwd()\"\"\"\n",
    "#     )\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "# @debug\n",
    "# def get_model_params_simple():\n",
    "#     dropout=0.2\n",
    "#     n_layers = 2\n",
    "#     n_hidden=32\n",
    "#     starting_learning_rate=1e-3\n",
    "#     print('n_iterations, n_layers, n_hidden, starting_learning_rate, dropout')\n",
    "#     return n_iterations, n_layers, n_hidden, starting_learning_rate, dropout\n",
    "\n",
    "\n",
    "# update fonts\n",
    "FONTSIZE = 14\n",
    "font = {\"family\": \"serif\", \"weight\": \"normal\", \"size\": FONTSIZE}\n",
    "mp.rc(\"font\", **font)\n",
    "\n",
    "# set usetex = False if LaTex is not\n",
    "# available on your system or if the\n",
    "# rendering is too slow\n",
    "mp.rc(\"text\", usetex=True)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "# seed = 128\n",
    "# rnd  = np.random.RandomState(seed)\n",
    "# sometimes jupyter doesnt initialize MathJax automatically for latex, so do this:\n",
    "#######\n",
    "\n",
    "\n",
    "IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "utils_dir = os.path.join(IQN_BASE, \"utils/\")\n",
    "sys.path.append(utils_dir)\n",
    "import utils\n",
    "\n",
    "# usually its not recommended to import everything from a module, but we know\n",
    "# whats in it so its fine\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "DATA_DIR = os.environ[\"DATA_DIR\"]\n",
    "print(f\"using DATA_DIR={DATA_DIR}\")\n",
    "\n",
    "memory = Memory(DATA_DIR)\n",
    "################################### SET DATA CONFIGURATIONS ###################################\n",
    "\n",
    "y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p(p_T)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "    \"RecoDataeta\": \"$p(\\eta)$\",\n",
    "    \"RecoDataphi\": \"$p(\\phi)$\",\n",
    "    \"RecoDatam\": \"$p(m)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "}\n",
    "\n",
    "loss_y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p_T^{reco}$\",\n",
    "    \"RecoDataeta\": \"$\\eta^{reco}$\",\n",
    "    \"RecoDataphi\": \"$\\phi^{reco}$\",\n",
    "    \"RecoDatam\": \"$m^{reco}$\",\n",
    "}\n",
    "\n",
    "X = [\"genDatapT\", \"genDataeta\", \"genDataphi\", \"genDatam\", \"tau\"]\n",
    "\n",
    "# set order of training:\n",
    "# pT_first: pT->>m->eta->phi\n",
    "# m_first: m->pT->eta->phi\n",
    "\n",
    "\n",
    "ORDER = \"m_First\"\n",
    "\n",
    "if ORDER == \"m_First\":\n",
    "    FIELDS = {\n",
    "        \"RecoDatam\": {\n",
    "            \"inputs\": X,\n",
    "            \"xlabel\": r\"$m$ (GeV)\",\n",
    "            \"ylabel\": \"$m^{reco}$\",\n",
    "            \"xmin\": 0,\n",
    "            \"xmax\": 25,\n",
    "        },\n",
    "        \"RecoDatapT\": {\n",
    "            \"inputs\": [\"RecoDatam\"] + X,\n",
    "            \"xlabel\": r\"$p_T$ (GeV)\",\n",
    "            \"ylabel\": \"$p_T^{reco}$\",\n",
    "            \"xmin\": 20,\n",
    "            \"xmax\": 80,\n",
    "        },\n",
    "        \"RecoDataeta\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\"] + X,\n",
    "            \"xlabel\": r\"$\\eta$\",\n",
    "            \"ylabel\": \"$\\eta^{reco}$\",\n",
    "            \"xmin\": -5,\n",
    "            \"xmax\": 5,\n",
    "        },\n",
    "        \"RecoDataphi\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\", \"RecoDataeta\"] + X,\n",
    "            \"xlabel\": r\"$\\phi$\",\n",
    "            \"ylabel\": \"$\\phi^{reco}$\",\n",
    "            \"xmin\": -3.2,\n",
    "            \"xmax\": 3.2,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Load and explore raw (unscaled) dataframes\n",
    "\n",
    "\n",
    "all_variable_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "]\n",
    "all_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "    \"tau\",\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9654c3-0337-4d13-b223-c9750abd0afc",
   "metadata": {},
   "source": [
    "# 2.2: Load Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f9d1692-0259-40e5-8859-3fbbfc76f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Load unscaled dataframes ###################################\n",
    "################################### Load unscaled dataframes ###################################\n",
    "@memory.cache\n",
    "def load_raw_data():\n",
    "    \"\"\"Load raw train, test, and validation raw (unscaled) dataframes, in that order.\n",
    "\n",
    "    Returns:\n",
    "        list(pandas.DataFrame): train, test, valid raw datafranes\n",
    "    \"\"\"\n",
    "    print(f'SUBSAMPLE = {SUBSAMPLE}')\n",
    "    raw_train_data=pd.read_csv(os.path.join(DATA_DIR,'train_data_10M_2.csv'),\n",
    "                        usecols=all_cols,\n",
    "                        nrows=SUBSAMPLE\n",
    "                        )\n",
    "\n",
    "    raw_test_data=pd.read_csv(os.path.join(DATA_DIR,'test_data_10M_2.csv'),\n",
    "                        usecols=all_cols,\n",
    "                        nrows=SUBSAMPLE\n",
    "                        )\n",
    "\n",
    "    raw_valid_data=pd.read_csv(os.path.join(DATA_DIR,'validation_data_10M_2.csv'),\n",
    "                        usecols=all_cols,\n",
    "                        nrows=SUBSAMPLE\n",
    "                        )\n",
    "\n",
    "\n",
    "    print('\\n RAW TRAIN DATA SHAPE\\n')\n",
    "    print(raw_train_data.shape)\n",
    "    print('\\n RAW TRAIN DATA\\n')\n",
    "    raw_train_data.describe()#unscaled\n",
    "    print('\\n RAW TEST DATA\\ SHAPEn')\n",
    "    print(raw_test_data.shape)\n",
    "    print('\\n RAW TEST DATA\\n')\n",
    "    raw_test_data.describe()#unscaled\n",
    "\n",
    "    return raw_train_data, raw_test_data, raw_valid_data\n",
    "\n",
    "\n",
    "########## Generate scaled data###############\n",
    "# scaled_train_data = L_scale_df(raw_train_data, title='scaled_train_data_10M_2.csv',\n",
    "#                              save=True)\n",
    "# print('\\n\\n')\n",
    "# scaled_test_data = L_scale_df(raw_test_data,  title='scaled_test_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "# print('\\n\\n')\n",
    "\n",
    "# scaled_valid_data = L_scale_df(raw_valid_data,  title='scaled_valid_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "\n",
    "# explore_data(df=scaled_train_data, title='Braden Kronheim-L-scaled Dataframe', scaled=True)\n",
    "\n",
    "################ Load scaled data##############\n",
    "@time_type_of_func(tuning_or_training='loading')\n",
    "@memory.cache\n",
    "def load_scaled_dataframes():\n",
    "    \"\"\"Load L-scaled train, test and validation according to Braden scaling, in that order.\n",
    "\n",
    "    Returns:\n",
    "        list(pandas.DataFarme): L-scaled train, test, validation dataframes, in that order.\n",
    "    \"\"\"\n",
    "    # print(\"SCALED TRAIN DATA\")\n",
    "    scaled_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    # print(\"TRAINING FEATURES\\n\", scaled_train_data.head())\n",
    "\n",
    "    scaled_test_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_test_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    scaled_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_valid_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    return scaled_train_data, scaled_test_data, scaled_valid_data\n",
    "\n",
    "# print('\\nTESTING FEATURES\\n', test_data_m.head())\n",
    "\n",
    "# print('\\ntrain set shape:',  train_data_m.shape)\n",
    "# print('\\ntest set shape:  ', test_data_m.shape)\n",
    "# # print('validation set shape:', valid_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "def get_train_scale_dict(USE_BRADEN_SCALING):\n",
    "    \"\"\"Get a dictionary containing mean and standard deviation of each gen and reco feature. \n",
    "\n",
    "    Args:\n",
    "        USE_BRADEN_SCALING (bool): Whether you wish to use the Braden scaling. If True, it uses the L-scaled train dataframe. If False, it uses the unscaled dataframe.\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of floats containing mean and standard deviation of each gen and reco feature. \n",
    "    \"\"\"\n",
    "    if USE_BRADEN_SCALING==True:\n",
    "        TRAIN_SCALE_DICT = get_scaling_info(scaled_train_data)\n",
    "        print('BRADEN SCALING DICTIONARY')\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    else:\n",
    "        print('NORMAL UNSCALED DICTIONARY')\n",
    "        TRAIN_SCALE_DICT = get_scaling_info(raw_train_data)\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    return TRAIN_SCALE_DICT\n",
    "\n",
    "\n",
    "\n",
    "################################ SPLIT###########\n",
    "# Currently need the split function again here\n",
    "# @memory.cache\n",
    "def split_t_x(df, target, input_features):\n",
    "    \"\"\"Get the target as the ratio, according to the T equation.\n",
    "    \n",
    "    Returns:\n",
    "    list(numpy.array): list of numpy array of target and training features\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_train_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "# @memory.cache\n",
    "def normal_split_t_x(df, target, input_features):\n",
    "    \"\"\"splot dataframe into targets and feature arrays.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe of train, test or validation data.\n",
    "        target (str): Choice of \"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\",\"RecoDatam\" as target.\n",
    "        input_features (list(str)): list of training features labels\n",
    "\n",
    "    Returns:\n",
    "    list(numpy.array): list of numpy array of target and training features\n",
    " \"\"\"\n",
    "    # change from pandas dataframe format to a numpy \n",
    "    # array of the specified types\n",
    "    # t = np.array(df[target])\n",
    "    t = np.array(df[target])\n",
    "    x = np.array(df[input_features])\n",
    "    return t, x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ Apply Z scaling############\n",
    "def z(x):\n",
    "    \"\"\"Simple z-score standardization. Used for targets\"\"\"\n",
    "    eps = 1e-20\n",
    "    return (x - np.mean(x)) / (np.std(x) + eps)\n",
    "def z_inverse(xprime, x):\n",
    "    return xprime * np.std(x) + np.mean(x)\n",
    "\n",
    "# @memory.cache\n",
    "def z2(x, mean, std):\n",
    "    \"\"\"\n",
    "    The main z score function. Args:\n",
    "        x (numpy.array): feature 1-D array\n",
    "        mean (float): mean of the feature (in the training set)\n",
    "        std (float): standard deviation of the feature (in the training set)\n",
    "\n",
    "    Returns:\n",
    "        numpy.array: z-score-scaled 1-D feature\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    scaled = (x - mean) / (std + eps)\n",
    "    return np.array(scaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z_inverse2(xprime, train_mean, train_std):\n",
    "    \"\"\"\n",
    "        The main z score de-scaling function. \n",
    "        \n",
    "        Args:\n",
    "        xprime (numpy.array): z-score-scaled feature 1-D array\n",
    "        train_mean (float): mean of the feature (in the training set)\n",
    "        train_std (float): standard deviation of the feature (in the training set)\n",
    "        \"\"\"\n",
    "    return xprime * train_std + train_mean\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x):\n",
    "    \"\"\"TO ensure this z scaling is only applied once to the training features, we use a generator.\n",
    "    This doesn't change the shapes of anything, just applies z to all the feature columns other than tau.\n",
    "    \n",
    "    Args:\n",
    "    TRAIN_SCALE_DICT (dict(float)): dictionary of train set mean and standard deviation values\n",
    "    train_x (numpy.array): 2-D numpy array of training features\n",
    "    test_x (numpy.array):  2-D numpy array of test features\n",
    "    valid_x (numpy.array):  2-D numpy array of validation features\n",
    "    \"\"\"\n",
    "    for i in range(NFEATURES - 1):\n",
    "        variable = list(TRAIN_SCALE_DICT)[i]\n",
    "        train_mean = float(TRAIN_SCALE_DICT[variable][\"mean\"])\n",
    "        train_std = float(TRAIN_SCALE_DICT[variable][\"std\"])\n",
    "        train_x[:, i] = z2(train_x[:, i], mean=train_mean, std=train_std)\n",
    "        test_x[:, i] = z2(test_x[:, i], mean=train_mean, std=train_std)\n",
    "        valid_x[:, i] = z2(valid_x[:, i], mean=train_mean, std=train_std)\n",
    "    yield train_x\n",
    "    yield test_x\n",
    "    yield valid_x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_targets(train_t, test_t, valid_t):\n",
    "    \"\"\"apply z-score scaling to target columns\n",
    "\n",
    "    Args:\n",
    "        train_t (numpy.array): target column in the training set\n",
    "        test_t (numpy.array): target column in the test set\n",
    "        valid_t (numpy.array): target column in the validation set\n",
    "\n",
    "    Yields:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    train_mean = np.mean(train_t)\n",
    "    train_std = np.std(train_t)\n",
    "    train_t_ = z2(train_t, mean=train_mean, std=train_std)\n",
    "    test_t_ = z2(test_t, mean=train_mean, std=train_std)\n",
    "    valid_t_ = z2(valid_t, mean=train_mean, std=train_std)\n",
    "\n",
    "    yield train_t_\n",
    "    yield test_t_\n",
    "    yield valid_t_\n",
    "\n",
    "\n",
    "\n",
    "# check that it looks correct\n",
    "# fig = plt.figure(figsize=(10, 4))\n",
    "# ax = fig.add_subplot(autoscale_on=True)\n",
    "# ax.grid()\n",
    "# for i in range(NFEATURES):\n",
    "#     ax.hist(train_x[:,i], alpha=0.35, label=f'feature {i}' )\n",
    "#     # set_axes(ax=ax, xlabel=\"Transformed features X' \",title=\"training features post-z score: X'=z(L(X))\")\n",
    "# ax.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "######### Get beset hyperparameters\n",
    "# tuned_dir = os.path.join(IQN_BASE,'best_params')\n",
    "# tuned_filename=os.path.join(tuned_dir,'best_params_mass_%s_trials.csv' % str(int(n_trials)))\n",
    "# BEST_PARAMS = pd.read_csv(os.path.join(IQN_BASE, 'best_params','best_params_Test_Trials.csv'))\n",
    "# BEST_PARAMS=pd.read_csv(tuned_filename)\n",
    "# print(BEST_PARAMS)\n",
    "\n",
    "\n",
    "\n",
    "def load_untrained_model(PARAMS):\n",
    "    \"\"\"Load an untrained model (with weights initiatted) according to model paramateters in the \n",
    "    PARAMS dictionary\n",
    "\n",
    "    Args:\n",
    "        PARAMS (dict): dictionary of model/training parameters: i.e. hyperparameters and training parameters.\n",
    "\n",
    "    Returns:\n",
    "        utils.RegularizedRegressionModel object\n",
    "    \"\"\"\n",
    "    model = RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    # model.apply(initialize_weights)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SaveModelCheckpoint:\n",
    "    \"\"\"Continuous model-checkpointing class. Updates the latest checkpoint of an object based o validation loss each time its called. \n",
    "    \"\"\"\n",
    "    def __init__(self, best_valid_loss=np.inf):\n",
    "        \"\"\"Initiate an instance of the class based on filename and best_valid_loss/\n",
    "\n",
    "        Args:\n",
    "            best_valid_loss (float, optional): Best possible validation loss of a checkpoint object. Defaults to np.inf.\n",
    "        \"\"\"\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        self.filename_model=filename_model\n",
    "\n",
    "    def __call__(self, model, current_valid_loss, filename_model):\n",
    "        \"\"\"When an object of the calss is called, its validation loss gets updated and the model based \n",
    "        on the latest validation loss is saved.\n",
    "\n",
    "        Args:\n",
    "            model: utils.RegularizedRegressionModel object.\n",
    "            current_valid_loss (float): current (latest) validation loss of this model during the training process.\n",
    "            filename_model (str): filename in which the latest model will be saved. Can be a relative or local path. \n",
    "        \"\"\"\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            # update the best loss\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            # filename_model='Trained_IQNx4_%s_%sK_iter.dict' % (target, str(int(n_iterations/1000)) )\n",
    "            # filename_model = \"Trained_IQNx4_%s_TUNED_2lin_with_noise.dict\" % target\n",
    "\n",
    "            # note that n_iterations is the total n_iterations, we dont want to save a million files for each iteration\n",
    "            trained_models_dir = \"trained_models\"\n",
    "            mkdir(trained_models_dir)\n",
    "            # on cluster, Im using another TRAIN directory\n",
    "            PATH_model = os.path.join(\n",
    "                IQN_BASE,\n",
    "                \"JupyterBook\",\n",
    "                \"Cluster\",\n",
    "                \"TRAIN\",\n",
    "                trained_models_dir,\n",
    "                filename_model,\n",
    "            )\n",
    "            torch.save(model.state_dict(), PATH_model)\n",
    "            print(\n",
    "                f\"\\nCurrent valid loss: {current_valid_loss};  saved better model at {PATH_model}\"\n",
    "            )\n",
    "            # save using .pth object which if a dictionary of dicionaries, so that I can have PARAMS saved in the same file\n",
    "\n",
    "\n",
    "def train(\n",
    "    target,\n",
    "    model,\n",
    "    avloss,\n",
    "    getbatch,\n",
    "    train_x,\n",
    "    train_t,\n",
    "    valid_x,\n",
    "    valid_t,\n",
    "    PARAMS,\n",
    "    traces,\n",
    "    step,\n",
    "    window,\n",
    "):\n",
    "    \"\"\"Training Function. \n",
    "\n",
    "    Args:\n",
    "        target (str): hoice of \"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\",\"RecoDatam\" as target.\n",
    "        model a torch NN model, e.g utils.RegularizedRegressionModel.\n",
    "        avloss (float): average training losss\n",
    "        getbatch (function): a get_batch function\n",
    "        train_x (numpy.DataFrame): 2-D numpy array of training features\n",
    "        train_t (numpy.DataFrame:  1-D numpy array of training targets\n",
    "        valid_x (numpy.DataFrame): 2-D numpy array of validation features\n",
    "        valid_t (numpy.DataFrame: 1-D numpy array of validation targets\n",
    "        PARAMS (dict): dictionary of model/training parameters \n",
    "        traces (tuple): tuple of  \n",
    "        (iteration, training accuracy, validation accuracy, running average of validation accuracy) \n",
    "        = (xx, yy_t, yy_v, yy_v_avg) \n",
    "        step (int): number of iterations to take a printout step of the traces\n",
    "        window (int): window of running average of validation loss  \n",
    "\n",
    "    Returns:\n",
    "        tuple: traces\n",
    "    \"\"\"\n",
    "    batch_size = PARAMS['batch_size']\n",
    "    n_iterations = PARAMS['n_iterations']\n",
    "    # to keep track of average losses\n",
    "    xx, yy_t, yy_v, yy_v_avg = traces\n",
    "    model_checkpoint = SaveModelCheckpoint()\n",
    "    n = len(valid_x)\n",
    "    \n",
    "    print(\"Iteration vs average loss\")\n",
    "    print(\"%10s\\t%10s\\t%10s\" % (\"iteration\", \"train-set\", \"test-set\"))\n",
    "\n",
    "    for ii in range(n_iterations):\n",
    "        \n",
    "        learning_rate= 1e-1\n",
    "        \n",
    "        if 25000 < ii < 55000:\n",
    "            learning_rate=1e-2\n",
    "            \n",
    "        if 55000 < ii < 100000:\n",
    "            learning_rate=1e-3\n",
    "        if ii > 100000:\n",
    "            learning_rate = decay_LR(ii)\n",
    "            \n",
    "        \n",
    "        # add weight decay (important regularization to reduce overfitting)\n",
    "        L2 = 1\n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2)\n",
    "        #SGD allows for: momentum=0, dampening=0, weight_decay=0, nesterov=boolean, differentiable=boolean\n",
    "\n",
    "        optimizer = getattr(torch.optim, optimizer_name)(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "         amsgrad=True, \n",
    "\n",
    "        #  weight_decay=L2,#\n",
    "        # differentiable=True,\n",
    "        #For SGD nesterov, it requires momentum and zero dampening\n",
    "        # dampening=0,\n",
    "        # momentum=momentum,\n",
    "        # nesterov=True\n",
    "        # BUT no one should ever use SGD in 2022! Adam converges much better and faster.\n",
    "        )\n",
    "\n",
    "        #if ii > 1e4: learning_rate=1e-4\n",
    "        # set mode to training so that training specific\n",
    "        # operations such as dropout are enabled.\n",
    "        # time_p_start = time.perf_counter()\n",
    "        model.train()\n",
    "\n",
    "        # get a random sample (a batch) of data (as numpy arrays)\n",
    "        batch_x, batch_t = getbatch(train_x, train_t, batch_size)\n",
    "        # Take df/ dtau\n",
    "        # x = torch.from_numpy(batch_x).float()\n",
    "        # # print('x is leaf: ', x.is_leaf)\n",
    "        # x.requires_grad_(True)\n",
    "        # # x.retain_grad()\n",
    "        # # print('x is leaf after retain: ', x.is_leaf)\n",
    "        # # x.requires_grad_(True)\n",
    "        # # x.retain_grad()\n",
    "        # f = model(x)\n",
    "        # f = f.view(-1)\n",
    "        # #multiply the model by its ransverse, remember we can only take gradients of scalars\n",
    "        # #and f will be a vector before this\n",
    "        # f = f @ f.t()\n",
    "        # f.retain_grad()\n",
    "        # f.backward(gradient=torch.ones_like(f), retain_graph=True)\n",
    "        # df_dx = x.grad\n",
    "        # df_dtau = df_dx[:,-1]\n",
    "        # x.grad.zero_()\n",
    "        \n",
    "        #add noise to training data\n",
    "        # batch_x = add_noise(batch_x)\n",
    "        # batch_t = add_noise(batch_t)\n",
    "\n",
    "        with torch.no_grad():  # no need to compute gradients\n",
    "            # wrt. x and t\n",
    "            x = torch.from_numpy(batch_x).float()\n",
    "            t = torch.from_numpy(batch_t).float()\n",
    "\n",
    "        outputs = model(x).reshape(t.shape)\n",
    "\n",
    "        empirical_risk = avloss(outputs, t, x)\n",
    "\n",
    "        optimizer.zero_grad()  # clear previous gradients\n",
    "        empirical_risk.backward()  # compute gradients\n",
    "\n",
    "        optimizer.step()  # move one step towards the minimum of the loss function using an SGD-like algorithm.\n",
    "\n",
    "        if ii % step == 0:\n",
    "\n",
    "            print(f\"\\t\\tCURRENT LEARNING RATE: {learning_rate}\")\n",
    "            acc_t = validate(model, avloss, train_x[:n], train_t[:n])\n",
    "            #acc_t: list of training losses\n",
    "            acc_v = validate(model, avloss, valid_x[:n], valid_t[:n])\n",
    "            #acc_v: list of validation losses\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "            # save better models based on valid loss\n",
    "            # filename_model=\"Trained_IQNx4_%s_TUNED_0lin_with_high_noise3.dict\" % target\n",
    "            filename_model=get_model_filename(target, PARAMS)\n",
    "             \n",
    "            model_checkpoint(model=model, filename_model =filename_model ,current_valid_loss=acc_v)\n",
    "            # compute running average for validation data\n",
    "            len_yy_v = len(yy_v)\n",
    "            if len_yy_v < window:\n",
    "                yy_v_avg.append(yy_v[-1])\n",
    "            elif len_yy_v == window:\n",
    "                yy_v_avg.append(sum(yy_v) / window)\n",
    "            else:\n",
    "                acc_v_avg = yy_v_avg[-1] * window\n",
    "                acc_v_avg += yy_v[-1] - yy_v[-window - 1]\n",
    "                yy_v_avg.append(acc_v_avg / window)\n",
    "\n",
    "            if len(xx) < 1:\n",
    "                xx.append(0)\n",
    "                print(\"%10d\\t%10.6f\\t%10.6f\" % (xx[-1], yy_t[-1], yy_v[-1]))\n",
    "            else:\n",
    "                xx.append(xx[-1] + step)\n",
    "\n",
    "                print(\n",
    "                    \"\\r%10d\\t%10.6f\\t%10.6f\\t%10.6f\"\n",
    "                    % (xx[-1], yy_t[-1], yy_v[-1], yy_v_avg[-1]),\n",
    "                    end=\"\",\n",
    "                )\n",
    "        # time_p_end = time.perf_counter()\n",
    "        # time_for_this_iter = time_p_end-time_p_start\n",
    "        # time_per_example = time_for_this_iter/batch_size\n",
    "        # print(f'training time for one example: {time_per_example}')\n",
    "\n",
    "    print()\n",
    "    return (xx, yy_t, yy_v, yy_v_avg)\n",
    "\n",
    "\n",
    "@time_type_of_func(tuning_or_training=\"training\")\n",
    "def run(\n",
    "    target,\n",
    "    model,\n",
    "    train_x,\n",
    "    train_t,\n",
    "    valid_x,\n",
    "    valid_t,\n",
    "    traces,\n",
    "    PARAMS,\n",
    "    traces_step,\n",
    "    traces_window,\n",
    "    save_model,\n",
    "):\n",
    "\n",
    "\n",
    "    traces = train(\n",
    "        target,\n",
    "        model,\n",
    "        average_quantile_loss,\n",
    "        get_batch,\n",
    "        train_x,\n",
    "        train_t,\n",
    "        valid_x,\n",
    "        valid_t,\n",
    "        PARAMS,\n",
    "        traces,\n",
    "        step=traces_step,\n",
    "        window=traces_window,\n",
    "    )\n",
    "\n",
    "    if save_model:\n",
    "        filename = \"Trained_IQNx4_%s_%sK_iter.dict\" % (\n",
    "            target,\n",
    "            str(int(n_iterations / 1000)),\n",
    "        )\n",
    "        PATH = os.path.join(IQN_BASE, \"trained_models\", filename)\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ## See if trainig works on T ratio\n",
    "\n",
    "\n",
    "@debug\n",
    "def save_model(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "@debug\n",
    "def save_model_params(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "@debug\n",
    "def load_model(PATH):\n",
    "    # n_layers = int(BEST_PARAMS[\"n_layers\"])\n",
    "    # hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "    # dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "    # optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "    # learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "    # batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "    model = RegularizedRegressionModel(\n",
    "        nfeatures=train_x.shape[1],\n",
    "        ntargets=1,\n",
    "        nlayers=n_layers,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    # OR\n",
    "    # model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED!\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_trained_model(PATH, PARAMS):\n",
    "    model = RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    print(model)\n",
    "    model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891fa0d7-ee43-4ee0-8d25-eafa4fc40b02",
   "metadata": {},
   "source": [
    "# 2.3 Load Data, split, scale, and Train Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a34491a-0c10-4e71-8967-c0514d211f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-4220973962.load_raw_data...\n",
      "load_raw_data()\n",
      "SUBSAMPLE = None\n",
      "\n",
      " RAW TRAIN DATA SHAPE\n",
      "\n",
      "(8000000, 9)\n",
      "\n",
      " RAW TRAIN DATA\n",
      "\n",
      "\n",
      " RAW TEST DATA\\ SHAPEn\n",
      "(1000000, 9)\n",
      "\n",
      " RAW TEST DATA\n",
      "\n",
      "___________________________________________________load_raw_data - 11.7s, 0.2min\n",
      "timing this arbitrary function\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-4220973962.load_scaled_dataframes...\n",
      "load_scaled_dataframes()\n",
      "__________________________________________load_scaled_dataframes - 33.5s, 0.6min\n",
      "this arbirary function took 33.4521 secs\n",
      "spliting data for RecoDatam\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 5)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 5)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 5)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 3.27223764e+01  6.98189368e-04 -8.95543973e-04  6.96116528e+00\n",
      "  5.00485136e-01] [15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]\n",
      "[ 3.26952341e+01 -1.78188172e-03 -3.83090331e-04  6.96299435e+00\n",
      "  4.99915289e-01] [14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]\n",
      "5.5514112643334546 2.664124544901276\n",
      "5.555567451922438 2.664339857066051\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[ 1.81700902e-03  1.12510099e-03 -2.82526482e-04 -6.57626105e-04\n",
      "  5.00485136e-01] [1.01748627 0.9999745  0.99989115 0.99987283 0.28852734]\n",
      "[ 1.61650249e-15 -1.10134124e-18 -3.12905257e-17  5.01036723e-15\n",
      "  4.99915289e-01] [1.         1.         1.         1.         0.28867295]\n",
      "-0.0015599314696879494 0.9999191874249058\n",
      "6.996216939114674e-16 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING=False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(1e5)  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "raw_train_data, raw_test_data, raw_valid_data =load_raw_data()\n",
    "# Load scaled data\n",
    "scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()\n",
    "\n",
    "\n",
    "# Get targets and features\n",
    "if USE_BRADEN_SCALING==True:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = split_t_x(\n",
    "        df=scaled_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = split_t_x(\n",
    "        df=scaled_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = split_t_x(df=scaled_test_data, target=target, input_features=features)\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"spliting data for {target}\")\n",
    "    train_t, train_x = normal_split_t_x(\n",
    "    df=raw_train_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "    print(\"\\n Training features:\\n\")\n",
    "    print(train_x)\n",
    "    valid_t, valid_x = normal_split_t_x(\n",
    "    df=raw_valid_data, target=target, input_features=features\n",
    "    )\n",
    "    print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "    test_t, test_x = normal_split_t_x(df=raw_test_data, target=target, input_features=features)\n",
    "    print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "NFEATURES = train_x.shape[1]\n",
    "######################################################\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "TRAIN_SCALE_DICT=get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(\n",
    "    train_t, test_t, valid_t\n",
    ")\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087482e-fb15-4e02-a894-c5c6fc05638e",
   "metadata": {},
   "source": [
    "Decide Whether to use Braden Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54bd29f-e1e9-46e5-a607-b56699cb9ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[-2.18861727e+00  1.31877232e-03  5.54429671e-05 -2.50371114e+00\n",
      "  5.00485136e-01] [0.06811426 0.45364518 0.55126104 0.35949423 0.28852734]\n",
      "[-2.18873890e+00  8.08362666e-04  2.11205765e-04 -2.50347470e+00\n",
      "  4.99915289e-01] [0.06694367 0.45365675 0.55132105 0.35953996 0.28867295]\n",
      "-0.0015599314696879494 0.9999191874249058\n",
      "6.996216939114674e-16 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "TRAIN_SCALE_DICT=get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(\n",
    "    train_t, test_t, valid_t\n",
    ")\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d597bc6-64df-4427-b268-9f1aeb3cd183",
   "metadata": {},
   "source": [
    "# Define Mass Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b8a342c-6c66-4d39-aaeb-eb421bfa3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Decide on parameters for this model and training\n",
    "PARAMS_m = {\n",
    "\"n_layers\": int(13),\n",
    "\"hidden_size\": int(5),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(1e-3),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(1024),\n",
    "    'n_iterations': int(3e5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708af01e-612a-405a-81c8-41a2e3739a07",
   "metadata": {},
   "source": [
    "## Train Mass\n",
    "\n",
    "### The model that needs the longest time in training is mass. Click here to scroll down to train $p_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f695aaf2-a37e-4ba1-86aa-0563957f09dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "training for 300000 iteration, which is  38.4 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (3): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.3)\n",
      "    (5): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (6): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (9): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.3)\n",
      "    (11): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (12): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.3)\n",
      "    (14): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (15): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.3)\n",
      "    (17): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (18): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.3)\n",
      "    (20): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (21): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.3)\n",
      "    (23): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (24): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): LeakyReLU(negative_slope=0.3)\n",
      "    (26): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (27): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (28): LeakyReLU(negative_slope=0.3)\n",
      "    (29): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (30): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): LeakyReLU(negative_slope=0.3)\n",
      "    (32): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (33): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (34): LeakyReLU(negative_slope=0.3)\n",
      "    (35): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (36): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (37): LeakyReLU(negative_slope=0.3)\n",
      "    (38): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "training IQN \n",
      "Iteration vs average loss\n",
      " iteration\t train-set\t  test-set\n",
      "\t\tCURRENT LEARNING RATE: 0.1\n",
      "\n",
      "Current valid loss: 0.36293846368789673;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "         0\t  0.363164\t  0.362938\n",
      "\t\tCURRENT LEARNING RATE: 0.1\n",
      "\n",
      "Current valid loss: 0.28018859028816223;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "      3000\t  0.280959\t  0.280189\t  0.280189\t\tCURRENT LEARNING RATE: 0.1\n",
      "\n",
      "Current valid loss: 0.1923537254333496;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "      6000\t  0.192717\t  0.192354\t  0.192354\t\tCURRENT LEARNING RATE: 0.1\n",
      "      9000\t  0.231056\t  0.230407\t  0.230407\t\tCURRENT LEARNING RATE: 0.1\n",
      "     12000\t  0.204524\t  0.203989\t  0.203989\t\tCURRENT LEARNING RATE: 0.1\n",
      "     15000\t  0.199903\t  0.199574\t  0.199574\t\tCURRENT LEARNING RATE: 0.1\n",
      "     18000\t  0.207830\t  0.207589\t  0.207589\t\tCURRENT LEARNING RATE: 0.1\n",
      "     21000\t  0.234235\t  0.233789\t  0.233789\t\tCURRENT LEARNING RATE: 0.1\n",
      "     24000\t  0.196765\t  0.196701\t  0.196701\t\tCURRENT LEARNING RATE: 0.01\n",
      "\n",
      "Current valid loss: 0.16104522347450256;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "     27000\t  0.161222\t  0.161045\t  0.161045\t\tCURRENT LEARNING RATE: 0.01\n",
      "     30000\t  0.161839\t  0.161675\t  0.161675\t\tCURRENT LEARNING RATE: 0.01\n",
      "     33000\t  0.161276\t  0.161110\t  0.161110\t\tCURRENT LEARNING RATE: 0.01\n",
      "\n",
      "Current valid loss: 0.16071198880672455;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "     36000\t  0.160865\t  0.160712\t  0.160712\t\tCURRENT LEARNING RATE: 0.01\n",
      "\n",
      "Current valid loss: 0.160553440451622;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "     39000\t  0.160767\t  0.160553\t  0.160553\t\tCURRENT LEARNING RATE: 0.01\n",
      "     42000\t  0.160771\t  0.160560\t  0.160560\t\tCURRENT LEARNING RATE: 0.01\n",
      "     45000\t  0.160962\t  0.160742\t  0.160742\t\tCURRENT LEARNING RATE: 0.01\n",
      "     48000\t  0.160863\t  0.160630\t  0.160630\t\tCURRENT LEARNING RATE: 0.01\n",
      "     51000\t  0.161146\t  0.160950\t  0.160950\t\tCURRENT LEARNING RATE: 0.01\n",
      "\n",
      "Current valid loss: 0.16035549342632294;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "     54000\t  0.160574\t  0.160355\t  0.160355\t\tCURRENT LEARNING RATE: 0.001\n",
      "\n",
      "Current valid loss: 0.16030780971050262;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "     57000\t  0.160525\t  0.160308\t  0.160308\t\tCURRENT LEARNING RATE: 0.001\n",
      "\n",
      "Current valid loss: 0.1601782739162445;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "     60000\t  0.160379\t  0.160178\t  0.160178\t\tCURRENT LEARNING RATE: 0.001\n",
      "     63000\t  0.160395\t  0.160193\t  0.160193\t\tCURRENT LEARNING RATE: 0.001\n",
      "     66000\t  0.160424\t  0.160217\t  0.160217\t\tCURRENT LEARNING RATE: 0.001\n",
      "     69000\t  0.160473\t  0.160276\t  0.160276\t\tCURRENT LEARNING RATE: 0.001\n",
      "\n",
      "Current valid loss: 0.16016395390033722;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "     72000\t  0.160365\t  0.160164\t  0.160164\t\tCURRENT LEARNING RATE: 0.001\n",
      "     75000\t  0.160375\t  0.160165\t  0.160165\t\tCURRENT LEARNING RATE: 0.001\n",
      "\n",
      "Current valid loss: 0.1601569652557373;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "     78000\t  0.160365\t  0.160157\t  0.160157\t\tCURRENT LEARNING RATE: 0.001\n",
      "     81000\t  0.160433\t  0.160228\t  0.160228\t\tCURRENT LEARNING RATE: 0.001\n",
      "     84000\t  0.160421\t  0.160201\t  0.160201\t\tCURRENT LEARNING RATE: 0.001\n",
      "     87000\t  0.160385\t  0.160182\t  0.160182\t\tCURRENT LEARNING RATE: 0.001\n",
      "     90000\t  0.160378\t  0.160167\t  0.160167\t\tCURRENT LEARNING RATE: 0.001\n",
      "\n",
      "Current valid loss: 0.1601404994726181;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "     93000\t  0.160340\t  0.160140\t  0.160140\t\tCURRENT LEARNING RATE: 0.001\n",
      "     96000\t  0.160383\t  0.160178\t  0.160178\t\tCURRENT LEARNING RATE: 0.001\n",
      "\n",
      "Current valid loss: 0.1601308435201645;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "     99000\t  0.160333\t  0.160131\t  0.160131\t\tCURRENT LEARNING RATE: 0.000989851843582095\n",
      "    102000\t  0.160357\t  0.160164\t  0.160164\t\tCURRENT LEARNING RATE: 0.0009895549325678993\n",
      "    105000\t  0.160408\t  0.160196\t  0.160196\t\tCURRENT LEARNING RATE: 0.0009892581106136483\n",
      "    108000\t  0.160359\t  0.160167\t  0.160167\t\tCURRENT LEARNING RATE: 0.0009889613776926278\n",
      "    111000\t  0.160396\t  0.160192\t  0.160192\t\tCURRENT LEARNING RATE: 0.000988664733778132\n",
      "\n",
      "Current valid loss: 0.1601124405860901;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    114000\t  0.160308\t  0.160112\t  0.160112\t\tCURRENT LEARNING RATE: 0.000988368178843463\n",
      "    117000\t  0.160322\t  0.160126\t  0.160126\t\tCURRENT LEARNING RATE: 0.0009880717128619306\n",
      "\n",
      "Current valid loss: 0.1601080745458603;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    120000\t  0.160324\t  0.160108\t  0.160108\t\tCURRENT LEARNING RATE: 0.000987775335806853\n",
      "\n",
      "Current valid loss: 0.16009771823883057;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    123000\t  0.160304\t  0.160098\t  0.160098\t\tCURRENT LEARNING RATE: 0.0009874790476515564\n",
      "    126000\t  0.160349\t  0.160119\t  0.160119\t\tCURRENT LEARNING RATE: 0.0009871828483693748\n",
      "    129000\t  0.160332\t  0.160118\t  0.160118\t\tCURRENT LEARNING RATE: 0.0009868867379336504\n",
      "    132000\t  0.160331\t  0.160121\t  0.160121\t\tCURRENT LEARNING RATE: 0.0009865907163177326\n",
      "    135000\t  0.160318\t  0.160118\t  0.160118\t\tCURRENT LEARNING RATE: 0.0009862947834949802\n",
      "    138000\t  0.160386\t  0.160175\t  0.160175\t\tCURRENT LEARNING RATE: 0.000985998939438759\n",
      "    141000\t  0.160343\t  0.160105\t  0.160105\t\tCURRENT LEARNING RATE: 0.0009857031841224429\n",
      "    144000\t  0.160326\t  0.160108\t  0.160108\t\tCURRENT LEARNING RATE: 0.0009854075175194143\n",
      "\n",
      "Current valid loss: 0.16006778180599213;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    147000\t  0.160277\t  0.160068\t  0.160068\t\tCURRENT LEARNING RATE: 0.0009851119396030628\n",
      "    150000\t  0.160298\t  0.160094\t  0.160094\t\tCURRENT LEARNING RATE: 0.0009848164503467865\n",
      "\n",
      "Current valid loss: 0.16006453335285187;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    153000\t  0.160280\t  0.160065\t  0.160065\t\tCURRENT LEARNING RATE: 0.0009845210497239913\n",
      "    156000\t  0.160299\t  0.160090\t  0.160090\t\tCURRENT LEARNING RATE: 0.0009842257377080913\n",
      "    159000\t  0.160337\t  0.160111\t  0.160111\t\tCURRENT LEARNING RATE: 0.0009839305142725083\n",
      "\n",
      "Current valid loss: 0.16005659103393555;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    162000\t  0.160266\t  0.160057\t  0.160057\t\tCURRENT LEARNING RATE: 0.0009836353793906724\n",
      "    165000\t  0.160312\t  0.160086\t  0.160086\t\tCURRENT LEARNING RATE: 0.0009833403330360212\n",
      "    168000\t  0.160303\t  0.160092\t  0.160092\t\tCURRENT LEARNING RATE: 0.0009830453751820008\n",
      "    171000\t  0.160360\t  0.160134\t  0.160134\t\tCURRENT LEARNING RATE: 0.0009827505058020646\n",
      "\n",
      "Current valid loss: 0.1600562483072281;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    174000\t  0.160255\t  0.160056\t  0.160056\t\tCURRENT LEARNING RATE: 0.0009824557248696746\n",
      "    177000\t  0.160263\t  0.160066\t  0.160066\t\tCURRENT LEARNING RATE: 0.0009821610323583008\n",
      "    180000\t  0.160263\t  0.160060\t  0.160060\t\tCURRENT LEARNING RATE: 0.0009818664282414203\n",
      "    183000\t  0.160315\t  0.160101\t  0.160101\t\tCURRENT LEARNING RATE: 0.0009815719124925191\n",
      "    186000\t  0.160323\t  0.160119\t  0.160119\t\tCURRENT LEARNING RATE: 0.0009812774850850906\n",
      "    189000\t  0.160273\t  0.160086\t  0.160086\t\tCURRENT LEARNING RATE: 0.0009809831459926365\n",
      "    192000\t  0.160295\t  0.160089\t  0.160089\t\tCURRENT LEARNING RATE: 0.0009806888951886662\n",
      "    195000\t  0.160261\t  0.160074\t  0.160074\t\tCURRENT LEARNING RATE: 0.000980394732646697\n",
      "    198000\t  0.160370\t  0.160143\t  0.160143\t\tCURRENT LEARNING RATE: 0.0009801006583402547\n",
      "\n",
      "Current valid loss: 0.16003349423408508;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    201000\t  0.160231\t  0.160033\t  0.160033\t\tCURRENT LEARNING RATE: 0.0009798066722428722\n",
      "    204000\t  0.160278\t  0.160079\t  0.160079\t\tCURRENT LEARNING RATE: 0.0009795127743280907\n",
      "    207000\t  0.160333\t  0.160151\t  0.160151\t\tCURRENT LEARNING RATE: 0.0009792189645694595\n",
      "\n",
      "Current valid loss: 0.1600334644317627;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    210000\t  0.160227\t  0.160033\t  0.160033\t\tCURRENT LEARNING RATE: 0.000978925242940536\n",
      "\n",
      "Current valid loss: 0.1600281000137329;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    213000\t  0.160224\t  0.160028\t  0.160028\t\tCURRENT LEARNING RATE: 0.0009786316094148849\n",
      "    216000\t  0.160272\t  0.160077\t  0.160077\t\tCURRENT LEARNING RATE: 0.0009783380639660794\n",
      "    219000\t  0.160248\t  0.160050\t  0.160050\t\tCURRENT LEARNING RATE: 0.0009780446065677003\n",
      "    222000\t  0.160292\t  0.160101\t  0.160101\t\tCURRENT LEARNING RATE: 0.0009777512371933364\n",
      "    225000\t  0.160260\t  0.160058\t  0.160058\t\tCURRENT LEARNING RATE: 0.0009774579558165845\n",
      "    228000\t  0.160331\t  0.160124\t  0.160124\t\tCURRENT LEARNING RATE: 0.0009771647624110494\n",
      "    231000\t  0.160250\t  0.160067\t  0.160067\t\tCURRENT LEARNING RATE: 0.0009768716569503433\n",
      "    234000\t  0.160281\t  0.160088\t  0.160088\t\tCURRENT LEARNING RATE: 0.0009765786394080873\n",
      "    237000\t  0.160243\t  0.160063\t  0.160063\t\tCURRENT LEARNING RATE: 0.0009762857097579093\n",
      "    240000\t  0.160301\t  0.160112\t  0.160112\t\tCURRENT LEARNING RATE: 0.000975992867973446\n",
      "    243000\t  0.160247\t  0.160068\t  0.160068\t\tCURRENT LEARNING RATE: 0.0009757001140283414\n",
      "    246000\t  0.160369\t  0.160183\t  0.160183\t\tCURRENT LEARNING RATE: 0.0009754074478962477\n",
      "    249000\t  0.160228\t  0.160039\t  0.160039\t\tCURRENT LEARNING RATE: 0.0009751148695508249\n",
      "    252000\t  0.160248\t  0.160076\t  0.160076\t\tCURRENT LEARNING RATE: 0.0009748223789657411\n",
      "    255000\t  0.160229\t  0.160042\t  0.160042\t\tCURRENT LEARNING RATE: 0.0009745299761146721\n",
      "    258000\t  0.160242\t  0.160070\t  0.160070\t\tCURRENT LEARNING RATE: 0.0009742376609713015\n",
      "    261000\t  0.160256\t  0.160071\t  0.160071\t\tCURRENT LEARNING RATE: 0.0009739454335093211\n",
      "    264000\t  0.160238\t  0.160058\t  0.160058\t\tCURRENT LEARNING RATE: 0.0009736532937024304\n",
      "    267000\t  0.160252\t  0.160078\t  0.160078\t\tCURRENT LEARNING RATE: 0.0009733612415243368\n",
      "    270000\t  0.160294\t  0.160115\t  0.160115\t\tCURRENT LEARNING RATE: 0.0009730692769487556\n",
      "    273000\t  0.160212\t  0.160032\t  0.160032\t\tCURRENT LEARNING RATE: 0.0009727773999494099\n",
      "    276000\t  0.160243\t  0.160049\t  0.160049\t\tCURRENT LEARNING RATE: 0.0009724856105000309\n",
      "    279000\t  0.160215\t  0.160039\t  0.160039\t\tCURRENT LEARNING RATE: 0.0009721939085743576\n",
      "    282000\t  0.160208\t  0.160031\t  0.160031\t\tCURRENT LEARNING RATE: 0.0009719022941461367\n",
      "\n",
      "Current valid loss: 0.16000664234161377;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_13_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict\n",
      "    285000\t  0.160189\t  0.160007\t  0.160007\t\tCURRENT LEARNING RATE: 0.0009716107671891228\n",
      "    288000\t  0.160215\t  0.160028\t  0.160028\t\tCURRENT LEARNING RATE: 0.0009713193276770786\n",
      "    291000\t  0.160225\t  0.160045\t  0.160045\t\tCURRENT LEARNING RATE: 0.0009710279755837746\n",
      "    294000\t  0.160214\t  0.160036\t  0.160036\t\tCURRENT LEARNING RATE: 0.0009707367108829891\n",
      "    297000\t  0.160301\t  0.160096\t  0.160096\n",
      "training target distribution using 'run' in 2077.5259 secs\n"
     ]
    }
   ],
   "source": [
    "optimizer_name=PARAMS_m['optimizer_name']\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS=PARAMS_m['n_iterations']\n",
    "BATCHSIZE=PARAMS_m['batch_size']\n",
    "comment=''\n",
    "\n",
    "\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(f\"training for {NITERATIONS} iteration, which is  {N_epochs} epochs\")\n",
    "\n",
    "\n",
    "filename_model = get_model_filename(target, PARAMS_m)\n",
    "trained_models_dir = \"trained_models\"\n",
    "mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE, #the loaction of the repo\n",
    "    \"JupyterBook\", #up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\", \n",
    "    \"TRAIN\",\n",
    "    trained_models_dir, #/trained_models \n",
    "    filename_model # utils.get_model_filename has the saved file format \n",
    ")\n",
    "\n",
    "#LOAD EITHER TRAINED OR UNTRAINED MODEL\n",
    "# to load untrained model (start training from scratch), uncomment the next line\n",
    "untrained_model = load_untrained_model(PARAMS_m)\n",
    "# to continune training of model (pickup where the previous training left off), uncomment below\n",
    "# trained_model =load_trained_model(PATH=PATH_model, PARAMS=PARAMS_m)\n",
    "\n",
    "IQN_trace = ([], [], [], [])\n",
    "traces_step = int(3e3)\n",
    "traces_window = traces_step\n",
    "IQN = run(\n",
    "    target=target,\n",
    "    model=untrained_model,\n",
    "    train_x=train_x_z_scaled,\n",
    "    train_t=train_t_z_scaled,\n",
    "    valid_x=test_x_z_scaled,\n",
    "    valid_t=test_t_z_scaled,\n",
    "    traces=IQN_trace,\n",
    "    PARAMS=PARAMS_m,\n",
    "    traces_step=traces_step,\n",
    "    traces_window=traces_window,\n",
    "    save_model=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "SAVE_LAST_MODEL=False\n",
    "if SAVE_LAST_MODEL:\n",
    "    # ## Save last iteration of trained model \n",
    "    #dont save the last model, it might be worse than previous iterations, which were automatically savedby model checkpoints\n",
    "\n",
    "    final_path = get_model_filename(target, PARAMS_m).split('.dict')[0]+'_FINAL.dict'\n",
    "\n",
    "    trained_models_dir = \"trained_models\"\n",
    "    mkdir(trained_models_dir)\n",
    "    # on cluster, Im using another TRAIN directory\n",
    "    PATH_final_model = os.path.join(\n",
    "    IQN_BASE, \"JupyterBook\", \"Cluster\", \"TRAIN\", trained_models_dir, final_path\n",
    "    )\n",
    "\n",
    "    save_model(IQN, PATH_final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cbb92-e01f-4c25-a910-9c4fc7bd793a",
   "metadata": {},
   "source": [
    "# 2.4: Train $p_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e7a24-afc4-486f-9bf1-7c11cc1dbdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
