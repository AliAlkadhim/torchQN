{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de81fd7f-8c91-44e5-9b5e-878882120c1e",
   "metadata": {},
   "source": [
    "# IQNx4 Chapter 2: Train All Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ddfd24-43c6-44ac-b171-7269c28f4f93",
   "metadata": {},
   "source": [
    "# 2.1 Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf410b4-9e3a-4b0d-9bac-fd65f3997d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using torch version 1.9.0\n",
      "matplotlib version=  3.5.1\n",
      "using (optional) optuna version 2.8.0\n",
      "BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN\n",
      "using DATA_DIR=/home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import scipy as sp; import scipy.stats as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"using torch version {torch.__version__}\")\n",
    "# use numba's just-in-time compiler to speed things up\n",
    "# from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "\n",
    "print(\"matplotlib version= \", mp.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reset matplotlib stle/parameters\n",
    "import matplotlib as mpl\n",
    "\n",
    "# reset matplotlib parameters to their defaults\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use(\"seaborn-deep\")\n",
    "mp.rcParams[\"agg.path.chunksize\"] = 10000\n",
    "font_legend = 15\n",
    "font_axes = 15\n",
    "# %matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "#or use joblib for caching on disk\n",
    "from joblib import  Memory\n",
    "# from IPython.display import Image, display\n",
    "# from importlib import import_module\n",
    "# import plotly\n",
    "try:\n",
    "    import optuna\n",
    "\n",
    "    print(f\"using (optional) optuna version {optuna.__version__}\")\n",
    "except Exception:\n",
    "    print(\"optuna is only used for hyperparameter tuning, not critical!\")\n",
    "    pass\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# import sympy as sy\n",
    "# import ipywidgets as wid;\n",
    "\n",
    "\n",
    "# try:\n",
    "#     IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "#     print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "#     utils_dir = os.path.join(IQN_BASE, \"utils/\")\n",
    "#     sys.path.append(utils_dir)\n",
    "#     import utils\n",
    "\n",
    "#     # usually its not recommended to import everything from a module, but we know\n",
    "#     # whats in it so its fine\n",
    "#     from utils import *\n",
    "\n",
    "#     print(\"DATA directory also properly set, in %s\" % os.environ[\"DATA_DIR\"])\n",
    "# except Exception:\n",
    "#     # IQN_BASE=os.getcwd()\n",
    "#     print(\n",
    "#         \"\"\"\\nBASE directory not properly set. Read repo README.    If you need a function from utils, use the decorator below, or add utils to sys.path\\n\n",
    "#     You can also do \n",
    "#     os.environ['IQN_BASE']=<ABSOLUTE PATH FOR THE IQN REPO>\n",
    "#     or\n",
    "#     os.environ['IQN_BASE']=os.getcwd()\"\"\"\n",
    "#     )\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "# @debug\n",
    "# def get_model_params_simple():\n",
    "#     dropout=0.2\n",
    "#     n_layers = 2\n",
    "#     n_hidden=32\n",
    "#     starting_learning_rate=1e-3\n",
    "#     print('n_iterations, n_layers, n_hidden, starting_learning_rate, dropout')\n",
    "#     return n_iterations, n_layers, n_hidden, starting_learning_rate, dropout\n",
    "\n",
    "\n",
    "# update fonts\n",
    "FONTSIZE = 14\n",
    "font = {\"family\": \"serif\", \"weight\": \"normal\", \"size\": FONTSIZE}\n",
    "mp.rc(\"font\", **font)\n",
    "\n",
    "# set usetex = False if LaTex is not\n",
    "# available on your system or if the\n",
    "# rendering is too slow\n",
    "mp.rc(\"text\", usetex=True)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "# seed = 128\n",
    "# rnd  = np.random.RandomState(seed)\n",
    "# sometimes jupyter doesnt initialize MathJax automatically for latex, so do this:\n",
    "#######\n",
    "\n",
    "\n",
    "IQN_BASE = os.environ[\"IQN_BASE\"]\n",
    "print(\"BASE directoy properly set = \", IQN_BASE)\n",
    "utils_dir = os.path.join(IQN_BASE, \"utils/\")\n",
    "sys.path.append(utils_dir)\n",
    "import utils\n",
    "\n",
    "# usually its not recommended to import everything from a module, but we know\n",
    "# whats in it so its fine\n",
    "from utils import *\n",
    "\n",
    "\n",
    "DATA_DIR = os.environ[\"DATA_DIR\"]\n",
    "print(f\"using DATA_DIR={DATA_DIR}\")\n",
    "\n",
    "memory = Memory(DATA_DIR)\n",
    "################################### SET DATA CONFIGURATIONS ###################################\n",
    "\n",
    "y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p(p_T)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "    \"RecoDataeta\": \"$p(\\eta)$\",\n",
    "    \"RecoDataphi\": \"$p(\\phi)$\",\n",
    "    \"RecoDatam\": \"$p(m)$\" + \" [ GeV\" + \"$^{-1} $\" + \"]\",\n",
    "}\n",
    "\n",
    "loss_y_label_dict = {\n",
    "    \"RecoDatapT\": \"$p_T^{reco}$\",\n",
    "    \"RecoDataeta\": \"$\\eta^{reco}$\",\n",
    "    \"RecoDataphi\": \"$\\phi^{reco}$\",\n",
    "    \"RecoDatam\": \"$m^{reco}$\",\n",
    "}\n",
    "\n",
    "X = [\"genDatapT\", \"genDataeta\", \"genDataphi\", \"genDatam\", \"tau\"]\n",
    "\n",
    "# set order of training:\n",
    "# pT_first: pT->>m->eta->phi\n",
    "# m_first: m->pT->eta->phi\n",
    "\n",
    "\n",
    "ORDER = \"m_First\"\n",
    "\n",
    "if ORDER == \"m_First\":\n",
    "    FIELDS = {\n",
    "        \"RecoDatam\": {\n",
    "            \"inputs\": X,\n",
    "            \"xlabel\": r\"$m$ (GeV)\",\n",
    "            \"ylabel\": \"$m^{reco}$\",\n",
    "            \"xmin\": 0,\n",
    "            \"xmax\": 25,\n",
    "        },\n",
    "        \"RecoDatapT\": {\n",
    "            \"inputs\": [\"RecoDatam\"] + X,\n",
    "            \"xlabel\": r\"$p_T$ (GeV)\",\n",
    "            \"ylabel\": \"$p_T^{reco}$\",\n",
    "            \"xmin\": 20,\n",
    "            \"xmax\": 80,\n",
    "        },\n",
    "        \"RecoDataeta\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\"] + X,\n",
    "            \"xlabel\": r\"$\\eta$\",\n",
    "            \"ylabel\": \"$\\eta^{reco}$\",\n",
    "            \"xmin\": -5,\n",
    "            \"xmax\": 5,\n",
    "        },\n",
    "        \"RecoDataphi\": {\n",
    "            \"inputs\": [\"RecoDatam\", \"RecoDatapT\", \"RecoDataeta\"] + X,\n",
    "            \"xlabel\": r\"$\\phi$\",\n",
    "            \"ylabel\": \"$\\phi^{reco}$\",\n",
    "            \"xmin\": -3.2,\n",
    "            \"xmax\": 3.2,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Load and explore raw (unscaled) dataframes\n",
    "\n",
    "\n",
    "all_variable_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "]\n",
    "all_cols = [\n",
    "    \"genDatapT\",\n",
    "    \"genDataeta\",\n",
    "    \"genDataphi\",\n",
    "    \"genDatam\",\n",
    "    \"RecoDatapT\",\n",
    "    \"RecoDataeta\",\n",
    "    \"RecoDataphi\",\n",
    "    \"RecoDatam\",\n",
    "    \"tau\",\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9654c3-0337-4d13-b223-c9750abd0afc",
   "metadata": {},
   "source": [
    "# 2.2: Load Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90f34e6d-4f15-4e82-96ae-64d047b38eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LR_Cooler:\n",
    "    def __init__(self, starting_lr: float, total_iterations: int, iter_: int) -> float:\n",
    "        self.starting_lr=starting_lr\n",
    "        self.iter_=iter_\n",
    "        self.total_iterations= total_iterations\n",
    "    def exponential_decay(self):\n",
    "        return self.starting_lr * (np.exp(-  self.iter_/1e5 ))\n",
    "    def exponential_decay_2(self):\n",
    "        decay_rate=1e-3\n",
    "        return self.starting_lr * np.exp(- decay_rate* self.iter)\n",
    "    \n",
    "    def fractional_decay(self):\n",
    "        final_time = 1\n",
    "        return self.starting_lr/(self.iter + final_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30f02bc1-6d36-44e8-b412-5356ef425eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR_sched = LR_Cooler(1e-3, np.arange(1e6), np.arange(1e6))\n",
    "LR = LR_sched.exponential_decay()\n",
    "plt.plot(np.arange(1e6), LR);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0f9d1692-0259-40e5-8859-3fbbfc76f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Load unscaled dataframes ###################################\n",
    "################################### Load unscaled dataframes ###################################\n",
    "@memory.cache\n",
    "def load_raw_data():\n",
    "    \"\"\"Load raw train, test, and validation raw (unscaled) dataframes, in that order.\n",
    "\n",
    "    Returns:\n",
    "        list(pandas.DataFrame): train, test, valid raw datafranes\n",
    "    \"\"\"\n",
    "    print(f'SUBSAMPLE = {SUBSAMPLE}')\n",
    "    raw_train_data=pd.read_csv(os.path.join(DATA_DIR,'train_data_10M_2.csv'),\n",
    "                        usecols=all_cols,\n",
    "                        nrows=SUBSAMPLE\n",
    "                        )\n",
    "\n",
    "    raw_test_data=pd.read_csv(os.path.join(DATA_DIR,'test_data_10M_2.csv'),\n",
    "                        usecols=all_cols,\n",
    "                        nrows=SUBSAMPLE\n",
    "                        )\n",
    "\n",
    "    raw_valid_data=pd.read_csv(os.path.join(DATA_DIR,'validation_data_10M_2.csv'),\n",
    "                        usecols=all_cols,\n",
    "                        nrows=SUBSAMPLE\n",
    "                        )\n",
    "\n",
    "\n",
    "    print('\\n RAW TRAIN DATA SHAPE\\n')\n",
    "    print(raw_train_data.shape)\n",
    "    print('\\n RAW TRAIN DATA\\n')\n",
    "    raw_train_data.describe()#unscaled\n",
    "    print('\\n RAW TEST DATA\\ SHAPEn')\n",
    "    print(raw_test_data.shape)\n",
    "    print('\\n RAW TEST DATA\\n')\n",
    "    raw_test_data.describe()#unscaled\n",
    "\n",
    "    return raw_train_data, raw_test_data, raw_valid_data\n",
    "\n",
    "\n",
    "########## Generate scaled data###############\n",
    "# scaled_train_data = L_scale_df(raw_train_data, title='scaled_train_data_10M_2.csv',\n",
    "#                              save=True)\n",
    "# print('\\n\\n')\n",
    "# scaled_test_data = L_scale_df(raw_test_data,  title='scaled_test_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "# print('\\n\\n')\n",
    "\n",
    "# scaled_valid_data = L_scale_df(raw_valid_data,  title='scaled_valid_data_10M_2.csv',\n",
    "#                             save=True)\n",
    "\n",
    "# explore_data(df=scaled_train_data, title='Braden Kronheim-L-scaled Dataframe', scaled=True)\n",
    "\n",
    "################ Load scaled data##############\n",
    "@utils.time_type_of_func(tuning_or_training='loading')\n",
    "@memory.cache\n",
    "def load_scaled_dataframes():\n",
    "    \"\"\"Load L-scaled train, test and validation according to Braden scaling, in that order.\n",
    "\n",
    "    Returns:\n",
    "        list(pandas.DataFarme): L-scaled train, test, validation dataframes, in that order.\n",
    "    \"\"\"\n",
    "    # print(\"SCALED TRAIN DATA\")\n",
    "    scaled_train_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_train_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    # print(\"TRAINING FEATURES\\n\", scaled_train_data.head())\n",
    "\n",
    "    scaled_test_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_test_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "\n",
    "    scaled_valid_data = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"scaled_valid_data_10M_2.csv\"),\n",
    "        usecols=all_cols,\n",
    "        nrows=SUBSAMPLE,\n",
    "    )\n",
    "    return scaled_train_data, scaled_test_data, scaled_valid_data\n",
    "\n",
    "# print('\\nTESTING FEATURES\\n', test_data_m.head())\n",
    "\n",
    "# print('\\ntrain set shape:',  train_data_m.shape)\n",
    "# print('\\ntest set shape:  ', test_data_m.shape)\n",
    "# # print('validation set shape:', valid_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "def get_train_scale_dict(USE_BRADEN_SCALING):\n",
    "    \"\"\"Get a dictionary containing mean and standard deviation of each gen and reco feature. \n",
    "\n",
    "    Args:\n",
    "        USE_BRADEN_SCALING (bool): Whether you wish to use the Braden scaling. If True, it uses the L-scaled train dataframe. If False, it uses the unscaled dataframe.\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of floats containing mean and standard deviation of each gen and reco feature. \n",
    "    \"\"\"\n",
    "    if USE_BRADEN_SCALING==True:\n",
    "        TRAIN_SCALE_DICT = get_scaling_info(scaled_train_data)\n",
    "        print('BRADEN SCALING DICTIONARY')\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    else:\n",
    "        print('NORMAL UNSCALED DICTIONARY')\n",
    "        TRAIN_SCALE_DICT = get_scaling_info(raw_train_data)\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    return TRAIN_SCALE_DICT\n",
    "\n",
    "\n",
    "\n",
    "################################ SPLIT###########\n",
    "# Currently need the split function again here\n",
    "# @memory.cache\n",
    "def split_t_x(df, target, input_features):\n",
    "    \"\"\"Get the target as the ratio, according to the T equation.\n",
    "    \n",
    "    Returns:\n",
    "    list(numpy.array): list of numpy array of target and training features\"\"\"\n",
    "\n",
    "    if target == \"RecoDatam\":\n",
    "        t = T(\"m\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDatapT\":\n",
    "        t = T(\"pT\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataeta\":\n",
    "        t = T(\"eta\", scaled_df=scaled_train_data)\n",
    "    if target == \"RecoDataphi\":\n",
    "        t = T(\"phi\", scaled_df=scaled_train_data)\n",
    "    x = np.array(df[input_features])\n",
    "    return np.array(t), x\n",
    "\n",
    "# @memory.cache\n",
    "def normal_split_t_x(df, target, input_features):\n",
    "    \"\"\"splot dataframe into targets and feature arrays.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe of train, test or validation data.\n",
    "        target (str): Choice of \"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\",\"RecoDatam\" as target.\n",
    "        input_features (list(str)): list of training features labels\n",
    "\n",
    "    Returns:\n",
    "    list(numpy.array): list of numpy array of target and training features\n",
    " \"\"\"\n",
    "    # change from pandas dataframe format to a numpy \n",
    "    # array of the specified types\n",
    "    # t = np.array(df[target])\n",
    "    t = np.array(df[target])\n",
    "    x = np.array(df[input_features])\n",
    "    return t, x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ Apply Z scaling############\n",
    "def z(x):\n",
    "    \"\"\"Simple z-score standardization. Used for targets\"\"\"\n",
    "    eps = 1e-20\n",
    "    return (x - np.mean(x)) / (np.std(x) + eps)\n",
    "def z_inverse(xprime, x):\n",
    "    return xprime * np.std(x) + np.mean(x)\n",
    "\n",
    "# @memory.cache\n",
    "def z2(x, mean, std):\n",
    "    \"\"\"\n",
    "    The main z score function. Args:\n",
    "        x (numpy.array): feature 1-D array\n",
    "        mean (float): mean of the feature (in the training set)\n",
    "        std (float): standard deviation of the feature (in the training set)\n",
    "\n",
    "    Returns:\n",
    "        numpy.array: z-score-scaled 1-D feature\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    scaled = (x - mean) / (std + eps)\n",
    "    return np.array(scaled, dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def z_inverse2(xprime, train_mean, train_std):\n",
    "    \"\"\"\n",
    "        The main z score de-scaling function. \n",
    "        \n",
    "        Args:\n",
    "        xprime (numpy.array): z-score-scaled feature 1-D array\n",
    "        train_mean (float): mean of the feature (in the training set)\n",
    "        train_std (float): standard deviation of the feature (in the training set)\n",
    "        \"\"\"\n",
    "    return xprime * train_std + train_mean\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x):\n",
    "    \"\"\"TO ensure this z scaling is only applied once to the training features, we use a generator.\n",
    "    This doesn't change the shapes of anything, just applies z to all the feature columns other than tau.\n",
    "    \n",
    "    Args:\n",
    "    TRAIN_SCALE_DICT (dict(float)): dictionary of train set mean and standard deviation values\n",
    "    train_x (numpy.array): 2-D numpy array of training features\n",
    "    test_x (numpy.array):  2-D numpy array of test features\n",
    "    valid_x (numpy.array):  2-D numpy array of validation features\n",
    "    \"\"\"\n",
    "    for i in range(NFEATURES - 1):\n",
    "        variable = list(TRAIN_SCALE_DICT)[i]\n",
    "        train_mean = float(TRAIN_SCALE_DICT[variable][\"mean\"])\n",
    "        train_std = float(TRAIN_SCALE_DICT[variable][\"std\"])\n",
    "        train_x[:, i] = z2(train_x[:, i], mean=train_mean, std=train_std)\n",
    "        test_x[:, i] = z2(test_x[:, i], mean=train_mean, std=train_std)\n",
    "        valid_x[:, i] = z2(valid_x[:, i], mean=train_mean, std=train_std)\n",
    "    yield train_x\n",
    "    yield test_x\n",
    "    yield valid_x\n",
    "\n",
    "\n",
    "# @memory.cache\n",
    "def apply_z_to_targets(train_t, test_t, valid_t):\n",
    "    \"\"\"apply z-score scaling to target columns\n",
    "\n",
    "    Args:\n",
    "        train_t (numpy.array): target column in the training set\n",
    "        test_t (numpy.array): target column in the test set\n",
    "        valid_t (numpy.array): target column in the validation set\n",
    "\n",
    "    Yields:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    train_mean = np.mean(train_t)\n",
    "    train_std = np.std(train_t)\n",
    "    train_t_ = z2(train_t, mean=train_mean, std=train_std)\n",
    "    test_t_ = z2(test_t, mean=train_mean, std=train_std)\n",
    "    valid_t_ = z2(valid_t, mean=train_mean, std=train_std)\n",
    "\n",
    "    yield train_t_\n",
    "    yield test_t_\n",
    "    yield valid_t_\n",
    "\n",
    "\n",
    "\n",
    "# check that it looks correct\n",
    "# fig = plt.figure(figsize=(10, 4))\n",
    "# ax = fig.add_subplot(autoscale_on=True)\n",
    "# ax.grid()\n",
    "# for i in range(NFEATURES):\n",
    "#     ax.hist(train_x[:,i], alpha=0.35, label=f'feature {i}' )\n",
    "#     # set_axes(ax=ax, xlabel=\"Transformed features X' \",title=\"training features post-z score: X'=z(L(X))\")\n",
    "# ax.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "######### Get beset hyperparameters\n",
    "# tuned_dir = os.path.join(IQN_BASE,'best_params')\n",
    "# tuned_filename=os.path.join(tuned_dir,'best_params_mass_%s_trials.csv' % str(int(n_trials)))\n",
    "# BEST_PARAMS = pd.read_csv(os.path.join(IQN_BASE, 'best_params','best_params_Test_Trials.csv'))\n",
    "# BEST_PARAMS=pd.read_csv(tuned_filename)\n",
    "# print(BEST_PARAMS)\n",
    "\n",
    "\n",
    "\n",
    "def load_untrained_model(PARAMS):\n",
    "    \"\"\"Load an untrained model (with weights initiatted) according to model paramateters in the \n",
    "    PARAMS dictionary\n",
    "\n",
    "    Args:\n",
    "        PARAMS (dict): dictionary of model/training parameters: i.e. hyperparameters and training parameters.\n",
    "\n",
    "    Returns:\n",
    "        utils.RegularizedRegressionModel object\n",
    "    \"\"\"\n",
    "    model = utils.RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    # model.apply(initialize_weights)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SaveModelCheckpoint:\n",
    "    \"\"\"Continuous model-checkpointing class. Updates the latest checkpoint of an object based o validation loss each time its called. \n",
    "    \"\"\"\n",
    "    def __init__(self, best_valid_loss=np.inf):\n",
    "        \"\"\"Initiate an instance of the class based on filename and best_valid_loss/\n",
    "\n",
    "        Args:\n",
    "            best_valid_loss (float, optional): Best possible validation loss of a checkpoint object. Defaults to np.inf.\n",
    "        \"\"\"\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        self.filename_model=filename_model\n",
    "\n",
    "    def __call__(self, model, current_valid_loss, filename_model):\n",
    "        \"\"\"When an object of the calss is called, its validation loss gets updated and the model based \n",
    "        on the latest validation loss is saved.\n",
    "\n",
    "        Args:\n",
    "            model: utils.RegularizedRegressionModel object.\n",
    "            current_valid_loss (float): current (latest) validation loss of this model during the training process.\n",
    "            filename_model (str): filename in which the latest model will be saved. Can be a relative or local path. \n",
    "        \"\"\"\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            # update the best loss\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            # filename_model='Trained_IQNx4_%s_%sK_iter.dict' % (target, str(int(n_iterations/1000)) )\n",
    "            # filename_model = \"Trained_IQNx4_%s_TUNED_2lin_with_noise.dict\" % target\n",
    "\n",
    "            # note that n_iterations is the total n_iterations, we dont want to save a million files for each iteration\n",
    "            trained_models_dir = \"trained_models\"\n",
    "            mkdir(trained_models_dir)\n",
    "            # on cluster, Im using another TRAIN directory\n",
    "            PATH_model = os.path.join(\n",
    "                IQN_BASE,\n",
    "                \"JupyterBook\",\n",
    "                \"Cluster\",\n",
    "                \"TRAIN\",\n",
    "                trained_models_dir,\n",
    "                filename_model,\n",
    "            )\n",
    "            torch.save(model.state_dict(), PATH_model)\n",
    "            print(\n",
    "                f\"\\nCurrent valid loss: {current_valid_loss};  saved better model at {PATH_model}\"\n",
    "            )\n",
    "            # save using .pth object which if a dictionary of dicionaries, so that I can have PARAMS saved in the same file\n",
    "\n",
    "\n",
    "def train(\n",
    "    target,\n",
    "    model,\n",
    "    avloss,\n",
    "    getbatch,\n",
    "    train_x,\n",
    "    train_t,\n",
    "    valid_x,\n",
    "    valid_t,\n",
    "    PARAMS,\n",
    "    traces,\n",
    "    step,\n",
    "    window,\n",
    "):\n",
    "    \"\"\"Training Function. \n",
    "\n",
    "    Args:\n",
    "        target (str): hoice of \"RecoDatapT\", \"RecoDataeta\", \"RecoDataphi\",\"RecoDatam\" as target.\n",
    "        model a torch NN model, e.g utils.RegularizedRegressionModel.\n",
    "        avloss (float): average training losss\n",
    "        getbatch (function): a get_batch function\n",
    "        train_x (numpy.DataFrame): 2-D numpy array of training features\n",
    "        train_t (numpy.DataFrame:  1-D numpy array of training targets\n",
    "        valid_x (numpy.DataFrame): 2-D numpy array of validation features\n",
    "        valid_t (numpy.DataFrame: 1-D numpy array of validation targets\n",
    "        PARAMS (dict): dictionary of model/training parameters \n",
    "        traces (tuple): tuple of  \n",
    "        (iteration, training accuracy, validation accuracy, running average of validation accuracy) \n",
    "        = (xx, yy_t, yy_v, yy_v_avg) \n",
    "        step (int): number of iterations to take a printout step of the traces\n",
    "        window (int): window of running average of validation loss  \n",
    "\n",
    "    Returns:\n",
    "        tuple: traces\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: obviously, for reference, the \"traces\" should be saved as a 2D numpy array\n",
    "    # with the same naming format as the \"model_filename\", so that it can be opened later and \n",
    "    # plot loss curves for different models.\n",
    "    \n",
    "    # TODO: decay the stepsize, such that steps (and hence checkpointing) are large in the beginnig to the learning\n",
    "    # process (which corresponds to high learning rates), and decrease as time steps increase.\n",
    "    batch_size = PARAMS['batch_size']\n",
    "    n_iterations = PARAMS['n_iterations']\n",
    "    # to keep track of average losses\n",
    "    xx, yy_t, yy_v, yy_v_avg = traces\n",
    "    model_checkpoint = SaveModelCheckpoint()\n",
    "    n = len(valid_x)\n",
    "    \n",
    "    print(\"Iteration vs average loss\")\n",
    "    print(\"%10s\\t%10s\\t%10s\" % (\"iteration\", \"train-set\", \"test-set\"))\n",
    "    \n",
    "    fifth_n_iterations=int(n_iterations//5)\n",
    "    starting_learning_rate = PARAMS['starting_learning_rate']\n",
    "    for ii in range(n_iterations):\n",
    "        #experiment with annealing LR from beginning\n",
    "        # LR_sched=LR_Cooler(starting_lr=starting_learning_rate, total_iterations=n_iterations, iter_=ii)\n",
    "        # learning_rate=LR_sched.exponential_decay()\n",
    "#         # starting learning rate (first fifth)\n",
    "        learning_rate= starting_learning_rate\n",
    "        \n",
    "#         #second fifth\n",
    "#         if 2* fifth_n_iterations < ii < 3*fifth_n_iterations:\n",
    "#             learning_rate=starting_learning_rate/10 #1e-2\n",
    "            \n",
    "#         #third fifth\n",
    "#         if 3*fifth_n_iterations < ii < 4*fifth_n_iterations:\n",
    "#             learning_rate=starting_learning_rate/100 #1e-3\n",
    "#         #frouth fifth: stary decay LR\n",
    "#         if ii > 4*fifth_n_iterations:\n",
    "#             learning_rate = decay_LR(ii)\n",
    "            \n",
    "        \n",
    "        # add weight decay (important regularization to reduce overfitting)\n",
    "        L2 = 1\n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2)\n",
    "        #SGD allows for: momentum=0, dampening=0, weight_decay=0, nesterov=boolean, differentiable=boolean\n",
    "\n",
    "        optimizer = getattr(torch.optim, optimizer_name)(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "         # amsgrad=True, \n",
    "\n",
    "        #  weight_decay=L2,#\n",
    "        # differentiable=True,\n",
    "        #For SGD nesterov, it requires momentum and zero dampening\n",
    "        # dampening=0,\n",
    "        # momentum=momentum,\n",
    "        # nesterov=True\n",
    "        # BUT no one should ever use SGD in 2022! Adam converges much better and faster.\n",
    "        )\n",
    "        \n",
    "        #if ii > 1e4: learning_rate=1e-4\n",
    "        # set mode to training so that training specific\n",
    "        # operations such as dropout are enabled.\n",
    "        # time_p_start = time.perf_counter()\n",
    "        model.train()\n",
    "\n",
    "        # get a random sample (a batch) of data (as numpy arrays)\n",
    "        batch_x, batch_t = getbatch(train_x, train_t, batch_size)\n",
    "        # Take df/ dtau\n",
    "        # x = torch.from_numpy(batch_x).float()\n",
    "        # # print('x is leaf: ', x.is_leaf)\n",
    "        # x.requires_grad_(True)\n",
    "        # # x.retain_grad()\n",
    "        # # print('x is leaf after retain: ', x.is_leaf)\n",
    "        # # x.requires_grad_(True)\n",
    "        # # x.retain_grad()\n",
    "        # f = model(x)\n",
    "        # f = f.view(-1)\n",
    "        # #multiply the model by its ransverse, remember we can only take gradients of scalars\n",
    "        # #and f will be a vector before this\n",
    "        # f = f @ f.t()\n",
    "        # f.retain_grad()\n",
    "        # f.backward(gradient=torch.ones_like(f), retain_graph=True)\n",
    "        # df_dx = x.grad\n",
    "        # df_dtau = df_dx[:,-1]\n",
    "        # x.grad.zero_()\n",
    "        \n",
    "        #add noise to training data\n",
    "        # batch_x = add_noise(batch_x)\n",
    "        # batch_t = add_noise(batch_t)\n",
    "        \n",
    "        # Try torch scheduler\n",
    "        scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():  # no need to compute gradients\n",
    "            # wrt. x and t\n",
    "            x = torch.from_numpy(batch_x).float()\n",
    "            t = torch.from_numpy(batch_t).float()\n",
    "\n",
    "        outputs = model(x).reshape(t.shape)\n",
    "\n",
    "        empirical_risk = avloss(outputs, t, x)\n",
    "\n",
    "        optimizer.zero_grad()  # clear previous gradients\n",
    "        empirical_risk.backward()  # compute gradients\n",
    "\n",
    "        optimizer.step()  # move one step towards the minimum of the loss function using an SGD-like algorithm.\n",
    "        \n",
    "        \n",
    "\n",
    "        if ii % step == 0:\n",
    "\n",
    "            print(f\"\\t\\tCURRENT LEARNING RATE: {learning_rate}\")\n",
    "            acc_t = validate(model, avloss, train_x[:n], train_t[:n])\n",
    "            #acc_t: list of training losses\n",
    "            acc_v = validate(model, avloss, valid_x[:n], valid_t[:n])\n",
    "            #acc_v: list of validation losses\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "            previous_iter_valid_loss = yy_v[-1]\n",
    "            print(f'previous_iter_valid_loss : {previous_iter_valid_loss}\\n')\n",
    "            # save better models based on valid loss\n",
    "            # filename_model=\"Trained_IQNx4_%s_TUNED_0lin_with_high_noise3.dict\" % target\n",
    "            filename_model=get_model_filename(target, PARAMS)\n",
    "             \n",
    "            model_checkpoint(model=model, filename_model =filename_model ,current_valid_loss=acc_v)\n",
    "            # compute running average for validation data\n",
    "            len_yy_v = len(yy_v)\n",
    "            if len_yy_v < window:\n",
    "                yy_v_avg.append(yy_v[-1])\n",
    "            elif len_yy_v == window:\n",
    "                yy_v_avg.append(sum(yy_v) / window)\n",
    "            else:\n",
    "                acc_v_avg = yy_v_avg[-1] * window\n",
    "                acc_v_avg += yy_v[-1] - yy_v[-window - 1]\n",
    "                yy_v_avg.append(acc_v_avg / window)\n",
    "\n",
    "            if len(xx) < 1:\n",
    "                xx.append(0)\n",
    "                print(\"%10d\\t%10.6f\\t%10.6f\" % (xx[-1], yy_t[-1], yy_v[-1]))\n",
    "            else:\n",
    "                xx.append(xx[-1] + step)\n",
    "\n",
    "                print(\n",
    "                    \"\\r%10d\\t%10.6f\\t%10.6f\\t%10.6f\"\n",
    "                    % (xx[-1], yy_t[-1], yy_v[-1], yy_v_avg[-1]),\n",
    "                    end=\"\",\n",
    "                )\n",
    "        # time_p_end = time.perf_counter()\n",
    "        # time_for_this_iter = time_p_end-time_p_start\n",
    "        # time_per_example = time_for_this_iter/batch_size\n",
    "        # print(f'training time for one example: {time_per_example}')\n",
    "\n",
    "    print()\n",
    "    return (xx, yy_t, yy_v, yy_v_avg)\n",
    "\n",
    "\n",
    "@utils.time_type_of_func(tuning_or_training=\"training\")\n",
    "def run(\n",
    "    target,\n",
    "    model,\n",
    "    train_x,\n",
    "    train_t,\n",
    "    valid_x,\n",
    "    valid_t,\n",
    "    traces,\n",
    "    PARAMS,\n",
    "    traces_step,\n",
    "    traces_window,\n",
    "    save_model,\n",
    "):\n",
    "\n",
    "\n",
    "    traces = train(\n",
    "        target,\n",
    "        model,\n",
    "        average_quantile_loss,\n",
    "        get_batch,\n",
    "        train_x,\n",
    "        train_t,\n",
    "        valid_x,\n",
    "        valid_t,\n",
    "        PARAMS,\n",
    "        traces,\n",
    "        step=traces_step,\n",
    "        window=traces_window,\n",
    "    )\n",
    "\n",
    "    if save_model:\n",
    "        filename = \"Trained_IQNx4_%s_%sK_iter.dict\" % (\n",
    "            target,\n",
    "            str(int(n_iterations / 1000)),\n",
    "        )\n",
    "        PATH = os.path.join(IQN_BASE, \"trained_models\", filename)\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_train_scale_dict(USE_BRADEN_SCALING):\n",
    "    if USE_BRADEN_SCALING==True:\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(scaled_train_data)\n",
    "        print(\"BRADEN SCALING DICTIONARY\")\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    else:\n",
    "        print(\"NORMAL UNSCALED DICTIONARY\")\n",
    "        TRAIN_SCALE_DICT = utils.get_scaling_info(raw_train_data)\n",
    "        print(TRAIN_SCALE_DICT)\n",
    "        print(\"\\n\\n\")\n",
    "        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)\n",
    "        # print(TEST_SCALE_DICT)\n",
    "    return TRAIN_SCALE_DICT\n",
    "\n",
    "\n",
    "@utils.debug\n",
    "def save_model(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "@utils.debug\n",
    "def save_model_params(model, PATH):\n",
    "    print(model)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(\"\\ntrained model dictionary saved in %s\" % PATH)\n",
    "\n",
    "\n",
    "@utils.debug\n",
    "def load_model(PATH):\n",
    "    # n_layers = int(BEST_PARAMS[\"n_layers\"])\n",
    "    # hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "    # dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "    # optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "    # learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "    # batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "    model = utils.RegularizedRegressionModel(\n",
    "        nfeatures=train_x.shape[1],\n",
    "        ntargets=1,\n",
    "        nlayers=n_layers,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    # OR\n",
    "    # model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED!\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_trained_model(PATH, PARAMS):\n",
    "    model = utils.RegularizedRegressionModel(\n",
    "        nfeatures=NFEATURES,\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout_1=PARAMS[\"dropout_1\"],\n",
    "        dropout_2=PARAMS[\"dropout_2\"],\n",
    "        activation=PARAMS[\"activation\"],\n",
    "    )\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    print(model)\n",
    "    model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891fa0d7-ee43-4ee0-8d25-eafa4fc40b02",
   "metadata": {},
   "source": [
    "# 2.3 Load Data, split, scale, and Train Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4a34491a-0c10-4e71-8967-c0514d211f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['genDatapT', 'genDataeta', 'genDataphi', 'genDatam', 'tau']\n",
      "\n",
      "Target =  RecoDatam\n",
      "USING NEW DATASET\n",
      "\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--tmp-ipykernel-4125311998.load_raw_data...\n",
      "load_raw_data()\n",
      "SUBSAMPLE = None\n",
      "\n",
      " RAW TRAIN DATA SHAPE\n",
      "\n",
      "(8000000, 9)\n",
      "\n",
      " RAW TRAIN DATA\n",
      "\n",
      "\n",
      " RAW TEST DATA\\ SHAPEn\n",
      "(1000000, 9)\n",
      "\n",
      " RAW TEST DATA\n",
      "\n",
      "___________________________________________________load_raw_data - 12.8s, 0.2min\n",
      "spliting data for RecoDatam\n",
      "train_t shape =  (8000000,) train_x shape =  (8000000, 5)\n",
      "\n",
      " Training features:\n",
      "\n",
      "[[29.4452      0.828187    2.90213     2.85348     0.36130954]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.12689925]\n",
      " [24.3193     -1.16351     0.636469    5.83685     0.96230681]\n",
      " ...\n",
      " [41.4192     -2.23358    -2.81921     7.19348     0.08421659]\n",
      " [35.4637     -1.12318     0.356494    6.06597     0.05535172]\n",
      " [26.5586     -1.09427    -1.49334     4.25409     0.07489863]]\n",
      "valid_t shape =  (1000000,) valid_x shape =  (1000000, 5)\n",
      "test_t shape =  (1000000,) test_x shape =  (1000000, 5)\n",
      "no need to train_test_split since we already have the split dataframes\n",
      "[ 3.27223764e+01  6.98189368e-04 -8.95543973e-04  6.96116528e+00\n",
      "  5.00485136e-01] [15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]\n",
      "[ 3.26952341e+01 -1.78188172e-03 -3.83090331e-04  6.96299435e+00\n",
      "  4.99915289e-01] [14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]\n",
      "5.5514112643334546 2.664124544901276\n",
      "5.555567451922438 2.664339857066051\n",
      "NORMAL UNSCALED DICTIONARY\n",
      "{'genDatapT': {'mean': 32.695234084987476, 'std': 14.937932540562551}, 'genDataeta': {'mean': -0.0017818817154031672, 'std': 2.204309760627079}, 'genDataphi': {'mean': -0.0003830903308450233, 'std': 1.8138251604791067}, 'genDatam': {'mean': 6.962994352358474, 'std': 2.781332025286383}, 'RecoDatapT': {'mean': 32.86720151648752, 'std': 15.829355769531851}, 'RecoDataeta': {'mean': -0.0017898858568513964, 'std': 2.197968491495457}, 'RecoDataphi': {'mean': -0.0004719170328962474, 'std': 1.8144739820043825}, 'RecoDatam': {'mean': 5.555567451922438, 'std': 2.664339857066051}}\n",
      "\n",
      "\n",
      "\n",
      "[ 1.81700902e-03  1.12510099e-03 -2.82526482e-04 -6.57626105e-04\n",
      "  5.00485136e-01] [1.01748627 0.9999745  0.99989115 0.99987283 0.28852734]\n",
      "[ 1.61650249e-15 -1.10134124e-18 -3.12905257e-17  5.01036723e-15\n",
      "  4.99915289e-01] [1.         1.         1.         1.         0.28867295]\n",
      "-0.0015599314696879494 0.9999191874249058\n",
      "6.996216939114674e-16 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "target = \"RecoDatam\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING=False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(1e5)  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "raw_train_data, raw_test_data, raw_valid_data =load_raw_data()\n",
    "# Load scaled data\n",
    "# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()\n",
    "\n",
    "\n",
    "# Get targets and features\n",
    "# if USE_BRADEN_SCALING==True:\n",
    "#     print(f\"spliting data for {target}\")\n",
    "#     train_t, train_x = split_t_x(\n",
    "#         df=scaled_train_data, target=target, input_features=features\n",
    "#     )\n",
    "#     print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "#     print(\"\\n Training features:\\n\")\n",
    "#     print(train_x)\n",
    "#     valid_t, valid_x = split_t_x(\n",
    "#         df=scaled_valid_data, target=target, input_features=features\n",
    "#     )\n",
    "#     print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "#     test_t, test_x = split_t_x(df=scaled_test_data, target=target, input_features=features)\n",
    "#     print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "# else:\n",
    "#     print(f\"spliting data for {target}\")\n",
    "#     train_t, train_x = normal_split_t_x(\n",
    "#     df=raw_train_data, target=target, input_features=features\n",
    "#     )\n",
    "#     print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "#     print(\"\\n Training features:\\n\")\n",
    "#     print(train_x)\n",
    "#     valid_t, valid_x = normal_split_t_x(\n",
    "#     df=raw_valid_data, target=target, input_features=features\n",
    "#     )\n",
    "#     print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "#     test_t, test_x = normal_split_t_x(df=raw_test_data, target=target, input_features=features)\n",
    "#     print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "\n",
    "print(f\"spliting data for {target}\")\n",
    "train_t, train_x = normal_split_t_x(\n",
    "df=raw_train_data, target=target, input_features=features\n",
    ")\n",
    "print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "print(\"\\n Training features:\\n\")\n",
    "print(train_x)\n",
    "valid_t, valid_x = normal_split_t_x(\n",
    "df=raw_valid_data, target=target, input_features=features\n",
    ")\n",
    "print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "test_t, test_x = normal_split_t_x(df=raw_test_data, target=target, input_features=features)\n",
    "print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "NFEATURES = train_x.shape[1]\n",
    "######################################################\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "TRAIN_SCALE_DICT=get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(\n",
    "    train_t, test_t, valid_t\n",
    ")\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087482e-fb15-4e02-a894-c5c6fc05638e",
   "metadata": {},
   "source": [
    "Decide Whether to use Braden Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d597bc6-64df-4427-b268-9f1aeb3cd183",
   "metadata": {},
   "source": [
    "# Define Mass Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7b8a342c-6c66-4d39-aaeb-eb421bfa3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Decide on parameters for this model and training\n",
    "PARAMS_m = {\n",
    "\"n_layers\": int(20),\n",
    "\"hidden_size\": int(3),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(0.5),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(512),\n",
    "    'n_iterations': int(5e6),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708af01e-612a-405a-81c8-41a2e3739a07",
   "metadata": {},
   "source": [
    "## Train Mass\n",
    "\n",
    "### The model that needs the longest time in training is mass. Click here to scroll down to train $p_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f695aaf2-a37e-4ba1-86aa-0563957f09dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "training for 5000000 iteration, which is  320.0 epochs\n",
      "RegularizedRegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=3, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.3)\n",
      "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (3): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.3)\n",
      "    (5): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (6): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.3)\n",
      "    (8): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (9): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.3)\n",
      "    (11): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (12): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.3)\n",
      "    (14): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (15): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.3)\n",
      "    (17): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (18): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.3)\n",
      "    (20): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (21): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.3)\n",
      "    (23): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (24): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): LeakyReLU(negative_slope=0.3)\n",
      "    (26): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (27): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (28): LeakyReLU(negative_slope=0.3)\n",
      "    (29): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (30): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): LeakyReLU(negative_slope=0.3)\n",
      "    (32): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (33): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (34): LeakyReLU(negative_slope=0.3)\n",
      "    (35): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (36): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (37): LeakyReLU(negative_slope=0.3)\n",
      "    (38): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (39): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (40): LeakyReLU(negative_slope=0.3)\n",
      "    (41): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (42): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (43): LeakyReLU(negative_slope=0.3)\n",
      "    (44): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (45): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (46): LeakyReLU(negative_slope=0.3)\n",
      "    (47): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (48): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (49): LeakyReLU(negative_slope=0.3)\n",
      "    (50): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (51): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (52): LeakyReLU(negative_slope=0.3)\n",
      "    (53): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (54): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (55): LeakyReLU(negative_slope=0.3)\n",
      "    (56): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (57): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (58): LeakyReLU(negative_slope=0.3)\n",
      "    (59): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "training IQN \n",
      "Iteration vs average loss\n",
      " iteration\t train-set\t  test-set\n",
      "\t\tCURRENT LEARNING RATE: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_iter_valid_loss : 0.3664275109767914\n",
      "\n",
      "\n",
      "Current valid loss: 0.3664275109767914;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_20_layer3_hiddenLeakyReLU_activation512_batchsize5000_Kiteration.dict\n",
      "         0\t  0.366663\t  0.366428\n",
      "\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.318241149187088\n",
      "\n",
      "\n",
      "Current valid loss: 0.318241149187088;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_20_layer3_hiddenLeakyReLU_activation512_batchsize5000_Kiteration.dict\n",
      "        20\t  0.318823\t  0.318241\t  0.318241\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.3382752537727356\n",
      "\n",
      "        40\t  0.338497\t  0.338275\t  0.338275\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.4142429232597351\n",
      "\n",
      "        60\t  0.414315\t  0.414243\t  0.414243\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.4033059775829315\n",
      "\n",
      "        80\t  0.403708\t  0.403306\t  0.403306\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.32650357484817505\n",
      "\n",
      "       100\t  0.326247\t  0.326504\t  0.326504\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.42051562666893005\n",
      "\n",
      "       120\t  0.420987\t  0.420516\t  0.420516\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.9743950366973877\n",
      "\n",
      "       140\t  0.975405\t  0.974395\t  0.974395\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 1.1878925561904907\n",
      "\n",
      "       160\t  1.189670\t  1.187893\t  1.187893\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.20049522817134857\n",
      "\n",
      "\n",
      "Current valid loss: 0.20049522817134857;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_20_layer3_hiddenLeakyReLU_activation512_batchsize5000_Kiteration.dict\n",
      "       180\t  0.200739\t  0.200495\t  0.200495\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.2664671838283539\n",
      "\n",
      "       200\t  0.266614\t  0.266467\t  0.266467\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.217570498585701\n",
      "\n",
      "       220\t  0.218200\t  0.217570\t  0.217570\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.4117608070373535\n",
      "\n",
      "       240\t  0.412009\t  0.411761\t  0.411761\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.20280355215072632\n",
      "\n",
      "       260\t  0.202918\t  0.202804\t  0.202804\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.23898771405220032\n",
      "\n",
      "       280\t  0.239259\t  0.238988\t  0.238988\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.5546733736991882\n",
      "\n",
      "       300\t  0.555227\t  0.554673\t  0.554673\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.2718827724456787\n",
      "\n",
      "       320\t  0.272222\t  0.271883\t  0.271883\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.20671525597572327\n",
      "\n",
      "       340\t  0.207322\t  0.206715\t  0.206715\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.3099561333656311\n",
      "\n",
      "       360\t  0.310573\t  0.309956\t  0.309956\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.33409327268600464\n",
      "\n",
      "       380\t  0.334195\t  0.334093\t  0.398260\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.35455989837646484\n",
      "\n",
      "       400\t  0.354758\t  0.354560\t  0.397667\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.6772529482841492\n",
      "\n",
      "       420\t  0.677241\t  0.677253\t  0.415617\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.34573084115982056\n",
      "\n",
      "       440\t  0.346043\t  0.345731\t  0.415990\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.8707666397094727\n",
      "\n",
      "       460\t  0.870336\t  0.870767\t  0.438816\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.36494821310043335\n",
      "\n",
      "       480\t  0.365430\t  0.364948\t  0.436899\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.2843315601348877\n",
      "\n",
      "       500\t  0.284747\t  0.284332\t  0.434790\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.2532341778278351\n",
      "\n",
      "       520\t  0.253375\t  0.253234\t  0.426426\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.2087298184633255\n",
      "\n",
      "       540\t  0.209187\t  0.208730\t  0.388143\t\tCURRENT LEARNING RATE: 0.5\n",
      "previous_iter_valid_loss : 0.21838919818401337\n",
      "\n",
      "       560\t  0.218773\t  0.218389\t  0.339667\t\tCURRENT LEARNING RATE: 0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_123759/626982273.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtraces_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraces_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtraces_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraces_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Pulled_Github_Repositories/torchQN/utils/utils.py\u001b[0m in \u001b[0;36mwrapper_timer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"timing this arbitrary function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_123759/4125311998.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(target, model, train_x, train_t, valid_x, valid_t, traces, PARAMS, traces_step, traces_window, save_model)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mtraces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraces_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraces_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m     )\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_123759/4125311998.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(target, model, avloss, getbatch, train_x, train_t, valid_x, valid_t, PARAMS, traces, step, window)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\t\\tCURRENT LEARNING RATE: {learning_rate}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0macc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;31m#acc_t: list of training losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0macc_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Pulled_Github_Repositories/torchQN/utils/utils.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, avloss, inputs, targets)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;31m# remember to reshape!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mavloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Pulled_Github_Repositories/torchQN/utils/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1472\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer_name=PARAMS_m['optimizer_name']\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS=PARAMS_m['n_iterations']\n",
    "BATCHSIZE=PARAMS_m['batch_size']\n",
    "comment=''\n",
    "\n",
    "\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(f\"training for {NITERATIONS} iteration, which is  {N_epochs} epochs\")\n",
    "\n",
    "\n",
    "filename_model = get_model_filename(target, PARAMS_m)\n",
    "trained_models_dir = \"trained_models\"\n",
    "mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE, #the loaction of the repo\n",
    "    \"JupyterBook\", #up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\", \n",
    "    \"TRAIN\",\n",
    "    trained_models_dir, #/trained_models \n",
    "    filename_model # utils.get_model_filename has the saved file format \n",
    ")\n",
    "\n",
    "#LOAD EITHER TRAINED OR UNTRAINED MODEL\n",
    "# to load untrained model (start training from scratch), uncomment the next line\n",
    "untrained_model = load_untrained_model(PARAMS_m)\n",
    "# to continune training of model (pickup where the previous training left off), uncomment below\n",
    "# trained_model =load_trained_model(PATH=PATH_model, PARAMS=PARAMS_m)\n",
    "\n",
    "IQN_trace = ([], [], [], [])\n",
    "traces_step = int(20)\n",
    "traces_window = traces_step\n",
    "IQN = run(\n",
    "    target=target,\n",
    "    model=untrained_model,\n",
    "    train_x=train_x_z_scaled,\n",
    "    train_t=train_t_z_scaled,\n",
    "    valid_x=test_x_z_scaled,\n",
    "    valid_t=test_t_z_scaled,\n",
    "    traces=IQN_trace,\n",
    "    PARAMS=PARAMS_m,\n",
    "    traces_step=traces_step,\n",
    "    traces_window=traces_window,\n",
    "    save_model=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "SAVE_LAST_MODEL=False\n",
    "if SAVE_LAST_MODEL:\n",
    "    # ## Save last iteration of trained model \n",
    "    #dont save the last model, it might be worse than previous iterations, which were automatically savedby model checkpoints\n",
    "\n",
    "    final_path = get_model_filename(target, PARAMS_m).split('.dict')[0]+'_FINAL.dict'\n",
    "\n",
    "    trained_models_dir = \"trained_models\"\n",
    "    mkdir(trained_models_dir)\n",
    "    # on cluster, Im using another TRAIN directory\n",
    "    PATH_final_model = os.path.join(\n",
    "    IQN_BASE, \"JupyterBook\", \"Cluster\", \"TRAIN\", trained_models_dir, final_path\n",
    "    )\n",
    "\n",
    "    save_model(IQN, PATH_final_model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47056189-c407-486b-aa44-11bc24248062",
   "metadata": {},
   "source": [
    "##### 2.4: Train $p_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e7a24-afc4-486f-9bf1-7c11cc1dbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "target = \"RecoDatapT\"\n",
    "source = FIELDS[target]\n",
    "features = source[\"inputs\"]\n",
    "print(\"Training Features:\\n\", features)\n",
    "print(\"\\nTarget = \", target)\n",
    "\n",
    "print(\"USING NEW DATASET\\n\")\n",
    "######################################\n",
    "USE_BRADEN_SCALING=False\n",
    "#####################################\n",
    "################################### CONFIGURATIONS ###################################\n",
    "\n",
    "JUPYTER = True\n",
    "use_subsample = False\n",
    "# use_subsample = True\n",
    "if use_subsample:\n",
    "    SUBSAMPLE = int(\n",
    "        1e5\n",
    "    )  # subsample use for development - in production use whole dataset\n",
    "else:\n",
    "    SUBSAMPLE = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "raw_train_data, raw_test_data, raw_valid_data =load_raw_data()\n",
    "# Load scaled data\n",
    "# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()\n",
    "\n",
    "\n",
    "# Get targets and features\n",
    "# if USE_BRADEN_SCALING==True:\n",
    "#     print(f\"spliting data for {target}\")\n",
    "#     train_t, train_x = split_t_x(\n",
    "#         df=scaled_train_data, target=target, input_features=features\n",
    "#     )\n",
    "#     print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "#     print(\"\\n Training features:\\n\")\n",
    "#     print(train_x)\n",
    "#     valid_t, valid_x = split_t_x(\n",
    "#         df=scaled_valid_data, target=target, input_features=features\n",
    "#     )\n",
    "#     print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "#     test_t, test_x = split_t_x(df=scaled_test_data, target=target, input_features=features)\n",
    "#     print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "# else:\n",
    "#     print(f\"spliting data for {target}\")\n",
    "#     train_t, train_x = normal_split_t_x(\n",
    "#     df=raw_train_data, target=target, input_features=features\n",
    "#     )\n",
    "#     print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "#     print(\"\\n Training features:\\n\")\n",
    "#     print(train_x)\n",
    "#     valid_t, valid_x = normal_split_t_x(\n",
    "#     df=raw_valid_data, target=target, input_features=features\n",
    "#     )\n",
    "#     print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "#     test_t, test_x = normal_split_t_x(df=raw_test_data, target=target, input_features=features)\n",
    "#     print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "\n",
    "print(f\"spliting data for {target}\")\n",
    "train_t, train_x = normal_split_t_x(\n",
    "df=raw_train_data, target=target, input_features=features\n",
    ")\n",
    "print(\"train_t shape = \", train_t.shape, \"train_x shape = \", train_x.shape)\n",
    "print(\"\\n Training features:\\n\")\n",
    "print(train_x)\n",
    "valid_t, valid_x = normal_split_t_x(\n",
    "df=raw_valid_data, target=target, input_features=features\n",
    ")\n",
    "print(\"valid_t shape = \", valid_t.shape, \"valid_x shape = \", valid_x.shape)\n",
    "test_t, test_x = normal_split_t_x(df=raw_test_data, target=target, input_features=features)\n",
    "print(\"test_t shape = \", test_t.shape, \"test_x shape = \", test_x.shape)\n",
    "\n",
    "\n",
    "print(\"no need to train_test_split since we already have the split dataframes\")\n",
    "print(valid_x.mean(axis=0), valid_x.std(axis=0))\n",
    "print(train_x.mean(axis=0), train_x.std(axis=0))\n",
    "print(valid_t.mean(), valid_t.std())\n",
    "print(train_t.mean(), train_t.std())\n",
    "NFEATURES = train_x.shape[1]\n",
    "######################################################\n",
    "\n",
    "# Apply z scaling to features and targets\n",
    "# to features\n",
    "TRAIN_SCALE_DICT=get_train_scale_dict(USE_BRADEN_SCALING)\n",
    "apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)\n",
    "train_x_z_scaled = next(apply_z_generator)\n",
    "test_x_z_scaled = next(apply_z_generator)\n",
    "valid_x_z_scaled = next(apply_z_generator)\n",
    "print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))\n",
    "print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))\n",
    "\n",
    "# to targets\n",
    "apply_z_to_targets_generator = apply_z_to_targets(\n",
    "    train_t, test_t, valid_t\n",
    ")\n",
    "train_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "test_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "valid_t_z_scaled = next(apply_z_to_targets_generator)\n",
    "print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())\n",
    "print(train_t_z_scaled.mean(), train_t_z_scaled.std())\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# Decide on parameters for this model and training\n",
    "PARAMS_pT = {\n",
    "\"n_layers\": int(10),\n",
    "\"hidden_size\": int(6),\n",
    "\"dropout_1\": float(0.6),\n",
    "\"dropout_2\": float(0.1),\n",
    "\"activation\": \"LeakyReLU\",\n",
    "    'optimizer_name':'Adam',\n",
    "    'starting_learning_rate':float(1e-4),\n",
    "    'momentum':float(0.6),\n",
    "    'batch_size':int(1024),\n",
    "    'n_iterations': int(3e5),\n",
    "}\n",
    "\n",
    "optimizer_name=PARAMS_pT['optimizer_name']\n",
    "print(type(optimizer_name))\n",
    "# optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "NITERATIONS=PARAMS_pT['n_iterations']\n",
    "BATCHSIZE=PARAMS_pT['batch_size']\n",
    "comment=''\n",
    "\n",
    "\n",
    "\n",
    "# N_epochs X N_train_examples = N_iterations X batch_size\n",
    "N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])\n",
    "print(f\"training for {NITERATIONS} iteration, which is  {N_epochs} epochs\")\n",
    "\n",
    "#train model from scratch\n",
    "filename_model = utils.get_model_filename(target, PARAMS_pT)\n",
    "#or pick up trained model\n",
    "filename_model = 'Trained_IQNx4_RecoDatapT_10_layer6_hiddenLeakyReLU_activation512_batchsize300_Kiteration.dict'\n",
    "\n",
    "trained_models_dir = \"trained_models\"\n",
    "utils.mkdir(trained_models_dir)\n",
    "# on cluster, Im using another TRAIN directory\n",
    "PATH_model = os.path.join(\n",
    "    IQN_BASE, #the loaction of the repo\n",
    "    \"JupyterBook\", #up tp TRAIN could be combined in a srs dicretory\n",
    "    \"Cluster\", \n",
    "    \"TRAIN\",\n",
    "    trained_models_dir, #/trained_models \n",
    "    filename_model # utils.get_model_filename has the saved file format \n",
    ")\n",
    "\n",
    "#LOAD EITHER TRAINED OR UNTRAINED MODEL\n",
    "# to load untrained model (start training from scratch), uncomment the next line\n",
    "untrained_model = load_untrained_model(PARAMS_pT)\n",
    "# to continune training of model (pickup where the previous training left off), uncomment below\n",
    "trained_model =load_trained_model(PATH=PATH_model, PARAMS=PARAMS_pT)\n",
    "\n",
    "IQN_trace = ([], [], [], [])\n",
    "traces_step = 20\n",
    "traces_window = traces_step\n",
    "IQN = run(\n",
    "    target=target,\n",
    "    model=trained_model,\n",
    "    train_x=train_x_z_scaled,\n",
    "    train_t=train_t_z_scaled,\n",
    "    valid_x=test_x_z_scaled,\n",
    "    valid_t=test_t_z_scaled,\n",
    "    traces=IQN_trace,\n",
    "    PARAMS=PARAMS_pT,\n",
    "    traces_step=traces_step,\n",
    "    traces_window=traces_window,\n",
    "    save_model=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "SAVE_LAST_MODEL=False\n",
    "if SAVE_LAST_MODEL:\n",
    "    # ## Save last iteration of trained model \n",
    "    #dont save the last model, it might be worse than previous iterations, which were automatically savedby model checkpoints\n",
    "\n",
    "    final_path = utils.get_model_filename(target, PARAMS_pT).split('.dict')[0]+'_FINAL.dict'\n",
    "\n",
    "    trained_models_dir = \"trained_models\"\n",
    "    utils.mkdir(trained_models_dir)\n",
    "    # on cluster, Im using another TRAIN directory\n",
    "    PATH_final_model = os.path.join(\n",
    "    IQN_BASE, \"JupyterBook\", \"Cluster\", \"TRAIN\", trained_models_dir, final_path\n",
    "    )\n",
    "\n",
    "    save_model(IQN, PATH_final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f98e6c-b1af-4b28-853f-25121451acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to continune training of model (pickup where the previous training left off), uncomment below\n",
    "trained_model =load_trained_model(PATH=PATH_model, PARAMS=PARAMS_pT)\n",
    "\n",
    "IQN_trace = ([], [], [], [])\n",
    "traces_step = 20\n",
    "traces_window = traces_step\n",
    "IQN = run(\n",
    "    target=target,\n",
    "    model=trained_model,\n",
    "    train_x=train_x_z_scaled,\n",
    "    train_t=train_t_z_scaled,\n",
    "    valid_x=test_x_z_scaled,\n",
    "    valid_t=test_t_z_scaled,\n",
    "    traces=IQN_trace,\n",
    "    PARAMS=PARAMS_pT,\n",
    "    traces_step=traces_step,\n",
    "    traces_window=traces_window,\n",
    "    save_model=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6e75c-af85-4e41-9c7b-591ae354c773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
