
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>IQNx4 Chapter 2: Train All Networks &#8212; torchIQNx4</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">torchIQNx4</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the torchIQNx4
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_Setup_and_Preprocess.html">
   IQNx4: Chapter 1. Setup and Preprocess
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2_Train_all_IQNs.html">
   IQNx4 Chapter 2: Train All Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_Autoregressive_Evaluation.html">
   IQNx4: Chapter 3: Autoregressive Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_SupplementaryMaterials_OptunaTuning_Theory_ML_Background.html">
   Chapter 4: Optional Supplementary Material
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5_Normalizing_Flows.html">
   train
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/AliAlkadhim/torchQN/HEAD?labpath=JupyterBook/v2/gh/AliAlkadhim/torchQN/master?urlpath=tree/JupyterBook/Cockpit_2_Train_all_IQNs.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/Cockpit_2_Train_all_IQNs.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   IQNx4 Chapter 2: Train All Networks
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports-and-configurations">
   2.1 Imports and Configurations
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-required-functions">
   2.2: Load Required Functions
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-data-split-scale-and-train-mass">
   2.3 Load Data, split, scale, and Train Mass
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#configurations">
     Configurations
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#oversample-regions-with-11-m-25-gev">
     Oversample regions with
     <span class="math notranslate nohighlight">
      \(11 &lt; m &lt; 25\)
     </span>
     GeV.
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-mass-model-parameters">
   Define Mass Model Parameters
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-mass">
     Train Mass
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-model-that-needs-the-longest-time-in-training-is-mass-click-here-to-scroll-down-to-train-p-t">
       The model that needs the longest time in training is mass. Click here to scroll down to train
       <span class="math notranslate nohighlight">
        \(p_T\)
       </span>
       .
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#train-p-t">
       2.4: Train
       <span class="math notranslate nohighlight">
        \(p_T\)
       </span>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>IQNx4 Chapter 2: Train All Networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   IQNx4 Chapter 2: Train All Networks
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports-and-configurations">
   2.1 Imports and Configurations
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-required-functions">
   2.2: Load Required Functions
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-data-split-scale-and-train-mass">
   2.3 Load Data, split, scale, and Train Mass
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#configurations">
     Configurations
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#oversample-regions-with-11-m-25-gev">
     Oversample regions with
     <span class="math notranslate nohighlight">
      \(11 &lt; m &lt; 25\)
     </span>
     GeV.
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-mass-model-parameters">
   Define Mass Model Parameters
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-mass">
     Train Mass
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-model-that-needs-the-longest-time-in-training-is-mass-click-here-to-scroll-down-to-train-p-t">
       The model that needs the longest time in training is mass. Click here to scroll down to train
       <span class="math notranslate nohighlight">
        \(p_T\)
       </span>
       .
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#train-p-t">
       2.4: Train
       <span class="math notranslate nohighlight">
        \(p_T\)
       </span>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="iqnx4-chapter-2-train-all-networks">
<h1>IQNx4 Chapter 2: Train All Networks<a class="headerlink" href="#iqnx4-chapter-2-train-all-networks" title="Permalink to this headline">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="imports-and-configurations">
<h1>2.1 Imports and Configurations<a class="headerlink" href="#imports-and-configurations" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># import scipy as sp; import scipy.stats as st</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;using torch version </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span><span class="c1">#old torch version: 1.9.0</span>
<span class="c1"># use numba&#39;s just-in-time compiler to speed things up</span>
<span class="c1"># from numba import njit</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mp</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;matplotlib version= &quot;</span><span class="p">,</span> <span class="n">mp</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># reset matplotlib stle/parameters</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>

<span class="c1"># reset matplotlib parameters to their defaults</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">mpl</span><span class="o">.</span><span class="n">rcParamsDefault</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;seaborn-deep&quot;</span><span class="p">)</span>
<span class="n">mp</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;agg.path.chunksize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">font_legend</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">font_axes</span> <span class="o">=</span> <span class="mi">15</span>
<span class="c1"># %matplotlib inline</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="c1">#or use joblib for caching on disk</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span>  <span class="n">Memory</span>
<span class="c1"># from IPython.display import Image, display</span>
<span class="c1"># from importlib import import_module</span>
<span class="c1"># import plotly</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">optuna</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;using (optional) optuna version </span><span class="si">{</span><span class="n">optuna</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;optuna is only used for hyperparameter tuning, not critical!&quot;</span><span class="p">)</span>
    <span class="k">pass</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># import sympy as sy</span>
<span class="c1"># import ipywidgets as wid;</span>


<span class="c1"># try:</span>
<span class="c1">#     IQN_BASE = os.environ[&quot;IQN_BASE&quot;]</span>
<span class="c1">#     print(&quot;BASE directoy properly set = &quot;, IQN_BASE)</span>
<span class="c1">#     utils_dir = os.path.join(IQN_BASE, &quot;utils/&quot;)</span>
<span class="c1">#     sys.path.append(utils_dir)</span>
<span class="c1">#     import utils</span>

<span class="c1">#     # usually its not recommended to import everything from a module, but we know</span>
<span class="c1">#     # whats in it so its fine</span>
<span class="c1">#     from utils import *</span>

<span class="c1">#     print(&quot;DATA directory also properly set, in %s&quot; % os.environ[&quot;DATA_DIR&quot;])</span>
<span class="c1"># except Exception:</span>
<span class="c1">#     # IQN_BASE=os.getcwd()</span>
<span class="c1">#     print(</span>
<span class="c1">#         &quot;&quot;&quot;\nBASE directory not properly set. Read repo README.    If you need a function from utils, use the decorator below, or add utils to sys.path\n</span>
<span class="c1">#     You can also do </span>
<span class="c1">#     os.environ[&#39;IQN_BASE&#39;]=&lt;ABSOLUTE PATH FOR THE IQN REPO&gt;</span>
<span class="c1">#     or</span>
<span class="c1">#     os.environ[&#39;IQN_BASE&#39;]=os.getcwd()&quot;&quot;&quot;</span>
<span class="c1">#     )</span>
<span class="c1">#     pass</span>




<span class="c1"># from IPython.core.magic import register_cell_magic</span>


<span class="c1"># @debug</span>
<span class="c1"># def get_model_params_simple():</span>
<span class="c1">#     dropout=0.2</span>
<span class="c1">#     n_layers = 2</span>
<span class="c1">#     n_hidden=32</span>
<span class="c1">#     starting_learning_rate=1e-3</span>
<span class="c1">#     print(&#39;n_iterations, n_layers, n_hidden, starting_learning_rate, dropout&#39;)</span>
<span class="c1">#     return n_iterations, n_layers, n_hidden, starting_learning_rate, dropout</span>


<span class="c1"># update fonts</span>
<span class="n">FONTSIZE</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;family&quot;</span><span class="p">:</span> <span class="s2">&quot;serif&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="n">FONTSIZE</span><span class="p">}</span>
<span class="n">mp</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s2">&quot;font&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>

<span class="c1"># set usetex = False if LaTex is not</span>
<span class="c1"># available on your system or if the</span>
<span class="c1"># rendering is too slow</span>
<span class="n">mp</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="n">usetex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># set a seed to ensure reproducibility</span>
<span class="c1"># seed = 128</span>
<span class="c1"># rnd  = np.random.RandomState(seed)</span>
<span class="c1"># sometimes jupyter doesnt initialize MathJax automatically for latex, so do this:</span>
<span class="c1">#######</span>


<span class="n">IQN_BASE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;IQN_BASE&quot;</span><span class="p">]</span><span class="c1"># BASE GIT REPO PATH</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BASE directoy properly set = &quot;</span><span class="p">,</span> <span class="n">IQN_BASE</span><span class="p">)</span>
<span class="n">utils_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">IQN_BASE</span><span class="p">,</span> <span class="s2">&quot;utils/&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">utils_dir</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">utils</span>

<span class="c1"># usually its not recommended to import everything from a module, but we know</span>
<span class="c1"># whats in it so its fine</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>


<span class="n">DATA_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;DATA_DIR&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;using DATA_DIR=</span><span class="si">{</span><span class="n">DATA_DIR</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">memory</span> <span class="o">=</span> <span class="n">Memory</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">)</span>
<span class="c1">################################### SET DATA CONFIGURATIONS ###################################</span>

<span class="n">y_label_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;RecoDatapT&quot;</span><span class="p">:</span> <span class="s2">&quot;$p(p_T)$&quot;</span> <span class="o">+</span> <span class="s2">&quot; [ GeV&quot;</span> <span class="o">+</span> <span class="s2">&quot;$^{-1} $&quot;</span> <span class="o">+</span> <span class="s2">&quot;]&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDataeta&quot;</span><span class="p">:</span> <span class="s2">&quot;$p(\eta)$&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDataphi&quot;</span><span class="p">:</span> <span class="s2">&quot;$p(\phi)$&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDatam&quot;</span><span class="p">:</span> <span class="s2">&quot;$p(m)$&quot;</span> <span class="o">+</span> <span class="s2">&quot; [ GeV&quot;</span> <span class="o">+</span> <span class="s2">&quot;$^{-1} $&quot;</span> <span class="o">+</span> <span class="s2">&quot;]&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">loss_y_label_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;RecoDatapT&quot;</span><span class="p">:</span> <span class="s2">&quot;$p_T^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDataeta&quot;</span><span class="p">:</span> <span class="s2">&quot;$\eta^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDataphi&quot;</span><span class="p">:</span> <span class="s2">&quot;$\phi^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDatam&quot;</span><span class="p">:</span> <span class="s2">&quot;$m^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;genDatapT&quot;</span><span class="p">,</span> <span class="s2">&quot;genDataeta&quot;</span><span class="p">,</span> <span class="s2">&quot;genDataphi&quot;</span><span class="p">,</span> <span class="s2">&quot;genDatam&quot;</span><span class="p">,</span> <span class="s2">&quot;tau&quot;</span><span class="p">]</span>

<span class="c1"># set order of training:</span>
<span class="c1"># pT_first: pT-&gt;&gt;m-&gt;eta-&gt;phi</span>
<span class="c1"># m_first: m-&gt;pT-&gt;eta-&gt;phi</span>


<span class="n">ORDER</span> <span class="o">=</span> <span class="s2">&quot;m_First&quot;</span>

<span class="k">if</span> <span class="n">ORDER</span> <span class="o">==</span> <span class="s2">&quot;m_First&quot;</span><span class="p">:</span>
    <span class="n">FIELDS</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;RecoDatam&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
            <span class="s2">&quot;xlabel&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;$m$ (GeV)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ylabel&quot;</span><span class="p">:</span> <span class="s2">&quot;$m^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
            <span class="s2">&quot;xmin&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;xmax&quot;</span><span class="p">:</span> <span class="mi">25</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;RecoDatapT&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;RecoDatam&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">,</span>
            <span class="s2">&quot;xlabel&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;$p_T$ (GeV)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ylabel&quot;</span><span class="p">:</span> <span class="s2">&quot;$p_T^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
            <span class="s2">&quot;xmin&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
            <span class="s2">&quot;xmax&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;RecoDataeta&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;RecoDatam&quot;</span><span class="p">,</span> <span class="s2">&quot;RecoDatapT&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">,</span>
            <span class="s2">&quot;xlabel&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;$\eta$&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ylabel&quot;</span><span class="p">:</span> <span class="s2">&quot;$\eta^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
            <span class="s2">&quot;xmin&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span>
            <span class="s2">&quot;xmax&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;RecoDataphi&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;RecoDatam&quot;</span><span class="p">,</span> <span class="s2">&quot;RecoDatapT&quot;</span><span class="p">,</span> <span class="s2">&quot;RecoDataeta&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">,</span>
            <span class="s2">&quot;xlabel&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;$\phi$&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ylabel&quot;</span><span class="p">:</span> <span class="s2">&quot;$\phi^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
            <span class="s2">&quot;xmin&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">,</span>
            <span class="s2">&quot;xmax&quot;</span><span class="p">:</span> <span class="mf">3.2</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">}</span>
<span class="k">elif</span> <span class="n">ORDER</span><span class="o">==</span> <span class="s2">&quot;phi_first&quot;</span><span class="p">:</span>
    <span class="n">FIELDS</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;RecoDataphi&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
        <span class="s2">&quot;xlabel&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;$\phi$&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ylabel&quot;</span><span class="p">:</span> <span class="s2">&quot;$\phi^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="s2">&quot;xmin&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">,</span>
        <span class="s2">&quot;xmax&quot;</span><span class="p">:</span> <span class="mf">3.2</span><span class="p">,</span>
    <span class="p">},</span>

    <span class="s2">&quot;RecoDatam&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;RecoDataphi&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">,</span>
        <span class="s2">&quot;xlabel&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;$m$ (GeV)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ylabel&quot;</span><span class="p">:</span> <span class="s2">&quot;$m^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="s2">&quot;xmin&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;xmax&quot;</span><span class="p">:</span> <span class="mi">25</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;RecoDatapT&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;RecoDataphi&quot;</span><span class="p">,</span> <span class="s2">&quot;RecoDatam&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">,</span>
        <span class="s2">&quot;xlabel&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;$p_T$ (GeV)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ylabel&quot;</span><span class="p">:</span> <span class="s2">&quot;$p_T^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="s2">&quot;xmin&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="s2">&quot;xmax&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;RecoDataeta&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;RecoDataphi&quot;</span><span class="p">,</span> <span class="s2">&quot;RecoDatam&quot;</span><span class="p">,</span> <span class="s2">&quot;RecoDatapT&quot;</span><span class="p">]</span>  <span class="o">+</span> <span class="n">X</span><span class="p">,</span>
        <span class="s2">&quot;xlabel&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;$\eta$&quot;</span><span class="p">,</span>
        <span class="s2">&quot;ylabel&quot;</span><span class="p">:</span> <span class="s2">&quot;$\eta^</span><span class="si">{reco}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="s2">&quot;xmin&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span>
        <span class="s2">&quot;xmax&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="p">},</span>

    <span class="p">}</span>


<span class="c1"># Load and explore raw (unscaled) dataframes</span>


<span class="n">all_variable_cols</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;genDatapT&quot;</span><span class="p">,</span>
    <span class="s2">&quot;genDataeta&quot;</span><span class="p">,</span>
    <span class="s2">&quot;genDataphi&quot;</span><span class="p">,</span>
    <span class="s2">&quot;genDatam&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDatapT&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDataeta&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDataphi&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDatam&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">all_cols</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;genDatapT&quot;</span><span class="p">,</span>
    <span class="s2">&quot;genDataeta&quot;</span><span class="p">,</span>
    <span class="s2">&quot;genDataphi&quot;</span><span class="p">,</span>
    <span class="s2">&quot;genDatam&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDatapT&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDataeta&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDataphi&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecoDatam&quot;</span><span class="p">,</span>
    <span class="s2">&quot;tau&quot;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>using torch version 1.13.1+cu117
matplotlib version=  3.5.1
using (optional) optuna version 2.8.0
BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN
using torch version 1.13.1+cu117
matplotlib version=  3.5.1
using (optional) optuna version 2.8.0
BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN
DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data
using DATA_DIR=/home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Cockpit</span>
<span class="kn">from</span> <span class="nn">backpack</span> <span class="kn">import</span> <span class="n">extend</span>

<span class="kn">from</span> <span class="nn">cockpit</span> <span class="kn">import</span> <span class="n">Cockpit</span><span class="p">,</span> <span class="n">CockpitPlotter</span>
<span class="kn">from</span> <span class="nn">cockpit.utils.configuration</span> <span class="kn">import</span> <span class="n">configuration</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="load-required-functions">
<h1>2.2: Load Required Functions<a class="headerlink" href="#load-required-functions" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LR_Cooler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">starting_lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">total_iterations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">iter_</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">starting_lr</span><span class="o">=</span><span class="n">starting_lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter_</span><span class="o">=</span><span class="n">iter_</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_iterations</span><span class="o">=</span> <span class="n">total_iterations</span>
    <span class="k">def</span> <span class="nf">exponential_decay</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">starting_lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span>  <span class="bp">self</span><span class="o">.</span><span class="n">iter_</span><span class="o">/</span><span class="mf">1e5</span> <span class="p">))</span>
    <span class="k">def</span> <span class="nf">exponential_decay_2</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">decay_rate</span><span class="o">=</span><span class="mf">1e-3</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">starting_lr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">decay_rate</span><span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">fractional_decay</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">final_time</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">starting_lr</span><span class="o">/</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iter</span> <span class="o">+</span> <span class="n">final_time</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LR_sched</span> <span class="o">=</span> <span class="n">LR_Cooler</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="n">LR</span> <span class="o">=</span> <span class="n">LR_sched</span><span class="o">.</span><span class="n">exponential_decay</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span> <span class="n">LR</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Cockpit_2_Train_all_IQNs_6_0.png" src="_images/Cockpit_2_Train_all_IQNs_6_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">################################### Load unscaled dataframes ###################################</span>
<span class="nd">@memory</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">load_raw_data</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Load raw train, test, and validation raw (unscaled) dataframes, in that order.</span>

<span class="sd">    Returns:</span>
<span class="sd">        list(pandas.DataFrame): train, test, valid raw datafranes</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;SUBSAMPLE = </span><span class="si">{</span><span class="n">SUBSAMPLE</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">raw_train_data</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span><span class="s1">&#39;train_data_10M_2.csv&#39;</span><span class="p">),</span>
                        <span class="n">usecols</span><span class="o">=</span><span class="n">all_cols</span><span class="p">,</span>
                        <span class="n">nrows</span><span class="o">=</span><span class="n">SUBSAMPLE</span>
                        <span class="p">)</span>

    <span class="n">raw_test_data</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span><span class="s1">&#39;test_data_10M_2.csv&#39;</span><span class="p">),</span>
                        <span class="n">usecols</span><span class="o">=</span><span class="n">all_cols</span><span class="p">,</span>
                        <span class="n">nrows</span><span class="o">=</span><span class="n">SUBSAMPLE</span>
                        <span class="p">)</span>

    <span class="n">raw_valid_data</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span><span class="s1">&#39;validation_data_10M_2.csv&#39;</span><span class="p">),</span>
                        <span class="n">usecols</span><span class="o">=</span><span class="n">all_cols</span><span class="p">,</span>
                        <span class="n">nrows</span><span class="o">=</span><span class="n">SUBSAMPLE</span>
                        <span class="p">)</span>


    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> RAW TRAIN DATA SHAPE</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">raw_train_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> RAW TRAIN DATA</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">raw_train_data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="c1">#unscaled</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> RAW TEST DATA\ SHAPEn&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">raw_test_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> RAW TEST DATA</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">raw_test_data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="c1">#unscaled</span>

    <span class="k">return</span> <span class="n">raw_train_data</span><span class="p">,</span> <span class="n">raw_test_data</span><span class="p">,</span> <span class="n">raw_valid_data</span>


<span class="c1">########## Generate scaled data###############</span>
<span class="c1"># scaled_train_data = L_scale_df(raw_train_data, title=&#39;scaled_train_data_10M_2.csv&#39;,</span>
<span class="c1">#                              save=True)</span>
<span class="c1"># print(&#39;\n\n&#39;)</span>
<span class="c1"># scaled_test_data = L_scale_df(raw_test_data,  title=&#39;scaled_test_data_10M_2.csv&#39;,</span>
<span class="c1">#                             save=True)</span>
<span class="c1"># print(&#39;\n\n&#39;)</span>

<span class="c1"># scaled_valid_data = L_scale_df(raw_valid_data,  title=&#39;scaled_valid_data_10M_2.csv&#39;,</span>
<span class="c1">#                             save=True)</span>

<span class="c1"># explore_data(df=scaled_train_data, title=&#39;Braden Kronheim-L-scaled Dataframe&#39;, scaled=True)</span>

<span class="c1">################ Load scaled data##############</span>
<span class="nd">@utils</span><span class="o">.</span><span class="n">time_type_of_func</span><span class="p">(</span><span class="n">tuning_or_training</span><span class="o">=</span><span class="s1">&#39;loading&#39;</span><span class="p">)</span>
<span class="nd">@memory</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">load_scaled_dataframes</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Load L-scaled train, test and validation according to Braden scaling, in that order.</span>

<span class="sd">    Returns:</span>
<span class="sd">        list(pandas.DataFarme): L-scaled train, test, validation dataframes, in that order.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># print(&quot;SCALED TRAIN DATA&quot;)</span>
    <span class="n">scaled_train_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s2">&quot;scaled_train_data_10M_2.csv&quot;</span><span class="p">),</span>
        <span class="n">usecols</span><span class="o">=</span><span class="n">all_cols</span><span class="p">,</span>
        <span class="n">nrows</span><span class="o">=</span><span class="n">SUBSAMPLE</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># print(&quot;TRAINING FEATURES\n&quot;, scaled_train_data.head())</span>

    <span class="n">scaled_test_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s2">&quot;scaled_test_data_10M_2.csv&quot;</span><span class="p">),</span>
        <span class="n">usecols</span><span class="o">=</span><span class="n">all_cols</span><span class="p">,</span>
        <span class="n">nrows</span><span class="o">=</span><span class="n">SUBSAMPLE</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">scaled_valid_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s2">&quot;scaled_valid_data_10M_2.csv&quot;</span><span class="p">),</span>
        <span class="n">usecols</span><span class="o">=</span><span class="n">all_cols</span><span class="p">,</span>
        <span class="n">nrows</span><span class="o">=</span><span class="n">SUBSAMPLE</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">scaled_train_data</span><span class="p">,</span> <span class="n">scaled_test_data</span><span class="p">,</span> <span class="n">scaled_valid_data</span>

<span class="c1"># print(&#39;\nTESTING FEATURES\n&#39;, test_data_m.head())</span>

<span class="c1"># print(&#39;\ntrain set shape:&#39;,  train_data_m.shape)</span>
<span class="c1"># print(&#39;\ntest set shape:  &#39;, test_data_m.shape)</span>
<span class="c1"># # print(&#39;validation set shape:&#39;, valid_data.shape)</span>



<span class="k">def</span> <span class="nf">get_train_scale_dict</span><span class="p">(</span><span class="n">USE_BRADEN_SCALING</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a dictionary containing mean and standard deviation of each gen and reco feature. </span>

<span class="sd">    Args:</span>
<span class="sd">        USE_BRADEN_SCALING (bool): Whether you wish to use the Braden scaling. If True, it uses the L-scaled train dataframe. If False, it uses the unscaled dataframe.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: dictionary of floats containing mean and standard deviation of each gen and reco feature. </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">USE_BRADEN_SCALING</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>
        <span class="n">TRAIN_SCALE_DICT</span> <span class="o">=</span> <span class="n">get_scaling_info</span><span class="p">(</span><span class="n">scaled_train_data</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;BRADEN SCALING DICTIONARY&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">TRAIN_SCALE_DICT</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># TEST_SCALE_DICT = get_scaling_info(scaled_test_data)</span>
        <span class="c1"># print(TEST_SCALE_DICT)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NORMAL UNSCALED DICTIONARY&#39;</span><span class="p">)</span>
        <span class="n">TRAIN_SCALE_DICT</span> <span class="o">=</span> <span class="n">get_scaling_info</span><span class="p">(</span><span class="n">raw_train_data</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">TRAIN_SCALE_DICT</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># TEST_SCALE_DICT = get_scaling_info(scaled_test_data)</span>
        <span class="c1"># print(TEST_SCALE_DICT)</span>
    <span class="k">return</span> <span class="n">TRAIN_SCALE_DICT</span>



<span class="c1">################################ SPLIT###########</span>
<span class="c1"># Currently need the split function again here</span>
<span class="c1"># @memory.cache</span>
<span class="k">def</span> <span class="nf">split_t_x</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">input_features</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get the target as the ratio, according to the T equation.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    list(numpy.array): list of numpy array of target and training features&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;RecoDatam&quot;</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">T</span><span class="p">(</span><span class="s2">&quot;m&quot;</span><span class="p">,</span> <span class="n">scaled_df</span><span class="o">=</span><span class="n">scaled_train_data</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;RecoDatapT&quot;</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">T</span><span class="p">(</span><span class="s2">&quot;pT&quot;</span><span class="p">,</span> <span class="n">scaled_df</span><span class="o">=</span><span class="n">scaled_train_data</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;RecoDataeta&quot;</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">T</span><span class="p">(</span><span class="s2">&quot;eta&quot;</span><span class="p">,</span> <span class="n">scaled_df</span><span class="o">=</span><span class="n">scaled_train_data</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;RecoDataphi&quot;</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">T</span><span class="p">(</span><span class="s2">&quot;phi&quot;</span><span class="p">,</span> <span class="n">scaled_df</span><span class="o">=</span><span class="n">scaled_train_data</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">input_features</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">x</span>

<span class="c1"># @memory.cache</span>
<span class="k">def</span> <span class="nf">normal_split_t_x</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">input_features</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;splot dataframe into targets and feature arrays.</span>

<span class="sd">    Args:</span>
<span class="sd">        df (pandas.DataFrame): Dataframe of train, test or validation data.</span>
<span class="sd">        target (str): Choice of &quot;RecoDatapT&quot;, &quot;RecoDataeta&quot;, &quot;RecoDataphi&quot;,&quot;RecoDatam&quot; as target.</span>
<span class="sd">        input_features (list(str)): list of training features labels</span>

<span class="sd">    Returns:</span>
<span class="sd">    list(numpy.array): list of numpy array of target and training features</span>
<span class="sd"> &quot;&quot;&quot;</span>
    <span class="c1"># change from pandas dataframe format to a numpy </span>
    <span class="c1"># array of the specified types</span>
    <span class="c1"># t = np.array(df[target])</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">target</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">input_features</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span>




<span class="c1">################ Apply Z scaling############</span>
<span class="k">def</span> <span class="nf">z</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simple z-score standardization. Used for targets&quot;&quot;&quot;</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-20</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">z_inverse</span><span class="p">(</span><span class="n">xprime</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">xprime</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># @memory.cache</span>
<span class="k">def</span> <span class="nf">z2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The main z score function. Args:</span>
<span class="sd">        x (numpy.array): feature 1-D array</span>
<span class="sd">        mean (float): mean of the feature (in the training set)</span>
<span class="sd">        std (float): standard deviation of the feature (in the training set)</span>

<span class="sd">    Returns:</span>
<span class="sd">        numpy.array: z-score-scaled 1-D feature</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-20</span>
    <span class="n">scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scaled</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>




<span class="c1"># @memory.cache</span>
<span class="k">def</span> <span class="nf">z_inverse2</span><span class="p">(</span><span class="n">xprime</span><span class="p">,</span> <span class="n">train_mean</span><span class="p">,</span> <span class="n">train_std</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The main z score de-scaling function. </span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">        xprime (numpy.array): z-score-scaled feature 1-D array</span>
<span class="sd">        train_mean (float): mean of the feature (in the training set)</span>
<span class="sd">        train_std (float): standard deviation of the feature (in the training set)</span>
<span class="sd">        &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">xprime</span> <span class="o">*</span> <span class="n">train_std</span> <span class="o">+</span> <span class="n">train_mean</span>

<span class="c1"># @memory.cache</span>
<span class="k">def</span> <span class="nf">apply_z_to_features</span><span class="p">(</span><span class="n">TRAIN_SCALE_DICT</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">test_x</span><span class="p">,</span> <span class="n">valid_x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;TO ensure this z scaling is only applied once to the training features, we use a generator.</span>
<span class="sd">    This doesn&#39;t change the shapes of anything, just applies z to all the feature columns other than tau.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">    TRAIN_SCALE_DICT (dict(float)): dictionary of train set mean and standard deviation values</span>
<span class="sd">    train_x (numpy.array): 2-D numpy array of training features</span>
<span class="sd">    test_x (numpy.array):  2-D numpy array of test features</span>
<span class="sd">    valid_x (numpy.array):  2-D numpy array of validation features</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># NFEATURES - 1 if not using tau, NFEATURES if using tau</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NFEATURES</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">variable</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">TRAIN_SCALE_DICT</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">train_mean</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">TRAIN_SCALE_DICT</span><span class="p">[</span><span class="n">variable</span><span class="p">][</span><span class="s2">&quot;mean&quot;</span><span class="p">])</span>
        <span class="n">train_std</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">TRAIN_SCALE_DICT</span><span class="p">[</span><span class="n">variable</span><span class="p">][</span><span class="s2">&quot;std&quot;</span><span class="p">])</span>
        <span class="n">train_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">z2</span><span class="p">(</span><span class="n">train_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">mean</span><span class="o">=</span><span class="n">train_mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">train_std</span><span class="p">)</span>
        <span class="n">test_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">z2</span><span class="p">(</span><span class="n">test_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">mean</span><span class="o">=</span><span class="n">train_mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">train_std</span><span class="p">)</span>
        <span class="n">valid_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">z2</span><span class="p">(</span><span class="n">valid_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">mean</span><span class="o">=</span><span class="n">train_mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">train_std</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">train_x</span>
    <span class="k">yield</span> <span class="n">test_x</span>
    <span class="k">yield</span> <span class="n">valid_x</span>


<span class="c1"># @memory.cache</span>
<span class="k">def</span> <span class="nf">apply_z_to_targets</span><span class="p">(</span><span class="n">train_t</span><span class="p">,</span> <span class="n">test_t</span><span class="p">,</span> <span class="n">valid_t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;apply z-score scaling to target columns</span>

<span class="sd">    Args:</span>
<span class="sd">        train_t (numpy.array): target column in the training set</span>
<span class="sd">        test_t (numpy.array): target column in the test set</span>
<span class="sd">        valid_t (numpy.array): target column in the validation set</span>

<span class="sd">    Yields:</span>
<span class="sd">        [type]: [description]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_t</span><span class="p">)</span>
    <span class="n">train_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_t</span><span class="p">)</span>
    <span class="n">train_t_</span> <span class="o">=</span> <span class="n">z2</span><span class="p">(</span><span class="n">train_t</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">train_mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">train_std</span><span class="p">)</span>
    <span class="n">test_t_</span> <span class="o">=</span> <span class="n">z2</span><span class="p">(</span><span class="n">test_t</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">train_mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">train_std</span><span class="p">)</span>
    <span class="n">valid_t_</span> <span class="o">=</span> <span class="n">z2</span><span class="p">(</span><span class="n">valid_t</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">train_mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">train_std</span><span class="p">)</span>

    <span class="k">yield</span> <span class="n">train_t_</span>
    <span class="k">yield</span> <span class="n">test_t_</span>
    <span class="k">yield</span> <span class="n">valid_t_</span>



<span class="c1"># check that it looks correct</span>
<span class="c1"># fig = plt.figure(figsize=(10, 4))</span>
<span class="c1"># ax = fig.add_subplot(autoscale_on=True)</span>
<span class="c1"># ax.grid()</span>
<span class="c1"># for i in range(NFEATURES):</span>
<span class="c1">#     ax.hist(train_x[:,i], alpha=0.35, label=f&#39;feature {i}&#39; )</span>
<span class="c1">#     # set_axes(ax=ax, xlabel=&quot;Transformed features X&#39; &quot;,title=&quot;training features post-z score: X&#39;=z(L(X))&quot;)</span>
<span class="c1"># ax.legend()</span>
<span class="c1"># plt.show()</span>


<span class="c1">######### Get beset hyperparameters</span>
<span class="c1"># tuned_dir = os.path.join(IQN_BASE,&#39;best_params&#39;)</span>
<span class="c1"># tuned_filename=os.path.join(tuned_dir,&#39;best_params_mass_%s_trials.csv&#39; % str(int(n_trials)))</span>
<span class="c1"># BEST_PARAMS = pd.read_csv(os.path.join(IQN_BASE, &#39;best_params&#39;,&#39;best_params_Test_Trials.csv&#39;))</span>
<span class="c1"># BEST_PARAMS=pd.read_csv(tuned_filename)</span>
<span class="c1"># print(BEST_PARAMS)</span>



<span class="k">def</span> <span class="nf">load_untrained_model</span><span class="p">(</span><span class="n">PARAMS</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Load an untrained model (with weights initiatted) according to model paramateters in the </span>
<span class="sd">    PARAMS dictionary</span>

<span class="sd">    Args:</span>
<span class="sd">        PARAMS (dict): dictionary of model/training parameters: i.e. hyperparameters and training parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        utils.RegularizedRegressionModel object</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">RegularizedRegressionModel</span><span class="p">(</span>
        <span class="n">nfeatures</span><span class="o">=</span><span class="n">NFEATURES</span><span class="p">,</span>
        <span class="n">ntargets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">nlayers</span><span class="o">=</span><span class="n">PARAMS</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">],</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="n">PARAMS</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">],</span>
        <span class="n">dropout_1</span><span class="o">=</span><span class="n">PARAMS</span><span class="p">[</span><span class="s2">&quot;dropout_1&quot;</span><span class="p">],</span>
        <span class="n">dropout_2</span><span class="o">=</span><span class="n">PARAMS</span><span class="p">[</span><span class="s2">&quot;dropout_2&quot;</span><span class="p">],</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">PARAMS</span><span class="p">[</span><span class="s2">&quot;activation&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="c1"># model.apply(initialize_weights)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>




<span class="k">class</span> <span class="nc">SaveModelCheckpoint</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Continuous model-checkpointing class. Updates the latest checkpoint of an object based o validation loss each time its called. </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">best_valid_loss</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initiate an instance of the class based on filename and best_valid_loss/</span>

<span class="sd">        Args:</span>
<span class="sd">            best_valid_loss (float, optional): Best possible validation loss of a checkpoint object. Defaults to np.inf.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_valid_loss</span> <span class="o">=</span> <span class="n">best_valid_loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filename_model</span><span class="o">=</span><span class="n">filename_model</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">current_valid_loss</span><span class="p">,</span> <span class="n">filename_model</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;When an object of the calss is called, its validation loss gets updated and the model based </span>
<span class="sd">        on the latest validation loss is saved.</span>

<span class="sd">        Args:</span>
<span class="sd">            model: utils.RegularizedRegressionModel object.</span>
<span class="sd">            current_valid_loss (float): current (latest) validation loss of this model during the training process.</span>
<span class="sd">            filename_model (str): filename in which the latest model will be saved. Can be a relative or local path. </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">current_valid_loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_valid_loss</span><span class="p">:</span>
            <span class="c1"># update the best loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_valid_loss</span> <span class="o">=</span> <span class="n">current_valid_loss</span>
            <span class="c1"># filename_model=&#39;Trained_IQNx4_%s_%sK_iter.dict&#39; % (target, str(int(n_iterations/1000)) )</span>
            <span class="c1"># filename_model = &quot;Trained_IQNx4_%s_TUNED_2lin_with_noise.dict&quot; % target</span>

            <span class="c1"># note that n_iterations is the total n_iterations, we dont want to save a million files for each iteration</span>
            <span class="n">trained_models_dir</span> <span class="o">=</span> <span class="s2">&quot;trained_models&quot;</span>
            <span class="n">mkdir</span><span class="p">(</span><span class="n">trained_models_dir</span><span class="p">)</span>
            <span class="c1"># on cluster, Im using another TRAIN directory</span>
            <span class="n">PATH_model</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">IQN_BASE</span><span class="p">,</span>
                <span class="s2">&quot;JupyterBook&quot;</span><span class="p">,</span>
                <span class="s2">&quot;Cluster&quot;</span><span class="p">,</span>
                <span class="s2">&quot;TRAIN&quot;</span><span class="p">,</span>
                <span class="n">trained_models_dir</span><span class="p">,</span>
                <span class="n">filename_model</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH_model</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Current valid loss: </span><span class="si">{</span><span class="n">current_valid_loss</span><span class="si">}</span><span class="s2">;  saved better model at </span><span class="si">{</span><span class="n">PATH_model</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="c1"># save using .pth object which if a dictionary of dicionaries, so that I can have PARAMS saved in the same file</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="n">target</span><span class="p">,</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">avloss</span><span class="p">,</span>
    <span class="n">getbatch</span><span class="p">,</span>
    <span class="n">train_x</span><span class="p">,</span>
    <span class="n">train_t</span><span class="p">,</span>
    <span class="n">valid_x</span><span class="p">,</span>
    <span class="n">valid_t</span><span class="p">,</span>
    <span class="n">PARAMS</span><span class="p">,</span>
    <span class="n">traces</span><span class="p">,</span>
    <span class="n">step</span><span class="p">,</span>
    <span class="n">window</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Training Function. </span>

<span class="sd">    Args:</span>
<span class="sd">        target (str): hoice of &quot;RecoDatapT&quot;, &quot;RecoDataeta&quot;, &quot;RecoDataphi&quot;,&quot;RecoDatam&quot; as target.</span>
<span class="sd">        model a torch NN model, e.g utils.RegularizedRegressionModel.</span>
<span class="sd">        avloss (float): average training losss</span>
<span class="sd">        getbatch (function): a get_batch function</span>
<span class="sd">        train_x (numpy.DataFrame): 2-D numpy array of training features</span>
<span class="sd">        train_t (numpy.DataFrame:  1-D numpy array of training targets</span>
<span class="sd">        valid_x (numpy.DataFrame): 2-D numpy array of validation features</span>
<span class="sd">        valid_t (numpy.DataFrame: 1-D numpy array of validation targets</span>
<span class="sd">        PARAMS (dict): dictionary of model/training parameters </span>
<span class="sd">        traces (tuple): tuple of  </span>
<span class="sd">        (iteration, training accuracy, validation accuracy, running average of validation accuracy) </span>
<span class="sd">        = (xx, yy_t, yy_v, yy_v_avg) </span>
<span class="sd">        step (int): number of iterations to take a printout step of the traces</span>
<span class="sd">        window (int): window of running average of validation loss  </span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple: traces</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># TODO: obviously, for reference, the &quot;traces&quot; should be saved as a 2D numpy array</span>
    <span class="c1"># with the same naming format as the &quot;model_filename&quot;, so that it can be opened later and </span>
    <span class="c1"># plot loss curves for different models.</span>
    
    <span class="c1"># TODO: decay the stepsize, such that steps (and hence checkpointing) are large in the beginnig to the learning</span>
    <span class="c1"># process (which corresponds to high learning rates), and decrease as time steps increase.</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">PARAMS</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
    <span class="n">n_iterations</span> <span class="o">=</span> <span class="n">PARAMS</span><span class="p">[</span><span class="s1">&#39;n_iterations&#39;</span><span class="p">]</span>
    <span class="c1"># to keep track of average losses</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy_t</span><span class="p">,</span> <span class="n">yy_v</span><span class="p">,</span> <span class="n">yy_v_avg</span> <span class="o">=</span> <span class="n">traces</span>
    <span class="n">model_checkpoint</span> <span class="o">=</span> <span class="n">SaveModelCheckpoint</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_x</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iteration vs average loss&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%10s</span><span class="se">\t</span><span class="si">%10s</span><span class="se">\t</span><span class="si">%10s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="s2">&quot;train-set&quot;</span><span class="p">,</span> <span class="s2">&quot;test-set&quot;</span><span class="p">))</span>
    
    <span class="n">fifth_n_iterations</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">n_iterations</span><span class="o">//</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">starting_learning_rate</span> <span class="o">=</span> <span class="n">PARAMS</span><span class="p">[</span><span class="s1">&#39;starting_learning_rate&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
        <span class="c1">#experiment with annealing LR from beginning</span>

<span class="c1">#         # starting learning rate (first fifth)</span>
        <span class="n">Cutoff_from_initial_LR</span> <span class="o">=</span> <span class="mi">15000</span>
        <span class="k">if</span> <span class="n">ii</span><span class="o">&lt;</span> <span class="n">Cutoff_from_initial_LR</span><span class="p">:</span>
            <span class="n">learning_rate</span><span class="o">=</span> <span class="n">starting_learning_rate</span>
        
<span class="c1">#         #second fifth</span>

<span class="c1">#         if 2* fifth_n_iterations &lt; ii &lt; 3*fifth_n_iterations:</span>
<span class="c1">#             learning_rate=starting_learning_rate/10 #1e-2</span>
        <span class="k">if</span> <span class="n">ii</span> <span class="o">&gt;</span> <span class="n">Cutoff_from_initial_LR</span><span class="p">:</span>
            <span class="n">LR_sched</span><span class="o">=</span><span class="n">LR_Cooler</span><span class="p">(</span><span class="n">starting_lr</span><span class="o">=</span><span class="n">starting_learning_rate</span><span class="p">,</span> <span class="n">total_iterations</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">,</span> <span class="n">iter_</span><span class="o">=</span><span class="n">ii</span><span class="p">)</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR_sched</span><span class="o">.</span><span class="n">exponential_decay</span><span class="p">()</span>
<span class="c1">#         #third fifth</span>
<span class="c1">#         if 3*fifth_n_iterations &lt; ii &lt; 4*fifth_n_iterations:</span>
<span class="c1">#             learning_rate=starting_learning_rate/100 #1e-3</span>
<span class="c1">#         #frouth fifth: stary decay LR</span>
<span class="c1">#         if ii &gt; 4*fifth_n_iterations:</span>
<span class="c1">#             learning_rate = decay_LR(ii)</span>
            
        
        <span class="c1"># add weight decay (important regularization to reduce overfitting)</span>
        <span class="n">L2</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1"># optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2)</span>
        <span class="c1">#SGD allows for: momentum=0, dampening=0, weight_decay=0, nesterov=boolean, differentiable=boolean</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="n">optimizer_name</span><span class="p">)(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
         <span class="c1"># amsgrad=True, </span>

        <span class="c1">#  weight_decay=L2,#</span>
        <span class="c1"># differentiable=True,</span>
        <span class="c1">#For SGD nesterov, it requires momentum and zero dampening</span>
        <span class="c1"># dampening=0,</span>
        <span class="c1"># momentum=momentum,</span>
        <span class="c1"># nesterov=True</span>
        <span class="c1"># BUT no one should ever use SGD in 2022! Adam converges much better and faster.</span>
        <span class="p">)</span>
        
        <span class="c1">#if ii &gt; 1e4: learning_rate=1e-4</span>
        <span class="c1"># set mode to training so that training specific</span>
        <span class="c1"># operations such as dropout are enabled.</span>
        <span class="c1"># time_p_start = time.perf_counter()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="c1"># get a random sample (a batch) of data (as numpy arrays)</span>
        <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_t</span> <span class="o">=</span> <span class="n">getbatch</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="c1"># Take df/ dtau</span>
        <span class="c1"># x = torch.from_numpy(batch_x).float()</span>
        <span class="c1"># # print(&#39;x is leaf: &#39;, x.is_leaf)</span>
        <span class="c1"># x.requires_grad_(True)</span>
        <span class="c1"># # x.retain_grad()</span>
        <span class="c1"># # print(&#39;x is leaf after retain: &#39;, x.is_leaf)</span>
        <span class="c1"># # x.requires_grad_(True)</span>
        <span class="c1"># # x.retain_grad()</span>
        <span class="c1"># f = model(x)</span>
        <span class="c1"># f = f.view(-1)</span>
        <span class="c1"># #multiply the model by its ransverse, remember we can only take gradients of scalars</span>
        <span class="c1"># #and f will be a vector before this</span>
        <span class="c1"># f = f @ f.t()</span>
        <span class="c1"># f.retain_grad()</span>
        <span class="c1"># f.backward(gradient=torch.ones_like(f), retain_graph=True)</span>
        <span class="c1"># df_dx = x.grad</span>
        <span class="c1"># df_dtau = df_dx[:,-1]</span>
        <span class="c1"># x.grad.zero_()</span>
        
        <span class="c1">#add noise to training data</span>
        <span class="c1"># batch_x = add_noise(batch_x)</span>
        <span class="c1"># batch_t = add_noise(batch_t)</span>
        
        <span class="c1"># Try torch scheduler</span>
        <span class="n">scheduler</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># no need to compute gradients</span>
            <span class="c1"># wrt. x and t</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_t</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">empirical_risk</span> <span class="o">=</span> <span class="n">avloss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># clear previous gradients</span>
        <span class="n">empirical_risk</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># compute gradients</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># move one step towards the minimum of the loss function using an SGD-like algorithm.</span>
        
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


        <span class="k">if</span> <span class="n">ii</span> <span class="o">%</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">CURRENT LEARNING RATE: </span><span class="si">{</span><span class="n">learning_rate</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">acc_t</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">avloss</span><span class="p">,</span> <span class="n">train_x</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">train_t</span><span class="p">[:</span><span class="n">n</span><span class="p">])</span>
            <span class="c1">#acc_t: list of training losses</span>
            <span class="n">acc_v</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">avloss</span><span class="p">,</span> <span class="n">valid_x</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">valid_t</span><span class="p">[:</span><span class="n">n</span><span class="p">])</span>
            <span class="c1">#acc_v: list of validation losses</span>
            <span class="n">yy_t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_t</span><span class="p">)</span>
            <span class="n">yy_v</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_v</span><span class="p">)</span>
            <span class="n">previous_iter_valid_loss</span> <span class="o">=</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;previous_iter_valid_loss : </span><span class="si">{</span><span class="n">previous_iter_valid_loss</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="c1"># save better models based on valid loss</span>
            <span class="c1"># filename_model=&quot;Trained_IQNx4_%s_TUNED_0lin_with_high_noise3.dict&quot; % target</span>
            <span class="n">filename_model</span><span class="o">=</span><span class="n">get_model_filename</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">PARAMS</span><span class="p">)</span>
             
            <span class="n">model_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">filename_model</span> <span class="o">=</span><span class="n">filename_model</span> <span class="p">,</span><span class="n">current_valid_loss</span><span class="o">=</span><span class="n">acc_v</span><span class="p">)</span>
            <span class="c1"># compute running average for validation data</span>
            <span class="n">len_yy_v</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">yy_v</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">len_yy_v</span> <span class="o">&lt;</span> <span class="n">window</span><span class="p">:</span>
                <span class="n">yy_v_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">elif</span> <span class="n">len_yy_v</span> <span class="o">==</span> <span class="n">window</span><span class="p">:</span>
                <span class="n">yy_v_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">yy_v</span><span class="p">)</span> <span class="o">/</span> <span class="n">window</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">acc_v_avg</span> <span class="o">=</span> <span class="n">yy_v_avg</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">window</span>
                <span class="n">acc_v_avg</span> <span class="o">+=</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="n">window</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
                <span class="n">yy_v_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_v_avg</span> <span class="o">/</span> <span class="n">window</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">xx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%10d</span><span class="se">\t</span><span class="si">%10.6f</span><span class="se">\t</span><span class="si">%10.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">xx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">step</span><span class="p">)</span>

                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;</span><span class="se">\r</span><span class="si">%10d</span><span class="se">\t</span><span class="si">%10.6f</span><span class="se">\t</span><span class="si">%10.6f</span><span class="se">\t</span><span class="si">%10.6f</span><span class="s2">&quot;</span>
                    <span class="o">%</span> <span class="p">(</span><span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_v_avg</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
                    <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="c1"># time_p_end = time.perf_counter()</span>
        <span class="c1"># time_for_this_iter = time_p_end-time_p_start</span>
        <span class="c1"># time_per_example = time_for_this_iter/batch_size</span>
        <span class="c1"># print(f&#39;training time for one example: {time_per_example}&#39;)</span>

    <span class="nb">print</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_t</span><span class="p">,</span> <span class="n">yy_v</span><span class="p">,</span> <span class="n">yy_v_avg</span><span class="p">)</span>


<span class="nd">@utils</span><span class="o">.</span><span class="n">time_type_of_func</span><span class="p">(</span><span class="n">tuning_or_training</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">run</span><span class="p">(</span>
    <span class="n">target</span><span class="p">,</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">train_x</span><span class="p">,</span>
    <span class="n">train_t</span><span class="p">,</span>
    <span class="n">valid_x</span><span class="p">,</span>
    <span class="n">valid_t</span><span class="p">,</span>
    <span class="n">traces</span><span class="p">,</span>
    <span class="n">PARAMS</span><span class="p">,</span>
    <span class="n">traces_step</span><span class="p">,</span>
    <span class="n">traces_window</span><span class="p">,</span>
    <span class="n">save_model</span><span class="p">,</span>
<span class="p">):</span>


    <span class="n">traces</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span>
        <span class="n">target</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">average_quantile_loss</span><span class="p">,</span>
        <span class="n">get_batch</span><span class="p">,</span>
        <span class="n">train_x</span><span class="p">,</span>
        <span class="n">train_t</span><span class="p">,</span>
        <span class="n">valid_x</span><span class="p">,</span>
        <span class="n">valid_t</span><span class="p">,</span>
        <span class="n">PARAMS</span><span class="p">,</span>
        <span class="n">traces</span><span class="p">,</span>
        <span class="n">step</span><span class="o">=</span><span class="n">traces_step</span><span class="p">,</span>
        <span class="n">window</span><span class="o">=</span><span class="n">traces_window</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">save_model</span><span class="p">:</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;Trained_IQNx4_</span><span class="si">%s</span><span class="s2">_</span><span class="si">%s</span><span class="s2">K_iter.dict&quot;</span> <span class="o">%</span> <span class="p">(</span>
            <span class="n">target</span><span class="p">,</span>
            <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n_iterations</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">)),</span>
        <span class="p">)</span>
        <span class="n">PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">IQN_BASE</span><span class="p">,</span> <span class="s2">&quot;trained_models&quot;</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">trained model dictionary saved in </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">PATH</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">get_train_scale_dict</span><span class="p">(</span><span class="n">USE_BRADEN_SCALING</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">USE_BRADEN_SCALING</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>
        <span class="n">TRAIN_SCALE_DICT</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_scaling_info</span><span class="p">(</span><span class="n">scaled_train_data</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BRADEN SCALING DICTIONARY&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">TRAIN_SCALE_DICT</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># TEST_SCALE_DICT = get_scaling_info(scaled_test_data)</span>
        <span class="c1"># print(TEST_SCALE_DICT)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NORMAL UNSCALED DICTIONARY&quot;</span><span class="p">)</span>
        <span class="n">TRAIN_SCALE_DICT</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_scaling_info</span><span class="p">(</span><span class="n">raw_train_data</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">TRAIN_SCALE_DICT</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># TEST_SCALE_DICT = get_scaling_info(scaled_test_data)</span>
        <span class="c1"># print(TEST_SCALE_DICT)</span>
    <span class="k">return</span> <span class="n">TRAIN_SCALE_DICT</span>


<span class="nd">@utils</span><span class="o">.</span><span class="n">debug</span>
<span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">PATH</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">trained model dictionary saved in </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">PATH</span><span class="p">)</span>


<span class="nd">@utils</span><span class="o">.</span><span class="n">debug</span>
<span class="k">def</span> <span class="nf">save_model_params</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">PATH</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">trained model dictionary saved in </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">PATH</span><span class="p">)</span>


<span class="nd">@utils</span><span class="o">.</span><span class="n">debug</span>
<span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">PATH</span><span class="p">):</span>
    <span class="c1"># n_layers = int(BEST_PARAMS[&quot;n_layers&quot;])</span>
    <span class="c1"># hidden_size = int(BEST_PARAMS[&quot;hidden_size&quot;])</span>
    <span class="c1"># dropout = float(BEST_PARAMS[&quot;dropout&quot;])</span>
    <span class="c1"># optimizer_name = BEST_PARAMS[&quot;optimizer_name&quot;].to_string().split()[1]</span>
    <span class="c1"># learning_rate =  float(BEST_PARAMS[&quot;learning_rate&quot;])</span>
    <span class="c1"># batch_size = int(BEST_PARAMS[&quot;batch_size&quot;])</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">RegularizedRegressionModel</span><span class="p">(</span>
        <span class="n">nfeatures</span><span class="o">=</span><span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">ntargets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">nlayers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
    <span class="c1"># OR</span>
    <span class="c1"># model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED!</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">load_trained_model</span><span class="p">(</span><span class="n">PATH</span><span class="p">,</span> <span class="n">PARAMS</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">RegularizedRegressionModel</span><span class="p">(</span>
        <span class="n">nfeatures</span><span class="o">=</span><span class="n">NFEATURES</span><span class="p">,</span>
        <span class="n">ntargets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">nlayers</span><span class="o">=</span><span class="n">PARAMS</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">],</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="n">PARAMS</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">],</span>
        <span class="n">dropout_1</span><span class="o">=</span><span class="n">PARAMS</span><span class="p">[</span><span class="s2">&quot;dropout_1&quot;</span><span class="p">],</span>
        <span class="n">dropout_2</span><span class="o">=</span><span class="n">PARAMS</span><span class="p">[</span><span class="s2">&quot;dropout_2&quot;</span><span class="p">],</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">PARAMS</span><span class="p">[</span><span class="s2">&quot;activation&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="load-data-split-scale-and-train-mass">
<h1>2.3 Load Data, split, scale, and Train Mass<a class="headerlink" href="#load-data-split-scale-and-train-mass" title="Permalink to this headline">#</a></h1>
<section id="configurations">
<h2>Configurations<a class="headerlink" href="#configurations" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">######################################</span>
<span class="n">USE_BRADEN_SCALING</span><span class="o">=</span><span class="kc">False</span>
<span class="c1">#####################################</span>
<span class="c1">################################### CONFIGURATIONS ###################################</span>

<span class="n">JUPYTER</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">use_subsample</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># use_subsample = True</span>
<span class="k">if</span> <span class="n">use_subsample</span><span class="p">:</span>
    <span class="n">SUBSAMPLE</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">)</span>  <span class="c1"># subsample use for development - in production use whole dataset</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">SUBSAMPLE</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">########################################################################################</span>
<span class="c1"># Load data only once, and with caching!</span>
<span class="n">raw_train_data</span><span class="p">,</span> <span class="n">raw_test_data</span><span class="p">,</span> <span class="n">raw_valid_data</span> <span class="o">=</span><span class="n">load_raw_data</span><span class="p">()</span>
<span class="c1"># Load scaled data</span>
<span class="c1"># scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="oversample-regions-with-11-m-25-gev">
<h2>Oversample regions with <span class="math notranslate nohighlight">\(11 &lt; m &lt; 25\)</span> GeV.<a class="headerlink" href="#oversample-regions-with-11-m-25-gev" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">oversample_mass</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">mass_min</span><span class="p">,</span> <span class="n">mass_max</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;df could be raw_test_data &quot;&quot;&quot;</span>
  <span class="n">first_mask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">RecoDatam</span> <span class="o">&lt;</span> <span class="mi">25</span> <span class="c1">#I think this condition might not make sense (we want to define just a min)</span>
  <span class="n">second_mask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">RecoDatam</span> <span class="o">&gt;</span> <span class="mi">10</span>
  <span class="n">df_resampled</span> <span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">second_mask</span><span class="p">]</span><span class="c1">#[first_mask]</span>
  <span class="n">frames</span> <span class="o">=</span> <span class="p">[</span><span class="n">df</span><span class="p">,</span> <span class="n">df_resampled</span><span class="p">]</span>
  <span class="n">df_combined</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">frames</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">df_combined</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># raw_test_data_ =oversample_mass(df=raw_test_data, mass_min=10, mass_max=25)</span>
<span class="c1"># raw_valid_data_ =oversample_mass(df=raw_valid_data, mass_min=10, mass_max=25)</span>
<span class="c1"># raw_train_data_ =oversample_mass(df=raw_train_data, mass_min=10, mass_max=25)</span>

<span class="c1"># raw_test_data, raw_valid_data, raw_train_data=raw_test_data_, raw_valid_data_, raw_train_data_</span>
<span class="c1"># raw_test_data_.describe()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#######################################</span>
<span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;RecoDatam&quot;</span>
<span class="n">source</span> <span class="o">=</span> <span class="n">FIELDS</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">source</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Features:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Target = &quot;</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;USING NEW DATASET</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>





<span class="c1"># Get targets and features</span>
<span class="c1"># if USE_BRADEN_SCALING==True:</span>
<span class="c1">#     print(f&quot;spliting data for {target}&quot;)</span>
<span class="c1">#     train_t, train_x = split_t_x(</span>
<span class="c1">#         df=scaled_train_data, target=target, input_features=features</span>
<span class="c1">#     )</span>
<span class="c1">#     print(&quot;train_t shape = &quot;, train_t.shape, &quot;train_x shape = &quot;, train_x.shape)</span>
<span class="c1">#     print(&quot;\n Training features:\n&quot;)</span>
<span class="c1">#     print(train_x)</span>
<span class="c1">#     valid_t, valid_x = split_t_x(</span>
<span class="c1">#         df=scaled_valid_data, target=target, input_features=features</span>
<span class="c1">#     )</span>
<span class="c1">#     print(&quot;valid_t shape = &quot;, valid_t.shape, &quot;valid_x shape = &quot;, valid_x.shape)</span>
<span class="c1">#     test_t, test_x = split_t_x(df=scaled_test_data, target=target, input_features=features)</span>
<span class="c1">#     print(&quot;test_t shape = &quot;, test_t.shape, &quot;test_x shape = &quot;, test_x.shape)</span>

<span class="c1"># else:</span>
<span class="c1">#     print(f&quot;spliting data for {target}&quot;)</span>
<span class="c1">#     train_t, train_x = normal_split_t_x(</span>
<span class="c1">#     df=raw_train_data, target=target, input_features=features</span>
<span class="c1">#     )</span>
<span class="c1">#     print(&quot;train_t shape = &quot;, train_t.shape, &quot;train_x shape = &quot;, train_x.shape)</span>
<span class="c1">#     print(&quot;\n Training features:\n&quot;)</span>
<span class="c1">#     print(train_x)</span>
<span class="c1">#     valid_t, valid_x = normal_split_t_x(</span>
<span class="c1">#     df=raw_valid_data, target=target, input_features=features</span>
<span class="c1">#     )</span>
<span class="c1">#     print(&quot;valid_t shape = &quot;, valid_t.shape, &quot;valid_x shape = &quot;, valid_x.shape)</span>
<span class="c1">#     test_t, test_x = normal_split_t_x(df=raw_test_data, target=target, input_features=features)</span>
<span class="c1">#     print(&quot;test_t shape = &quot;, test_t.shape, &quot;test_x shape = &quot;, test_x.shape)</span>


<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;spliting data for </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">train_t</span><span class="p">,</span> <span class="n">train_x</span> <span class="o">=</span> <span class="n">normal_split_t_x</span><span class="p">(</span>
<span class="n">df</span><span class="o">=</span><span class="n">raw_train_data</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="n">features</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train_t shape = &quot;</span><span class="p">,</span> <span class="n">train_t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;train_x shape = &quot;</span><span class="p">,</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Training features:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
<span class="n">valid_t</span><span class="p">,</span> <span class="n">valid_x</span> <span class="o">=</span> <span class="n">normal_split_t_x</span><span class="p">(</span>
<span class="n">df</span><span class="o">=</span><span class="n">raw_valid_data</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="n">features</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;valid_t shape = &quot;</span><span class="p">,</span> <span class="n">valid_t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;valid_x shape = &quot;</span><span class="p">,</span> <span class="n">valid_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">test_t</span><span class="p">,</span> <span class="n">test_x</span> <span class="o">=</span> <span class="n">normal_split_t_x</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">raw_test_data</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test_t shape = &quot;</span><span class="p">,</span> <span class="n">test_t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;test_x shape = &quot;</span><span class="p">,</span> <span class="n">test_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;no need to train_test_split since we already have the split dataframes&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">valid_x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">valid_x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">train_x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">valid_t</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">valid_t</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_t</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">train_t</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="n">NFEATURES</span> <span class="o">=</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">######################################################</span>

<span class="c1"># Apply z scaling to features and targets</span>
<span class="c1"># to features</span>
<span class="n">TRAIN_SCALE_DICT</span><span class="o">=</span><span class="n">get_train_scale_dict</span><span class="p">(</span><span class="n">USE_BRADEN_SCALING</span><span class="p">)</span>
<span class="n">apply_z_generator</span> <span class="o">=</span> <span class="n">apply_z_to_features</span><span class="p">(</span><span class="n">TRAIN_SCALE_DICT</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">test_x</span><span class="p">,</span> <span class="n">valid_x</span><span class="p">)</span>
<span class="n">train_x_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_generator</span><span class="p">)</span>
<span class="n">test_x_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_generator</span><span class="p">)</span>
<span class="n">valid_x_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_generator</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">valid_x_z_scaled</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">valid_x_z_scaled</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_x_z_scaled</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">train_x_z_scaled</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># to targets</span>
<span class="n">apply_z_to_targets_generator</span> <span class="o">=</span> <span class="n">apply_z_to_targets</span><span class="p">(</span>
    <span class="n">train_t</span><span class="p">,</span> <span class="n">test_t</span><span class="p">,</span> <span class="n">valid_t</span>
<span class="p">)</span>
<span class="n">train_t_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_to_targets_generator</span><span class="p">)</span>
<span class="n">test_t_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_to_targets_generator</span><span class="p">)</span>
<span class="n">valid_t_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_to_targets_generator</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">valid_t_z_scaled</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">valid_t_z_scaled</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_t_z_scaled</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">train_t_z_scaled</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Features:
 [&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;, &#39;tau&#39;]

Target =  RecoDatam
USING NEW DATASET

spliting data for RecoDatam
train_t shape =  (100000,) train_x shape =  (100000, 5)

 Training features:

[[29.4452      0.828187    2.90213     2.85348     0.36130954]
 [24.3193     -1.16351     0.636469    5.83685     0.12689925]
 [24.3193     -1.16351     0.636469    5.83685     0.96230681]
 ...
 [26.863       4.46097     0.381944    6.05116     0.30971054]
 [21.3142      4.15091    -2.81233     5.26289     0.07136162]
 [34.5862      2.1022     -0.435373    4.55711     0.19148971]]
valid_t shape =  (100000,) valid_x shape =  (100000, 5)
test_t shape =  (100000,) test_x shape =  (100000, 5)
no need to train_test_split since we already have the split dataframes
[ 3.26982106e+01 -2.19061436e-03 -1.07439844e-02  6.97319550e+00
  5.00222989e-01] [14.55610053  2.20102624  1.81151811  2.73757608  0.28808471]
[3.28398167e+01 2.20884209e-03 7.40700042e-03 6.98953116e+00
 5.00433356e-01] [14.90390069  2.21693324  1.80890606  2.78903949  0.28827815]
5.561710906429678 2.6282735020527532
5.56540205907344 2.685472287815592
NORMAL UNSCALED DICTIONARY
{&#39;genDatapT&#39;: {&#39;mean&#39;: 32.83981673500001, &#39;std&#39;: 14.90390068970772}, &#39;genDataeta&#39;: {&#39;mean&#39;: 0.0022088420911595004, &#39;std&#39;: 2.2169332378187043}, &#39;genDataphi&#39;: {&#39;mean&#39;: 0.007407000418604001, &#39;std&#39;: 1.8089060619308075}, &#39;genDatam&#39;: {&#39;mean&#39;: 6.989531155823706, &#39;std&#39;: 2.7890394898296575}, &#39;RecoDatapT&#39;: {&#39;mean&#39;: 32.967168834000006, &#39;std&#39;: 15.77965621193832}, &#39;RecoDataeta&#39;: {&#39;mean&#39;: 0.0022161224001759983, &#39;std&#39;: 2.210429783644192}, &#39;RecoDataphi&#39;: {&#39;mean&#39;: 0.007273818861474919, &#39;std&#39;: 1.8095924300645279}, &#39;RecoDatam&#39;: {&#39;mean&#39;: 5.56540205907344, &#39;std&#39;: 2.685472287815592}}



[-0.00950128 -0.00198448 -0.01003423 -0.00585709  0.50022299] [0.97666382 0.99282478 1.00144399 0.98154798 0.28808471]
[-4.38262759e-16  3.26849658e-18  1.05870868e-17 -7.61701813e-17
  5.00433356e-01] [1.         1.         1.         1.         0.28827815]
-0.0013744891952560954 0.9787006605793853
-1.7763568394002506e-16 1.0
</pre></div>
</div>
</div>
</div>
<p>Decide Whether to use Braden Scaling</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="define-mass-model-parameters">
<h1>Define Mass Model Parameters<a class="headerlink" href="#define-mass-model-parameters" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">###########################################################</span>
<span class="c1"># Decide on parameters for this model and training</span>
<span class="n">PARAMS_m</span> <span class="o">=</span> <span class="p">{</span>
<span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
<span class="s2">&quot;dropout_1&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="mf">0.6</span><span class="p">),</span>
<span class="s2">&quot;dropout_2&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span>
<span class="s2">&quot;activation&quot;</span><span class="p">:</span> <span class="s2">&quot;LeakyReLU&quot;</span><span class="p">,</span>
    <span class="s1">&#39;optimizer_name&#39;</span><span class="p">:</span><span class="s1">&#39;NAdam&#39;</span><span class="p">,</span>
    <span class="s1">&#39;starting_learning_rate&#39;</span><span class="p">:</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">&#39;momentum&#39;</span><span class="p">:</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.6</span><span class="p">),</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span><span class="nb">int</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
    <span class="s1">&#39;n_iterations&#39;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="mf">2e6</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<section id="train-mass">
<h2>Train Mass<a class="headerlink" href="#train-mass" title="Permalink to this headline">#</a></h2>
<section id="the-model-that-needs-the-longest-time-in-training-is-mass-click-here-to-scroll-down-to-train-p-t">
<h3>The model that needs the longest time in training is mass. Click here to scroll down to train <span class="math notranslate nohighlight">\(p_T\)</span>.<a class="headerlink" href="#the-model-that-needs-the-longest-time-in-training-is-mass-click-here-to-scroll-down-to-train-p-t" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer_name</span><span class="o">=</span><span class="n">PARAMS_m</span><span class="p">[</span><span class="s1">&#39;optimizer_name&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">))</span>
<span class="c1"># optimizer_name = BEST_PARAMS[&quot;optimizer_name&quot;].to_string().split()[1]</span>
<span class="n">NITERATIONS</span><span class="o">=</span><span class="n">PARAMS_m</span><span class="p">[</span><span class="s1">&#39;n_iterations&#39;</span><span class="p">]</span>
<span class="n">BATCHSIZE</span><span class="o">=</span><span class="n">PARAMS_m</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
<span class="n">comment</span><span class="o">=</span><span class="s1">&#39;&#39;</span>



<span class="c1"># N_epochs X N_train_examples = N_iterations X batch_size</span>
<span class="n">N_epochs</span> <span class="o">=</span> <span class="p">(</span><span class="n">NITERATIONS</span> <span class="o">*</span> <span class="n">BATCHSIZE</span><span class="p">)</span> <span class="o">/</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;training for </span><span class="si">{</span><span class="n">NITERATIONS</span><span class="si">}</span><span class="s2"> iteration, which is  </span><span class="si">{</span><span class="n">N_epochs</span><span class="si">}</span><span class="s2"> epochs&quot;</span><span class="p">)</span>


<span class="n">filename_model</span> <span class="o">=</span> <span class="n">get_model_filename</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">PARAMS_m</span><span class="p">)</span>
<span class="n">trained_models_dir</span> <span class="o">=</span> <span class="s2">&quot;trained_models&quot;</span>
<span class="n">mkdir</span><span class="p">(</span><span class="n">trained_models_dir</span><span class="p">)</span>
<span class="c1"># on cluster, Im using another TRAIN directory</span>
<span class="n">PATH_model</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">IQN_BASE</span><span class="p">,</span> <span class="c1">#the loaction of the repo</span>
    <span class="s2">&quot;JupyterBook&quot;</span><span class="p">,</span> <span class="c1">#up tp TRAIN could be combined in a srs dicretory</span>
    <span class="s2">&quot;Cluster&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;TRAIN&quot;</span><span class="p">,</span>
    <span class="n">trained_models_dir</span><span class="p">,</span> <span class="c1">#/trained_models </span>
    <span class="n">filename_model</span> <span class="c1"># utils.get_model_filename has the saved file format </span>
<span class="p">)</span>

<span class="c1">#LOAD EITHER TRAINED OR UNTRAINED MODEL</span>
<span class="c1"># to load untrained model (start training from scratch), uncomment the next line</span>
<span class="n">untrained_model</span> <span class="o">=</span> <span class="n">load_untrained_model</span><span class="p">(</span><span class="n">PARAMS_m</span><span class="p">)</span>
<span class="c1"># to continune training of model (pickup where the previous training left off), uncomment below</span>
<span class="c1"># trained_model =load_trained_model(PATH=PATH_model, PARAMS=PARAMS_m)</span>

<span class="n">IQN_trace</span> <span class="o">=</span> <span class="p">([],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[])</span>
<span class="n">traces_step</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">traces_window</span> <span class="o">=</span> <span class="n">traces_step</span>
<span class="n">IQN</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span>
    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">untrained_model</span><span class="p">,</span>
    <span class="n">train_x</span><span class="o">=</span><span class="n">train_x_z_scaled</span><span class="p">,</span>
    <span class="n">train_t</span><span class="o">=</span><span class="n">train_t_z_scaled</span><span class="p">,</span>
    <span class="n">valid_x</span><span class="o">=</span><span class="n">test_x_z_scaled</span><span class="p">,</span>
    <span class="n">valid_t</span><span class="o">=</span><span class="n">test_t_z_scaled</span><span class="p">,</span>
    <span class="n">traces</span><span class="o">=</span><span class="n">IQN_trace</span><span class="p">,</span>
    <span class="n">PARAMS</span><span class="o">=</span><span class="n">PARAMS_m</span><span class="p">,</span>
    <span class="n">traces_step</span><span class="o">=</span><span class="n">traces_step</span><span class="p">,</span>
    <span class="n">traces_window</span><span class="o">=</span><span class="n">traces_window</span><span class="p">,</span>
    <span class="n">save_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>



<span class="n">SAVE_LAST_MODEL</span><span class="o">=</span><span class="kc">False</span>
<span class="k">if</span> <span class="n">SAVE_LAST_MODEL</span><span class="p">:</span>
    <span class="c1"># ## Save last iteration of trained model </span>
    <span class="c1">#dont save the last model, it might be worse than previous iterations, which were automatically savedby model checkpoints</span>

    <span class="n">final_path</span> <span class="o">=</span> <span class="n">get_model_filename</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">PARAMS_m</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.dict&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;_FINAL.dict&#39;</span>

    <span class="n">trained_models_dir</span> <span class="o">=</span> <span class="s2">&quot;trained_models&quot;</span>
    <span class="n">mkdir</span><span class="p">(</span><span class="n">trained_models_dir</span><span class="p">)</span>
    <span class="c1"># on cluster, Im using another TRAIN directory</span>
    <span class="n">PATH_final_model</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">IQN_BASE</span><span class="p">,</span> <span class="s2">&quot;JupyterBook&quot;</span><span class="p">,</span> <span class="s2">&quot;Cluster&quot;</span><span class="p">,</span> <span class="s2">&quot;TRAIN&quot;</span><span class="p">,</span> <span class="n">trained_models_dir</span><span class="p">,</span> <span class="n">final_path</span>
    <span class="p">)</span>

    <span class="n">save_model</span><span class="p">(</span><span class="n">IQN</span><span class="p">,</span> <span class="n">PATH_final_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;str&#39;&gt;
training for 2000000 iteration, which is  1280.0 epochs
RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=10, bias=True)
    (1): LeakyReLU(negative_slope=0.3)
    (2): Linear(in_features=10, out_features=10, bias=True)
    (3): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.3)
    (5): Linear(in_features=10, out_features=10, bias=True)
    (6): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.3)
    (8): Linear(in_features=10, out_features=1, bias=True)
  )
)
training IQN 
Iteration vs average loss
 iteration	 train-set	  test-set
		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 3.4373116493225098


Current valid loss: 3.4373116493225098;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_3_layer10_hiddenLeakyReLU_activation64_batchsize2000_Kiteration.dict
         0	  3.440834	  3.437312
		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 2.313009738922119


Current valid loss: 2.313009738922119;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_3_layer10_hiddenLeakyReLU_activation64_batchsize2000_Kiteration.dict
       200	  2.313900	  2.313010	  2.313010		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 1.790529727935791


Current valid loss: 1.790529727935791;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_3_layer10_hiddenLeakyReLU_activation64_batchsize2000_Kiteration.dict
       400	  1.792286	  1.790530	  1.790530		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 2.2069709300994873

       600	  2.208208	  2.206971	  2.206971		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 5.5066633224487305

       800	  5.515627	  5.506663	  5.506663		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 3.0318331718444824

      1000	  3.034365	  3.031833	  3.031833		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 7.6721673011779785

      1200	  7.662749	  7.672167	  7.672167		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 3.0870087146759033

      1400	  3.091507	  3.087009	  3.087009		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 3.746440887451172

      1600	  3.748494	  3.746441	  3.746441		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 2.478487253189087

      1800	  2.478855	  2.478487	  2.478487		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 5.734546184539795

      2000	  5.751131	  5.734546	  5.734546		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 4.55300235748291

      2200	  4.562317	  4.553002	  4.553002		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 2.9482529163360596

      2400	  2.956784	  2.948253	  2.948253		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 4.012182712554932

      2600	  4.025727	  4.012183	  4.012183		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 6.367801666259766

      2800	  6.365440	  6.367802	  6.367802		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.6989338397979736


Current valid loss: 0.6989338397979736;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatam_3_layer10_hiddenLeakyReLU_activation64_batchsize2000_Kiteration.dict
      3000	  0.701813	  0.698934	  0.698934		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 2.1136515140533447

      3200	  2.114510	  2.113652	  2.113652		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 1.8501721620559692

      3400	  1.852114	  1.850172	  1.850172
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">30</span><span class="o">-</span><span class="n">f8cc6f0b3bdc</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">46</span>     <span class="n">traces_step</span><span class="o">=</span><span class="n">traces_step</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">47</span>     <span class="n">traces_window</span><span class="o">=</span><span class="n">traces_window</span><span class="p">,</span>
<span class="ne">---&gt; </span><span class="mi">48</span>     <span class="n">save_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">49</span> <span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">50</span> 

<span class="nn">~/Desktop/Pulled_Github_Repositories/torchQN/utils/utils.py</span> in <span class="ni">wrapper_timer</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">574</span>                 <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;timing this arbitrary function&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">575</span>             <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="ne">--&gt; </span><span class="mi">576</span>             <span class="n">value</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">577</span>             <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">578</span>             <span class="n">run_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>

<span class="nn">&lt;ipython-input-23-496357fd26d9&gt;</span> in <span class="ni">run</span><span class="nt">(target, model, train_x, train_t, valid_x, valid_t, traces, PARAMS, traces_step, traces_window, save_model)</span>
<span class="g g-Whitespace">    </span><span class="mi">557</span>         <span class="n">traces</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">558</span>         <span class="n">step</span><span class="o">=</span><span class="n">traces_step</span><span class="p">,</span>
<span class="ne">--&gt; </span><span class="mi">559</span>         <span class="n">window</span><span class="o">=</span><span class="n">traces_window</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">560</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">561</span> 

<span class="nn">&lt;ipython-input-23-496357fd26d9&gt;</span> in <span class="ni">train</span><span class="nt">(target, model, avloss, getbatch, train_x, train_t, valid_x, valid_t, PARAMS, traces, step, window)</span>
<span class="g g-Whitespace">    </span><span class="mi">477</span>         <span class="n">empirical_risk</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># compute gradients</span>
<span class="g g-Whitespace">    </span><span class="mi">478</span> 
<span class="ne">--&gt; </span><span class="mi">479</span>         <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># move one step towards the minimum of the loss function using an SGD-like algorithm.</span>
<span class="g g-Whitespace">    </span><span class="mi">480</span> 
<span class="g g-Whitespace">    </span><span class="mi">481</span>         <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="nn">~/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py</span> in <span class="ni">wrapper</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">66</span>                 <span class="n">instance</span><span class="o">.</span><span class="n">_step_count</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">     </span><span class="mi">67</span>                 <span class="n">wrapped</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="fm">__get__</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">68</span>                 <span class="k">return</span> <span class="n">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">69</span> 
<span class="g g-Whitespace">     </span><span class="mi">70</span>             <span class="c1"># Note that the returned function here is no longer a bound method,</span>

<span class="nn">~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py</span> in <span class="ni">wrapper</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">138</span>                 <span class="n">profile_name</span> <span class="o">=</span> <span class="s2">&quot;Optimizer.step#</span><span class="si">{}</span><span class="s2">.step&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">139</span>                 <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="n">profile_name</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">140</span>                     <span class="n">out</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">141</span>                     <span class="n">obj</span><span class="o">.</span><span class="n">_optimizer_step_code</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">142</span>                     <span class="k">return</span> <span class="n">out</span>

<span class="nn">~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py</span> in <span class="ni">decorate_context</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">25</span>         <span class="k">def</span> <span class="nf">decorate_context</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="g g-Whitespace">     </span><span class="mi">26</span>             <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone</span><span class="p">():</span>
<span class="ne">---&gt; </span><span class="mi">27</span>                 <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">28</span>         <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">decorate_context</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">29</span> 

<span class="nn">~/anaconda3/lib/python3.7/site-packages/torch/optim/nadam.py</span> in <span class="ni">step</span><span class="nt">(self, closure)</span>
<span class="g g-Whitespace">    </span><span class="mi">145</span>                   <span class="n">momentum_decay</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;momentum_decay&#39;</span><span class="p">],</span>
<span class="g g-Whitespace">    </span><span class="mi">146</span>                   <span class="n">eps</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">],</span>
<span class="ne">--&gt; </span><span class="mi">147</span>                   <span class="n">foreach</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;foreach&#39;</span><span class="p">])</span>
<span class="g g-Whitespace">    </span><span class="mi">148</span> 
<span class="g g-Whitespace">    </span><span class="mi">149</span>         <span class="k">return</span> <span class="n">loss</span>

<span class="nn">~/anaconda3/lib/python3.7/site-packages/torch/optim/nadam.py</span> in <span class="ni">nadam</span><span class="nt">(params, grads, exp_avgs, exp_avg_sqs, mu_products, state_steps, foreach, beta1, beta2, lr, weight_decay, momentum_decay, eps)</span>
<span class="g g-Whitespace">    </span><span class="mi">200</span>          <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">201</span>          <span class="n">momentum_decay</span><span class="o">=</span><span class="n">momentum_decay</span><span class="p">,</span>
<span class="ne">--&gt; </span><span class="mi">202</span>          <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">203</span> 
<span class="g g-Whitespace">    </span><span class="mi">204</span> 

<span class="nn">~/anaconda3/lib/python3.7/site-packages/torch/optim/nadam.py</span> in <span class="ni">_single_tensor_nadam</span><span class="nt">(params, grads, exp_avgs, exp_avg_sqs, mu_products, state_steps, beta1, beta2, lr, weight_decay, momentum_decay, eps)</span>
<span class="g g-Whitespace">    </span><span class="mi">241</span> 
<span class="g g-Whitespace">    </span><span class="mi">242</span>         <span class="c1"># decay the first and second moment running average coefficient</span>
<span class="ne">--&gt; </span><span class="mi">243</span>         <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">244</span>         <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">245</span> 

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-p-t">
<h3>2.4: Train <span class="math notranslate nohighlight">\(p_T\)</span><a class="headerlink" href="#train-p-t" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#######################################</span>
<span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;RecoDatapT&quot;</span>
<span class="n">source</span> <span class="o">=</span> <span class="n">FIELDS</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">source</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Features:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Target = &quot;</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;USING NEW DATASET</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1">######################################</span>
<span class="n">USE_BRADEN_SCALING</span><span class="o">=</span><span class="kc">False</span>
<span class="c1">#####################################</span>
<span class="c1">################################### CONFIGURATIONS ###################################</span>

<span class="n">JUPYTER</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">use_subsample</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1"># use_subsample = True</span>
<span class="k">if</span> <span class="n">use_subsample</span><span class="p">:</span>
    <span class="n">SUBSAMPLE</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
        <span class="mf">1e5</span>
    <span class="p">)</span>  <span class="c1"># subsample use for development - in production use whole dataset</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">SUBSAMPLE</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1"># Load scaled data</span>
<span class="c1"># scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()</span>


<span class="c1"># Get targets and features</span>
<span class="c1"># if USE_BRADEN_SCALING==True:</span>
<span class="c1">#     print(f&quot;spliting data for {target}&quot;)</span>
<span class="c1">#     train_t, train_x = split_t_x(</span>
<span class="c1">#         df=scaled_train_data, target=target, input_features=features</span>
<span class="c1">#     )</span>
<span class="c1">#     print(&quot;train_t shape = &quot;, train_t.shape, &quot;train_x shape = &quot;, train_x.shape)</span>
<span class="c1">#     print(&quot;\n Training features:\n&quot;)</span>
<span class="c1">#     print(train_x)</span>
<span class="c1">#     valid_t, valid_x = split_t_x(</span>
<span class="c1">#         df=scaled_valid_data, target=target, input_features=features</span>
<span class="c1">#     )</span>
<span class="c1">#     print(&quot;valid_t shape = &quot;, valid_t.shape, &quot;valid_x shape = &quot;, valid_x.shape)</span>
<span class="c1">#     test_t, test_x = split_t_x(df=scaled_test_data, target=target, input_features=features)</span>
<span class="c1">#     print(&quot;test_t shape = &quot;, test_t.shape, &quot;test_x shape = &quot;, test_x.shape)</span>

<span class="c1"># else:</span>
<span class="c1">#     print(f&quot;spliting data for {target}&quot;)</span>
<span class="c1">#     train_t, train_x = normal_split_t_x(</span>
<span class="c1">#     df=raw_train_data, target=target, input_features=features</span>
<span class="c1">#     )</span>
<span class="c1">#     print(&quot;train_t shape = &quot;, train_t.shape, &quot;train_x shape = &quot;, train_x.shape)</span>
<span class="c1">#     print(&quot;\n Training features:\n&quot;)</span>
<span class="c1">#     print(train_x)</span>
<span class="c1">#     valid_t, valid_x = normal_split_t_x(</span>
<span class="c1">#     df=raw_valid_data, target=target, input_features=features</span>
<span class="c1">#     )</span>
<span class="c1">#     print(&quot;valid_t shape = &quot;, valid_t.shape, &quot;valid_x shape = &quot;, valid_x.shape)</span>
<span class="c1">#     test_t, test_x = normal_split_t_x(df=raw_test_data, target=target, input_features=features)</span>
<span class="c1">#     print(&quot;test_t shape = &quot;, test_t.shape, &quot;test_x shape = &quot;, test_x.shape)</span>


<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;spliting data for </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">train_t</span><span class="p">,</span> <span class="n">train_x</span> <span class="o">=</span> <span class="n">normal_split_t_x</span><span class="p">(</span>
<span class="n">df</span><span class="o">=</span><span class="n">raw_train_data</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="n">features</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train_t shape = &quot;</span><span class="p">,</span> <span class="n">train_t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;train_x shape = &quot;</span><span class="p">,</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Training features:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
<span class="n">valid_t</span><span class="p">,</span> <span class="n">valid_x</span> <span class="o">=</span> <span class="n">normal_split_t_x</span><span class="p">(</span>
<span class="n">df</span><span class="o">=</span><span class="n">raw_valid_data</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="n">features</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;valid_t shape = &quot;</span><span class="p">,</span> <span class="n">valid_t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;valid_x shape = &quot;</span><span class="p">,</span> <span class="n">valid_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">test_t</span><span class="p">,</span> <span class="n">test_x</span> <span class="o">=</span> <span class="n">normal_split_t_x</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">raw_test_data</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">input_features</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test_t shape = &quot;</span><span class="p">,</span> <span class="n">test_t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;test_x shape = &quot;</span><span class="p">,</span> <span class="n">test_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;no need to train_test_split since we already have the split dataframes&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">valid_x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">valid_x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">train_x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">valid_t</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">valid_t</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_t</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">train_t</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="n">NFEATURES</span> <span class="o">=</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">######################################################</span>

<span class="c1"># Apply z scaling to features and targets</span>
<span class="c1"># to features</span>
<span class="n">TRAIN_SCALE_DICT</span><span class="o">=</span><span class="n">get_train_scale_dict</span><span class="p">(</span><span class="n">USE_BRADEN_SCALING</span><span class="p">)</span>
<span class="n">apply_z_generator</span> <span class="o">=</span> <span class="n">apply_z_to_features</span><span class="p">(</span><span class="n">TRAIN_SCALE_DICT</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">test_x</span><span class="p">,</span> <span class="n">valid_x</span><span class="p">)</span>
<span class="n">train_x_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_generator</span><span class="p">)</span>
<span class="n">test_x_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_generator</span><span class="p">)</span>
<span class="n">valid_x_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_generator</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">valid_x_z_scaled</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">valid_x_z_scaled</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_x_z_scaled</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">train_x_z_scaled</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># to targets</span>
<span class="n">apply_z_to_targets_generator</span> <span class="o">=</span> <span class="n">apply_z_to_targets</span><span class="p">(</span>
    <span class="n">train_t</span><span class="p">,</span> <span class="n">test_t</span><span class="p">,</span> <span class="n">valid_t</span>
<span class="p">)</span>
<span class="n">train_t_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_to_targets_generator</span><span class="p">)</span>
<span class="n">test_t_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_to_targets_generator</span><span class="p">)</span>
<span class="n">valid_t_z_scaled</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">apply_z_to_targets_generator</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">valid_t_z_scaled</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">valid_t_z_scaled</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_t_z_scaled</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">train_t_z_scaled</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>



<span class="c1">###########################################################</span>
<span class="c1"># Decide on parameters for this model and training</span>
<span class="n">PARAMS_pT</span> <span class="o">=</span> <span class="p">{</span>
<span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span>
<span class="s2">&quot;dropout_1&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="mf">0.6</span><span class="p">),</span>
<span class="s2">&quot;dropout_2&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span>
<span class="s2">&quot;activation&quot;</span><span class="p">:</span> <span class="s2">&quot;LeakyReLU&quot;</span><span class="p">,</span>
    <span class="s1">&#39;optimizer_name&#39;</span><span class="p">:</span><span class="s1">&#39;NAdam&#39;</span><span class="p">,</span>
    <span class="s1">&#39;starting_learning_rate&#39;</span><span class="p">:</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">&#39;momentum&#39;</span><span class="p">:</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.6</span><span class="p">),</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span><span class="nb">int</span><span class="p">(</span><span class="mi">1024</span><span class="p">),</span>
    <span class="s1">&#39;n_iterations&#39;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="mf">2e6</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">optimizer_name</span><span class="o">=</span><span class="n">PARAMS_pT</span><span class="p">[</span><span class="s1">&#39;optimizer_name&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">))</span>
<span class="c1"># optimizer_name = BEST_PARAMS[&quot;optimizer_name&quot;].to_string().split()[1]</span>
<span class="n">NITERATIONS</span><span class="o">=</span><span class="n">PARAMS_pT</span><span class="p">[</span><span class="s1">&#39;n_iterations&#39;</span><span class="p">]</span>
<span class="n">BATCHSIZE</span><span class="o">=</span><span class="n">PARAMS_pT</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
<span class="n">comment</span><span class="o">=</span><span class="s1">&#39;&#39;</span>



<span class="c1"># N_epochs X N_train_examples = N_iterations X batch_size</span>
<span class="n">N_epochs</span> <span class="o">=</span> <span class="p">(</span><span class="n">NITERATIONS</span> <span class="o">*</span> <span class="n">BATCHSIZE</span><span class="p">)</span> <span class="o">/</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;training for </span><span class="si">{</span><span class="n">NITERATIONS</span><span class="si">}</span><span class="s2"> iteration, which is  </span><span class="si">{</span><span class="n">N_epochs</span><span class="si">}</span><span class="s2"> epochs&quot;</span><span class="p">)</span>

<span class="c1">#train model from scratch</span>
<span class="n">filename_model</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_model_filename</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">PARAMS_pT</span><span class="p">)</span>
<span class="c1">#or pick up trained model</span>
<span class="c1"># filename_model = &#39;Trained_IQNx4_RecoDatapT_10_layer6_hiddenLeakyReLU_activation512_batchsize300_Kiteration.dict&#39;</span>

<span class="n">trained_models_dir</span> <span class="o">=</span> <span class="s2">&quot;trained_models&quot;</span>
<span class="n">utils</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">trained_models_dir</span><span class="p">)</span>
<span class="c1"># on cluster, Im using another TRAIN directory</span>
<span class="n">PATH_model</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">IQN_BASE</span><span class="p">,</span> <span class="c1">#the loaction of the repo</span>
    <span class="s2">&quot;JupyterBook&quot;</span><span class="p">,</span> <span class="c1">#up tp TRAIN could be combined in a srs dicretory</span>
    <span class="s2">&quot;Cluster&quot;</span><span class="p">,</span> 
    <span class="s2">&quot;TRAIN&quot;</span><span class="p">,</span>
    <span class="n">trained_models_dir</span><span class="p">,</span> <span class="c1">#/trained_models </span>
    <span class="n">filename_model</span> <span class="c1"># utils.get_model_filename has the saved file format </span>
<span class="p">)</span>

<span class="c1">#LOAD EITHER TRAINED OR UNTRAINED MODEL</span>
<span class="c1"># to load untrained model (start training from scratch), uncomment the next line</span>
<span class="n">untrained_model</span> <span class="o">=</span> <span class="n">load_untrained_model</span><span class="p">(</span><span class="n">PARAMS_pT</span><span class="p">)</span>
<span class="c1"># to continune training of model (pickup where the previous training left off), uncomment below</span>
<span class="c1"># trained_model =load_trained_model(PATH=PATH_model, PARAMS=PARAMS_pT)</span>

<span class="n">IQN_trace</span> <span class="o">=</span> <span class="p">([],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[])</span>
<span class="n">traces_step</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">traces_window</span> <span class="o">=</span> <span class="n">traces_step</span>
<span class="n">IQN</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span>
    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">untrained_model</span><span class="p">,</span>
    <span class="n">train_x</span><span class="o">=</span><span class="n">train_x_z_scaled</span><span class="p">,</span>
    <span class="n">train_t</span><span class="o">=</span><span class="n">train_t_z_scaled</span><span class="p">,</span>
    <span class="n">valid_x</span><span class="o">=</span><span class="n">test_x_z_scaled</span><span class="p">,</span>
    <span class="n">valid_t</span><span class="o">=</span><span class="n">test_t_z_scaled</span><span class="p">,</span>
    <span class="n">traces</span><span class="o">=</span><span class="n">IQN_trace</span><span class="p">,</span>
    <span class="n">PARAMS</span><span class="o">=</span><span class="n">PARAMS_pT</span><span class="p">,</span>
    <span class="n">traces_step</span><span class="o">=</span><span class="n">traces_step</span><span class="p">,</span>
    <span class="n">traces_window</span><span class="o">=</span><span class="n">traces_window</span><span class="p">,</span>
    <span class="n">save_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>



<span class="n">SAVE_LAST_MODEL</span><span class="o">=</span><span class="kc">False</span>
<span class="k">if</span> <span class="n">SAVE_LAST_MODEL</span><span class="p">:</span>
    <span class="c1"># ## Save last iteration of trained model </span>
    <span class="c1">#dont save the last model, it might be worse than previous iterations, which were automatically savedby model checkpoints</span>

    <span class="n">final_path</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_model_filename</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">PARAMS_pT</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.dict&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;_FINAL.dict&#39;</span>

    <span class="n">trained_models_dir</span> <span class="o">=</span> <span class="s2">&quot;trained_models&quot;</span>
    <span class="n">utils</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">trained_models_dir</span><span class="p">)</span>
    <span class="c1"># on cluster, Im using another TRAIN directory</span>
    <span class="n">PATH_final_model</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">IQN_BASE</span><span class="p">,</span> <span class="s2">&quot;JupyterBook&quot;</span><span class="p">,</span> <span class="s2">&quot;Cluster&quot;</span><span class="p">,</span> <span class="s2">&quot;TRAIN&quot;</span><span class="p">,</span> <span class="n">trained_models_dir</span><span class="p">,</span> <span class="n">final_path</span>
    <span class="p">)</span>

    <span class="n">save_model</span><span class="p">(</span><span class="n">IQN</span><span class="p">,</span> <span class="n">PATH_final_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Features:
 [&#39;RecoDatam&#39;, &#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;, &#39;tau&#39;]

Target =  RecoDatapT
USING NEW DATASET

spliting data for RecoDatapT
train_t shape =  (8000000,) train_x shape =  (8000000, 6)

 Training features:

[[ 2.59587    29.4452      0.828187    2.90213     2.85348     0.36130954]
 [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.12689925]
 [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.96230681]
 ...
 [ 6.25659    41.4192     -2.23358    -2.81921     7.19348     0.08421659]
 [ 6.11213    35.4637     -1.12318     0.356494    6.06597     0.05535172]
 [ 4.17483    26.5586     -1.09427    -1.49334     4.25409     0.07489863]]
valid_t shape =  (1000000,) valid_x shape =  (1000000, 6)
test_t shape =  (1000000,) test_x shape =  (1000000, 6)
no need to train_test_split since we already have the split dataframes
[ 5.55141126e+00  3.27223764e+01  6.98189368e-04 -8.95543973e-04
  6.96116528e+00  5.00485136e-01] [ 2.66412454 15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]
[ 5.55556745e+00  3.26952341e+01 -1.78188172e-03 -3.83090331e-04
  6.96299435e+00  4.99915289e-01] [ 2.66433986 14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]
32.881453465999996 16.02400426348493
32.86720151648752 15.829355769531851
NORMAL UNSCALED DICTIONARY
{&#39;genDatapT&#39;: {&#39;mean&#39;: 32.695234084987476, &#39;std&#39;: 14.937932540562551}, &#39;genDataeta&#39;: {&#39;mean&#39;: -0.0017818817154031672, &#39;std&#39;: 2.204309760627079}, &#39;genDataphi&#39;: {&#39;mean&#39;: -0.0003830903308450233, &#39;std&#39;: 1.8138251604791067}, &#39;genDatam&#39;: {&#39;mean&#39;: 6.962994352358474, &#39;std&#39;: 2.781332025286383}, &#39;RecoDatapT&#39;: {&#39;mean&#39;: 32.86720151648752, &#39;std&#39;: 15.829355769531851}, &#39;RecoDataeta&#39;: {&#39;mean&#39;: -0.0017898858568513964, &#39;std&#39;: 2.197968491495457}, &#39;RecoDataphi&#39;: {&#39;mean&#39;: -0.0004719170328962474, &#39;std&#39;: 1.8144739820043825}, &#39;RecoDatam&#39;: {&#39;mean&#39;: 5.555567451922438, &#39;std&#39;: 2.664339857066051}}



[-1.81710707e+00  1.48455353e+01  5.96132264e-04 -2.50379668e+00
 -1.63658184e+00  5.00485136e-01] [0.17834627 6.89519305 1.2152514  0.65207164 0.17568487 0.28852734]
[-1.81682884e+00  1.48332220e+01 -7.71183141e-04 -2.50361243e+00
 -1.63646629e+00  4.99915289e-01] [0.17836068 6.77669391 1.21528238 0.65214262 0.17570722 0.28867295]
0.0009003493079555966 1.0122966781963252
-1.2048033681821834e-15 1.0000000000000002
&lt;class &#39;str&#39;&gt;
training for 2000000 iteration, which is  256.0 epochs
RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=16, bias=True)
    (1): LeakyReLU(negative_slope=0.3)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.3)
    (5): Linear(in_features=16, out_features=16, bias=True)
    (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.3)
    (8): Linear(in_features=16, out_features=1, bias=True)
  )
)
training IQN 
Iteration vs average loss
 iteration	 train-set	  test-set
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.5987200140953064


Current valid loss: 0.5987200140953064;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
         0	  0.600579	  0.598720
		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.14327415823936462


Current valid loss: 0.14327415823936462;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
       100	  0.142855	  0.143274	  0.143274		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1359773427248001


Current valid loss: 0.1359773427248001;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
       200	  0.136279	  0.135977	  0.135977		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11432049423456192


Current valid loss: 0.11432049423456192;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
       300	  0.114511	  0.114320	  0.114320		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08805301040410995


Current valid loss: 0.08805301040410995;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
       400	  0.088157	  0.088053	  0.088053		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08550194650888443


Current valid loss: 0.08550194650888443;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
       500	  0.085494	  0.085502	  0.085502		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1109614223241806

       600	  0.111296	  0.110961	  0.110961		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.33752644062042236

       700	  0.338428	  0.337526	  0.337526		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1085544228553772

       800	  0.108280	  0.108554	  0.108554		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.20174911618232727

       900	  0.202449	  0.201749	  0.201749		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0968456044793129

      1000	  0.096569	  0.096846	  0.096846		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.21917271614074707

      1100	  0.219687	  0.219173	  0.219173		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09528733789920807

      1200	  0.095409	  0.095287	  0.095287		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.13177266716957092

      1300	  0.131997	  0.131773	  0.131773		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.21874766051769257

      1400	  0.219383	  0.218748	  0.218748		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1472843736410141

      1500	  0.146945	  0.147284	  0.147284		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.108138807117939

      1600	  0.107729	  0.108139	  0.108139		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.19834953546524048

      1700	  0.199303	  0.198350	  0.198350		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08799073100090027

      1800	  0.088087	  0.087991	  0.087991		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.07833113521337509


Current valid loss: 0.07833113521337509;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
      1900	  0.078295	  0.078331	  0.078331		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10393134504556656

      2000	  0.103753	  0.103931	  0.103931		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08745770156383514

      2100	  0.087513	  0.087458	  0.087458		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08412045240402222

      2200	  0.084095	  0.084120	  0.084120		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08317507803440094

      2300	  0.083125	  0.083175	  0.083175		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08781371265649796

      2400	  0.087798	  0.087814	  0.087814		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09140736609697342

      2500	  0.091283	  0.091407	  0.091407		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12309848517179489

      2600	  0.122790	  0.123098	  0.123098		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1378583461046219

      2700	  0.138528	  0.137858	  0.137858		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08843953162431717

      2800	  0.088497	  0.088440	  0.088440		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09436282515525818

      2900	  0.094189	  0.094363	  0.094363		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0800202488899231

      3000	  0.080035	  0.080020	  0.080020		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1318553239107132

      3100	  0.132398	  0.131855	  0.131855		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08702273666858673

      3200	  0.086990	  0.087023	  0.087023		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10442271828651428

      3300	  0.104360	  0.104423	  0.104423		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.07873428612947464

      3400	  0.078731	  0.078734	  0.078734		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10214599221944809

      3500	  0.102306	  0.102146	  0.102146		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08991770446300507

      3600	  0.089934	  0.089918	  0.089918		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12549999356269836

      3700	  0.125793	  0.125500	  0.125500		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08406978845596313

      3800	  0.083877	  0.084070	  0.084070		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10047253221273422

      3900	  0.100651	  0.100473	  0.100473		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09253895282745361

      4000	  0.092539	  0.092539	  0.092539		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08424456417560577

      4100	  0.084232	  0.084245	  0.084245		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.18504059314727783

      4200	  0.185560	  0.185041	  0.185041		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.2963695824146271

      4300	  0.296985	  0.296370	  0.296370		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0937710553407669

      4400	  0.093788	  0.093771	  0.093771		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09388059377670288

      4500	  0.093863	  0.093881	  0.093881		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08279455453157425

      4600	  0.082692	  0.082795	  0.082795		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0929548516869545

      4700	  0.092801	  0.092955	  0.092955		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12896119058132172

      4800	  0.129256	  0.128961	  0.128961		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10698778182268143

      4900	  0.107053	  0.106988	  0.106988		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09648514539003372

      5000	  0.096366	  0.096485	  0.096485		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09870883822441101

      5100	  0.098508	  0.098709	  0.098709		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09387663751840591

      5200	  0.093801	  0.093877	  0.093877		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0888291746377945

      5300	  0.088792	  0.088829	  0.088829		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09610298275947571

      5400	  0.095932	  0.096103	  0.096103		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1551860123872757

      5500	  0.155501	  0.155186	  0.155186		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1795654594898224

      5600	  0.180383	  0.179565	  0.179565		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12004590779542923

      5700	  0.119923	  0.120046	  0.120046		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09906575083732605

      5800	  0.099161	  0.099066	  0.099066		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12350724637508392

      5900	  0.123687	  0.123507	  0.123507		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0858503058552742

      6000	  0.085830	  0.085850	  0.085850		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08146058022975922

      6100	  0.081449	  0.081461	  0.081461		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08426325023174286

      6200	  0.084271	  0.084263	  0.084263		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10870661586523056

      6300	  0.108527	  0.108707	  0.108707		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09746822714805603

      6400	  0.097303	  0.097468	  0.097468		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1321294754743576

      6500	  0.131962	  0.132129	  0.132129		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09647223353385925

      6600	  0.096457	  0.096472	  0.096472		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11535345762968063

      6700	  0.115144	  0.115353	  0.115353		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09456070512533188

      6800	  0.094339	  0.094561	  0.094561		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11577510833740234

      6900	  0.115636	  0.115775	  0.115775		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09180919826030731

      7000	  0.091879	  0.091809	  0.091809		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1309339553117752

      7100	  0.130779	  0.130934	  0.130934		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11411381512880325

      7200	  0.113903	  0.114114	  0.114114		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1045396476984024

      7300	  0.104487	  0.104540	  0.104540		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.18942321836948395

      7400	  0.190079	  0.189423	  0.189423		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10695245862007141

      7500	  0.107210	  0.106952	  0.106952		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09459994733333588

      7600	  0.094731	  0.094600	  0.094600		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08579390496015549

      7700	  0.085749	  0.085794	  0.085794		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0926670953631401

      7800	  0.092481	  0.092667	  0.092667		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08704673498868942

      7900	  0.087027	  0.087047	  0.087047		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0912131816148758

      8000	  0.091008	  0.091213	  0.091213		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09549988806247711

      8100	  0.095231	  0.095500	  0.095500		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09099157154560089

      8200	  0.090859	  0.090992	  0.090992		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1419641375541687

      8300	  0.142479	  0.141964	  0.141964		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10836914926767349

      8400	  0.108474	  0.108369	  0.108369		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0860951766371727

      8500	  0.086029	  0.086095	  0.086095		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.14122559130191803

      8600	  0.141289	  0.141226	  0.141226		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11224314570426941

      8700	  0.112124	  0.112243	  0.112243		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08431847393512726

      8800	  0.084114	  0.084318	  0.084318		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10582946985960007

      8900	  0.105583	  0.105829	  0.105829		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10101267695426941

      9000	  0.100757	  0.101013	  0.101013		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11927693337202072

      9100	  0.119004	  0.119277	  0.119277		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09422948211431503

      9200	  0.094019	  0.094229	  0.094229		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12899287045001984

      9300	  0.128716	  0.128993	  0.128993		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1413053721189499

      9400	  0.141019	  0.141305	  0.141305		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.090022973716259

      9500	  0.089829	  0.090023	  0.090023		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.13384956121444702

      9600	  0.133796	  0.133850	  0.133850		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10227558016777039

      9700	  0.102371	  0.102276	  0.102276		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09924706071615219

      9800	  0.098917	  0.099247	  0.099247		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11046256124973297

      9900	  0.110079	  0.110463	  0.119696		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08762479573488235

     10000	  0.087418	  0.087625	  0.114585		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11442088335752487

     10100	  0.114046	  0.114421	  0.114297		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09054090827703476

     10200	  0.090396	  0.090541	  0.113843		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12565839290618896

     10300	  0.125325	  0.125658	  0.113956		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.13652268052101135

     10400	  0.136152	  0.136523	  0.114441		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1302274763584137

     10500	  0.129918	  0.130227	  0.114888		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08429298549890518

     10600	  0.084194	  0.084293	  0.114621		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12229479849338531

     10700	  0.122008	  0.122295	  0.112469		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09290793538093567

     10800	  0.092796	  0.092908	  0.112312		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11660624295473099

     10900	  0.116242	  0.116606	  0.111461		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.1030367836356163

     11000	  0.103290	  0.103037	  0.111523		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11631660908460617

     11100	  0.116001	  0.116317	  0.110494		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.13986219465732574

     11200	  0.139622	  0.139862	  0.110940		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10265087336301804

     11300	  0.102608	  0.102651	  0.110649		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08664029836654663

     11400	  0.086700	  0.086640	  0.109328		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08358953148126602

     11500	  0.083664	  0.083590	  0.108691		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10868518054485321

     11600	  0.108520	  0.108685	  0.108696		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08662819117307663

     11700	  0.086718	  0.086628	  0.107579		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08990690857172012

     11800	  0.090294	  0.089907	  0.107598		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09865636378526688

     11900	  0.098847	  0.098656	  0.107802		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10415695607662201

     12000	  0.104487	  0.104157	  0.107804		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.14765676856040955

     12100	  0.147903	  0.147657	  0.108406		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.14286187291145325

     12200	  0.142601	  0.142862	  0.108993		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0887325257062912

     12300	  0.088770	  0.088733	  0.109049		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08724970370531082

     12400	  0.087344	  0.087250	  0.109043		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08632771670818329

     12500	  0.086169	  0.086328	  0.108992		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08073397725820541

     12600	  0.080651	  0.080734	  0.108569		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0844281017780304

     12700	  0.084375	  0.084428	  0.108034		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12755198776721954

     12800	  0.127089	  0.127552	  0.108426		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08550751209259033

     12900	  0.085519	  0.085508	  0.108337		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11579742282629013

     13000	  0.116109	  0.115797	  0.108695		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10228379815816879

     13100	  0.102565	  0.102284	  0.108399		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.13491258025169373

     13200	  0.135237	  0.134913	  0.108878		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.16681523621082306

     13300	  0.166888	  0.166815	  0.109502		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.0883251428604126

     13400	  0.088246	  0.088325	  0.109598		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10748699307441711

     13500	  0.107624	  0.107487	  0.109651		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09652940183877945

     13600	  0.096663	  0.096529	  0.109717		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.14641417562961578

     13700	  0.146679	  0.146414	  0.109926		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.11184067279100418

     13800	  0.112076	  0.111841	  0.110204		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10741748660802841

     13900	  0.107577	  0.107417	  0.110274		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10032019019126892

     14000	  0.100428	  0.100320	  0.110351		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.14485062658786774

     14100	  0.145334	  0.144851	  0.110957		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09206888824701309

     14200	  0.092137	  0.092069	  0.110028		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10443326085805893

     14300	  0.104504	  0.104433	  0.108108		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10546302050352097

     14400	  0.105725	  0.105463	  0.108225		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.10232437402009964

     14500	  0.102577	  0.102324	  0.108310		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12320293486118317

     14600	  0.123514	  0.123203	  0.108714		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.12175906449556351

     14700	  0.121966	  0.121759	  0.109002		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.08701157569885254

     14800	  0.087047	  0.087012	  0.108582		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.09333434700965881

     14900	  0.093264	  0.093334	  0.108446		CURRENT LEARNING RATE: 0.5
previous_iter_valid_loss : 0.17384056746959686

     15000	  0.174302	  0.173841	  0.109219		CURRENT LEARNING RATE: 0.42992384932960276
previous_iter_valid_loss : 0.10141874849796295

     15100	  0.101688	  0.101419	  0.109246		CURRENT LEARNING RATE: 0.4294941403705617
previous_iter_valid_loss : 0.12285413593053818

     15200	  0.122755	  0.122854	  0.109536		CURRENT LEARNING RATE: 0.4290648609056969
previous_iter_valid_loss : 0.08151457458734512

     15300	  0.081512	  0.081515	  0.109463		CURRENT LEARNING RATE: 0.42863601050572875
previous_iter_valid_loss : 0.08694326132535934

     15400	  0.086978	  0.086943	  0.109372		CURRENT LEARNING RATE: 0.42820758874180676
previous_iter_valid_loss : 0.10293997079133987

     15500	  0.102719	  0.102940	  0.108849		CURRENT LEARNING RATE: 0.42777959518550923
previous_iter_valid_loss : 0.09086598455905914

     15600	  0.090973	  0.090866	  0.107962		CURRENT LEARNING RATE: 0.42735202940884254
previous_iter_valid_loss : 0.10195006430149078

     15700	  0.102057	  0.101950	  0.107781		CURRENT LEARNING RATE: 0.42692489098424086
previous_iter_valid_loss : 0.08717059344053268

     15800	  0.087142	  0.087171	  0.107662		CURRENT LEARNING RATE: 0.42649817948456575
previous_iter_valid_loss : 0.08689522743225098

     15900	  0.086802	  0.086895	  0.107296		CURRENT LEARNING RATE: 0.4260718944831057
previous_iter_valid_loss : 0.09962745010852814

     16000	  0.099350	  0.099627	  0.107434		CURRENT LEARNING RATE: 0.42564603555357555
previous_iter_valid_loss : 0.07949534058570862

     16100	  0.079482	  0.079495	  0.107414		CURRENT LEARNING RATE: 0.4252206022701165
previous_iter_valid_loss : 0.10643906146287918

     16200	  0.106230	  0.106439	  0.107636		CURRENT LEARNING RATE: 0.4247955942072951
previous_iter_valid_loss : 0.08017057180404663

     16300	  0.080163	  0.080171	  0.107351		CURRENT LEARNING RATE: 0.4243710109401034
previous_iter_valid_loss : 0.12159326672554016

     16400	  0.121319	  0.121593	  0.107592		CURRENT LEARNING RATE: 0.42394685204395793
previous_iter_valid_loss : 0.08025999367237091

     16500	  0.080205	  0.080260	  0.107073		CURRENT LEARNING RATE: 0.4235231170946998
previous_iter_valid_loss : 0.15803225338459015

     16600	  0.157634	  0.158032	  0.107689		CURRENT LEARNING RATE: 0.4230998056685941
previous_iter_valid_loss : 0.09385281056165695

     16700	  0.093642	  0.093853	  0.107474		CURRENT LEARNING RATE: 0.4226769173423294
previous_iter_valid_loss : 0.08297383040189743

     16800	  0.083064	  0.082974	  0.107358		CURRENT LEARNING RATE: 0.4222544516930172
previous_iter_valid_loss : 0.08481478691101074

     16900	  0.084729	  0.084815	  0.107048		CURRENT LEARNING RATE: 0.42183240829819185
previous_iter_valid_loss : 0.12580454349517822

     17000	  0.126174	  0.125805	  0.107388		CURRENT LEARNING RATE: 0.42141078673580995
previous_iter_valid_loss : 0.08796104788780212

     17100	  0.088085	  0.087961	  0.106958		CURRENT LEARNING RATE: 0.42098958658424995
previous_iter_valid_loss : 0.07951153069734573

     17200	  0.079405	  0.079512	  0.106612		CURRENT LEARNING RATE: 0.4205688074223116
previous_iter_valid_loss : 0.11137047410011292

     17300	  0.111674	  0.111370	  0.106681		CURRENT LEARNING RATE: 0.4201484488292157
previous_iter_valid_loss : 0.11305618286132812

     17400	  0.113259	  0.113056	  0.105917		CURRENT LEARNING RATE: 0.4197285103846037
previous_iter_valid_loss : 0.10236392170190811

     17500	  0.102134	  0.102364	  0.105871		CURRENT LEARNING RATE: 0.419308991668537
previous_iter_valid_loss : 0.13482698798179626

     17600	  0.134557	  0.134827	  0.106273		CURRENT LEARNING RATE: 0.4188898922614969
previous_iter_valid_loss : 0.09218914061784744

     17700	  0.092277	  0.092189	  0.106337		CURRENT LEARNING RATE: 0.41847121174438406
previous_iter_valid_loss : 0.08350376039743423

     17800	  0.083529	  0.083504	  0.106246		CURRENT LEARNING RATE: 0.41805294969851775
previous_iter_valid_loss : 0.08447667211294174

     17900	  0.084359	  0.084477	  0.106220		CURRENT LEARNING RATE: 0.417635105705636
previous_iter_valid_loss : 0.09834224730730057

     18000	  0.098116	  0.098342	  0.106291		CURRENT LEARNING RATE: 0.4172176793478948
previous_iter_valid_loss : 0.10175202786922455

     18100	  0.101400	  0.101752	  0.106354		CURRENT LEARNING RATE: 0.41680067020786765
previous_iter_valid_loss : 0.09671907871961594

     18200	  0.096533	  0.096719	  0.106411		CURRENT LEARNING RATE: 0.4163840778685455
previous_iter_valid_loss : 0.10253266245126724

     18300	  0.102732	  0.102533	  0.106017		CURRENT LEARNING RATE: 0.4159679019133359
previous_iter_valid_loss : 0.12955841422080994

     18400	  0.129927	  0.129558	  0.106229		CURRENT LEARNING RATE: 0.4155521419260628
previous_iter_valid_loss : 0.09504400193691254

     18500	  0.095032	  0.095044	  0.106318		CURRENT LEARNING RATE: 0.4151367974909663
previous_iter_valid_loss : 0.13741862773895264

     18600	  0.137128	  0.137419	  0.106280		CURRENT LEARNING RATE: 0.41472186819270196
previous_iter_valid_loss : 0.09410625696182251

     18700	  0.094120	  0.094106	  0.106099		CURRENT LEARNING RATE: 0.4143073536163403
previous_iter_valid_loss : 0.08786896616220474

     18800	  0.087789	  0.087869	  0.106134		CURRENT LEARNING RATE: 0.4138932533473668
previous_iter_valid_loss : 0.0953281819820404

     18900	  0.095057	  0.095328	  0.106029		CURRENT LEARNING RATE: 0.41347956697168115
previous_iter_valid_loss : 0.08834917843341827

     19000	  0.088240	  0.088349	  0.105903		CURRENT LEARNING RATE: 0.4130662940755969
previous_iter_valid_loss : 0.14171290397644043

     19100	  0.141476	  0.141713	  0.106127		CURRENT LEARNING RATE: 0.41265343424584117
previous_iter_valid_loss : 0.09213795512914658

     19200	  0.092304	  0.092138	  0.106106		CURRENT LEARNING RATE: 0.4122409870695541
previous_iter_valid_loss : 0.10342463850975037

     19300	  0.103192	  0.103425	  0.105850		CURRENT LEARNING RATE: 0.41182895213428844
previous_iter_valid_loss : 0.08726269006729126

     19400	  0.087005	  0.087263	  0.105310		CURRENT LEARNING RATE: 0.4114173290280092
previous_iter_valid_loss : 0.08380208164453506

     19500	  0.083680	  0.083802	  0.105248		CURRENT LEARNING RATE: 0.41100611733909326
previous_iter_valid_loss : 0.12828439474105835

     19600	  0.128126	  0.128284	  0.105192		CURRENT LEARNING RATE: 0.410595316656329
previous_iter_valid_loss : 0.16432605683803558

     19700	  0.164744	  0.164326	  0.105813		CURRENT LEARNING RATE: 0.41018492656891553
previous_iter_valid_loss : 0.09749063849449158

     19800	  0.097426	  0.097491	  0.105795		CURRENT LEARNING RATE: 0.4097749466664628
previous_iter_valid_loss : 0.09571252763271332

     19900	  0.095577	  0.095713	  0.105648		CURRENT LEARNING RATE: 0.4093653765389909
previous_iter_valid_loss : 0.10882081091403961

     20000	  0.108755	  0.108821	  0.105860		CURRENT LEARNING RATE: 0.4089562157769297
previous_iter_valid_loss : 0.089748315513134

     20100	  0.089638	  0.089748	  0.105613		CURRENT LEARNING RATE: 0.4085474639711183
previous_iter_valid_loss : 0.08992284536361694

     20200	  0.089846	  0.089923	  0.105607		CURRENT LEARNING RATE: 0.40813912071280495
previous_iter_valid_loss : 0.09649526327848434

     20300	  0.096591	  0.096495	  0.105315		CURRENT LEARNING RATE: 0.40773118559364635
previous_iter_valid_loss : 0.09212086349725723

     20400	  0.092132	  0.092121	  0.104871		CURRENT LEARNING RATE: 0.40732365820570726
previous_iter_valid_loss : 0.1533787101507187

     20500	  0.153789	  0.153379	  0.105102		CURRENT LEARNING RATE: 0.40691653814146034
previous_iter_valid_loss : 0.10682164132595062

     20600	  0.106695	  0.106822	  0.105328		CURRENT LEARNING RATE: 0.4065098249937855
previous_iter_valid_loss : 0.12628397345542908

     20700	  0.126136	  0.126284	  0.105368		CURRENT LEARNING RATE: 0.40610351835596953
previous_iter_valid_loss : 0.08850273489952087

     20800	  0.088578	  0.088503	  0.105324		CURRENT LEARNING RATE: 0.4056976178217057
previous_iter_valid_loss : 0.09927627444267273

     20900	  0.099091	  0.099276	  0.105150		CURRENT LEARNING RATE: 0.40529212298509354
previous_iter_valid_loss : 0.08795277774333954

     21000	  0.087708	  0.087953	  0.104999		CURRENT LEARNING RATE: 0.40488703344063814
previous_iter_valid_loss : 0.09203683584928513

     21100	  0.092021	  0.092037	  0.104757		CURRENT LEARNING RATE: 0.4044823487832499
previous_iter_valid_loss : 0.12322340160608292

     21200	  0.122918	  0.123223	  0.104590		CURRENT LEARNING RATE: 0.4040780686082442
previous_iter_valid_loss : 0.08938281983137131

     21300	  0.089299	  0.089383	  0.104458		CURRENT LEARNING RATE: 0.40367419251134073
previous_iter_valid_loss : 0.10979052633047104

     21400	  0.109646	  0.109791	  0.104689		CURRENT LEARNING RATE: 0.40327072008866344
previous_iter_valid_loss : 0.0893155187368393

     21500	  0.089206	  0.089316	  0.104746		CURRENT LEARNING RATE: 0.4028676509367398
previous_iter_valid_loss : 0.12437214702367783

     21600	  0.124188	  0.124372	  0.104903		CURRENT LEARNING RATE: 0.40246498465250075
previous_iter_valid_loss : 0.09067587554454803

     21700	  0.090701	  0.090676	  0.104944		CURRENT LEARNING RATE: 0.4020627208332798
previous_iter_valid_loss : 0.10242900252342224

     21800	  0.102620	  0.102429	  0.105069		CURRENT LEARNING RATE: 0.40166085907681326
previous_iter_valid_loss : 0.08718515187501907

     21900	  0.087217	  0.087185	  0.104954		CURRENT LEARNING RATE: 0.40125939898123925
previous_iter_valid_loss : 0.09717357903718948

     22000	  0.097296	  0.097174	  0.104884		CURRENT LEARNING RATE: 0.40085834014509764
previous_iter_valid_loss : 0.13749022781848907

     22100	  0.137874	  0.137490	  0.104783		CURRENT LEARNING RATE: 0.4004576821673296
previous_iter_valid_loss : 0.12022849917411804

     22200	  0.120487	  0.120228	  0.104556		CURRENT LEARNING RATE: 0.40005742464727706
previous_iter_valid_loss : 0.11457132548093796

     22300	  0.114802	  0.114571	  0.104815		CURRENT LEARNING RATE: 0.39965756718468254
previous_iter_valid_loss : 0.08894907683134079

     22400	  0.089141	  0.088949	  0.104832		CURRENT LEARNING RATE: 0.39925810937968853
previous_iter_valid_loss : 0.09392176568508148

     22500	  0.094075	  0.093922	  0.104908		CURRENT LEARNING RATE: 0.3988590508328371
previous_iter_valid_loss : 0.0983455628156662

     22600	  0.098563	  0.098346	  0.105084		CURRENT LEARNING RATE: 0.3984603911450698
previous_iter_valid_loss : 0.08470027148723602

     22700	  0.084761	  0.084700	  0.105087		CURRENT LEARNING RATE: 0.3980621299177269
previous_iter_valid_loss : 0.09216451644897461

     22800	  0.092348	  0.092165	  0.104733		CURRENT LEARNING RATE: 0.39766426675254696
previous_iter_valid_loss : 0.09065820276737213

     22900	  0.090810	  0.090658	  0.104784		CURRENT LEARNING RATE: 0.397266801251667
previous_iter_valid_loss : 0.09497220069169998

     23000	  0.095213	  0.094972	  0.104576		CURRENT LEARNING RATE: 0.39686973301762135
previous_iter_valid_loss : 0.12236075103282928

     23100	  0.122728	  0.122361	  0.104777		CURRENT LEARNING RATE: 0.39647306165334184
previous_iter_valid_loss : 0.10625285655260086

     23200	  0.106498	  0.106253	  0.104490		CURRENT LEARNING RATE: 0.396076786762157
previous_iter_valid_loss : 0.09927107393741608

     23300	  0.099470	  0.099271	  0.103815		CURRENT LEARNING RATE: 0.3956809079477919
previous_iter_valid_loss : 0.10133687406778336

     23400	  0.101084	  0.101337	  0.103945		CURRENT LEARNING RATE: 0.3952854248143678
previous_iter_valid_loss : 0.1321852207183838

     23500	  0.132323	  0.132185	  0.104192		CURRENT LEARNING RATE: 0.39489033696640136
previous_iter_valid_loss : 0.1472516506910324

     23600	  0.147553	  0.147252	  0.104699		CURRENT LEARNING RATE: 0.3944956440088049
previous_iter_valid_loss : 0.11934874206781387

     23700	  0.119608	  0.119349	  0.104428		CURRENT LEARNING RATE: 0.3941013455468852
previous_iter_valid_loss : 0.12819111347198486

     23800	  0.128018	  0.128191	  0.104592		CURRENT LEARNING RATE: 0.393707441186344
previous_iter_valid_loss : 0.09793074429035187

     23900	  0.097917	  0.097931	  0.104497		CURRENT LEARNING RATE: 0.39331393053327673
previous_iter_valid_loss : 0.09872864186763763

     24000	  0.098715	  0.098729	  0.104481		CURRENT LEARNING RATE: 0.39292081319417277
previous_iter_valid_loss : 0.08851319551467896

     24100	  0.088361	  0.088513	  0.103918		CURRENT LEARNING RATE: 0.3925280887759148
previous_iter_valid_loss : 0.08196848630905151

     24200	  0.081827	  0.081968	  0.103817		CURRENT LEARNING RATE: 0.39213575688577823
previous_iter_valid_loss : 0.08515111356973648

     24300	  0.084954	  0.085151	  0.103624		CURRENT LEARNING RATE: 0.39174381713143125
previous_iter_valid_loss : 0.08591873943805695

     24400	  0.085706	  0.085919	  0.103428		CURRENT LEARNING RATE: 0.39135226912093407
previous_iter_valid_loss : 0.08599668741226196

     24500	  0.085789	  0.085997	  0.103265		CURRENT LEARNING RATE: 0.3909611124627386
previous_iter_valid_loss : 0.08379204571247101

     24600	  0.083604	  0.083792	  0.102871		CURRENT LEARNING RATE: 0.39057034676568825
previous_iter_valid_loss : 0.08277377486228943

     24700	  0.082651	  0.082774	  0.102481		CURRENT LEARNING RATE: 0.39017997163901713
previous_iter_valid_loss : 0.11572964489459991

     24800	  0.115526	  0.115730	  0.102768		CURRENT LEARNING RATE: 0.3897899866923502
previous_iter_valid_loss : 0.143362894654274

     24900	  0.143065	  0.143363	  0.103269		CURRENT LEARNING RATE: 0.38940039153570244
previous_iter_valid_loss : 0.09600789099931717

     25000	  0.095765	  0.096008	  0.102490		CURRENT LEARNING RATE: 0.38901118577947863
previous_iter_valid_loss : 0.09209823608398438

     25100	  0.091911	  0.092098	  0.102397		CURRENT LEARNING RATE: 0.38862236903447306
previous_iter_valid_loss : 0.12210050225257874

     25200	  0.121920	  0.122101	  0.102390		CURRENT LEARNING RATE: 0.3882339409118689
previous_iter_valid_loss : 0.0855553075671196

     25300	  0.085616	  0.085555	  0.102430		CURRENT LEARNING RATE: 0.387845901023238
previous_iter_valid_loss : 0.08252166956663132

     25400	  0.082408	  0.082522	  0.102386		CURRENT LEARNING RATE: 0.3874582489805405
previous_iter_valid_loss : 0.10038963705301285

     25500	  0.100206	  0.100390	  0.102360		CURRENT LEARNING RATE: 0.3870709843961242
previous_iter_valid_loss : 0.08776238560676575

     25600	  0.087768	  0.087762	  0.102329		CURRENT LEARNING RATE: 0.38668410688272453
previous_iter_valid_loss : 0.091314896941185

     25700	  0.091144	  0.091315	  0.102223		CURRENT LEARNING RATE: 0.386297616053464
previous_iter_valid_loss : 0.08455979824066162

     25800	  0.084457	  0.084560	  0.102197		CURRENT LEARNING RATE: 0.3859115115218517
previous_iter_valid_loss : 0.09570977836847305

     25900	  0.095484	  0.095710	  0.102285		CURRENT LEARNING RATE: 0.3855257929017831
previous_iter_valid_loss : 0.09218674898147583

     26000	  0.092199	  0.092187	  0.102210		CURRENT LEARNING RATE: 0.3851404598075396
previous_iter_valid_loss : 0.1052704006433487

     26100	  0.104986	  0.105270	  0.102468		CURRENT LEARNING RATE: 0.3847555118537879
previous_iter_valid_loss : 0.08576733618974686

     26200	  0.085658	  0.085767	  0.102262		CURRENT LEARNING RATE: 0.38437094865558014
previous_iter_valid_loss : 0.088468998670578

     26300	  0.088298	  0.088469	  0.102345		CURRENT LEARNING RATE: 0.38398676982835306
previous_iter_valid_loss : 0.08441569656133652

     26400	  0.084267	  0.084416	  0.101973		CURRENT LEARNING RATE: 0.38360297498792784
previous_iter_valid_loss : 0.08242955058813095

     26500	  0.082311	  0.082430	  0.101994		CURRENT LEARNING RATE: 0.3832195637505096
previous_iter_valid_loss : 0.1164826899766922

     26600	  0.116281	  0.116483	  0.101579		CURRENT LEARNING RATE: 0.38283653573268694
previous_iter_valid_loss : 0.11609163880348206

     26700	  0.115917	  0.116092	  0.101801		CURRENT LEARNING RATE: 0.382453890551432
previous_iter_valid_loss : 0.09664051234722137

     26800	  0.096522	  0.096641	  0.101938		CURRENT LEARNING RATE: 0.3820716278240995
previous_iter_valid_loss : 0.11276381462812424

     26900	  0.112506	  0.112764	  0.102217		CURRENT LEARNING RATE: 0.3816897471684266
previous_iter_valid_loss : 0.08674507588148117

     27000	  0.086567	  0.086745	  0.101827		CURRENT LEARNING RATE: 0.3813082482025327
previous_iter_valid_loss : 0.12127192318439484

     27100	  0.121042	  0.121272	  0.102160		CURRENT LEARNING RATE: 0.3809271305449188
previous_iter_valid_loss : 0.1265217661857605

     27200	  0.126265	  0.126522	  0.102630		CURRENT LEARNING RATE: 0.38054639381446714
previous_iter_valid_loss : 0.13486288487911224

     27300	  0.134589	  0.134863	  0.102865		CURRENT LEARNING RATE: 0.38016603763044104
previous_iter_valid_loss : 0.09395608305931091

     27400	  0.093835	  0.093956	  0.102674		CURRENT LEARNING RATE: 0.37978606161248424
previous_iter_valid_loss : 0.08425984531641006

     27500	  0.084087	  0.084260	  0.102493		CURRENT LEARNING RATE: 0.37940646538062067
previous_iter_valid_loss : 0.10050422698259354

     27600	  0.100243	  0.100504	  0.102150		CURRENT LEARNING RATE: 0.37902724855525416
previous_iter_valid_loss : 0.10180951654911041

     27700	  0.101532	  0.101810	  0.102246		CURRENT LEARNING RATE: 0.37864841075716776
previous_iter_valid_loss : 0.09129150211811066

     27800	  0.091035	  0.091292	  0.102324		CURRENT LEARNING RATE: 0.3782699516075237
previous_iter_valid_loss : 0.08090167492628098

     27900	  0.080816	  0.080902	  0.102288		CURRENT LEARNING RATE: 0.37789187072786273
previous_iter_valid_loss : 0.09493912756443024

     28000	  0.094679	  0.094939	  0.102254		CURRENT LEARNING RATE: 0.377514167740104
previous_iter_valid_loss : 0.09305892884731293

     28100	  0.092871	  0.093059	  0.102167		CURRENT LEARNING RATE: 0.3771368422665445
previous_iter_valid_loss : 0.0960196852684021

     28200	  0.095885	  0.096020	  0.102160		CURRENT LEARNING RATE: 0.37675989392985865
previous_iter_valid_loss : 0.1396554410457611

     28300	  0.139448	  0.139655	  0.102531		CURRENT LEARNING RATE: 0.3763833223530981
previous_iter_valid_loss : 0.10893946886062622

     28400	  0.108695	  0.108939	  0.102325		CURRENT LEARNING RATE: 0.3760071271596913
previous_iter_valid_loss : 0.0853106901049614

     28500	  0.085285	  0.085311	  0.102228		CURRENT LEARNING RATE: 0.375631307973443
previous_iter_valid_loss : 0.09893658012151718

     28600	  0.098700	  0.098937	  0.101843		CURRENT LEARNING RATE: 0.375255864418534
previous_iter_valid_loss : 0.11211526393890381

     28700	  0.111877	  0.112115	  0.102023		CURRENT LEARNING RATE: 0.37488079611952063
previous_iter_valid_loss : 0.11418966203927994

     28800	  0.113946	  0.114190	  0.102286		CURRENT LEARNING RATE: 0.37450610270133466
previous_iter_valid_loss : 0.09575623273849487

     28900	  0.095573	  0.095756	  0.102291		CURRENT LEARNING RATE: 0.37413178378928263
previous_iter_valid_loss : 0.08961904793977737

     29000	  0.089332	  0.089619	  0.102303		CURRENT LEARNING RATE: 0.3737578390090455
previous_iter_valid_loss : 0.1363552212715149

     29100	  0.136126	  0.136355	  0.102250		CURRENT LEARNING RATE: 0.37338426798667856
previous_iter_valid_loss : 0.14224280416965485

     29200	  0.141957	  0.142243	  0.102751		CURRENT LEARNING RATE: 0.37301107034861075
previous_iter_valid_loss : 0.1262453943490982

     29300	  0.126020	  0.126245	  0.102979		CURRENT LEARNING RATE: 0.37263824572164433
previous_iter_valid_loss : 0.09876969456672668

     29400	  0.098611	  0.098770	  0.103094		CURRENT LEARNING RATE: 0.3722657937329547
previous_iter_valid_loss : 0.16633997857570648

     29500	  0.166228	  0.166340	  0.103919		CURRENT LEARNING RATE: 0.3718937140100898
previous_iter_valid_loss : 0.12651264667510986

     29600	  0.126280	  0.126513	  0.103902		CURRENT LEARNING RATE: 0.3715220061809699
previous_iter_valid_loss : 0.10816919058561325

     29700	  0.107931	  0.108169	  0.103340		CURRENT LEARNING RATE: 0.3711506698738872
previous_iter_valid_loss : 0.09137509018182755

     29800	  0.091326	  0.091375	  0.103279		CURRENT LEARNING RATE: 0.37077970471750527
previous_iter_valid_loss : 0.1651495099067688

     29900	  0.164912	  0.165150	  0.103973		CURRENT LEARNING RATE: 0.37040911034085894
previous_iter_valid_loss : 0.12522153556346893

     30000	  0.125350	  0.125222	  0.104137		CURRENT LEARNING RATE: 0.3700388863733538
previous_iter_valid_loss : 0.09030041843652725

     30100	  0.090208	  0.090300	  0.104143		CURRENT LEARNING RATE: 0.36966903244476595
previous_iter_valid_loss : 0.08383217453956604

     30200	  0.083844	  0.083832	  0.104082		CURRENT LEARNING RATE: 0.3692995481852413
previous_iter_valid_loss : 0.1311464011669159

     30300	  0.131382	  0.131146	  0.104428		CURRENT LEARNING RATE: 0.36893043322529556
previous_iter_valid_loss : 0.09958239644765854

     30400	  0.099610	  0.099582	  0.104503		CURRENT LEARNING RATE: 0.3685616871958139
previous_iter_valid_loss : 0.13169309496879578

     30500	  0.131534	  0.131693	  0.104286		CURRENT LEARNING RATE: 0.36819330972805003
previous_iter_valid_loss : 0.09909709542989731

     30600	  0.098909	  0.099097	  0.104209		CURRENT LEARNING RATE: 0.36782530045362666
previous_iter_valid_loss : 0.11169499903917313

     30700	  0.111466	  0.111695	  0.104063		CURRENT LEARNING RATE: 0.36745765900453436
previous_iter_valid_loss : 0.088705874979496

     30800	  0.088600	  0.088706	  0.104065		CURRENT LEARNING RATE: 0.3670903850131317
previous_iter_valid_loss : 0.12366500496864319

     30900	  0.123502	  0.123665	  0.104309		CURRENT LEARNING RATE: 0.3667234781121446
previous_iter_valid_loss : 0.08835713565349579

     31000	  0.088328	  0.088357	  0.104313		CURRENT LEARNING RATE: 0.3663569379346662
previous_iter_valid_loss : 0.10767855495214462

     31100	  0.107462	  0.107679	  0.104469		CURRENT LEARNING RATE: 0.3659907641141563
previous_iter_valid_loss : 0.10302754491567612

     31200	  0.102852	  0.103028	  0.104268		CURRENT LEARNING RATE: 0.365624956284441
previous_iter_valid_loss : 0.1270969808101654

     31300	  0.126869	  0.127097	  0.104645		CURRENT LEARNING RATE: 0.36525951407971247
previous_iter_valid_loss : 0.12633496522903442

     31400	  0.126046	  0.126335	  0.104810		CURRENT LEARNING RATE: 0.3648944371345284
previous_iter_valid_loss : 0.10726732015609741

     31500	  0.107007	  0.107267	  0.104990		CURRENT LEARNING RATE: 0.3645297250838119
previous_iter_valid_loss : 0.10439161211252213

     31600	  0.104209	  0.104392	  0.104790		CURRENT LEARNING RATE: 0.36416537756285083
previous_iter_valid_loss : 0.13874609768390656

     31700	  0.138620	  0.138746	  0.105271		CURRENT LEARNING RATE: 0.36380139420729773
previous_iter_valid_loss : 0.08183712512254715

     31800	  0.081863	  0.081837	  0.105065		CURRENT LEARNING RATE: 0.3634377746531691
previous_iter_valid_loss : 0.09981681406497955

     31900	  0.099621	  0.099817	  0.105191		CURRENT LEARNING RATE: 0.36307451853684547
previous_iter_valid_loss : 0.10977133363485336

     32000	  0.109585	  0.109771	  0.105317		CURRENT LEARNING RATE: 0.3627116254950706
previous_iter_valid_loss : 0.11210214346647263

     32100	  0.111994	  0.112102	  0.105063		CURRENT LEARNING RATE: 0.36234909516495145
previous_iter_valid_loss : 0.16093824803829193

     32200	  0.160739	  0.160938	  0.105470		CURRENT LEARNING RATE: 0.36198692718395764
previous_iter_valid_loss : 0.09911143779754639

     32300	  0.098950	  0.099111	  0.105316		CURRENT LEARNING RATE: 0.3616251211899212
previous_iter_valid_loss : 0.15209414064884186

     32400	  0.151953	  0.152094	  0.105947		CURRENT LEARNING RATE: 0.3612636768210361
previous_iter_valid_loss : 0.10105939209461212

     32500	  0.100882	  0.101059	  0.106018		CURRENT LEARNING RATE: 0.3609025937158579
previous_iter_valid_loss : 0.10993076860904694

     32600	  0.109771	  0.109931	  0.106134		CURRENT LEARNING RATE: 0.3605418715133035
previous_iter_valid_loss : 0.11987842619419098

     32700	  0.120082	  0.119878	  0.106486		CURRENT LEARNING RATE: 0.3601815098526507
previous_iter_valid_loss : 0.10195080935955048

     32800	  0.101809	  0.101951	  0.106584		CURRENT LEARNING RATE: 0.3598215083735377
previous_iter_valid_loss : 0.08726280182600021

     32900	  0.087321	  0.087263	  0.106550		CURRENT LEARNING RATE: 0.3594618667159631
previous_iter_valid_loss : 0.18525399267673492

     33000	  0.186076	  0.185254	  0.107453		CURRENT LEARNING RATE: 0.35910258452028515
previous_iter_valid_loss : 0.09613440930843353

     33100	  0.095879	  0.096134	  0.107190		CURRENT LEARNING RATE: 0.35874366142722164
previous_iter_valid_loss : 0.09636104851961136

     33200	  0.096366	  0.096361	  0.107092		CURRENT LEARNING RATE: 0.3583850970778495
previous_iter_valid_loss : 0.14216512441635132

     33300	  0.142370	  0.142165	  0.107520		CURRENT LEARNING RATE: 0.35802689111360425
previous_iter_valid_loss : 0.12682123482227325

     33400	  0.126663	  0.126821	  0.107775		CURRENT LEARNING RATE: 0.35766904317627995
previous_iter_valid_loss : 0.0910254642367363

     33500	  0.090986	  0.091025	  0.107364		CURRENT LEARNING RATE: 0.35731155290802863
previous_iter_valid_loss : 0.14651893079280853

     33600	  0.146334	  0.146519	  0.107356		CURRENT LEARNING RATE: 0.35695441995136
previous_iter_valid_loss : 0.12450253218412399

     33700	  0.124226	  0.124503	  0.107408		CURRENT LEARNING RATE: 0.3565976439491411
previous_iter_valid_loss : 0.10143040120601654

     33800	  0.101262	  0.101430	  0.107140		CURRENT LEARNING RATE: 0.3562412245445959
previous_iter_valid_loss : 0.09921354055404663

     33900	  0.099532	  0.099214	  0.107153		CURRENT LEARNING RATE: 0.3558851613813048
previous_iter_valid_loss : 0.11343451589345932

     34000	  0.113472	  0.113435	  0.107300		CURRENT LEARNING RATE: 0.35552945410320486
previous_iter_valid_loss : 0.16037985682487488

     34100	  0.160259	  0.160380	  0.108019		CURRENT LEARNING RATE: 0.35517410235458863
previous_iter_valid_loss : 0.11542438715696335

     34200	  0.115622	  0.115424	  0.108353		CURRENT LEARNING RATE: 0.35481910578010434
previous_iter_valid_loss : 0.11572015285491943

     34300	  0.115923	  0.115720	  0.108659		CURRENT LEARNING RATE: 0.3544644640247554
previous_iter_valid_loss : 0.10866004228591919

     34400	  0.108491	  0.108660	  0.108887		CURRENT LEARNING RATE: 0.3541101767339
previous_iter_valid_loss : 0.12263309955596924

     34500	  0.122849	  0.122633	  0.109253		CURRENT LEARNING RATE: 0.35375624355325086
previous_iter_valid_loss : 0.11570873111486435

     34600	  0.115537	  0.115709	  0.109572		CURRENT LEARNING RATE: 0.3534026641288747
previous_iter_valid_loss : 0.1041320413351059

     34700	  0.103927	  0.104132	  0.109786		CURRENT LEARNING RATE: 0.3530494381071922
previous_iter_valid_loss : 0.09164367616176605

     34800	  0.091580	  0.091644	  0.109545		CURRENT LEARNING RATE: 0.3526965651349772
previous_iter_valid_loss : 0.13053035736083984

     34900	  0.130676	  0.130530	  0.109416		CURRENT LEARNING RATE: 0.3523440448593567
previous_iter_valid_loss : 0.09243594855070114

     35000	  0.092407	  0.092436	  0.109381		CURRENT LEARNING RATE: 0.3519918769278105
previous_iter_valid_loss : 0.0886850580573082

     35100	  0.088592	  0.088685	  0.109347		CURRENT LEARNING RATE: 0.35164006098817047
previous_iter_valid_loss : 0.0996028482913971

     35200	  0.099364	  0.099603	  0.109122		CURRENT LEARNING RATE: 0.3512885966886208
previous_iter_valid_loss : 0.09351246803998947

     35300	  0.093301	  0.093512	  0.109201		CURRENT LEARNING RATE: 0.350937483677697
previous_iter_valid_loss : 0.10243789106607437

     35400	  0.102276	  0.102438	  0.109400		CURRENT LEARNING RATE: 0.3505867216042862
previous_iter_valid_loss : 0.09568509459495544

     35500	  0.095674	  0.095685	  0.109353		CURRENT LEARNING RATE: 0.3502363101176262
previous_iter_valid_loss : 0.10462941229343414

     35600	  0.104714	  0.104629	  0.109522		CURRENT LEARNING RATE: 0.3498862488673055
previous_iter_valid_loss : 0.1248648464679718

     35700	  0.125141	  0.124865	  0.109858		CURRENT LEARNING RATE: 0.34953653750326286
previous_iter_valid_loss : 0.12563298642635345

     35800	  0.125862	  0.125633	  0.110268		CURRENT LEARNING RATE: 0.3491871756757868
previous_iter_valid_loss : 0.13850611448287964

     35900	  0.138850	  0.138506	  0.110696		CURRENT LEARNING RATE: 0.3488381630355155
previous_iter_valid_loss : 0.09006308019161224

     36000	  0.090013	  0.090063	  0.110675		CURRENT LEARNING RATE: 0.34848949923343636
previous_iter_valid_loss : 0.09618927538394928

     36100	  0.096137	  0.096189	  0.110584		CURRENT LEARNING RATE: 0.3481411839208855
previous_iter_valid_loss : 0.10546843707561493

     36200	  0.105330	  0.105468	  0.110781		CURRENT LEARNING RATE: 0.34779321674954755
previous_iter_valid_loss : 0.13633939623832703

     36300	  0.136221	  0.136339	  0.111260		CURRENT LEARNING RATE: 0.3474455973714553
previous_iter_valid_loss : 0.15073031187057495

     36400	  0.151077	  0.150730	  0.111923		CURRENT LEARNING RATE: 0.34709832543898944
previous_iter_valid_loss : 0.11261563748121262

     36500	  0.112502	  0.112616	  0.112225		CURRENT LEARNING RATE: 0.3467514006048779
previous_iter_valid_loss : 0.09772790968418121

     36600	  0.097616	  0.097728	  0.112037		CURRENT LEARNING RATE: 0.34640482252219584
previous_iter_valid_loss : 0.1221037358045578

     36700	  0.122406	  0.122104	  0.112097		CURRENT LEARNING RATE: 0.3460585908443652
previous_iter_valid_loss : 0.15455062687397003

     36800	  0.154961	  0.154551	  0.112677		CURRENT LEARNING RATE: 0.34571270522515424
previous_iter_valid_loss : 0.11071690917015076

     36900	  0.110937	  0.110717	  0.112656		CURRENT LEARNING RATE: 0.34536716531867734
previous_iter_valid_loss : 0.08863630145788193

     37000	  0.088548	  0.088636	  0.112675		CURRENT LEARNING RATE: 0.3450219707793945
previous_iter_valid_loss : 0.1078014075756073

     37100	  0.107622	  0.107801	  0.112540		CURRENT LEARNING RATE: 0.3446771212621112
previous_iter_valid_loss : 0.08622020483016968

     37200	  0.086049	  0.086220	  0.112137		CURRENT LEARNING RATE: 0.3443326164219779
previous_iter_valid_loss : 0.1003781110048294

     37300	  0.100206	  0.100378	  0.111792		CURRENT LEARNING RATE: 0.34398845591448973
previous_iter_valid_loss : 0.10376420617103577

     37400	  0.103593	  0.103764	  0.111891		CURRENT LEARNING RATE: 0.3436446393954861
previous_iter_valid_loss : 0.09096792340278625

     37500	  0.090844	  0.090968	  0.111958		CURRENT LEARNING RATE: 0.3433011665211505
previous_iter_valid_loss : 0.0954994186758995

     37600	  0.095408	  0.095499	  0.111908		CURRENT LEARNING RATE: 0.3429580369480101
previous_iter_valid_loss : 0.0908980444073677

     37700	  0.090783	  0.090898	  0.111798		CURRENT LEARNING RATE: 0.34261525033293516
previous_iter_valid_loss : 0.12758320569992065

     37800	  0.127348	  0.127583	  0.112161		CURRENT LEARNING RATE: 0.3422728063331391
previous_iter_valid_loss : 0.16527898609638214

     37900	  0.165066	  0.165279	  0.113005		CURRENT LEARNING RATE: 0.3419307046061779
previous_iter_valid_loss : 0.0966026708483696

     38000	  0.096466	  0.096603	  0.113022		CURRENT LEARNING RATE: 0.34158894480994983
previous_iter_valid_loss : 0.14499224722385406

     38100	  0.144875	  0.144992	  0.113541		CURRENT LEARNING RATE: 0.34124752660269503
previous_iter_valid_loss : 0.10280416905879974

     38200	  0.102718	  0.102804	  0.113609		CURRENT LEARNING RATE: 0.34090644964299527
previous_iter_valid_loss : 0.10235586017370224

     38300	  0.102228	  0.102356	  0.113236		CURRENT LEARNING RATE: 0.34056571358977356
previous_iter_valid_loss : 0.08064392954111099

     38400	  0.080590	  0.080644	  0.112953		CURRENT LEARNING RATE: 0.34022531810229384
previous_iter_valid_loss : 0.11118976026773453

     38500	  0.111376	  0.111190	  0.113212		CURRENT LEARNING RATE: 0.3398852628401605
previous_iter_valid_loss : 0.09554728865623474

     38600	  0.095708	  0.095547	  0.113178		CURRENT LEARNING RATE: 0.3395455474633184
previous_iter_valid_loss : 0.146156907081604

     38700	  0.146522	  0.146157	  0.113518		CURRENT LEARNING RATE: 0.339206171632052
previous_iter_valid_loss : 0.11264656484127045

     38800	  0.112921	  0.112647	  0.113503		CURRENT LEARNING RATE: 0.33886713500698556
previous_iter_valid_loss : 0.09077232331037521

     38900	  0.090761	  0.090772	  0.113453		CURRENT LEARNING RATE: 0.3385284372490823
previous_iter_valid_loss : 0.1094791516661644

     39000	  0.109539	  0.109479	  0.113652		CURRENT LEARNING RATE: 0.33819007801964457
previous_iter_valid_loss : 0.0951555073261261

     39100	  0.095110	  0.095156	  0.113240		CURRENT LEARNING RATE: 0.337852056980313
previous_iter_valid_loss : 0.09265194088220596

     39200	  0.092798	  0.092652	  0.112744		CURRENT LEARNING RATE: 0.3375143737930666
previous_iter_valid_loss : 0.09794074296951294

     39300	  0.098106	  0.097941	  0.112461		CURRENT LEARNING RATE: 0.3371770281202221
previous_iter_valid_loss : 0.12476977705955505

     39400	  0.125213	  0.124770	  0.112721		CURRENT LEARNING RATE: 0.33684001962443383
previous_iter_valid_loss : 0.1000494733452797

     39500	  0.100141	  0.100049	  0.112058		CURRENT LEARNING RATE: 0.3365033479686932
previous_iter_valid_loss : 0.1205260306596756

     39600	  0.120725	  0.120526	  0.111998		CURRENT LEARNING RATE: 0.3361670128163286
previous_iter_valid_loss : 0.116043820977211

     39700	  0.116199	  0.116044	  0.112077		CURRENT LEARNING RATE: 0.3358310138310049
previous_iter_valid_loss : 0.13751840591430664

     39800	  0.137886	  0.137518	  0.112538		CURRENT LEARNING RATE: 0.3354953506767229
previous_iter_valid_loss : 0.14282958209514618

     39900	  0.143313	  0.142830	  0.112315		CURRENT LEARNING RATE: 0.33516002301781966
previous_iter_valid_loss : 0.11795996874570847

     40000	  0.118335	  0.117960	  0.112242		CURRENT LEARNING RATE: 0.3348250305189673
previous_iter_valid_loss : 0.09415736794471741

     40100	  0.094387	  0.094157	  0.112281		CURRENT LEARNING RATE: 0.33449037284517336
previous_iter_valid_loss : 0.11405572295188904

     40200	  0.114407	  0.114056	  0.112583		CURRENT LEARNING RATE: 0.33415604966178014
previous_iter_valid_loss : 0.08707379549741745

     40300	  0.087290	  0.087074	  0.112142		CURRENT LEARNING RATE: 0.33382206063446446
previous_iter_valid_loss : 0.09478344023227692

     40400	  0.094982	  0.094783	  0.112094		CURRENT LEARNING RATE: 0.3334884054292372
previous_iter_valid_loss : 0.10836593061685562

     40500	  0.108311	  0.108366	  0.111861		CURRENT LEARNING RATE: 0.3331550837124432
previous_iter_valid_loss : 0.11336249858140945

     40600	  0.113554	  0.113362	  0.112004		CURRENT LEARNING RATE: 0.3328220951507606
previous_iter_valid_loss : 0.13480915129184723

     40700	  0.135187	  0.134809	  0.112235		CURRENT LEARNING RATE: 0.33248943941120096
previous_iter_valid_loss : 0.09945613890886307

     40800	  0.099540	  0.099456	  0.112342		CURRENT LEARNING RATE: 0.3321571161611084
previous_iter_valid_loss : 0.11043991893529892

     40900	  0.110741	  0.110440	  0.112210		CURRENT LEARNING RATE: 0.3318251250681597
previous_iter_valid_loss : 0.09805972129106522

     41000	  0.098219	  0.098060	  0.112307		CURRENT LEARNING RATE: 0.3314934658003637
previous_iter_valid_loss : 0.0961456224322319

     41100	  0.096177	  0.096146	  0.112192		CURRENT LEARNING RATE: 0.33116213802606115
previous_iter_valid_loss : 0.09831403940916061

     41200	  0.098456	  0.098314	  0.112145		CURRENT LEARNING RATE: 0.33083114141392417
previous_iter_valid_loss : 0.09668917208909988

     41300	  0.096646	  0.096689	  0.111841		CURRENT LEARNING RATE: 0.33050047563295626
previous_iter_valid_loss : 0.10814699530601501

     41400	  0.108378	  0.108147	  0.111659		CURRENT LEARNING RATE: 0.3301701403524914
previous_iter_valid_loss : 0.12178361415863037

     41500	  0.122000	  0.121784	  0.111804		CURRENT LEARNING RATE: 0.3298401352421945
previous_iter_valid_loss : 0.10891389846801758

     41600	  0.108985	  0.108914	  0.111849		CURRENT LEARNING RATE: 0.32951045997206035
previous_iter_valid_loss : 0.10512981563806534

     41700	  0.105292	  0.105130	  0.111513		CURRENT LEARNING RATE: 0.3291811142124136
previous_iter_valid_loss : 0.11437064409255981

     41800	  0.114598	  0.114371	  0.111838		CURRENT LEARNING RATE: 0.3288520976339085
previous_iter_valid_loss : 0.11585374921560287

     41900	  0.116081	  0.115854	  0.111999		CURRENT LEARNING RATE: 0.3285234099075284
previous_iter_valid_loss : 0.09383751451969147

     42000	  0.093794	  0.093838	  0.111839		CURRENT LEARNING RATE: 0.3281950507045856
previous_iter_valid_loss : 0.08799322694540024

     42100	  0.087949	  0.087993	  0.111598		CURRENT LEARNING RATE: 0.3278670196967209
previous_iter_valid_loss : 0.09382109344005585

     42200	  0.093734	  0.093821	  0.110927		CURRENT LEARNING RATE: 0.3275393165559031
previous_iter_valid_loss : 0.12048470228910446

     42300	  0.120330	  0.120485	  0.111141		CURRENT LEARNING RATE: 0.3272119409544293
previous_iter_valid_loss : 0.08679228276014328

     42400	  0.086691	  0.086792	  0.110488		CURRENT LEARNING RATE: 0.32688489256492365
previous_iter_valid_loss : 0.14452821016311646

     42500	  0.144331	  0.144528	  0.110922		CURRENT LEARNING RATE: 0.3265581710603378
previous_iter_valid_loss : 0.08283314108848572

     42600	  0.082742	  0.082833	  0.110651		CURRENT LEARNING RATE: 0.3262317761139502
previous_iter_valid_loss : 0.08315511047840118

     42700	  0.083167	  0.083155	  0.110284		CURRENT LEARNING RATE: 0.325905707399366
previous_iter_valid_loss : 0.10274989902973175

     42800	  0.102594	  0.102750	  0.110292		CURRENT LEARNING RATE: 0.32557996459051625
previous_iter_valid_loss : 0.08724098652601242

     42900	  0.087140	  0.087241	  0.110292		CURRENT LEARNING RATE: 0.32525454736165826
previous_iter_valid_loss : 0.12704136967658997

     43000	  0.126735	  0.127041	  0.109710		CURRENT LEARNING RATE: 0.3249294553873748
previous_iter_valid_loss : 0.10767338424921036

     43100	  0.107427	  0.107673	  0.109825		CURRENT LEARNING RATE: 0.3246046883425737
previous_iter_valid_loss : 0.11594488471746445

     43200	  0.116208	  0.115945	  0.110021		CURRENT LEARNING RATE: 0.32428024590248805
previous_iter_valid_loss : 0.09585385024547577

     43300	  0.095917	  0.095854	  0.109558		CURRENT LEARNING RATE: 0.3239561277426753
previous_iter_valid_loss : 0.10243693739175797

     43400	  0.102696	  0.102437	  0.109314		CURRENT LEARNING RATE: 0.3236323335390173
previous_iter_valid_loss : 0.11150677502155304

     43500	  0.111663	  0.111507	  0.109519		CURRENT LEARNING RATE: 0.3233088629677198
previous_iter_valid_loss : 0.09120018780231476

     43600	  0.091248	  0.091200	  0.108966		CURRENT LEARNING RATE: 0.32298571570531226
previous_iter_valid_loss : 0.09764761477708817

     43700	  0.097730	  0.097648	  0.108697		CURRENT LEARNING RATE: 0.3226628914286473
previous_iter_valid_loss : 0.11119037866592407

     43800	  0.111070	  0.111190	  0.108795		CURRENT LEARNING RATE: 0.32234038981490065
previous_iter_valid_loss : 0.10211717337369919

     43900	  0.102049	  0.102117	  0.108824		CURRENT LEARNING RATE: 0.3220182105415707
previous_iter_valid_loss : 0.11959504336118698

     44000	  0.119489	  0.119595	  0.108885		CURRENT LEARNING RATE: 0.3216963532864781
previous_iter_valid_loss : 0.08964052051305771

     44100	  0.089460	  0.089641	  0.108178		CURRENT LEARNING RATE: 0.3213748177277656
previous_iter_valid_loss : 0.10707646608352661

     44200	  0.106837	  0.107076	  0.108095		CURRENT LEARNING RATE: 0.3210536035438976
previous_iter_valid_loss : 0.08980307728052139

     44300	  0.089803	  0.089803	  0.107835		CURRENT LEARNING RATE: 0.3207327104136599
previous_iter_valid_loss : 0.13411086797714233

     44400	  0.133917	  0.134111	  0.108090		CURRENT LEARNING RATE: 0.32041213801615936
previous_iter_valid_loss : 0.1150795966386795

     44500	  0.114923	  0.115080	  0.108014		CURRENT LEARNING RATE: 0.32009188603082356
previous_iter_valid_loss : 0.10544431209564209

     44600	  0.105260	  0.105444	  0.107912		CURRENT LEARNING RATE: 0.31977195413740045
previous_iter_valid_loss : 0.0831368938088417

     44700	  0.083021	  0.083137	  0.107702		CURRENT LEARNING RATE: 0.3194523420159581
previous_iter_valid_loss : 0.09167855978012085

     44800	  0.091550	  0.091679	  0.107702		CURRENT LEARNING RATE: 0.3191330493468844
previous_iter_valid_loss : 0.1197531595826149

     44900	  0.119534	  0.119753	  0.107594		CURRENT LEARNING RATE: 0.31881407581088667
previous_iter_valid_loss : 0.09642300009727478

     45000	  0.096298	  0.096423	  0.107634		CURRENT LEARNING RATE: 0.31849542108899126
previous_iter_valid_loss : 0.08281798660755157

     45100	  0.082755	  0.082818	  0.107576		CURRENT LEARNING RATE: 0.31817708486254354
previous_iter_valid_loss : 0.11990877240896225

     45200	  0.119772	  0.119909	  0.107779		CURRENT LEARNING RATE: 0.31785906681320714
previous_iter_valid_loss : 0.10205124318599701

     45300	  0.101915	  0.102051	  0.107864		CURRENT LEARNING RATE: 0.31754136662296406
previous_iter_valid_loss : 0.09560459107160568

     45400	  0.095478	  0.095605	  0.107796		CURRENT LEARNING RATE: 0.3172239839741141
previous_iter_valid_loss : 0.0954190045595169

     45500	  0.095251	  0.095419	  0.107793		CURRENT LEARNING RATE: 0.3169069185492745
previous_iter_valid_loss : 0.10788042843341827

     45600	  0.107707	  0.107880	  0.107826		CURRENT LEARNING RATE: 0.3165901700313799
previous_iter_valid_loss : 0.08785378187894821

     45700	  0.087802	  0.087854	  0.107455		CURRENT LEARNING RATE: 0.3162737381036817
previous_iter_valid_loss : 0.10440871864557266

     45800	  0.104272	  0.104409	  0.107243		CURRENT LEARNING RATE: 0.31595762244974795
previous_iter_valid_loss : 0.1023348942399025

     45900	  0.102197	  0.102335	  0.106882		CURRENT LEARNING RATE: 0.315641822753463
previous_iter_valid_loss : 0.11520854383707047

     46000	  0.115091	  0.115209	  0.107133		CURRENT LEARNING RATE: 0.3153263386990271
previous_iter_valid_loss : 0.08406348526477814

     46100	  0.084051	  0.084063	  0.107012		CURRENT LEARNING RATE: 0.31501116997095613
previous_iter_valid_loss : 0.08031827956438065

     46200	  0.080358	  0.080318	  0.106760		CURRENT LEARNING RATE: 0.31469631625408145
previous_iter_valid_loss : 0.15165476500988007

     46300	  0.152048	  0.151655	  0.106913		CURRENT LEARNING RATE: 0.3143817772335492
previous_iter_valid_loss : 0.11816371977329254

     46400	  0.118328	  0.118164	  0.106588		CURRENT LEARNING RATE: 0.3140675525948204
previous_iter_valid_loss : 0.1403590887784958

     46500	  0.140201	  0.140359	  0.106865		CURRENT LEARNING RATE: 0.31375364202367034
previous_iter_valid_loss : 0.11625605821609497

     46600	  0.116284	  0.116256	  0.107050		CURRENT LEARNING RATE: 0.3134400452061885
previous_iter_valid_loss : 0.09502896666526794

     46700	  0.095080	  0.095029	  0.106780		CURRENT LEARNING RATE: 0.31312676182877797
previous_iter_valid_loss : 0.09209839254617691

     46800	  0.092208	  0.092098	  0.106155		CURRENT LEARNING RATE: 0.31281379157815536
previous_iter_valid_loss : 0.11614784598350525

     46900	  0.116125	  0.116148	  0.106209		CURRENT LEARNING RATE: 0.3125011341413504
previous_iter_valid_loss : 0.1049671620130539

     47000	  0.104919	  0.104967	  0.106373		CURRENT LEARNING RATE: 0.31218878920570564
previous_iter_valid_loss : 0.12909294664859772

     47100	  0.129295	  0.129093	  0.106586		CURRENT LEARNING RATE: 0.3118767564588761
previous_iter_valid_loss : 0.08562346547842026

     47200	  0.085555	  0.085623	  0.106580		CURRENT LEARNING RATE: 0.3115650355888289
previous_iter_valid_loss : 0.1053413525223732

     47300	  0.105254	  0.105341	  0.106629		CURRENT LEARNING RATE: 0.3112536262838434
previous_iter_valid_loss : 0.10250051319599152

     47400	  0.102455	  0.102501	  0.106617		CURRENT LEARNING RATE: 0.31094252823251006
previous_iter_valid_loss : 0.09096085280179977

     47500	  0.090960	  0.090961	  0.106617		CURRENT LEARNING RATE: 0.3106317411237309
previous_iter_valid_loss : 0.08952971547842026

     47600	  0.089535	  0.089530	  0.106557		CURRENT LEARNING RATE: 0.31032126464671866
previous_iter_valid_loss : 0.08999692648649216

     47700	  0.089850	  0.089997	  0.106548		CURRENT LEARNING RATE: 0.310011098490997
previous_iter_valid_loss : 0.09845775365829468

     47800	  0.098447	  0.098458	  0.106257		CURRENT LEARNING RATE: 0.3097012423463996
previous_iter_valid_loss : 0.1353062391281128

     47900	  0.135174	  0.135306	  0.105957		CURRENT LEARNING RATE: 0.3093916959030704
previous_iter_valid_loss : 0.09948360919952393

     48000	  0.099410	  0.099484	  0.105986		CURRENT LEARNING RATE: 0.3090824588514629
previous_iter_valid_loss : 0.1284775286912918

     48100	  0.128752	  0.128478	  0.105821		CURRENT LEARNING RATE: 0.30877353088234
previous_iter_valid_loss : 0.10018936544656754

     48200	  0.100340	  0.100189	  0.105794		CURRENT LEARNING RATE: 0.3084649116867737
previous_iter_valid_loss : 0.08814780414104462

     48300	  0.087973	  0.088148	  0.105652		CURRENT LEARNING RATE: 0.30815660095614483
previous_iter_valid_loss : 0.09162117540836334

     48400	  0.092040	  0.091621	  0.105762		CURRENT LEARNING RATE: 0.30784859838214257
previous_iter_valid_loss : 0.09940982609987259

     48500	  0.099554	  0.099410	  0.105644		CURRENT LEARNING RATE: 0.30754090365676434
previous_iter_valid_loss : 0.08433296531438828

     48600	  0.084423	  0.084333	  0.105532		CURRENT LEARNING RATE: 0.3072335164723154
previous_iter_valid_loss : 0.08610039204359055

     48700	  0.086127	  0.086100	  0.104932		CURRENT LEARNING RATE: 0.30692643652140855
previous_iter_valid_loss : 0.09620263427495956

     48800	  0.096346	  0.096203	  0.104767		CURRENT LEARNING RATE: 0.30661966349696373
previous_iter_valid_loss : 0.10386303067207336

     48900	  0.103877	  0.103863	  0.104898		CURRENT LEARNING RATE: 0.30631319709220806
previous_iter_valid_loss : 0.09778489172458649

     49000	  0.098147	  0.097785	  0.104781		CURRENT LEARNING RATE: 0.30600703700067494
previous_iter_valid_loss : 0.08812340348958969

     49100	  0.088284	  0.088123	  0.104711		CURRENT LEARNING RATE: 0.30570118291620435
previous_iter_valid_loss : 0.09725047647953033

     49200	  0.097302	  0.097250	  0.104757		CURRENT LEARNING RATE: 0.3053956345329421
previous_iter_valid_loss : 0.08464011549949646

     49300	  0.084604	  0.084640	  0.104624		CURRENT LEARNING RATE: 0.3050903915453399
previous_iter_valid_loss : 0.1661348193883896

     49400	  0.165945	  0.166135	  0.105037		CURRENT LEARNING RATE: 0.3047854536481546
previous_iter_valid_loss : 0.10127337276935577

     49500	  0.101255	  0.101273	  0.105050		CURRENT LEARNING RATE: 0.3044808205364484
previous_iter_valid_loss : 0.08729583024978638

     49600	  0.087487	  0.087296	  0.104717		CURRENT LEARNING RATE: 0.30417649190558815
previous_iter_valid_loss : 0.12818415462970734

     49700	  0.128042	  0.128184	  0.104839		CURRENT LEARNING RATE: 0.3038724674512451
previous_iter_valid_loss : 0.10543975234031677

     49800	  0.105568	  0.105440	  0.104518		CURRENT LEARNING RATE: 0.30356874686939483
previous_iter_valid_loss : 0.11933688819408417

     49900	  0.119259	  0.119337	  0.104283		CURRENT LEARNING RATE: 0.3032653298563167
previous_iter_valid_loss : 0.10663320124149323

     50000	  0.106540	  0.106633	  0.104170		CURRENT LEARNING RATE: 0.30296221610859375
previous_iter_valid_loss : 0.1024366021156311

     50100	  0.102376	  0.102437	  0.104253		CURRENT LEARNING RATE: 0.30265940532311214
previous_iter_valid_loss : 0.09483630955219269

     50200	  0.094753	  0.094836	  0.104060		CURRENT LEARNING RATE: 0.30235689719706105
previous_iter_valid_loss : 0.08874307572841644

     50300	  0.088793	  0.088743	  0.104077		CURRENT LEARNING RATE: 0.30205469142793234
previous_iter_valid_loss : 0.09257412701845169

     50400	  0.092461	  0.092574	  0.104055		CURRENT LEARNING RATE: 0.30175278771352027
previous_iter_valid_loss : 0.09538578242063522

     50500	  0.095401	  0.095386	  0.103925		CURRENT LEARNING RATE: 0.30145118575192104
previous_iter_valid_loss : 0.08831580728292465

     50600	  0.088198	  0.088316	  0.103675		CURRENT LEARNING RATE: 0.3011498852415327
previous_iter_valid_loss : 0.09975399821996689

     50700	  0.099776	  0.099754	  0.103324		CURRENT LEARNING RATE: 0.3008488858810547
previous_iter_valid_loss : 0.09194225817918777

     50800	  0.091952	  0.091942	  0.103249		CURRENT LEARNING RATE: 0.30054818736948763
previous_iter_valid_loss : 0.1395755112171173

     50900	  0.139797	  0.139576	  0.103540		CURRENT LEARNING RATE: 0.30024778940613295
previous_iter_valid_loss : 0.11256876587867737

     51000	  0.112672	  0.112569	  0.103685		CURRENT LEARNING RATE: 0.29994769169059277
previous_iter_valid_loss : 0.13623374700546265

     51100	  0.136528	  0.136234	  0.104086		CURRENT LEARNING RATE: 0.2996478939227692
previous_iter_valid_loss : 0.10943804681301117

     51200	  0.109553	  0.109438	  0.104198		CURRENT LEARNING RATE: 0.2993483958028646
previous_iter_valid_loss : 0.11923748254776001

     51300	  0.119584	  0.119237	  0.104423		CURRENT LEARNING RATE: 0.29904919703138066
previous_iter_valid_loss : 0.17700503766536713

     51400	  0.177016	  0.177005	  0.105112		CURRENT LEARNING RATE: 0.29875029730911873
previous_iter_valid_loss : 0.1630498319864273

     51500	  0.163381	  0.163050	  0.105524		CURRENT LEARNING RATE: 0.298451696337179
previous_iter_valid_loss : 0.1054920181632042

     51600	  0.105585	  0.105492	  0.105490		CURRENT LEARNING RATE: 0.2981533938169605
previous_iter_valid_loss : 0.10685461759567261

     51700	  0.106967	  0.106855	  0.105507		CURRENT LEARNING RATE: 0.2978553894501606
previous_iter_valid_loss : 0.11280091851949692

     51800	  0.112911	  0.112801	  0.105492		CURRENT LEARNING RATE: 0.29755768293877505
previous_iter_valid_loss : 0.10887020081281662

     51900	  0.108955	  0.108870	  0.105422		CURRENT LEARNING RATE: 0.2972602739850972
previous_iter_valid_loss : 0.10750147700309753

     52000	  0.107686	  0.107501	  0.105558		CURRENT LEARNING RATE: 0.29696316229171804
previous_iter_valid_loss : 0.09177859872579575

     52100	  0.091870	  0.091779	  0.105596		CURRENT LEARNING RATE: 0.296666347561526
previous_iter_valid_loss : 0.10212303698062897

     52200	  0.102256	  0.102123	  0.105679		CURRENT LEARNING RATE: 0.29636982949770624
previous_iter_valid_loss : 0.11268626898527145

     52300	  0.112917	  0.112686	  0.105601		CURRENT LEARNING RATE: 0.29607360780374065
previous_iter_valid_loss : 0.10650499910116196

     52400	  0.106583	  0.106505	  0.105798		CURRENT LEARNING RATE: 0.29577768218340755
previous_iter_valid_loss : 0.09806253015995026

     52500	  0.098115	  0.098063	  0.105334		CURRENT LEARNING RATE: 0.2954820523407813
previous_iter_valid_loss : 0.10942843556404114

     52600	  0.109610	  0.109428	  0.105600		CURRENT LEARNING RATE: 0.29518671798023194
previous_iter_valid_loss : 0.09342773258686066

     52700	  0.093509	  0.093428	  0.105703		CURRENT LEARNING RATE: 0.2948916788064252
previous_iter_valid_loss : 0.11406415700912476

     52800	  0.114210	  0.114064	  0.105816		CURRENT LEARNING RATE: 0.29459693452432184
previous_iter_valid_loss : 0.11493764817714691

     52900	  0.115223	  0.114938	  0.106093		CURRENT LEARNING RATE: 0.2943024848391776
previous_iter_valid_loss : 0.11778444051742554

     53000	  0.117926	  0.117784	  0.106000		CURRENT LEARNING RATE: 0.2940083294565427
previous_iter_valid_loss : 0.10321055352687836

     53100	  0.103277	  0.103211	  0.105955		CURRENT LEARNING RATE: 0.2937144680822617
previous_iter_valid_loss : 0.09901183098554611

     53200	  0.099023	  0.099012	  0.105786		CURRENT LEARNING RATE: 0.2934209004224733
previous_iter_valid_loss : 0.10281557589769363

     53300	  0.102797	  0.102816	  0.105856		CURRENT LEARNING RATE: 0.2931276261836098
previous_iter_valid_loss : 0.13038143515586853

     53400	  0.130172	  0.130381	  0.106135		CURRENT LEARNING RATE: 0.29283464507239687
previous_iter_valid_loss : 0.11827082186937332

     53500	  0.118770	  0.118271	  0.106203		CURRENT LEARNING RATE: 0.29254195679585343
previous_iter_valid_loss : 0.18220871686935425

     53600	  0.182630	  0.182209	  0.107113		CURRENT LEARNING RATE: 0.2922495610612912
previous_iter_valid_loss : 0.12099520862102509

     53700	  0.121037	  0.120995	  0.107346		CURRENT LEARNING RATE: 0.29195745757631436
previous_iter_valid_loss : 0.16186833381652832

     53800	  0.162057	  0.161868	  0.107853		CURRENT LEARNING RATE: 0.2916656460488194
previous_iter_valid_loss : 0.11281844973564148

     53900	  0.112489	  0.112818	  0.107960		CURRENT LEARNING RATE: 0.2913741261869948
previous_iter_valid_loss : 0.08925281465053558

     54000	  0.089081	  0.089253	  0.107657		CURRENT LEARNING RATE: 0.2910828976993207
previous_iter_valid_loss : 0.09904725104570389

     54100	  0.098875	  0.099047	  0.107751		CURRENT LEARNING RATE: 0.29079196029456855
previous_iter_valid_loss : 0.12307108938694

     54200	  0.123139	  0.123071	  0.107911		CURRENT LEARNING RATE: 0.29050131368180093
previous_iter_valid_loss : 0.09091538190841675

     54300	  0.090877	  0.090915	  0.107922		CURRENT LEARNING RATE: 0.2902109575703712
previous_iter_valid_loss : 0.10796163976192474

     54400	  0.107831	  0.107962	  0.107660		CURRENT LEARNING RATE: 0.2899208916699232
previous_iter_valid_loss : 0.13220755755901337

     54500	  0.132264	  0.132208	  0.107832		CURRENT LEARNING RATE: 0.289631115690391
previous_iter_valid_loss : 0.14014531672000885

     54600	  0.140364	  0.140145	  0.108179		CURRENT LEARNING RATE: 0.2893416293419987
previous_iter_valid_loss : 0.12262271344661713

     54700	  0.122912	  0.122623	  0.108574		CURRENT LEARNING RATE: 0.2890524323352598
previous_iter_valid_loss : 0.19311222434043884

     54800	  0.193440	  0.193112	  0.109588		CURRENT LEARNING RATE: 0.28876352438097735
previous_iter_valid_loss : 0.12302795052528381

     54900	  0.123088	  0.123028	  0.109621		CURRENT LEARNING RATE: 0.2884749051902433
previous_iter_valid_loss : 0.1626885086297989

     55000	  0.162899	  0.162689	  0.110283		CURRENT LEARNING RATE: 0.2881865744744386
previous_iter_valid_loss : 0.15247370302677155

     55100	  0.152483	  0.152474	  0.110980		CURRENT LEARNING RATE: 0.28789853194523224
previous_iter_valid_loss : 0.11988821625709534

     55200	  0.120014	  0.119888	  0.110980		CURRENT LEARNING RATE: 0.2876107773145819
previous_iter_valid_loss : 0.1693728119134903

     55300	  0.169715	  0.169373	  0.111653		CURRENT LEARNING RATE: 0.28732331029473285
previous_iter_valid_loss : 0.12071296572685242

     55400	  0.120863	  0.120713	  0.111904		CURRENT LEARNING RATE: 0.287036130598218
previous_iter_valid_loss : 0.10393699258565903

     55500	  0.103949	  0.103937	  0.111989		CURRENT LEARNING RATE: 0.28674923793785767
previous_iter_valid_loss : 0.09465103596448898

     55600	  0.094487	  0.094651	  0.111857		CURRENT LEARNING RATE: 0.2864626320267592
previous_iter_valid_loss : 0.10933075845241547

     55700	  0.109061	  0.109331	  0.112072		CURRENT LEARNING RATE: 0.2861763125783166
previous_iter_valid_loss : 0.11855850368738174

     55800	  0.118771	  0.118559	  0.112213		CURRENT LEARNING RATE: 0.28589027930621047
previous_iter_valid_loss : 0.11647379398345947

     55900	  0.116554	  0.116474	  0.112354		CURRENT LEARNING RATE: 0.28560453192440743
previous_iter_valid_loss : 0.12088998407125473

     56000	  0.120869	  0.120890	  0.112411		CURRENT LEARNING RATE: 0.2853190701471601
previous_iter_valid_loss : 0.11537837982177734

     56100	  0.115367	  0.115378	  0.112724		CURRENT LEARNING RATE: 0.2850338936890067
previous_iter_valid_loss : 0.10167977213859558

     56200	  0.101646	  0.101680	  0.112938		CURRENT LEARNING RATE: 0.28474900226477085
previous_iter_valid_loss : 0.18188993632793427

     56300	  0.181737	  0.181890	  0.113240		CURRENT LEARNING RATE: 0.2844643955895609
previous_iter_valid_loss : 0.12599842250347137

     56400	  0.126130	  0.125998	  0.113319		CURRENT LEARNING RATE: 0.28418007337877027
previous_iter_valid_loss : 0.17090550065040588

     56500	  0.171347	  0.170906	  0.113624		CURRENT LEARNING RATE: 0.28389603534807667
previous_iter_valid_loss : 0.1788066178560257

     56600	  0.179319	  0.178807	  0.114250		CURRENT LEARNING RATE: 0.28361228121344206
previous_iter_valid_loss : 0.1563185155391693

     56700	  0.155978	  0.156319	  0.114863		CURRENT LEARNING RATE: 0.2833288106911123
previous_iter_valid_loss : 0.17112518846988678

     56800	  0.170830	  0.171125	  0.115653		CURRENT LEARNING RATE: 0.2830456234976169
previous_iter_valid_loss : 0.14165790379047394

     56900	  0.141907	  0.141658	  0.115908		CURRENT LEARNING RATE: 0.28276271934976854
previous_iter_valid_loss : 0.12230861186981201

     57000	  0.122518	  0.122309	  0.116081		CURRENT LEARNING RATE: 0.2824800979646631
previous_iter_valid_loss : 0.17444531619548798

     57100	  0.174878	  0.174445	  0.116535		CURRENT LEARNING RATE: 0.2821977590596792
previous_iter_valid_loss : 0.13725855946540833

     57200	  0.137899	  0.137259	  0.117051		CURRENT LEARNING RATE: 0.28191570235247787
previous_iter_valid_loss : 0.13806405663490295

     57300	  0.138735	  0.138064	  0.117378		CURRENT LEARNING RATE: 0.28163392756100236
previous_iter_valid_loss : 0.10636543482542038

     57400	  0.106680	  0.106365	  0.117417		CURRENT LEARNING RATE: 0.28135243440347785
previous_iter_valid_loss : 0.13004432618618011

     57500	  0.130102	  0.130044	  0.117808		CURRENT LEARNING RATE: 0.2810712225984112
previous_iter_valid_loss : 0.146540567278862

     57600	  0.146623	  0.146541	  0.118378		CURRENT LEARNING RATE: 0.28079029186459065
previous_iter_valid_loss : 0.12720635533332825

     57700	  0.127308	  0.127206	  0.118750		CURRENT LEARNING RATE: 0.2805096419210853
previous_iter_valid_loss : 0.14505714178085327

     57800	  0.145024	  0.145057	  0.119216		CURRENT LEARNING RATE: 0.2802292724872452
previous_iter_valid_loss : 0.11979607492685318

     57900	  0.120275	  0.119796	  0.119061		CURRENT LEARNING RATE: 0.279949183282701
previous_iter_valid_loss : 0.13891002535820007

     58000	  0.139341	  0.138910	  0.119455		CURRENT LEARNING RATE: 0.2796693740273634
previous_iter_valid_loss : 0.10903854668140411

     58100	  0.109709	  0.109039	  0.119261		CURRENT LEARNING RATE: 0.2793898444414232
previous_iter_valid_loss : 0.12832848727703094

     58200	  0.128212	  0.128328	  0.119542		CURRENT LEARNING RATE: 0.2791105942453506
previous_iter_valid_loss : 0.10447899997234344

     58300	  0.104619	  0.104479	  0.119706		CURRENT LEARNING RATE: 0.2788316231598956
previous_iter_valid_loss : 0.1206066831946373

     58400	  0.120818	  0.120607	  0.119995		CURRENT LEARNING RATE: 0.27855293090608696
previous_iter_valid_loss : 0.12711161375045776

     58500	  0.127142	  0.127112	  0.120272		CURRENT LEARNING RATE: 0.27827451720523244
previous_iter_valid_loss : 0.11030193418264389

     58600	  0.110200	  0.110302	  0.120532		CURRENT LEARNING RATE: 0.27799638177891833
previous_iter_valid_loss : 0.13890309631824493

     58700	  0.139552	  0.138903	  0.121060		CURRENT LEARNING RATE: 0.2777185243490091
previous_iter_valid_loss : 0.11186394095420837

     58800	  0.112024	  0.111864	  0.121217		CURRENT LEARNING RATE: 0.27744094463764746
previous_iter_valid_loss : 0.11020328104496002

     58900	  0.110342	  0.110203	  0.121280		CURRENT LEARNING RATE: 0.27716364236725355
previous_iter_valid_loss : 0.17828160524368286

     59000	  0.178320	  0.178282	  0.122085		CURRENT LEARNING RATE: 0.27688661726052505
previous_iter_valid_loss : 0.11434690654277802

     59100	  0.114560	  0.114347	  0.122347		CURRENT LEARNING RATE: 0.27660986904043694
previous_iter_valid_loss : 0.13470883667469025

     59200	  0.134742	  0.134709	  0.122722		CURRENT LEARNING RATE: 0.2763333974302409
previous_iter_valid_loss : 0.10810084640979767

     59300	  0.108529	  0.108101	  0.122957		CURRENT LEARNING RATE: 0.2760572021534653
previous_iter_valid_loss : 0.20177243649959564

     59400	  0.202224	  0.201772	  0.123313		CURRENT LEARNING RATE: 0.2757812829339149
previous_iter_valid_loss : 0.11136766523122787

     59500	  0.111277	  0.111368	  0.123414		CURRENT LEARNING RATE: 0.27550563949567036
previous_iter_valid_loss : 0.10481578856706619

     59600	  0.104886	  0.104816	  0.123589		CURRENT LEARNING RATE: 0.2752302715630883
previous_iter_valid_loss : 0.12897683680057526

     59700	  0.129414	  0.128977	  0.123597		CURRENT LEARNING RATE: 0.2749551788608008
previous_iter_valid_loss : 0.10734729468822479

     59800	  0.107582	  0.107347	  0.123616		CURRENT LEARNING RATE: 0.274680361113715
previous_iter_valid_loss : 0.13165660202503204

     59900	  0.131784	  0.131657	  0.123739		CURRENT LEARNING RATE: 0.2744058180470132
previous_iter_valid_loss : 0.10997194051742554

     60000	  0.109994	  0.109972	  0.123773		CURRENT LEARNING RATE: 0.27413154938615236
previous_iter_valid_loss : 0.10331501066684723

     60100	  0.103216	  0.103315	  0.123782		CURRENT LEARNING RATE: 0.27385755485686375
previous_iter_valid_loss : 0.16316920518875122

     60200	  0.163154	  0.163169	  0.124465		CURRENT LEARNING RATE: 0.27358383418515275
previous_iter_valid_loss : 0.1228930801153183

     60300	  0.122851	  0.122893	  0.124806		CURRENT LEARNING RATE: 0.2733103870972988
previous_iter_valid_loss : 0.09867504984140396

     60400	  0.098859	  0.098675	  0.124867		CURRENT LEARNING RATE: 0.2730372133198547
previous_iter_valid_loss : 0.10625994950532913

     60500	  0.106108	  0.106260	  0.124976		CURRENT LEARNING RATE: 0.2727643125796467
previous_iter_valid_loss : 0.16107046604156494

     60600	  0.160932	  0.161070	  0.125704		CURRENT LEARNING RATE: 0.272491684603774
previous_iter_valid_loss : 0.12148407101631165

     60700	  0.121673	  0.121484	  0.125921		CURRENT LEARNING RATE: 0.27221932911960856
previous_iter_valid_loss : 0.1260213851928711

     60800	  0.126314	  0.126021	  0.126262		CURRENT LEARNING RATE: 0.27194724585479496
previous_iter_valid_loss : 0.16988913714885712

     60900	  0.170044	  0.169889	  0.126565		CURRENT LEARNING RATE: 0.2716754345372499
previous_iter_valid_loss : 0.13386930525302887

     61000	  0.133765	  0.133869	  0.126778		CURRENT LEARNING RATE: 0.271403894895162
previous_iter_valid_loss : 0.13192781805992126

     61100	  0.131917	  0.131928	  0.126735		CURRENT LEARNING RATE: 0.2711326266569916
previous_iter_valid_loss : 0.11195313930511475

     61200	  0.111833	  0.111953	  0.126760		CURRENT LEARNING RATE: 0.2708616295514705
previous_iter_valid_loss : 0.12064829468727112

     61300	  0.120780	  0.120648	  0.126774		CURRENT LEARNING RATE: 0.27059090330760144
previous_iter_valid_loss : 0.12487748265266418

     61400	  0.125048	  0.124877	  0.126253		CURRENT LEARNING RATE: 0.2703204476546583
previous_iter_valid_loss : 0.13628555834293365

     61500	  0.137036	  0.136286	  0.125985		CURRENT LEARNING RATE: 0.2700502623221853
previous_iter_valid_loss : 0.1191275343298912

     61600	  0.119206	  0.119128	  0.126121		CURRENT LEARNING RATE: 0.26978034703999715
previous_iter_valid_loss : 0.1091669499874115

     61700	  0.109246	  0.109167	  0.126145		CURRENT LEARNING RATE: 0.2695107015381785
previous_iter_valid_loss : 0.11874358355998993

     61800	  0.118747	  0.118744	  0.126204		CURRENT LEARNING RATE: 0.2692413255470839
previous_iter_valid_loss : 0.1426088809967041

     61900	  0.143080	  0.142609	  0.126541		CURRENT LEARNING RATE: 0.26897221879733724
previous_iter_valid_loss : 0.1181892678141594

     62000	  0.118146	  0.118189	  0.126648		CURRENT LEARNING RATE: 0.2687033810198318
previous_iter_valid_loss : 0.10027223080396652

     62100	  0.100197	  0.100272	  0.126733		CURRENT LEARNING RATE: 0.26843481194572977
previous_iter_valid_loss : 0.11283345520496368

     62200	  0.113319	  0.112833	  0.126840		CURRENT LEARNING RATE: 0.2681665113064621
previous_iter_valid_loss : 0.1157369613647461

     62300	  0.116063	  0.115737	  0.126871		CURRENT LEARNING RATE: 0.267898478833728
previous_iter_valid_loss : 0.13161981105804443

     62400	  0.132203	  0.131620	  0.127122		CURRENT LEARNING RATE: 0.26763071425949514
previous_iter_valid_loss : 0.1195092648267746

     62500	  0.119946	  0.119509	  0.127336		CURRENT LEARNING RATE: 0.26736321731599877
previous_iter_valid_loss : 0.11824768036603928

     62600	  0.118438	  0.118248	  0.127425		CURRENT LEARNING RATE: 0.26709598773574206
previous_iter_valid_loss : 0.11300834268331528

     62700	  0.113301	  0.113008	  0.127620		CURRENT LEARNING RATE: 0.2668290252514953
previous_iter_valid_loss : 0.12897446751594543

     62800	  0.129514	  0.128974	  0.127770		CURRENT LEARNING RATE: 0.26656232959629605
previous_iter_valid_loss : 0.11350899189710617

     62900	  0.113522	  0.113509	  0.127755		CURRENT LEARNING RATE: 0.2662959005034486
previous_iter_valid_loss : 0.15037015080451965

     63000	  0.150386	  0.150370	  0.128081		CURRENT LEARNING RATE: 0.26602973770652383
previous_iter_valid_loss : 0.12253891676664352

     63100	  0.122886	  0.122539	  0.128274		CURRENT LEARNING RATE: 0.26576384093935895
previous_iter_valid_loss : 0.12683610618114471

     63200	  0.127337	  0.126836	  0.128553		CURRENT LEARNING RATE: 0.26549820993605716
previous_iter_valid_loss : 0.15977224707603455

     63300	  0.160206	  0.159772	  0.129122		CURRENT LEARNING RATE: 0.26523284443098744
previous_iter_valid_loss : 0.131491556763649

     63400	  0.131664	  0.131492	  0.129133		CURRENT LEARNING RATE: 0.26496774415878427
previous_iter_valid_loss : 0.12492664903402328

     63500	  0.124905	  0.124927	  0.129200		CURRENT LEARNING RATE: 0.2647029088543473
previous_iter_valid_loss : 0.14268627762794495

     63600	  0.142669	  0.142686	  0.128805		CURRENT LEARNING RATE: 0.26443833825284124
previous_iter_valid_loss : 0.1352114975452423

     63700	  0.135614	  0.135211	  0.128947		CURRENT LEARNING RATE: 0.2641740320896955
previous_iter_valid_loss : 0.21039354801177979

     63800	  0.211089	  0.210394	  0.129432		CURRENT LEARNING RATE: 0.26390999010060384
previous_iter_valid_loss : 0.1643722802400589

     63900	  0.165202	  0.164372	  0.129948		CURRENT LEARNING RATE: 0.2636462120215243
previous_iter_valid_loss : 0.15185411274433136

     64000	  0.152137	  0.151854	  0.130574		CURRENT LEARNING RATE: 0.2633826975886787
previous_iter_valid_loss : 0.17420731484889984

     64100	  0.175136	  0.174207	  0.131325		CURRENT LEARNING RATE: 0.2631194465385527
previous_iter_valid_loss : 0.21076065301895142

     64200	  0.210984	  0.210761	  0.132202		CURRENT LEARNING RATE: 0.26285645860789514
previous_iter_valid_loss : 0.13754276931285858

     64300	  0.137686	  0.137543	  0.132668		CURRENT LEARNING RATE: 0.26259373353371807
previous_iter_valid_loss : 0.1699838787317276

     64400	  0.169599	  0.169984	  0.133289		CURRENT LEARNING RATE: 0.26233127105329646
previous_iter_valid_loss : 0.19909945130348206

     64500	  0.200419	  0.199099	  0.133958		CURRENT LEARNING RATE: 0.2620690709041677
previous_iter_valid_loss : 0.2815985083580017

     64600	  0.281338	  0.281599	  0.135372		CURRENT LEARNING RATE: 0.26180713282413176
previous_iter_valid_loss : 0.22585910558700562

     64700	  0.226733	  0.225859	  0.136404		CURRENT LEARNING RATE: 0.2615454565512504
previous_iter_valid_loss : 0.1548367440700531

     64800	  0.155420	  0.154837	  0.136022		CURRENT LEARNING RATE: 0.2612840418238474
previous_iter_valid_loss : 0.1570092886686325

     64900	  0.157373	  0.157009	  0.136362		CURRENT LEARNING RATE: 0.261022888380508
previous_iter_valid_loss : 0.2641580104827881

     65000	  0.264574	  0.264158	  0.137376		CURRENT LEARNING RATE: 0.26076199596007876
previous_iter_valid_loss : 0.17756257951259613

     65100	  0.178438	  0.177563	  0.137627		CURRENT LEARNING RATE: 0.2605013643016672
previous_iter_valid_loss : 0.14561371505260468

     65200	  0.145660	  0.145614	  0.137884		CURRENT LEARNING RATE: 0.2602409931446416
previous_iter_valid_loss : 0.14027155935764313

     65300	  0.140050	  0.140272	  0.137593		CURRENT LEARNING RATE: 0.2599808822286309
previous_iter_valid_loss : 0.24470125138759613

     65400	  0.245704	  0.244701	  0.138833		CURRENT LEARNING RATE: 0.2597210312935241
previous_iter_valid_loss : 0.17199598252773285

     65500	  0.173320	  0.171996	  0.139514		CURRENT LEARNING RATE: 0.2594614400794702
previous_iter_valid_loss : 0.3931347727775574

     65600	  0.395755	  0.393135	  0.142499		CURRENT LEARNING RATE: 0.25920210832687796
previous_iter_valid_loss : 0.23000994324684143

     65700	  0.231151	  0.230010	  0.143705		CURRENT LEARNING RATE: 0.2589430357764157
previous_iter_valid_loss : 0.15258292853832245

     65800	  0.153885	  0.152583	  0.144046		CURRENT LEARNING RATE: 0.2586842221690108
previous_iter_valid_loss : 0.19850397109985352

     65900	  0.200287	  0.198504	  0.144866		CURRENT LEARNING RATE: 0.2584256672458496
previous_iter_valid_loss : 0.1833125501871109

     66000	  0.184106	  0.183313	  0.145490		CURRENT LEARNING RATE: 0.2581673707483772
previous_iter_valid_loss : 0.16471922397613525

     66100	  0.166796	  0.164719	  0.145984		CURRENT LEARNING RATE: 0.25790933241829705
previous_iter_valid_loss : 0.14622652530670166

     66200	  0.146196	  0.146227	  0.146429		CURRENT LEARNING RATE: 0.25765155199757084
previous_iter_valid_loss : 0.18849141895771027

     66300	  0.188692	  0.188491	  0.146495		CURRENT LEARNING RATE: 0.2573940292284181
previous_iter_valid_loss : 0.14969748258590698

     66400	  0.148875	  0.149697	  0.146732		CURRENT LEARNING RATE: 0.25713676385331596
previous_iter_valid_loss : 0.14953848719596863

     66500	  0.150268	  0.149538	  0.146518		CURRENT LEARNING RATE: 0.25687975561499915
previous_iter_valid_loss : 0.1737443059682846

     66600	  0.174821	  0.173744	  0.146468		CURRENT LEARNING RATE: 0.2566230042564594
previous_iter_valid_loss : 0.16320131719112396

     66700	  0.163400	  0.163201	  0.146537		CURRENT LEARNING RATE: 0.25636650952094525
previous_iter_valid_loss : 0.1737571805715561

     66800	  0.173513	  0.173757	  0.146563		CURRENT LEARNING RATE: 0.256110271151962
previous_iter_valid_loss : 0.16561654210090637

     66900	  0.166789	  0.165617	  0.146802		CURRENT LEARNING RATE: 0.2558542888932712
previous_iter_valid_loss : 0.20274831354618073

     67000	  0.204298	  0.202748	  0.147607		CURRENT LEARNING RATE: 0.2555985624888907
previous_iter_valid_loss : 0.12830857932567596

     67100	  0.129352	  0.128309	  0.147146		CURRENT LEARNING RATE: 0.25534309168309394
previous_iter_valid_loss : 0.23126216232776642

     67200	  0.232590	  0.231262	  0.148086		CURRENT LEARNING RATE: 0.2550878762204101
previous_iter_valid_loss : 0.14158058166503906

     67300	  0.141592	  0.141581	  0.148121		CURRENT LEARNING RATE: 0.2548329158456238
previous_iter_valid_loss : 0.14319251477718353

     67400	  0.143825	  0.143193	  0.148489		CURRENT LEARNING RATE: 0.2545782103037746
previous_iter_valid_loss : 0.10660335421562195

     67500	  0.106616	  0.106603	  0.148255		CURRENT LEARNING RATE: 0.25432375934015683
previous_iter_valid_loss : 0.2180967777967453

     67600	  0.219832	  0.218097	  0.148970		CURRENT LEARNING RATE: 0.25406956270031966
previous_iter_valid_loss : 0.1260339468717575

     67700	  0.125720	  0.126034	  0.148958		CURRENT LEARNING RATE: 0.25381562013006637
previous_iter_valid_loss : 0.16563677787780762

     67800	  0.165604	  0.165637	  0.149164		CURRENT LEARNING RATE: 0.2535619313754543
previous_iter_valid_loss : 0.1400383561849594

     67900	  0.141183	  0.140038	  0.149367		CURRENT LEARNING RATE: 0.2533084961827948
previous_iter_valid_loss : 0.18033145368099213

     68000	  0.180393	  0.180331	  0.149781		CURRENT LEARNING RATE: 0.2530553142986526
previous_iter_valid_loss : 0.230265274643898

     68100	  0.231369	  0.230265	  0.150993		CURRENT LEARNING RATE: 0.25280238546984574
previous_iter_valid_loss : 0.1579490303993225

     68200	  0.157678	  0.157949	  0.151289		CURRENT LEARNING RATE: 0.2525497094434454
previous_iter_valid_loss : 0.1579264998435974

     68300	  0.158234	  0.157926	  0.151824		CURRENT LEARNING RATE: 0.2522972859667756
previous_iter_valid_loss : 0.1947307139635086

     68400	  0.196204	  0.194731	  0.152565		CURRENT LEARNING RATE: 0.25204511478741276
previous_iter_valid_loss : 0.11117365211248398

     68500	  0.111233	  0.111174	  0.152406		CURRENT LEARNING RATE: 0.2517931956531857
previous_iter_valid_loss : 0.11432934552431107

     68600	  0.114885	  0.114329	  0.152446		CURRENT LEARNING RATE: 0.2515415283121753
previous_iter_valid_loss : 0.143791064620018

     68700	  0.144541	  0.143791	  0.152495		CURRENT LEARNING RATE: 0.2512901125127142
previous_iter_valid_loss : 0.1325177103281021

     68800	  0.133540	  0.132518	  0.152701		CURRENT LEARNING RATE: 0.25103894800338655
previous_iter_valid_loss : 0.17392121255397797

     68900	  0.174665	  0.173921	  0.153338		CURRENT LEARNING RATE: 0.2507880345330278
previous_iter_valid_loss : 0.1708492487668991

     69000	  0.171671	  0.170849	  0.153264		CURRENT LEARNING RATE: 0.25053737185072444
previous_iter_valid_loss : 0.18430186808109283

     69100	  0.185249	  0.184302	  0.153964		CURRENT LEARNING RATE: 0.2502869597058139
previous_iter_valid_loss : 0.1527101993560791

     69200	  0.153970	  0.152710	  0.154144		CURRENT LEARNING RATE: 0.25003679784788385
previous_iter_valid_loss : 0.11383169889450073

     69300	  0.114181	  0.113832	  0.154201		CURRENT LEARNING RATE: 0.2497868860267725
previous_iter_valid_loss : 0.1832360327243805

     69400	  0.184946	  0.183236	  0.154016		CURRENT LEARNING RATE: 0.249537223992568
previous_iter_valid_loss : 0.14634771645069122

     69500	  0.147518	  0.146348	  0.154365		CURRENT LEARNING RATE: 0.24928781149560827
previous_iter_valid_loss : 0.12458407878875732

     69600	  0.124817	  0.124584	  0.154563		CURRENT LEARNING RATE: 0.24903864828648084
previous_iter_valid_loss : 0.17584888637065887

     69700	  0.177056	  0.175849	  0.155032		CURRENT LEARNING RATE: 0.24878973411602243
previous_iter_valid_loss : 0.3162616193294525

     69800	  0.316125	  0.316262	  0.157121		CURRENT LEARNING RATE: 0.24854106873531887
previous_iter_valid_loss : 0.19096234440803528

     69900	  0.191482	  0.190962	  0.157714		CURRENT LEARNING RATE: 0.24829265189570476
previous_iter_valid_loss : 0.14739057421684265

     70000	  0.148357	  0.147391	  0.158088		CURRENT LEARNING RATE: 0.24804448334876325
previous_iter_valid_loss : 0.25150924921035767

     70100	  0.251829	  0.251509	  0.159570		CURRENT LEARNING RATE: 0.24779656284632573
previous_iter_valid_loss : 0.1606667935848236

     70200	  0.160379	  0.160667	  0.159545		CURRENT LEARNING RATE: 0.24754889014047174
previous_iter_valid_loss : 0.2633017599582672

     70300	  0.264558	  0.263302	  0.160949		CURRENT LEARNING RATE: 0.2473014649835285
previous_iter_valid_loss : 0.20193904638290405

     70400	  0.202453	  0.201939	  0.161982		CURRENT LEARNING RATE: 0.24705428712807084
previous_iter_valid_loss : 0.16568714380264282

     70500	  0.165767	  0.165687	  0.162576		CURRENT LEARNING RATE: 0.2468073563269209
previous_iter_valid_loss : 0.1673429161310196

     70600	  0.167486	  0.167343	  0.162639		CURRENT LEARNING RATE: 0.24656067233314788
previous_iter_valid_loss : 0.22277261316776276

     70700	  0.223604	  0.222773	  0.163652		CURRENT LEARNING RATE: 0.24631423490006774
previous_iter_valid_loss : 0.19772343337535858

     70800	  0.199333	  0.197723	  0.164369		CURRENT LEARNING RATE: 0.246068043781243
previous_iter_valid_loss : 0.19225506484508514

     70900	  0.192349	  0.192255	  0.164592		CURRENT LEARNING RATE: 0.24582209873048255
previous_iter_valid_loss : 0.222642719745636

     71000	  0.223047	  0.222643	  0.165480		CURRENT LEARNING RATE: 0.24557639950184132
previous_iter_valid_loss : 0.1955225020647049

     71100	  0.195999	  0.195523	  0.166116		CURRENT LEARNING RATE: 0.24533094584962006
previous_iter_valid_loss : 0.237925723195076

     71200	  0.238682	  0.237926	  0.167376		CURRENT LEARNING RATE: 0.2450857375283651
previous_iter_valid_loss : 0.2432265281677246

     71300	  0.245016	  0.243227	  0.168602		CURRENT LEARNING RATE: 0.2448407742928681
previous_iter_valid_loss : 0.22067496180534363

     71400	  0.221137	  0.220675	  0.169560		CURRENT LEARNING RATE: 0.24459605589816577
previous_iter_valid_loss : 0.3819999694824219

     71500	  0.384458	  0.382000	  0.172017		CURRENT LEARNING RATE: 0.24435158209953975
previous_iter_valid_loss : 0.1712937206029892

     71600	  0.171388	  0.171294	  0.172538		CURRENT LEARNING RATE: 0.24410735265251615
previous_iter_valid_loss : 0.3258943259716034

     71700	  0.326439	  0.325894	  0.174706		CURRENT LEARNING RATE: 0.2438633673128656
previous_iter_valid_loss : 0.17939144372940063

     71800	  0.179537	  0.179391	  0.175312		CURRENT LEARNING RATE: 0.24361962583660263
previous_iter_valid_loss : 0.4491672217845917

     71900	  0.449942	  0.449167	  0.178378		CURRENT LEARNING RATE: 0.24337612797998584
previous_iter_valid_loss : 0.4498920738697052

     72000	  0.452032	  0.449892	  0.181695		CURRENT LEARNING RATE: 0.2431328734995173
previous_iter_valid_loss : 0.18365763127803802

     72100	  0.184256	  0.183658	  0.182529		CURRENT LEARNING RATE: 0.2428898621519425
previous_iter_valid_loss : 0.20718948543071747

     72200	  0.207581	  0.207189	  0.183472		CURRENT LEARNING RATE: 0.2426470936942501
previous_iter_valid_loss : 0.3040449917316437

     72300	  0.305148	  0.304045	  0.185355		CURRENT LEARNING RATE: 0.24240456788367162
previous_iter_valid_loss : 0.29453906416893005

     72400	  0.296005	  0.294539	  0.186984		CURRENT LEARNING RATE: 0.24216228447768123
previous_iter_valid_loss : 0.22013446688652039

     72500	  0.220584	  0.220134	  0.187991		CURRENT LEARNING RATE: 0.2419202432339955
previous_iter_valid_loss : 0.2597969174385071

     72600	  0.259403	  0.259797	  0.189406		CURRENT LEARNING RATE: 0.24167844391057317
previous_iter_valid_loss : 0.39047351479530334

     72700	  0.391381	  0.390474	  0.192181		CURRENT LEARNING RATE: 0.24143688626561488
previous_iter_valid_loss : 0.29389917850494385

     72800	  0.294241	  0.293899	  0.193830		CURRENT LEARNING RATE: 0.24119557005756295
previous_iter_valid_loss : 0.2329312562942505

     72900	  0.232213	  0.232931	  0.195024		CURRENT LEARNING RATE: 0.24095449504510122
previous_iter_valid_loss : 0.19807922840118408

     73000	  0.198072	  0.198079	  0.195501		CURRENT LEARNING RATE: 0.2407136609871546
previous_iter_valid_loss : 0.28913065791130066

     73100	  0.289589	  0.289131	  0.197167		CURRENT LEARNING RATE: 0.240473067642889
previous_iter_valid_loss : 0.1878165900707245

     73200	  0.187890	  0.187817	  0.197777		CURRENT LEARNING RATE: 0.24023271477171113
previous_iter_valid_loss : 0.4289723038673401

     73300	  0.429527	  0.428972	  0.200469		CURRENT LEARNING RATE: 0.239992602133268
previous_iter_valid_loss : 0.23852483928203583

     73400	  0.239163	  0.238525	  0.201540		CURRENT LEARNING RATE: 0.23975272948744705
previous_iter_valid_loss : 0.2138507068157196

     73500	  0.214098	  0.213851	  0.202429		CURRENT LEARNING RATE: 0.23951309659437556
previous_iter_valid_loss : 0.2299024909734726

     73600	  0.230548	  0.229902	  0.203301		CURRENT LEARNING RATE: 0.2392737032144206
previous_iter_valid_loss : 0.3277130722999573

     73700	  0.328772	  0.327713	  0.205226		CURRENT LEARNING RATE: 0.2390345491081888
previous_iter_valid_loss : 0.3458576202392578

     73800	  0.345127	  0.345858	  0.206581		CURRENT LEARNING RATE: 0.23879563403652604
previous_iter_valid_loss : 0.2601795494556427

     73900	  0.260641	  0.260180	  0.207539		CURRENT LEARNING RATE: 0.2385569577605172
previous_iter_valid_loss : 0.23089106380939484

     74000	  0.231080	  0.230891	  0.208329		CURRENT LEARNING RATE: 0.23831852004148601
previous_iter_valid_loss : 0.4532659351825714

     74100	  0.454375	  0.453266	  0.211120		CURRENT LEARNING RATE: 0.2380803206409947
previous_iter_valid_loss : 0.19564040005207062

     74200	  0.195939	  0.195640	  0.210968		CURRENT LEARNING RATE: 0.2378423593208439
previous_iter_valid_loss : 0.20857445895671844

     74300	  0.208964	  0.208574	  0.211679		CURRENT LEARNING RATE: 0.23760463584307223
previous_iter_valid_loss : 0.47438767552375793

     74400	  0.475435	  0.474388	  0.214723		CURRENT LEARNING RATE: 0.2373671499699562
previous_iter_valid_loss : 0.4387768507003784

     74500	  0.438070	  0.438777	  0.217120		CURRENT LEARNING RATE: 0.23712990146400995
previous_iter_valid_loss : 0.5443402528762817

     74600	  0.547432	  0.544340	  0.219747		CURRENT LEARNING RATE: 0.2368928900879849
previous_iter_valid_loss : 0.3781405985355377

     74700	  0.377836	  0.378141	  0.221270		CURRENT LEARNING RATE: 0.23665611560486965
previous_iter_valid_loss : 0.25960713624954224

     74800	  0.260154	  0.259607	  0.222317		CURRENT LEARNING RATE: 0.23641957777788977
previous_iter_valid_loss : 0.29192379117012024

     74900	  0.293024	  0.291924	  0.223667		CURRENT LEARNING RATE: 0.23618327637050734
previous_iter_valid_loss : 0.24055728316307068

     75000	  0.241596	  0.240557	  0.223431		CURRENT LEARNING RATE: 0.235947211146421
previous_iter_valid_loss : 0.4427243769168854

     75100	  0.443602	  0.442724	  0.226082		CURRENT LEARNING RATE: 0.23571138186956545
previous_iter_valid_loss : 0.30830690264701843

     75200	  0.308135	  0.308307	  0.227709		CURRENT LEARNING RATE: 0.2354757883041114
previous_iter_valid_loss : 0.18783831596374512

     75300	  0.187292	  0.187838	  0.228185		CURRENT LEARNING RATE: 0.23524043021446528
previous_iter_valid_loss : 0.36517757177352905

     75400	  0.364450	  0.365178	  0.229390		CURRENT LEARNING RATE: 0.23500530736526898
previous_iter_valid_loss : 0.27995049953460693

     75500	  0.280904	  0.279950	  0.230469		CURRENT LEARNING RATE: 0.23477041952139963
previous_iter_valid_loss : 0.26264694333076477

     75600	  0.263743	  0.262647	  0.229164		CURRENT LEARNING RATE: 0.23453576644796936
previous_iter_valid_loss : 0.3068944811820984

     75700	  0.306415	  0.306894	  0.229933		CURRENT LEARNING RATE: 0.23430134791032511
previous_iter_valid_loss : 0.2575395405292511

     75800	  0.257388	  0.257540	  0.230983		CURRENT LEARNING RATE: 0.23406716367404826
previous_iter_valid_loss : 0.2699860632419586

     75900	  0.270564	  0.269986	  0.231697		CURRENT LEARNING RATE: 0.23383321350495462
previous_iter_valid_loss : 0.8461505770683289

     76000	  0.849140	  0.846151	  0.238326		CURRENT LEARNING RATE: 0.23359949716909395
previous_iter_valid_loss : 0.19207172095775604

     76100	  0.193511	  0.192072	  0.238599		CURRENT LEARNING RATE: 0.23336601443274993
previous_iter_valid_loss : 0.4923899471759796

     76200	  0.492580	  0.492390	  0.242061		CURRENT LEARNING RATE: 0.23313276506243977
previous_iter_valid_loss : 0.25611719489097595

     76300	  0.255704	  0.256117	  0.242737		CURRENT LEARNING RATE: 0.23289974882491413
previous_iter_valid_loss : 0.3324451744556427

     76400	  0.332610	  0.332445	  0.244565		CURRENT LEARNING RATE: 0.2326669654871567
previous_iter_valid_loss : 0.3973842263221741

     76500	  0.397213	  0.397384	  0.247043		CURRENT LEARNING RATE: 0.23243441481638413
previous_iter_valid_loss : 0.9807999134063721

     76600	  0.982489	  0.980800	  0.255114		CURRENT LEARNING RATE: 0.23220209658004579
previous_iter_valid_loss : 0.20804566144943237

     76700	  0.207982	  0.208046	  0.255562		CURRENT LEARNING RATE: 0.23197001054582336
previous_iter_valid_loss : 0.3548106849193573

     76800	  0.356082	  0.354811	  0.257373		CURRENT LEARNING RATE: 0.2317381564816308
previous_iter_valid_loss : 0.24448950588703156

     76900	  0.244295	  0.244490	  0.258161		CURRENT LEARNING RATE: 0.23150653415561404
previous_iter_valid_loss : 0.26809296011924744

     77000	  0.268754	  0.268093	  0.258815		CURRENT LEARNING RATE: 0.2312751433361507
previous_iter_valid_loss : 0.23444068431854248

     77100	  0.234907	  0.234441	  0.259876		CURRENT LEARNING RATE: 0.23104398379185
previous_iter_valid_loss : 0.2775888442993164

     77200	  0.278144	  0.277589	  0.260339		CURRENT LEARNING RATE: 0.23081305529155235
previous_iter_valid_loss : 0.6114619970321655

     77300	  0.612394	  0.611462	  0.265038		CURRENT LEARNING RATE: 0.2305823576043292
previous_iter_valid_loss : 0.5063275694847107

     77400	  0.507675	  0.506328	  0.268670		CURRENT LEARNING RATE: 0.2303518904994829
previous_iter_valid_loss : 0.3289245069026947

     77500	  0.328461	  0.328925	  0.270893		CURRENT LEARNING RATE: 0.23012165374654628
previous_iter_valid_loss : 0.3713109791278839

     77600	  0.371315	  0.371311	  0.272425		CURRENT LEARNING RATE: 0.2298916471152826
previous_iter_valid_loss : 0.5250962376594543

     77700	  0.523960	  0.525096	  0.276416		CURRENT LEARNING RATE: 0.22966187037568517
previous_iter_valid_loss : 0.2381364405155182

     77800	  0.239940	  0.238136	  0.277141		CURRENT LEARNING RATE: 0.22943232329797725
previous_iter_valid_loss : 0.4291289746761322

     77900	  0.430269	  0.429129	  0.280032		CURRENT LEARNING RATE: 0.22920300565261176
previous_iter_valid_loss : 0.2343161553144455

     78000	  0.236040	  0.234316	  0.280571		CURRENT LEARNING RATE: 0.22897391721027102
previous_iter_valid_loss : 0.20692245662212372

     78100	  0.208522	  0.206922	  0.280338		CURRENT LEARNING RATE: 0.22874505774186657
previous_iter_valid_loss : 0.33617374300956726

     78200	  0.338654	  0.336174	  0.282120		CURRENT LEARNING RATE: 0.22851642701853894
previous_iter_valid_loss : 0.19898174703121185

     78300	  0.199704	  0.198982	  0.282531		CURRENT LEARNING RATE: 0.22828802481165736
previous_iter_valid_loss : 0.28798824548721313

     78400	  0.289706	  0.287988	  0.283463		CURRENT LEARNING RATE: 0.2280598508928196
previous_iter_valid_loss : 0.20987026393413544

     78500	  0.210877	  0.209870	  0.284450		CURRENT LEARNING RATE: 0.22783190503385176
previous_iter_valid_loss : 0.33518365025520325

     78600	  0.336458	  0.335184	  0.286659		CURRENT LEARNING RATE: 0.22760418700680793
previous_iter_valid_loss : 0.2482379972934723

     78700	  0.249334	  0.248238	  0.287703		CURRENT LEARNING RATE: 0.22737669658397008
previous_iter_valid_loss : 0.4674381911754608

     78800	  0.468461	  0.467438	  0.291053		CURRENT LEARNING RATE: 0.22714943353784775
previous_iter_valid_loss : 0.27488112449645996

     78900	  0.275664	  0.274881	  0.292062		CURRENT LEARNING RATE: 0.22692239764117791
previous_iter_valid_loss : 0.18733206391334534

     79000	  0.187690	  0.187332	  0.292227		CURRENT LEARNING RATE: 0.2266955886669246
previous_iter_valid_loss : 0.2631548345088959

     79100	  0.263753	  0.263155	  0.293016		CURRENT LEARNING RATE: 0.22646900638827885
previous_iter_valid_loss : 0.3648971617221832

     79200	  0.367046	  0.364897	  0.295137		CURRENT LEARNING RATE: 0.22624265057865836
previous_iter_valid_loss : 0.37563449144363403

     79300	  0.376736	  0.375634	  0.297755		CURRENT LEARNING RATE: 0.22601652101170733
previous_iter_valid_loss : 0.43486523628234863

     79400	  0.435646	  0.434865	  0.300272		CURRENT LEARNING RATE: 0.2257906174612961
previous_iter_valid_loss : 0.27541518211364746

     79500	  0.275978	  0.275415	  0.301562		CURRENT LEARNING RATE: 0.22556493970152117
previous_iter_valid_loss : 0.33490344882011414

     79600	  0.337754	  0.334903	  0.303666		CURRENT LEARNING RATE: 0.22533948750670474
previous_iter_valid_loss : 0.21005618572235107

     79700	  0.210366	  0.210056	  0.304008		CURRENT LEARNING RATE: 0.22511426065139462
previous_iter_valid_loss : 0.5052704811096191

     79800	  0.505998	  0.505270	  0.305898		CURRENT LEARNING RATE: 0.22488925891036388
previous_iter_valid_loss : 0.33439144492149353

     79900	  0.336325	  0.334391	  0.307332		CURRENT LEARNING RATE: 0.22466448205861078
previous_iter_valid_loss : 0.3898635506629944

     80000	  0.389837	  0.389864	  0.309757		CURRENT LEARNING RATE: 0.2244399298713585
previous_iter_valid_loss : 0.573021650314331

     80100	  0.574095	  0.573022	  0.312972		CURRENT LEARNING RATE: 0.22421560212405475
previous_iter_valid_loss : 0.30427518486976624

     80200	  0.304767	  0.304275	  0.314408		CURRENT LEARNING RATE: 0.22399149859237183
previous_iter_valid_loss : 0.22851575911045074

     80300	  0.228775	  0.228516	  0.314060		CURRENT LEARNING RATE: 0.22376761905220618
previous_iter_valid_loss : 0.22185516357421875

     80400	  0.222508	  0.221855	  0.314259		CURRENT LEARNING RATE: 0.2235439632796782
previous_iter_valid_loss : 0.3338373005390167

     80500	  0.333913	  0.333837	  0.315941		CURRENT LEARNING RATE: 0.22332053105113217
previous_iter_valid_loss : 0.29366225004196167

     80600	  0.294966	  0.293662	  0.317204		CURRENT LEARNING RATE: 0.22309732214313577
previous_iter_valid_loss : 0.36910635232925415

     80700	  0.370342	  0.369106	  0.318667		CURRENT LEARNING RATE: 0.2228743363324801
previous_iter_valid_loss : 0.3250657021999359

     80800	  0.324779	  0.325066	  0.319941		CURRENT LEARNING RATE: 0.22265157339617936
previous_iter_valid_loss : 0.19370298087596893

     80900	  0.194199	  0.193703	  0.319955		CURRENT LEARNING RATE: 0.22242903311147055
previous_iter_valid_loss : 0.24690082669258118

     81000	  0.248112	  0.246901	  0.320198		CURRENT LEARNING RATE: 0.2222067152558134
previous_iter_valid_loss : 0.35485753417015076

     81100	  0.355030	  0.354858	  0.321791		CURRENT LEARNING RATE: 0.22198461960689003
previous_iter_valid_loss : 0.2551872432231903

     81200	  0.256835	  0.255187	  0.321964		CURRENT LEARNING RATE: 0.22176274594260476
previous_iter_valid_loss : 0.3029402494430542

     81300	  0.304902	  0.302940	  0.322561		CURRENT LEARNING RATE: 0.2215410940410839
previous_iter_valid_loss : 0.2810514271259308

     81400	  0.282315	  0.281051	  0.323165		CURRENT LEARNING RATE: 0.22131966368067557
previous_iter_valid_loss : 0.17043696343898773

     81500	  0.170573	  0.170437	  0.321049		CURRENT LEARNING RATE: 0.22109845463994934
previous_iter_valid_loss : 0.2199794501066208

     81600	  0.222138	  0.219979	  0.321536		CURRENT LEARNING RATE: 0.22087746669769617
previous_iter_valid_loss : 0.17064540088176727

     81700	  0.170938	  0.170645	  0.319983		CURRENT LEARNING RATE: 0.2206566996329281
previous_iter_valid_loss : 0.5930619239807129

     81800	  0.594657	  0.593062	  0.324120		CURRENT LEARNING RATE: 0.22043615322487808
previous_iter_valid_loss : 0.17858590185642242

     81900	  0.179546	  0.178586	  0.321414		CURRENT LEARNING RATE: 0.22021582725299965
previous_iter_valid_loss : 0.6843196749687195

     82000	  0.687637	  0.684320	  0.323759		CURRENT LEARNING RATE: 0.2199957214969668
previous_iter_valid_loss : 0.3327908217906952

     82100	  0.334436	  0.332791	  0.325250		CURRENT LEARNING RATE: 0.2197758357366738
previous_iter_valid_loss : 0.1887965053319931

     82200	  0.190236	  0.188797	  0.325066		CURRENT LEARNING RATE: 0.21955616975223485
previous_iter_valid_loss : 0.24551038444042206

     82300	  0.246440	  0.245510	  0.324481		CURRENT LEARNING RATE: 0.21933672332398393
previous_iter_valid_loss : 0.36932462453842163

     82400	  0.370125	  0.369325	  0.325229		CURRENT LEARNING RATE: 0.21911749623247462
previous_iter_valid_loss : 0.27148744463920593

     82500	  0.272693	  0.271487	  0.325742		CURRENT LEARNING RATE: 0.21889848825847982
previous_iter_valid_loss : 0.22057278454303741

     82600	  0.220600	  0.220573	  0.325350		CURRENT LEARNING RATE: 0.2186796991829915
previous_iter_valid_loss : 0.1912127137184143

     82700	  0.192147	  0.191213	  0.323357		CURRENT LEARNING RATE: 0.2184611287872206
previous_iter_valid_loss : 0.20547109842300415

     82800	  0.206717	  0.205471	  0.322473		CURRENT LEARNING RATE: 0.2182427768525967
previous_iter_valid_loss : 0.24662253260612488

     82900	  0.248394	  0.246623	  0.322610		CURRENT LEARNING RATE: 0.2180246431607678
previous_iter_valid_loss : 0.39683008193969727

     83000	  0.397742	  0.396830	  0.324597		CURRENT LEARNING RATE: 0.21780672749360025
previous_iter_valid_loss : 0.18956074118614197

     83100	  0.191050	  0.189561	  0.323602		CURRENT LEARNING RATE: 0.21758902963317836
previous_iter_valid_loss : 0.1902480572462082

     83200	  0.191120	  0.190248	  0.323626		CURRENT LEARNING RATE: 0.21737154936180422
previous_iter_valid_loss : 0.17706061899662018

     83300	  0.177481	  0.177061	  0.321107		CURRENT LEARNING RATE: 0.21715428646199755
previous_iter_valid_loss : 0.23294419050216675

     83400	  0.233381	  0.232944	  0.321051		CURRENT LEARNING RATE: 0.21693724071649545
previous_iter_valid_loss : 0.17886297404766083

     83500	  0.180467	  0.178863	  0.320701		CURRENT LEARNING RATE: 0.21672041190825214
previous_iter_valid_loss : 0.1710590422153473

     83600	  0.172247	  0.171059	  0.320113		CURRENT LEARNING RATE: 0.21650379982043882
previous_iter_valid_loss : 0.2393714040517807

     83700	  0.239947	  0.239371	  0.319229		CURRENT LEARNING RATE: 0.21628740423644333
previous_iter_valid_loss : 0.1595042198896408

     83800	  0.159633	  0.159504	  0.317366		CURRENT LEARNING RATE: 0.21607122493987013
previous_iter_valid_loss : 0.17846746742725372

     83900	  0.178293	  0.178467	  0.316549		CURRENT LEARNING RATE: 0.21585526171453986
previous_iter_valid_loss : 0.20250645279884338

     84000	  0.202704	  0.202506	  0.316265		CURRENT LEARNING RATE: 0.21563951434448927
previous_iter_valid_loss : 0.16877204179763794

     84100	  0.168793	  0.168772	  0.313420		CURRENT LEARNING RATE: 0.21542398261397103
previous_iter_valid_loss : 0.1819935142993927

     84200	  0.182361	  0.181994	  0.313283		CURRENT LEARNING RATE: 0.21520866630745333
previous_iter_valid_loss : 0.18850965797901154

     84300	  0.188749	  0.188510	  0.313083		CURRENT LEARNING RATE: 0.2149935652096199
previous_iter_valid_loss : 0.37936830520629883

     84400	  0.380796	  0.379368	  0.312133		CURRENT LEARNING RATE: 0.21477867910536957
previous_iter_valid_loss : 0.2360735982656479

     84500	  0.236970	  0.236074	  0.310106		CURRENT LEARNING RATE: 0.21456400777981627
previous_iter_valid_loss : 0.12359359860420227

     84600	  0.123751	  0.123594	  0.305898		CURRENT LEARNING RATE: 0.21434955101828862
previous_iter_valid_loss : 0.4560547471046448

     84700	  0.456398	  0.456055	  0.306677		CURRENT LEARNING RATE: 0.21413530860632984
previous_iter_valid_loss : 0.14131294190883636

     84800	  0.141519	  0.141313	  0.305494		CURRENT LEARNING RATE: 0.2139212803296975
previous_iter_valid_loss : 0.33538711071014404

     84900	  0.335975	  0.335387	  0.305929		CURRENT LEARNING RATE: 0.21370746597436335
previous_iter_valid_loss : 0.2014133185148239

     85000	  0.202922	  0.201413	  0.305537		CURRENT LEARNING RATE: 0.21349386532651296
previous_iter_valid_loss : 0.16189731657505035

     85100	  0.162764	  0.161897	  0.302729		CURRENT LEARNING RATE: 0.2132804781725457
previous_iter_valid_loss : 0.2188674956560135

     85200	  0.219845	  0.218867	  0.301835		CURRENT LEARNING RATE: 0.21306730429907436
previous_iter_valid_loss : 0.12892921268939972

     85300	  0.129746	  0.128929	  0.301246		CURRENT LEARNING RATE: 0.2128543434929251
previous_iter_valid_loss : 0.34933483600616455

     85400	  0.350637	  0.349335	  0.301087		CURRENT LEARNING RATE: 0.21264159554113707
previous_iter_valid_loss : 0.12984660267829895

     85500	  0.129757	  0.129847	  0.299586		CURRENT LEARNING RATE: 0.21242906023096228
previous_iter_valid_loss : 0.1459851861000061

     85600	  0.146753	  0.145985	  0.298420		CURRENT LEARNING RATE: 0.21221673734986546
previous_iter_valid_loss : 0.18721996247768402

     85700	  0.188162	  0.187220	  0.297223		CURRENT LEARNING RATE: 0.21200462668552364
previous_iter_valid_loss : 0.1357443779706955

     85800	  0.136550	  0.135744	  0.296005		CURRENT LEARNING RATE: 0.2117927280258262
previous_iter_valid_loss : 0.19821284711360931

     85900	  0.198816	  0.198213	  0.295287		CURRENT LEARNING RATE: 0.2115810411588744
previous_iter_valid_loss : 0.11380529403686523

     86000	  0.113991	  0.113805	  0.287964		CURRENT LEARNING RATE: 0.21136956587298142
previous_iter_valid_loss : 0.12804727256298065

     86100	  0.128248	  0.128047	  0.287323		CURRENT LEARNING RATE: 0.21115830195667193
previous_iter_valid_loss : 0.212192103266716

     86200	  0.212416	  0.212192	  0.284521		CURRENT LEARNING RATE: 0.21094724919868196
previous_iter_valid_loss : 0.19399133324623108

     86300	  0.194236	  0.193991	  0.283900		CURRENT LEARNING RATE: 0.21073640738795882
previous_iter_valid_loss : 0.16980253159999847

     86400	  0.169680	  0.169803	  0.282274		CURRENT LEARNING RATE: 0.2105257763136606
previous_iter_valid_loss : 0.17114605009555817

     86500	  0.170954	  0.171146	  0.280011		CURRENT LEARNING RATE: 0.21031535576515623
previous_iter_valid_loss : 0.20453591644763947

     86600	  0.205335	  0.204536	  0.272249		CURRENT LEARNING RATE: 0.21010514553202514
previous_iter_valid_loss : 0.12978915870189667

     86700	  0.130072	  0.129789	  0.271466		CURRENT LEARNING RATE: 0.20989514540405713
previous_iter_valid_loss : 0.4548710286617279

     86800	  0.456320	  0.454871	  0.272467		CURRENT LEARNING RATE: 0.20968535517125197
previous_iter_valid_loss : 0.1863722950220108

     86900	  0.186386	  0.186372	  0.271886		CURRENT LEARNING RATE: 0.2094757746238195
previous_iter_valid_loss : 0.1687859147787094

     87000	  0.168496	  0.168786	  0.270893		CURRENT LEARNING RATE: 0.20926640355217907
previous_iter_valid_loss : 0.22117392718791962

     87100	  0.221344	  0.221174	  0.270760		CURRENT LEARNING RATE: 0.20905724174695967
previous_iter_valid_loss : 0.12249673157930374

     87200	  0.122560	  0.122497	  0.269209		CURRENT LEARNING RATE: 0.2088482889989994
previous_iter_valid_loss : 0.17186927795410156

     87300	  0.172066	  0.171869	  0.264813		CURRENT LEARNING RATE: 0.20863954509934557
previous_iter_valid_loss : 0.1562000960111618

     87400	  0.156473	  0.156200	  0.261312		CURRENT LEARNING RATE: 0.2084310098392542
previous_iter_valid_loss : 0.21861079335212708

     87500	  0.219112	  0.218611	  0.260209		CURRENT LEARNING RATE: 0.20822268301019004
previous_iter_valid_loss : 0.14884264767169952

     87600	  0.148632	  0.148843	  0.257984		CURRENT LEARNING RATE: 0.20801456440382626
previous_iter_valid_loss : 0.19516675174236298

     87700	  0.195102	  0.195167	  0.254685		CURRENT LEARNING RATE: 0.2078066538120442
previous_iter_valid_loss : 0.13640999794006348

     87800	  0.136468	  0.136410	  0.253667		CURRENT LEARNING RATE: 0.2075989510269333
previous_iter_valid_loss : 0.247078999876976

     87900	  0.247678	  0.247079	  0.251847		CURRENT LEARNING RATE: 0.2073914558407907
previous_iter_valid_loss : 0.1797972321510315

     88000	  0.180349	  0.179797	  0.251302		CURRENT LEARNING RATE: 0.20718416804612122
previous_iter_valid_loss : 0.1444658786058426

     88100	  0.144830	  0.144466	  0.250677		CURRENT LEARNING RATE: 0.20697708743563706
previous_iter_valid_loss : 0.1369049996137619

     88200	  0.137123	  0.136905	  0.248684		CURRENT LEARNING RATE: 0.20677021380225757
previous_iter_valid_loss : 0.19226188957691193

     88300	  0.191972	  0.192262	  0.248617		CURRENT LEARNING RATE: 0.2065635469391091
previous_iter_valid_loss : 0.17345081269741058

     88400	  0.173637	  0.173451	  0.247472		CURRENT LEARNING RATE: 0.20635708663952482
previous_iter_valid_loss : 0.31606805324554443

     88500	  0.316819	  0.316068	  0.248534		CURRENT LEARNING RATE: 0.20615083269704437
previous_iter_valid_loss : 0.16967080533504486

     88600	  0.169433	  0.169671	  0.246879		CURRENT LEARNING RATE: 0.2059447849054138
previous_iter_valid_loss : 0.24557463824748993

     88700	  0.245605	  0.245575	  0.246852		CURRENT LEARNING RATE: 0.20573894305858528
previous_iter_valid_loss : 0.16155153512954712

     88800	  0.161929	  0.161552	  0.243793		CURRENT LEARNING RATE: 0.20553330695071698
previous_iter_valid_loss : 0.30612432956695557

     88900	  0.306860	  0.306124	  0.244106		CURRENT LEARNING RATE: 0.20532787637617275
previous_iter_valid_loss : 0.1761591136455536

     89000	  0.176726	  0.176159	  0.243994		CURRENT LEARNING RATE: 0.205122651129522
previous_iter_valid_loss : 0.16838058829307556

     89100	  0.168807	  0.168381	  0.243046		CURRENT LEARNING RATE: 0.20491763100553947
previous_iter_valid_loss : 0.19807296991348267

     89200	  0.199116	  0.198073	  0.241378		CURRENT LEARNING RATE: 0.20471281579920503
previous_iter_valid_loss : 0.139900341629982

     89300	  0.140201	  0.139900	  0.239020		CURRENT LEARNING RATE: 0.20450820530570346
previous_iter_valid_loss : 0.19642890989780426

     89400	  0.195998	  0.196429	  0.236636		CURRENT LEARNING RATE: 0.20430379932042422
previous_iter_valid_loss : 0.1144033819437027

     89500	  0.114383	  0.114403	  0.235026		CURRENT LEARNING RATE: 0.20409959763896135
previous_iter_valid_loss : 0.18668624758720398

     89600	  0.186275	  0.186686	  0.233544		CURRENT LEARNING RATE: 0.2038956000571131
previous_iter_valid_loss : 0.18727189302444458

     89700	  0.187992	  0.187272	  0.233316		CURRENT LEARNING RATE: 0.2036918063708819
previous_iter_valid_loss : 0.18243075907230377

     89800	  0.183018	  0.182431	  0.230088		CURRENT LEARNING RATE: 0.20348821637647407
previous_iter_valid_loss : 0.1328241229057312

     89900	  0.133108	  0.132824	  0.228072		CURRENT LEARNING RATE: 0.20328482987029955
previous_iter_valid_loss : 0.2510569393634796

     90000	  0.251996	  0.251057	  0.226684		CURRENT LEARNING RATE: 0.20308164664897185
previous_iter_valid_loss : 0.20027917623519897

     90100	  0.200874	  0.200279	  0.222956		CURRENT LEARNING RATE: 0.20287866650930772
previous_iter_valid_loss : 0.17429737746715546

     90200	  0.174931	  0.174297	  0.221657		CURRENT LEARNING RATE: 0.202675889248327
previous_iter_valid_loss : 0.23117446899414062

     90300	  0.231875	  0.231174	  0.221683		CURRENT LEARNING RATE: 0.20247331466325244
previous_iter_valid_loss : 0.17511437833309174

     90400	  0.175013	  0.175114	  0.221216		CURRENT LEARNING RATE: 0.2022709425515094
previous_iter_valid_loss : 0.186366006731987

     90500	  0.185997	  0.186366	  0.219741		CURRENT LEARNING RATE: 0.20206877271072576
previous_iter_valid_loss : 0.17148597538471222

     90600	  0.171376	  0.171486	  0.218519		CURRENT LEARNING RATE: 0.2018668049387317
previous_iter_valid_loss : 0.18264935910701752

     90700	  0.182570	  0.182649	  0.216655		CURRENT LEARNING RATE: 0.20166503903355937
previous_iter_valid_loss : 0.2469993382692337

     90800	  0.248003	  0.246999	  0.215874		CURRENT LEARNING RATE: 0.20146347479344287
previous_iter_valid_loss : 0.20448358356952667

     90900	  0.205117	  0.204484	  0.215982		CURRENT LEARNING RATE: 0.20126211201681798
previous_iter_valid_loss : 0.1387668401002884

     91000	  0.138764	  0.138767	  0.214901		CURRENT LEARNING RATE: 0.20106095050232187
previous_iter_valid_loss : 0.17647263407707214

     91100	  0.175813	  0.176473	  0.213117		CURRENT LEARNING RATE: 0.200859990048793
previous_iter_valid_loss : 0.1386931985616684

     91200	  0.138788	  0.138693	  0.211952		CURRENT LEARNING RATE: 0.20065923045527095
previous_iter_valid_loss : 0.31961962580680847

     91300	  0.320345	  0.319620	  0.212119		CURRENT LEARNING RATE: 0.20045867152099606
previous_iter_valid_loss : 0.2456173151731491

     91400	  0.246410	  0.245617	  0.211764		CURRENT LEARNING RATE: 0.2002583130454094
previous_iter_valid_loss : 0.15534928441047668

     91500	  0.155547	  0.155349	  0.211613		CURRENT LEARNING RATE: 0.20005815482815248
previous_iter_valid_loss : 0.24812021851539612

     91600	  0.247716	  0.248120	  0.211895		CURRENT LEARNING RATE: 0.19985819666906704
previous_iter_valid_loss : 0.3420814275741577

     91700	  0.342685	  0.342081	  0.213609		CURRENT LEARNING RATE: 0.19965843836819494
previous_iter_valid_loss : 0.2713744640350342

     91800	  0.271425	  0.271374	  0.210392		CURRENT LEARNING RATE: 0.19945887972577783
previous_iter_valid_loss : 0.18637047708034515

     91900	  0.187228	  0.186370	  0.210470		CURRENT LEARNING RATE: 0.19925952054225707
previous_iter_valid_loss : 0.311688631772995

     92000	  0.311368	  0.311689	  0.206744		CURRENT LEARNING RATE: 0.19906036061827348
previous_iter_valid_loss : 0.1879439353942871

     92100	  0.188169	  0.187944	  0.205295		CURRENT LEARNING RATE: 0.19886139975466707
previous_iter_valid_loss : 0.11731027811765671

     92200	  0.117368	  0.117310	  0.204580		CURRENT LEARNING RATE: 0.198662637752477
previous_iter_valid_loss : 0.13510741293430328

     92300	  0.135480	  0.135107	  0.203476		CURRENT LEARNING RATE: 0.19846407441294123
previous_iter_valid_loss : 0.14644865691661835

     92400	  0.146662	  0.146449	  0.201248		CURRENT LEARNING RATE: 0.19826570953749642
previous_iter_valid_loss : 0.15336693823337555

     92500	  0.153767	  0.153367	  0.200066		CURRENT LEARNING RATE: 0.19806754292777767
previous_iter_valid_loss : 0.18285422027111053

     92600	  0.183427	  0.182854	  0.199689		CURRENT LEARNING RATE: 0.19786957438561836
previous_iter_valid_loss : 0.17039795219898224

     92700	  0.170896	  0.170398	  0.199481		CURRENT LEARNING RATE: 0.1976718037130499
previous_iter_valid_loss : 0.16670505702495575

     92800	  0.167259	  0.166705	  0.199093		CURRENT LEARNING RATE: 0.19747423071230163
previous_iter_valid_loss : 0.1425771862268448

     92900	  0.142510	  0.142577	  0.198053		CURRENT LEARNING RATE: 0.19727685518580054
previous_iter_valid_loss : 0.1961687058210373

     93000	  0.196482	  0.196169	  0.196046		CURRENT LEARNING RATE: 0.1970796769361711
previous_iter_valid_loss : 0.20374028384685516

     93100	  0.203422	  0.203740	  0.196188		CURRENT LEARNING RATE: 0.196882695766235
previous_iter_valid_loss : 0.1271502822637558

     93200	  0.127456	  0.127150	  0.195557		CURRENT LEARNING RATE: 0.19668591147901104
previous_iter_valid_loss : 0.16730190813541412

     93300	  0.167607	  0.167302	  0.195460		CURRENT LEARNING RATE: 0.19648932387771498
previous_iter_valid_loss : 0.2152869701385498

     93400	  0.215932	  0.215287	  0.195283		CURRENT LEARNING RATE: 0.19629293276575918
previous_iter_valid_loss : 0.3249877989292145

     93500	  0.325391	  0.324988	  0.196744		CURRENT LEARNING RATE: 0.19609673794675248
previous_iter_valid_loss : 0.18102610111236572

     93600	  0.180880	  0.181026	  0.196844		CURRENT LEARNING RATE: 0.19590073922450008
previous_iter_valid_loss : 0.13151942193508148

     93700	  0.131898	  0.131519	  0.195765		CURRENT LEARNING RATE: 0.19570493640300327
previous_iter_valid_loss : 0.15938793122768402

     93800	  0.159422	  0.159388	  0.195764		CURRENT LEARNING RATE: 0.19550932928645912
previous_iter_valid_loss : 0.17397555708885193

     93900	  0.174895	  0.173976	  0.195719		CURRENT LEARNING RATE: 0.19531391767926057
previous_iter_valid_loss : 0.20133313536643982

     94000	  0.201720	  0.201333	  0.195708		CURRENT LEARNING RATE: 0.19511870138599596
previous_iter_valid_loss : 0.23570027947425842

     94100	  0.235484	  0.235700	  0.196377		CURRENT LEARNING RATE: 0.19492368021144899
previous_iter_valid_loss : 0.2334364801645279

     94200	  0.233109	  0.233436	  0.196891		CURRENT LEARNING RATE: 0.1947288539605985
previous_iter_valid_loss : 0.16615857183933258

     94300	  0.166204	  0.166159	  0.196668		CURRENT LEARNING RATE: 0.19453422243861818
previous_iter_valid_loss : 0.18158414959907532

     94400	  0.181117	  0.181584	  0.194690		CURRENT LEARNING RATE: 0.19433978545087652
previous_iter_valid_loss : 0.14319241046905518

     94500	  0.143861	  0.143192	  0.193761		CURRENT LEARNING RATE: 0.1941455428029365
previous_iter_valid_loss : 0.17027829587459564

     94600	  0.170317	  0.170278	  0.194228		CURRENT LEARNING RATE: 0.19395149430055544
previous_iter_valid_loss : 0.2284150868654251

     94700	  0.227729	  0.228415	  0.191952		CURRENT LEARNING RATE: 0.19375763974968488
previous_iter_valid_loss : 0.2150687426328659

     94800	  0.214963	  0.215069	  0.192689		CURRENT LEARNING RATE: 0.1935639789564702
previous_iter_valid_loss : 0.13965678215026855

     94900	  0.139784	  0.139657	  0.190732		CURRENT LEARNING RATE: 0.19337051172725062
previous_iter_valid_loss : 0.19886094331741333

     95000	  0.199042	  0.198861	  0.190706		CURRENT LEARNING RATE: 0.19317723786855887
previous_iter_valid_loss : 0.30929329991340637

     95100	  0.308826	  0.309293	  0.192180		CURRENT LEARNING RATE: 0.19298415718712109
previous_iter_valid_loss : 0.20729894936084747

     95200	  0.207091	  0.207299	  0.192065		CURRENT LEARNING RATE: 0.19279126948985656
previous_iter_valid_loss : 0.1652153879404068

     95300	  0.165038	  0.165215	  0.192428		CURRENT LEARNING RATE: 0.1925985745838776
previous_iter_valid_loss : 0.15242138504981995

     95400	  0.152476	  0.152421	  0.190458		CURRENT LEARNING RATE: 0.1924060722764893
previous_iter_valid_loss : 0.17555809020996094

     95500	  0.175264	  0.175558	  0.190916		CURRENT LEARNING RATE: 0.19221376237518928
previous_iter_valid_loss : 0.17228759825229645

     95600	  0.172477	  0.172288	  0.191179		CURRENT LEARNING RATE: 0.19202164468766764
previous_iter_valid_loss : 0.24271385371685028

     95700	  0.242959	  0.242714	  0.191733		CURRENT LEARNING RATE: 0.1918297190218067
previous_iter_valid_loss : 0.19091551005840302

     95800	  0.190199	  0.190916	  0.192285		CURRENT LEARNING RATE: 0.1916379851856808
previous_iter_valid_loss : 0.19303488731384277

     95900	  0.192713	  0.193035	  0.192233		CURRENT LEARNING RATE: 0.19144644298755603
previous_iter_valid_loss : 0.1974422186613083

     96000	  0.197348	  0.197442	  0.193070		CURRENT LEARNING RATE: 0.19125509223589018
previous_iter_valid_loss : 0.11267165839672089

     96100	  0.112532	  0.112672	  0.192916		CURRENT LEARNING RATE: 0.19106393273933253
previous_iter_valid_loss : 0.13155783712863922

     96200	  0.131803	  0.131558	  0.192110		CURRENT LEARNING RATE: 0.19087296430672354
previous_iter_valid_loss : 0.16981922090053558

     96300	  0.169260	  0.169819	  0.191868		CURRENT LEARNING RATE: 0.19068218674709478
previous_iter_valid_loss : 0.110172338783741

     96400	  0.110056	  0.110172	  0.191272		CURRENT LEARNING RATE: 0.19049159986966863
previous_iter_valid_loss : 0.15802600979804993

     96500	  0.157732	  0.158026	  0.191140		CURRENT LEARNING RATE: 0.19030120348385823
previous_iter_valid_loss : 0.1317148208618164

     96600	  0.131396	  0.131715	  0.190412		CURRENT LEARNING RATE: 0.19011099739926718
previous_iter_valid_loss : 0.28204140067100525

     96700	  0.281506	  0.282041	  0.191935		CURRENT LEARNING RATE: 0.18992098142568936
previous_iter_valid_loss : 0.20393121242523193

     96800	  0.203796	  0.203931	  0.189425		CURRENT LEARNING RATE: 0.18973115537310878
previous_iter_valid_loss : 0.1254459023475647

     96900	  0.125442	  0.125446	  0.188816		CURRENT LEARNING RATE: 0.18954151905169941
previous_iter_valid_loss : 0.13739530742168427

     97000	  0.137141	  0.137395	  0.188502		CURRENT LEARNING RATE: 0.18935207227182488
previous_iter_valid_loss : 0.21051743626594543

     97100	  0.209939	  0.210517	  0.188396		CURRENT LEARNING RATE: 0.1891628148440384
previous_iter_valid_loss : 0.24645565450191498

     97200	  0.246319	  0.246456	  0.189635		CURRENT LEARNING RATE: 0.18897374657908253
previous_iter_valid_loss : 0.2030620276927948

     97300	  0.202595	  0.203062	  0.189947		CURRENT LEARNING RATE: 0.18878486728788899
previous_iter_valid_loss : 0.18793939054012299

     97400	  0.187688	  0.187939	  0.190265		CURRENT LEARNING RATE: 0.18859617678157847
previous_iter_valid_loss : 0.19112075865268707

     97500	  0.191245	  0.191121	  0.189990		CURRENT LEARNING RATE: 0.18840767487146043
previous_iter_valid_loss : 0.13042695820331573

     97600	  0.130564	  0.130427	  0.189805		CURRENT LEARNING RATE: 0.18821936136903297
previous_iter_valid_loss : 0.1971427947282791

     97700	  0.197753	  0.197143	  0.189825		CURRENT LEARNING RATE: 0.18803123608598257
previous_iter_valid_loss : 0.15852659940719604

     97800	  0.158746	  0.158527	  0.190046		CURRENT LEARNING RATE: 0.18784329883418394
previous_iter_valid_loss : 0.09990742057561874

     97900	  0.100107	  0.099907	  0.188575		CURRENT LEARNING RATE: 0.18765554942569979
previous_iter_valid_loss : 0.149430051445961

     98000	  0.149621	  0.149430	  0.188271		CURRENT LEARNING RATE: 0.18746798767278067
previous_iter_valid_loss : 0.18232423067092896

     98100	  0.182163	  0.182324	  0.188650		CURRENT LEARNING RATE: 0.1872806133878649
previous_iter_valid_loss : 0.1835341602563858

     98200	  0.183310	  0.183534	  0.189116		CURRENT LEARNING RATE: 0.18709342638357807
previous_iter_valid_loss : 0.16812248528003693

     98300	  0.168328	  0.168122	  0.188875		CURRENT LEARNING RATE: 0.18690642647273326
previous_iter_valid_loss : 0.18040330708026886

     98400	  0.180403	  0.180403	  0.188944		CURRENT LEARNING RATE: 0.18671961346833046
previous_iter_valid_loss : 0.20185516774654388

     98500	  0.201646	  0.201855	  0.187802		CURRENT LEARNING RATE: 0.1865329871835567
previous_iter_valid_loss : 0.14311394095420837

     98600	  0.143431	  0.143114	  0.187536		CURRENT LEARNING RATE: 0.1863465474317857
previous_iter_valid_loss : 0.12086272239685059

     98700	  0.120716	  0.120863	  0.186289		CURRENT LEARNING RATE: 0.1861602940265776
previous_iter_valid_loss : 0.15772825479507446

     98800	  0.157479	  0.157728	  0.186251		CURRENT LEARNING RATE: 0.1859742267816791
previous_iter_valid_loss : 0.14175890386104584

     98900	  0.141830	  0.141759	  0.184607		CURRENT LEARNING RATE: 0.18578834551102286
previous_iter_valid_loss : 0.14585259556770325

     99000	  0.145999	  0.145853	  0.184304		CURRENT LEARNING RATE: 0.18560265002872758
previous_iter_valid_loss : 0.15916596353054047

     99100	  0.159495	  0.159166	  0.184212		CURRENT LEARNING RATE: 0.18541714014909783
previous_iter_valid_loss : 0.11449739336967468

     99200	  0.114393	  0.114497	  0.183376		CURRENT LEARNING RATE: 0.18523181568662367
previous_iter_valid_loss : 0.19128091633319855

     99300	  0.191566	  0.191281	  0.183890		CURRENT LEARNING RATE: 0.18504667645598064
previous_iter_valid_loss : 0.1559116542339325

     99400	  0.156215	  0.155912	  0.183485		CURRENT LEARNING RATE: 0.1848617222720295
previous_iter_valid_loss : 0.14578700065612793

     99500	  0.145860	  0.145787	  0.183799		CURRENT LEARNING RATE: 0.184676952949816
previous_iter_valid_loss : 0.13325147330760956

     99600	  0.133018	  0.133251	  0.183264		CURRENT LEARNING RATE: 0.1844923683045709
previous_iter_valid_loss : 0.13035964965820312

     99700	  0.130435	  0.130360	  0.182695		CURRENT LEARNING RATE: 0.1843079681517094
previous_iter_valid_loss : 0.1657290756702423

     99800	  0.165818	  0.165729	  0.182528		CURRENT LEARNING RATE: 0.18412375230683145
previous_iter_valid_loss : 0.14804790914058685

     99900	  0.148057	  0.148048	  0.182681		CURRENT LEARNING RATE: 0.18393972058572117
previous_iter_valid_loss : 0.16299866139888763

    100000	  0.163010	  0.162999	  0.181800		CURRENT LEARNING RATE: 0.1837558728043468
previous_iter_valid_loss : 0.22129228711128235

    100100	  0.221581	  0.221292	  0.182010		CURRENT LEARNING RATE: 0.18357220877886052
previous_iter_valid_loss : 0.23278243839740753

    100200	  0.232857	  0.232782	  0.182595		CURRENT LEARNING RATE: 0.18338872832559833
previous_iter_valid_loss : 0.22742298245429993

    100300	  0.227572	  0.227423	  0.182557		CURRENT LEARNING RATE: 0.18320543126107974
previous_iter_valid_loss : 0.11634564399719238

    100400	  0.116200	  0.116346	  0.181970		CURRENT LEARNING RATE: 0.1830223174020077
previous_iter_valid_loss : 0.1385367214679718

    100500	  0.139060	  0.138537	  0.181492		CURRENT LEARNING RATE: 0.18283938656526827
previous_iter_valid_loss : 0.12789574265480042

    100600	  0.127797	  0.127896	  0.181056		CURRENT LEARNING RATE: 0.18265663856793068
previous_iter_valid_loss : 0.12832632660865784

    100700	  0.128376	  0.128326	  0.180512		CURRENT LEARNING RATE: 0.18247407322724687
previous_iter_valid_loss : 0.13165351748466492

    100800	  0.131777	  0.131654	  0.179359		CURRENT LEARNING RATE: 0.18229169036065151
previous_iter_valid_loss : 0.15809720754623413

    100900	  0.158275	  0.158097	  0.178895		CURRENT LEARNING RATE: 0.18210948978576166
previous_iter_valid_loss : 0.14172634482383728

    101000	  0.141876	  0.141726	  0.178925		CURRENT LEARNING RATE: 0.18192747132037682
previous_iter_valid_loss : 0.12308412045240402

    101100	  0.123325	  0.123084	  0.178391		CURRENT LEARNING RATE: 0.1817456347824784
previous_iter_valid_loss : 0.25514936447143555

    101200	  0.255144	  0.255149	  0.179555		CURRENT LEARNING RATE: 0.18156397999022997
previous_iter_valid_loss : 0.13461057841777802

    101300	  0.134615	  0.134611	  0.177705		CURRENT LEARNING RATE: 0.18138250676197662
previous_iter_valid_loss : 0.12084795534610748

    101400	  0.120997	  0.120848	  0.176458		CURRENT LEARNING RATE: 0.1812012149162452
previous_iter_valid_loss : 0.14349809288978577

    101500	  0.143218	  0.143498	  0.176339		CURRENT LEARNING RATE: 0.18102010427174373
previous_iter_valid_loss : 0.13839790225028992

    101600	  0.138481	  0.138398	  0.175242		CURRENT LEARNING RATE: 0.18083917464736166
previous_iter_valid_loss : 0.18705765902996063

    101700	  0.186750	  0.187058	  0.173692		CURRENT LEARNING RATE: 0.18065842586216926
previous_iter_valid_loss : 0.12396161258220673

    101800	  0.124048	  0.123962	  0.172217		CURRENT LEARNING RATE: 0.18047785773541783
previous_iter_valid_loss : 0.11206773668527603

    101900	  0.112029	  0.112068	  0.171474		CURRENT LEARNING RATE: 0.18029747008653915
previous_iter_valid_loss : 0.12514062225818634

    102000	  0.125009	  0.125141	  0.169609		CURRENT LEARNING RATE: 0.1801172627351456
previous_iter_valid_loss : 0.11398830264806747

    102100	  0.113693	  0.113988	  0.168869		CURRENT LEARNING RATE: 0.17993723550102977
previous_iter_valid_loss : 0.12000573426485062

    102200	  0.119933	  0.120006	  0.168896		CURRENT LEARNING RATE: 0.17975738820416445
previous_iter_valid_loss : 0.12326301634311676

    102300	  0.123280	  0.123263	  0.168778		CURRENT LEARNING RATE: 0.1795777206647023
previous_iter_valid_loss : 0.13229280710220337

    102400	  0.131998	  0.132293	  0.168636		CURRENT LEARNING RATE: 0.1793982327029758
previous_iter_valid_loss : 0.1314980685710907

    102500	  0.131015	  0.131498	  0.168418		CURRENT LEARNING RATE: 0.17921892413949694
previous_iter_valid_loss : 0.20184984803199768

    102600	  0.201586	  0.201850	  0.168608		CURRENT LEARNING RATE: 0.17903979479495719
previous_iter_valid_loss : 0.1621532142162323

    102700	  0.161937	  0.162153	  0.168525		CURRENT LEARNING RATE: 0.1788608444902271
previous_iter_valid_loss : 0.10868050903081894

    102800	  0.109023	  0.108681	  0.167945		CURRENT LEARNING RATE: 0.17868207304635644
previous_iter_valid_loss : 0.14518241584300995

    102900	  0.145520	  0.145182	  0.167971		CURRENT LEARNING RATE: 0.1785034802845737
previous_iter_valid_loss : 0.11676174402236938

    103000	  0.117104	  0.116762	  0.167177		CURRENT LEARNING RATE: 0.17832506602628612
previous_iter_valid_loss : 0.16160063445568085

    103100	  0.161833	  0.161601	  0.166755		CURRENT LEARNING RATE: 0.1781468300930794
previous_iter_valid_loss : 0.17675887048244476

    103200	  0.177278	  0.176759	  0.167252		CURRENT LEARNING RATE: 0.17796877230671768
previous_iter_valid_loss : 0.132703498005867

    103300	  0.132835	  0.132703	  0.166906		CURRENT LEARNING RATE: 0.17779089248914307
previous_iter_valid_loss : 0.11316339671611786

    103400	  0.113365	  0.113163	  0.165884		CURRENT LEARNING RATE: 0.17761319046247576
previous_iter_valid_loss : 0.12128333747386932

    103500	  0.121658	  0.121283	  0.163847		CURRENT LEARNING RATE: 0.1774356660490137
previous_iter_valid_loss : 0.11738641560077667

    103600	  0.117547	  0.117386	  0.163211		CURRENT LEARNING RATE: 0.17725831907123252
previous_iter_valid_loss : 0.13893002271652222

    103700	  0.139383	  0.138930	  0.163285		CURRENT LEARNING RATE: 0.17708114935178512
previous_iter_valid_loss : 0.13712021708488464

    103800	  0.137468	  0.137120	  0.163062		CURRENT LEARNING RATE: 0.17690415671350188
previous_iter_valid_loss : 0.12376021593809128

    103900	  0.124080	  0.123760	  0.162560		CURRENT LEARNING RATE: 0.17672734097939008
previous_iter_valid_loss : 0.12573717534542084

    104000	  0.125982	  0.125737	  0.161804		CURRENT LEARNING RATE: 0.176550701972634
previous_iter_valid_loss : 0.11557183414697647

    104100	  0.115864	  0.115572	  0.160603		CURRENT LEARNING RATE: 0.17637423951659456
previous_iter_valid_loss : 0.10170970112085342

    104200	  0.101929	  0.101710	  0.159286		CURRENT LEARNING RATE: 0.17619795343480937
previous_iter_valid_loss : 0.2079937607049942

    104300	  0.208539	  0.207994	  0.159704		CURRENT LEARNING RATE: 0.1760218435509923
previous_iter_valid_loss : 0.12764884531497955

    104400	  0.127905	  0.127649	  0.159165		CURRENT LEARNING RATE: 0.17584590968903346
previous_iter_valid_loss : 0.09952046722173691

    104500	  0.099502	  0.099520	  0.158728		CURRENT LEARNING RATE: 0.1756701516729989
previous_iter_valid_loss : 0.13960687816143036

    104600	  0.140151	  0.139607	  0.158421		CURRENT LEARNING RATE: 0.17549456932713073
previous_iter_valid_loss : 0.1024702936410904

    104700	  0.102620	  0.102470	  0.157162		CURRENT LEARNING RATE: 0.17531916247584647
previous_iter_valid_loss : 0.12539586424827576

    104800	  0.125798	  0.125396	  0.156265		CURRENT LEARNING RATE: 0.1751439309437393
previous_iter_valid_loss : 0.1261250376701355

    104900	  0.126299	  0.126125	  0.156130		CURRENT LEARNING RATE: 0.17496887455557766
previous_iter_valid_loss : 0.10072938352823257

    105000	  0.100980	  0.100729	  0.155148		CURRENT LEARNING RATE: 0.1747939931363052
previous_iter_valid_loss : 0.12535984814167023

    105100	  0.125940	  0.125360	  0.153309		CURRENT LEARNING RATE: 0.1746192865110404
previous_iter_valid_loss : 0.12305554002523422

    105200	  0.123159	  0.123056	  0.152467		CURRENT LEARNING RATE: 0.17444475450507668
previous_iter_valid_loss : 0.19587203860282898

    105300	  0.195836	  0.195872	  0.152773		CURRENT LEARNING RATE: 0.174270396943882
previous_iter_valid_loss : 0.13674665987491608

    105400	  0.136442	  0.136747	  0.152616		CURRENT LEARNING RATE: 0.1740962136530988
previous_iter_valid_loss : 0.12852677702903748

    105500	  0.128412	  0.128527	  0.152146		CURRENT LEARNING RATE: 0.1739222044585437
previous_iter_valid_loss : 0.09602806717157364

    105600	  0.095958	  0.096028	  0.151384		CURRENT LEARNING RATE: 0.17374836918620762
previous_iter_valid_loss : 0.19518554210662842

    105700	  0.195612	  0.195186	  0.150908		CURRENT LEARNING RATE: 0.17357470766225516
previous_iter_valid_loss : 0.09569922089576721

    105800	  0.095665	  0.095699	  0.149956		CURRENT LEARNING RATE: 0.17340121971302488
previous_iter_valid_loss : 0.1059425100684166

    105900	  0.106072	  0.105943	  0.149085		CURRENT LEARNING RATE: 0.1732279051650287
previous_iter_valid_loss : 0.1015143170952797

    106000	  0.101588	  0.101514	  0.148126		CURRENT LEARNING RATE: 0.1730547638449522
previous_iter_valid_loss : 0.26718711853027344

    106100	  0.267542	  0.267187	  0.149671		CURRENT LEARNING RATE: 0.17288179557965389
previous_iter_valid_loss : 0.1597471833229065

    106200	  0.160369	  0.159747	  0.149953		CURRENT LEARNING RATE: 0.1727090001961656
previous_iter_valid_loss : 0.13012175261974335

    106300	  0.129847	  0.130122	  0.149556		CURRENT LEARNING RATE: 0.17253637752169187
previous_iter_valid_loss : 0.12239893525838852

    106400	  0.122662	  0.122399	  0.149678		CURRENT LEARNING RATE: 0.1723639273836101
previous_iter_valid_loss : 0.1357364058494568

    106500	  0.135545	  0.135736	  0.149455		CURRENT LEARNING RATE: 0.17219164960947
previous_iter_valid_loss : 0.10680807381868362

    106600	  0.106557	  0.106808	  0.149206		CURRENT LEARNING RATE: 0.17201954402699393
previous_iter_valid_loss : 0.10258922725915909

    106700	  0.102329	  0.102589	  0.147412		CURRENT LEARNING RATE: 0.17184761046407618
previous_iter_valid_loss : 0.12186530977487564

    106800	  0.121730	  0.121865	  0.146591		CURRENT LEARNING RATE: 0.17167584874878325
previous_iter_valid_loss : 0.1692519634962082

    106900	  0.169604	  0.169252	  0.147029		CURRENT LEARNING RATE: 0.17150425870935332
previous_iter_valid_loss : 0.0914575457572937

    107000	  0.091315	  0.091458	  0.146570		CURRENT LEARNING RATE: 0.17133284017419645
previous_iter_valid_loss : 0.12231606245040894

    107100	  0.122008	  0.122316	  0.145688		CURRENT LEARNING RATE: 0.17116159297189398
previous_iter_valid_loss : 0.09120550751686096

    107200	  0.091060	  0.091206	  0.144135		CURRENT LEARNING RATE: 0.1709905169311988
previous_iter_valid_loss : 0.11308813840150833

    107300	  0.112809	  0.113088	  0.143236		CURRENT LEARNING RATE: 0.17081961188103473
previous_iter_valid_loss : 0.09784871339797974

    107400	  0.097874	  0.097849	  0.142335		CURRENT LEARNING RATE: 0.17064887765049686
previous_iter_valid_loss : 0.12488959729671478

    107500	  0.125070	  0.124890	  0.141672		CURRENT LEARNING RATE: 0.17047831406885078
previous_iter_valid_loss : 0.2374844253063202

    107600	  0.237350	  0.237484	  0.142743		CURRENT LEARNING RATE: 0.17030792096553304
previous_iter_valid_loss : 0.09017588943243027

    107700	  0.090018	  0.090176	  0.141673		CURRENT LEARNING RATE: 0.1701376981701504
previous_iter_valid_loss : 0.13004127144813538

    107800	  0.129890	  0.130041	  0.141388		CURRENT LEARNING RATE: 0.16996764551248017
previous_iter_valid_loss : 0.14308324456214905

    107900	  0.142790	  0.143083	  0.141820		CURRENT LEARNING RATE: 0.16979776282246956
previous_iter_valid_loss : 0.10064512491226196

    108000	  0.100409	  0.100645	  0.141332		CURRENT LEARNING RATE: 0.16962804993023597
previous_iter_valid_loss : 0.12329329550266266

    108100	  0.123007	  0.123293	  0.140742		CURRENT LEARNING RATE: 0.1694585066660664
previous_iter_valid_loss : 0.17825299501419067

    108200	  0.178030	  0.178253	  0.140689		CURRENT LEARNING RATE: 0.16928913286041766
previous_iter_valid_loss : 0.15192148089408875

    108300	  0.152272	  0.151921	  0.140527		CURRENT LEARNING RATE: 0.16911992834391584
previous_iter_valid_loss : 0.11048858612775803

    108400	  0.110788	  0.110489	  0.139828		CURRENT LEARNING RATE: 0.16895089294735652
previous_iter_valid_loss : 0.11598425358533859

    108500	  0.116227	  0.115984	  0.138969		CURRENT LEARNING RATE: 0.16878202650170418
previous_iter_valid_loss : 0.15849286317825317

    108600	  0.158289	  0.158493	  0.139123		CURRENT LEARNING RATE: 0.16861332883809244
previous_iter_valid_loss : 0.0902954488992691

    108700	  0.090278	  0.090295	  0.138817		CURRENT LEARNING RATE: 0.16844479978782353
previous_iter_valid_loss : 0.1253690868616104

    108800	  0.125104	  0.125369	  0.138494		CURRENT LEARNING RATE: 0.1682764391823685
previous_iter_valid_loss : 0.14617018401622772

    108900	  0.146560	  0.146170	  0.138538		CURRENT LEARNING RATE: 0.16810824685336667
previous_iter_valid_loss : 0.08831244707107544

    109000	  0.088205	  0.088312	  0.137963		CURRENT LEARNING RATE: 0.1679402226326257
previous_iter_valid_loss : 0.10041341930627823

    109100	  0.100388	  0.100413	  0.137375		CURRENT LEARNING RATE: 0.16777236635212134
previous_iter_valid_loss : 0.10782178491353989

    109200	  0.107537	  0.107822	  0.137308		CURRENT LEARNING RATE: 0.16760467784399732
previous_iter_valid_loss : 0.11182118952274323

    109300	  0.111531	  0.111821	  0.136514		CURRENT LEARNING RATE: 0.1674371569405651
previous_iter_valid_loss : 0.10829515010118484

    109400	  0.108104	  0.108295	  0.136037		CURRENT LEARNING RATE: 0.1672698034743038
previous_iter_valid_loss : 0.12993158400058746

    109500	  0.129761	  0.129932	  0.135879		CURRENT LEARNING RATE: 0.16710261727785988
previous_iter_valid_loss : 0.11116406321525574

    109600	  0.111064	  0.111164	  0.135658		CURRENT LEARNING RATE: 0.1669355981840472
previous_iter_valid_loss : 0.11119458824396133

    109700	  0.110890	  0.111195	  0.135466		CURRENT LEARNING RATE: 0.1667687460258466
previous_iter_valid_loss : 0.12124545872211456

    109800	  0.120929	  0.121245	  0.135022		CURRENT LEARNING RATE: 0.16660206063640592
previous_iter_valid_loss : 0.1544499546289444

    109900	  0.154839	  0.154450	  0.135086		CURRENT LEARNING RATE: 0.16643554184903978
previous_iter_valid_loss : 0.152427077293396

    110000	  0.152811	  0.152427	  0.134980		CURRENT LEARNING RATE: 0.16626918949722935
previous_iter_valid_loss : 0.10406351834535599

    110100	  0.104042	  0.104064	  0.133808		CURRENT LEARNING RATE: 0.16610300341462222
previous_iter_valid_loss : 0.13318435847759247

    110200	  0.133427	  0.133184	  0.132812		CURRENT LEARNING RATE: 0.16593698343503244
previous_iter_valid_loss : 0.09918411076068878

    110300	  0.099251	  0.099184	  0.131529		CURRENT LEARNING RATE: 0.16577112939243985
previous_iter_valid_loss : 0.1268506795167923

    110400	  0.127418	  0.126851	  0.131634		CURRENT LEARNING RATE: 0.1656054411209905
previous_iter_valid_loss : 0.08722826838493347

    110500	  0.087082	  0.087228	  0.131121		CURRENT LEARNING RATE: 0.16543991845499603
previous_iter_valid_loss : 0.11054015904664993

    110600	  0.110637	  0.110540	  0.130948		CURRENT LEARNING RATE: 0.16527456122893386
previous_iter_valid_loss : 0.1286126971244812

    110700	  0.128874	  0.128613	  0.130951		CURRENT LEARNING RATE: 0.16510936927744665
previous_iter_valid_loss : 0.09895416349172592

    110800	  0.098758	  0.098954	  0.130624		CURRENT LEARNING RATE: 0.1649443424353425
previous_iter_valid_loss : 0.1167871281504631

    110900	  0.116669	  0.116787	  0.130210		CURRENT LEARNING RATE: 0.16477948053759453
previous_iter_valid_loss : 0.09447026252746582

    111000	  0.094510	  0.094470	  0.129738		CURRENT LEARNING RATE: 0.16461478341934083
previous_iter_valid_loss : 0.10042651742696762

    111100	  0.100263	  0.100427	  0.129511		CURRENT LEARNING RATE: 0.16445025091588425
previous_iter_valid_loss : 0.10655266046524048

    111200	  0.106430	  0.106553	  0.128025		CURRENT LEARNING RATE: 0.1642858828626923
previous_iter_valid_loss : 0.17418783903121948

    111300	  0.173970	  0.174188	  0.128421		CURRENT LEARNING RATE: 0.16412167909539688
previous_iter_valid_loss : 0.14468729496002197

    111400	  0.144874	  0.144687	  0.128660		CURRENT LEARNING RATE: 0.16395763944979427
previous_iter_valid_loss : 0.1284545511007309

    111500	  0.128675	  0.128455	  0.128509		CURRENT LEARNING RATE: 0.16379376376184476
previous_iter_valid_loss : 0.17437702417373657

    111600	  0.174199	  0.174377	  0.128869		CURRENT LEARNING RATE: 0.16363005186767265
previous_iter_valid_loss : 0.1317915916442871

    111700	  0.131959	  0.131792	  0.128316		CURRENT LEARNING RATE: 0.16346650360356604
previous_iter_valid_loss : 0.12583810091018677

    111800	  0.126124	  0.125838	  0.128335		CURRENT LEARNING RATE: 0.1633031188059767
previous_iter_valid_loss : 0.09138841181993484

    111900	  0.091343	  0.091388	  0.128128		CURRENT LEARNING RATE: 0.16313989731151973
previous_iter_valid_loss : 0.11025916039943695

    112000	  0.110468	  0.110259	  0.127979		CURRENT LEARNING RATE: 0.16297683895697368
previous_iter_valid_loss : 0.10638720542192459

    112100	  0.106370	  0.106387	  0.127903		CURRENT LEARNING RATE: 0.16281394357928017
previous_iter_valid_loss : 0.125003844499588

    112200	  0.125148	  0.125004	  0.127953		CURRENT LEARNING RATE: 0.1626512110155438
previous_iter_valid_loss : 0.13715492188930511

    112300	  0.137467	  0.137155	  0.128092		CURRENT LEARNING RATE: 0.162488641103032
previous_iter_valid_loss : 0.09430032968521118

    112400	  0.094297	  0.094300	  0.127712		CURRENT LEARNING RATE: 0.16232623367917487
previous_iter_valid_loss : 0.09532544016838074

    112500	  0.095240	  0.095325	  0.127351		CURRENT LEARNING RATE: 0.16216398858156494
previous_iter_valid_loss : 0.1362486183643341

    112600	  0.136528	  0.136249	  0.126695		CURRENT LEARNING RATE: 0.16200190564795708
previous_iter_valid_loss : 0.1350267231464386

    112700	  0.135413	  0.135027	  0.126423		CURRENT LEARNING RATE: 0.1618399847162684
previous_iter_valid_loss : 0.12882982194423676

    112800	  0.129140	  0.128830	  0.126625		CURRENT LEARNING RATE: 0.16167822562457787
previous_iter_valid_loss : 0.17014320194721222

    112900	  0.170690	  0.170143	  0.126874		CURRENT LEARNING RATE: 0.16151662821112647
previous_iter_valid_loss : 0.08649896085262299

    113000	  0.086410	  0.086499	  0.126572		CURRENT LEARNING RATE: 0.16135519231431675
previous_iter_valid_loss : 0.11256613582372665

    113100	  0.112782	  0.112566	  0.126081		CURRENT LEARNING RATE: 0.16119391777271277
previous_iter_valid_loss : 0.12231222540140152

    113200	  0.122679	  0.122312	  0.125537		CURRENT LEARNING RATE: 0.16103280442504
previous_iter_valid_loss : 0.18640649318695068

    113300	  0.187095	  0.186406	  0.126074		CURRENT LEARNING RATE: 0.1608718521101851
previous_iter_valid_loss : 0.10659375041723251

    113400	  0.106869	  0.106594	  0.126008		CURRENT LEARNING RATE: 0.16071106066719568
previous_iter_valid_loss : 0.10638224333524704

    113500	  0.106586	  0.106382	  0.125859		CURRENT LEARNING RATE: 0.16055042993528035
previous_iter_valid_loss : 0.10555055737495422

    113600	  0.105551	  0.105551	  0.125741		CURRENT LEARNING RATE: 0.1603899597538083
previous_iter_valid_loss : 0.10863770544528961

    113700	  0.108385	  0.108638	  0.125438		CURRENT LEARNING RATE: 0.1602296499623094
previous_iter_valid_loss : 0.09166733175516129

    113800	  0.091461	  0.091667	  0.124984		CURRENT LEARNING RATE: 0.1600695004004738
previous_iter_valid_loss : 0.1178738996386528

    113900	  0.118003	  0.117874	  0.124925		CURRENT LEARNING RATE: 0.15990951090815197
previous_iter_valid_loss : 0.13821908831596375

    114000	  0.138616	  0.138219	  0.125049		CURRENT LEARNING RATE: 0.15974968132535433
previous_iter_valid_loss : 0.14106668531894684

    114100	  0.141389	  0.141067	  0.125304		CURRENT LEARNING RATE: 0.15959001149225135
previous_iter_valid_loss : 0.18682746589183807

    114200	  0.187282	  0.186827	  0.126156		CURRENT LEARNING RATE: 0.15943050124917316
previous_iter_valid_loss : 0.12291908264160156

    114300	  0.123155	  0.122919	  0.125305		CURRENT LEARNING RATE: 0.1592711504366095
previous_iter_valid_loss : 0.13128255307674408

    114400	  0.131485	  0.131283	  0.125341		CURRENT LEARNING RATE: 0.15911195889520954
previous_iter_valid_loss : 0.1814076453447342

    114500	  0.181829	  0.181408	  0.126160		CURRENT LEARNING RATE: 0.15895292646578177
previous_iter_valid_loss : 0.12952446937561035

    114600	  0.129252	  0.129524	  0.126059		CURRENT LEARNING RATE: 0.15879405298929367
previous_iter_valid_loss : 0.149509996175766

    114700	  0.149236	  0.149510	  0.126530		CURRENT LEARNING RATE: 0.15863533830687182
previous_iter_valid_loss : 0.09373236447572708

    114800	  0.093503	  0.093732	  0.126213		CURRENT LEARNING RATE: 0.15847678225980147
previous_iter_valid_loss : 0.18506677448749542

    114900	  0.184928	  0.185067	  0.126802		CURRENT LEARNING RATE: 0.15831838468952664
previous_iter_valid_loss : 0.1109861508011818

    115000	  0.110880	  0.110986	  0.126905		CURRENT LEARNING RATE: 0.1581601454376496
previous_iter_valid_loss : 0.11261726915836334

    115100	  0.112317	  0.112617	  0.126778		CURRENT LEARNING RATE: 0.15800206434593128
previous_iter_valid_loss : 0.09774032980203629

    115200	  0.097693	  0.097740	  0.126524		CURRENT LEARNING RATE: 0.1578441412562904
previous_iter_valid_loss : 0.4049408733844757

    115300	  0.404746	  0.404941	  0.128615		CURRENT LEARNING RATE: 0.15768637601080399
previous_iter_valid_loss : 0.1630212366580963

    115400	  0.163454	  0.163021	  0.128878		CURRENT LEARNING RATE: 0.15752876845170666
previous_iter_valid_loss : 0.10722161084413528

    115500	  0.107299	  0.107222	  0.128665		CURRENT LEARNING RATE: 0.15737131842139096
previous_iter_valid_loss : 0.17586775124073029

    115600	  0.175657	  0.175868	  0.129463		CURRENT LEARNING RATE: 0.15721402576240678
previous_iter_valid_loss : 0.12260197848081589

    115700	  0.122342	  0.122602	  0.128737		CURRENT LEARNING RATE: 0.15705689031746148
previous_iter_valid_loss : 0.10055746138095856

    115800	  0.100616	  0.100557	  0.128786		CURRENT LEARNING RATE: 0.15689991192941954
previous_iter_valid_loss : 0.11848422884941101

    115900	  0.118323	  0.118484	  0.128911		CURRENT LEARNING RATE: 0.15674309044130266
previous_iter_valid_loss : 0.18689578771591187

    116000	  0.186708	  0.186896	  0.129765		CURRENT LEARNING RATE: 0.15658642569628925
previous_iter_valid_loss : 0.11342363059520721

    116100	  0.113366	  0.113424	  0.128228		CURRENT LEARNING RATE: 0.1564299175377146
previous_iter_valid_loss : 0.09353107959032059

    116200	  0.093482	  0.093531	  0.127565		CURRENT LEARNING RATE: 0.1562735658090705
previous_iter_valid_loss : 0.09799788892269135

    116300	  0.097790	  0.097998	  0.127244		CURRENT LEARNING RATE: 0.15611737035400527
previous_iter_valid_loss : 0.09741555154323578

    116400	  0.097506	  0.097416	  0.126994		CURRENT LEARNING RATE: 0.15596133101632337
previous_iter_valid_loss : 0.12633392214775085

    116500	  0.126366	  0.126334	  0.126900		CURRENT LEARNING RATE: 0.15580544763998552
previous_iter_valid_loss : 0.12619152665138245

    116600	  0.126435	  0.126192	  0.127094		CURRENT LEARNING RATE: 0.15564972006910824
previous_iter_valid_loss : 0.10046686232089996

    116700	  0.100571	  0.100467	  0.127073		CURRENT LEARNING RATE: 0.15549414814796406
previous_iter_valid_loss : 0.08491676300764084

    116800	  0.084938	  0.084917	  0.126703		CURRENT LEARNING RATE: 0.15533873172098092
previous_iter_valid_loss : 0.1420454978942871

    116900	  0.142415	  0.142045	  0.126431		CURRENT LEARNING RATE: 0.15518347063274252
previous_iter_valid_loss : 0.09212662279605865

    117000	  0.092182	  0.092127	  0.126438		CURRENT LEARNING RATE: 0.15502836472798762
previous_iter_valid_loss : 0.09919921308755875

    117100	  0.099305	  0.099199	  0.126207		CURRENT LEARNING RATE: 0.1548734138516104
previous_iter_valid_loss : 0.108446404337883

    117200	  0.108392	  0.108446	  0.126379		CURRENT LEARNING RATE: 0.15471861784865992
previous_iter_valid_loss : 0.09342872351408005

    117300	  0.093501	  0.093429	  0.126183		CURRENT LEARNING RATE: 0.1545639765643402
previous_iter_valid_loss : 0.13588127493858337

    117400	  0.136151	  0.135881	  0.126563		CURRENT LEARNING RATE: 0.15440948984400993
previous_iter_valid_loss : 0.13684609532356262

    117500	  0.137246	  0.136846	  0.126683		CURRENT LEARNING RATE: 0.15425515753318236
previous_iter_valid_loss : 0.1089121475815773

    117600	  0.109066	  0.108912	  0.125397		CURRENT LEARNING RATE: 0.15410097947752516
previous_iter_valid_loss : 0.12272854894399643

    117700	  0.122984	  0.122729	  0.125722		CURRENT LEARNING RATE: 0.1539469555228603
previous_iter_valid_loss : 0.12334617227315903

    117800	  0.123676	  0.123346	  0.125655		CURRENT LEARNING RATE: 0.1537930855151638
previous_iter_valid_loss : 0.12426043301820755

    117900	  0.124545	  0.124260	  0.125467		CURRENT LEARNING RATE: 0.15363936930056563
previous_iter_valid_loss : 0.13830922544002533

    118000	  0.138727	  0.138309	  0.125844		CURRENT LEARNING RATE: 0.15348580672534953
previous_iter_valid_loss : 0.1283835768699646

    118100	  0.128736	  0.128384	  0.125895		CURRENT LEARNING RATE: 0.153332397635953
previous_iter_valid_loss : 0.1271432787179947

    118200	  0.127555	  0.127143	  0.125384		CURRENT LEARNING RATE: 0.15317914187896683
previous_iter_valid_loss : 0.18797259032726288

    118300	  0.188634	  0.187973	  0.125744		CURRENT LEARNING RATE: 0.15302603930113534
previous_iter_valid_loss : 0.12097248435020447

    118400	  0.121263	  0.120972	  0.125849		CURRENT LEARNING RATE: 0.15287308974935587
previous_iter_valid_loss : 0.12659063935279846

    118500	  0.126948	  0.126591	  0.125955		CURRENT LEARNING RATE: 0.15272029307067894
previous_iter_valid_loss : 0.11028759181499481

    118600	  0.110537	  0.110288	  0.125473		CURRENT LEARNING RATE: 0.15256764911230775
previous_iter_valid_loss : 0.1378633677959442

    118700	  0.138264	  0.137863	  0.125949		CURRENT LEARNING RATE: 0.15241515772159842
previous_iter_valid_loss : 0.20056647062301636

    118800	  0.201253	  0.200566	  0.126701		CURRENT LEARNING RATE: 0.1522628187460595
previous_iter_valid_loss : 0.10064695030450821

    118900	  0.100786	  0.100647	  0.126245		CURRENT LEARNING RATE: 0.15211063203335204
previous_iter_valid_loss : 0.11038476973772049

    119000	  0.110593	  0.110385	  0.126466		CURRENT LEARNING RATE: 0.15195859743128926
previous_iter_valid_loss : 0.1319819986820221

    119100	  0.132218	  0.131982	  0.126782		CURRENT LEARNING RATE: 0.15180671478783658
previous_iter_valid_loss : 0.12722960114479065

    119200	  0.127581	  0.127230	  0.126976		CURRENT LEARNING RATE: 0.15165498395111132
previous_iter_valid_loss : 0.18471792340278625

    119300	  0.185309	  0.184718	  0.127705		CURRENT LEARNING RATE: 0.1515034047693827
previous_iter_valid_loss : 0.11129757761955261

    119400	  0.111679	  0.111298	  0.127735		CURRENT LEARNING RATE: 0.15135197709107143
previous_iter_valid_loss : 0.16550661623477936

    119500	  0.166060	  0.165507	  0.128091		CURRENT LEARNING RATE: 0.1512007007647499
previous_iter_valid_loss : 0.08582711219787598

    119600	  0.085962	  0.085827	  0.127837		CURRENT LEARNING RATE: 0.1510495756391417
previous_iter_valid_loss : 0.10609927773475647

    119700	  0.106371	  0.106099	  0.127786		CURRENT LEARNING RATE: 0.15089860156312174
previous_iter_valid_loss : 0.1119694709777832

    119800	  0.112248	  0.111969	  0.127694		CURRENT LEARNING RATE: 0.1507477783857159
previous_iter_valid_loss : 0.14724913239479065

    119900	  0.147698	  0.147249	  0.127622		CURRENT LEARNING RATE: 0.15059710595610107
previous_iter_valid_loss : 0.16586458683013916

    120000	  0.166399	  0.165865	  0.127756		CURRENT LEARNING RATE: 0.15044658412360468
previous_iter_valid_loss : 0.17545044422149658

    120100	  0.176045	  0.175450	  0.128470		CURRENT LEARNING RATE: 0.150296212737705
previous_iter_valid_loss : 0.12067065387964249

    120200	  0.120998	  0.120671	  0.128345		CURRENT LEARNING RATE: 0.15014599164803052
previous_iter_valid_loss : 0.08619125187397003

    120300	  0.086294	  0.086191	  0.128215		CURRENT LEARNING RATE: 0.14999592070436024
previous_iter_valid_loss : 0.10232773423194885

    120400	  0.102530	  0.102328	  0.127970		CURRENT LEARNING RATE: 0.14984599975662316
previous_iter_valid_loss : 0.11044111847877502

    120500	  0.110730	  0.110441	  0.128202		CURRENT LEARNING RATE: 0.14969622865489834
previous_iter_valid_loss : 0.1500658094882965

    120600	  0.150516	  0.150066	  0.128597		CURRENT LEARNING RATE: 0.14954660724941463
previous_iter_valid_loss : 0.11925409734249115

    120700	  0.119569	  0.119254	  0.128503		CURRENT LEARNING RATE: 0.14939713539055063
previous_iter_valid_loss : 0.1119500920176506

    120800	  0.112213	  0.111950	  0.128633		CURRENT LEARNING RATE: 0.14924781292883446
previous_iter_valid_loss : 0.18978892266750336

    120900	  0.190404	  0.189789	  0.129363		CURRENT LEARNING RATE: 0.1490986397149437
previous_iter_valid_loss : 0.1301160454750061

    121000	  0.130434	  0.130116	  0.129720		CURRENT LEARNING RATE: 0.14894961559970504
previous_iter_valid_loss : 0.12389370054006577

    121100	  0.124218	  0.123894	  0.129954		CURRENT LEARNING RATE: 0.14880074043409441
previous_iter_valid_loss : 0.12235917896032333

    121200	  0.122684	  0.122359	  0.130112		CURRENT LEARNING RATE: 0.1486520140692366
previous_iter_valid_loss : 0.17976492643356323

    121300	  0.180341	  0.179765	  0.130168		CURRENT LEARNING RATE: 0.14850343635640526
previous_iter_valid_loss : 0.12150277197360992

    121400	  0.121823	  0.121503	  0.129936		CURRENT LEARNING RATE: 0.14835500714702263
previous_iter_valid_loss : 0.12098345160484314

    121500	  0.121294	  0.120983	  0.129862		CURRENT LEARNING RATE: 0.14820672629265955
previous_iter_valid_loss : 0.10008673369884491

    121600	  0.100331	  0.100087	  0.129119		CURRENT LEARNING RATE: 0.14805859364503507
previous_iter_valid_loss : 0.13609543442726135

    121700	  0.136473	  0.136095	  0.129162		CURRENT LEARNING RATE: 0.1479106090560166
previous_iter_valid_loss : 0.13917893171310425

    121800	  0.139588	  0.139179	  0.129295		CURRENT LEARNING RATE: 0.1477627723776195
previous_iter_valid_loss : 0.1505340337753296

    121900	  0.150991	  0.150534	  0.129887		CURRENT LEARNING RATE: 0.1476150834620071
previous_iter_valid_loss : 0.09249930828809738

    122000	  0.092577	  0.092499	  0.129709		CURRENT LEARNING RATE: 0.14746754216149044
previous_iter_valid_loss : 0.1109488233923912

    122100	  0.111134	  0.110949	  0.129755		CURRENT LEARNING RATE: 0.14732014832852827
previous_iter_valid_loss : 0.10423934459686279

    122200	  0.104457	  0.104239	  0.129547		CURRENT LEARNING RATE: 0.14717290181572668
previous_iter_valid_loss : 0.10942023247480392

    122300	  0.109612	  0.109420	  0.129270		CURRENT LEARNING RATE: 0.14702580247583918
previous_iter_valid_loss : 0.15526293218135834

    122400	  0.155672	  0.155263	  0.129879		CURRENT LEARNING RATE: 0.14687885016176638
previous_iter_valid_loss : 0.10846669226884842

    122500	  0.108676	  0.108467	  0.130011		CURRENT LEARNING RATE: 0.14673204472655604
previous_iter_valid_loss : 0.11527527868747711

    122600	  0.115509	  0.115275	  0.129801		CURRENT LEARNING RATE: 0.1465853860234026
previous_iter_valid_loss : 0.17591993510723114

    122700	  0.176486	  0.175920	  0.130210		CURRENT LEARNING RATE: 0.14643887390564744
previous_iter_valid_loss : 0.1215725839138031

    122800	  0.121909	  0.121573	  0.130137		CURRENT LEARNING RATE: 0.14629250822677833
previous_iter_valid_loss : 0.12827861309051514

    122900	  0.128540	  0.128279	  0.129719		CURRENT LEARNING RATE: 0.1461462888404297
previous_iter_valid_loss : 0.12900203466415405

    123000	  0.129287	  0.129002	  0.130144		CURRENT LEARNING RATE: 0.14600021560038204
previous_iter_valid_loss : 0.13589921593666077

    123100	  0.136223	  0.135899	  0.130377		CURRENT LEARNING RATE: 0.1458542883605622
previous_iter_valid_loss : 0.15650828182697296

    123200	  0.156956	  0.156508	  0.130719		CURRENT LEARNING RATE: 0.14570850697504284
previous_iter_valid_loss : 0.11465165764093399

    123300	  0.114853	  0.114652	  0.130002		CURRENT LEARNING RATE: 0.1455628712980426
previous_iter_valid_loss : 0.14828351140022278

    123400	  0.148680	  0.148284	  0.130418		CURRENT LEARNING RATE: 0.14541738118392578
previous_iter_valid_loss : 0.10635284334421158

    123500	  0.106609	  0.106353	  0.130418		CURRENT LEARNING RATE: 0.1452720364872023
previous_iter_valid_loss : 0.09778626263141632

    123600	  0.097916	  0.097786	  0.130340		CURRENT LEARNING RATE: 0.14512683706252735
previous_iter_valid_loss : 0.1010560616850853

    123700	  0.101264	  0.101056	  0.130265		CURRENT LEARNING RATE: 0.1449817827647016
previous_iter_valid_loss : 0.10111985355615616

    123800	  0.101303	  0.101120	  0.130359		CURRENT LEARNING RATE: 0.1448368734486707
previous_iter_valid_loss : 0.11799091845750809

    123900	  0.118211	  0.117991	  0.130360		CURRENT LEARNING RATE: 0.1446921089695253
previous_iter_valid_loss : 0.151885524392128

    124000	  0.152304	  0.151886	  0.130497		CURRENT LEARNING RATE: 0.14454748918250093
previous_iter_valid_loss : 0.1004459485411644

    124100	  0.100631	  0.100446	  0.130091		CURRENT LEARNING RATE: 0.14440301394297783
previous_iter_valid_loss : 0.1408047080039978

    124200	  0.141142	  0.140805	  0.129631		CURRENT LEARNING RATE: 0.14425868310648066
previous_iter_valid_loss : 0.10456471145153046

    124300	  0.104725	  0.104565	  0.129447		CURRENT LEARNING RATE: 0.14411449652867864
previous_iter_valid_loss : 0.11875458061695099

    124400	  0.118985	  0.118755	  0.129322		CURRENT LEARNING RATE: 0.1439704540653851
previous_iter_valid_loss : 0.1112070307135582

    124500	  0.111363	  0.111207	  0.128620		CURRENT LEARNING RATE: 0.1438265555725577
previous_iter_valid_loss : 0.10027699917554855

    124600	  0.100330	  0.100277	  0.128327		CURRENT LEARNING RATE: 0.14368280090629781
previous_iter_valid_loss : 0.12016692757606506

    124700	  0.120401	  0.120167	  0.128034		CURRENT LEARNING RATE: 0.14353918992285084
previous_iter_valid_loss : 0.11817193031311035

    124800	  0.118370	  0.118172	  0.128278		CURRENT LEARNING RATE: 0.1433957224786057
previous_iter_valid_loss : 0.20388802886009216

    124900	  0.204419	  0.203888	  0.128467		CURRENT LEARNING RATE: 0.14325239843009505
previous_iter_valid_loss : 0.0929480567574501

    125000	  0.093004	  0.092948	  0.128286		CURRENT LEARNING RATE: 0.14310921763399476
previous_iter_valid_loss : 0.1674785614013672

    125100	  0.167878	  0.167479	  0.128835		CURRENT LEARNING RATE: 0.142966179947124
previous_iter_valid_loss : 0.12168639898300171

    125200	  0.121882	  0.121686	  0.129074		CURRENT LEARNING RATE: 0.1428232852264451
previous_iter_valid_loss : 0.14929400384426117

    125300	  0.149697	  0.149294	  0.126518		CURRENT LEARNING RATE: 0.14268053332906333
previous_iter_valid_loss : 0.14704987406730652

    125400	  0.147423	  0.147050	  0.126358		CURRENT LEARNING RATE: 0.1425379241122268
previous_iter_valid_loss : 0.1269991397857666

    125500	  0.127262	  0.126999	  0.126556		CURRENT LEARNING RATE: 0.14239545743332624
previous_iter_valid_loss : 0.11331004649400711

    125600	  0.113418	  0.113310	  0.125930		CURRENT LEARNING RATE: 0.142253133149895
previous_iter_valid_loss : 0.12142812460660934

    125700	  0.121657	  0.121428	  0.125918		CURRENT LEARNING RATE: 0.14211095111960872
previous_iter_valid_loss : 0.13337746262550354

    125800	  0.133658	  0.133377	  0.126247		CURRENT LEARNING RATE: 0.14196891120028546
previous_iter_valid_loss : 0.1083306297659874

    125900	  0.108463	  0.108331	  0.126145		CURRENT LEARNING RATE: 0.1418270132498852
previous_iter_valid_loss : 0.11256500333547592

    126000	  0.112761	  0.112565	  0.125402		CURRENT LEARNING RATE: 0.14168525712651
previous_iter_valid_loss : 0.1209566742181778

    126100	  0.121222	  0.120957	  0.125477		CURRENT LEARNING RATE: 0.14154364268840375
previous_iter_valid_loss : 0.11984950304031372

    126200	  0.120115	  0.119850	  0.125740		CURRENT LEARNING RATE: 0.14140216979395198
previous_iter_valid_loss : 0.09381544589996338

    126300	  0.093831	  0.093815	  0.125699		CURRENT LEARNING RATE: 0.1412608383016818
previous_iter_valid_loss : 0.12453900277614594

    126400	  0.124800	  0.124539	  0.125970		CURRENT LEARNING RATE: 0.14111964807026167
previous_iter_valid_loss : 0.14950169622898102

    126500	  0.149896	  0.149502	  0.126201		CURRENT LEARNING RATE: 0.14097859895850137
previous_iter_valid_loss : 0.16216044127941132

    126600	  0.162618	  0.162160	  0.126561		CURRENT LEARNING RATE: 0.1408376908253518
previous_iter_valid_loss : 0.11735494434833527

    126700	  0.117544	  0.117355	  0.126730		CURRENT LEARNING RATE: 0.14069692352990476
previous_iter_valid_loss : 0.10747639089822769

    126800	  0.107626	  0.107476	  0.126956		CURRENT LEARNING RATE: 0.14055629693139302
previous_iter_valid_loss : 0.09776758402585983

    126900	  0.097778	  0.097768	  0.126513		CURRENT LEARNING RATE: 0.1404158108891899
previous_iter_valid_loss : 0.13152340054512024

    127000	  0.131807	  0.131523	  0.126907		CURRENT LEARNING RATE: 0.14027546526280937
previous_iter_valid_loss : 0.13885143399238586

    127100	  0.139205	  0.138851	  0.127303		CURRENT LEARNING RATE: 0.14013525991190579
previous_iter_valid_loss : 0.14944683015346527

    127200	  0.150008	  0.149447	  0.127713		CURRENT LEARNING RATE: 0.1399951946962738
previous_iter_valid_loss : 0.09993390738964081

    127300	  0.100045	  0.099934	  0.127778		CURRENT LEARNING RATE: 0.13985526947584817
previous_iter_valid_loss : 0.09429573267698288

    127400	  0.094375	  0.094296	  0.127362		CURRENT LEARNING RATE: 0.13971548411070367
previous_iter_valid_loss : 0.11798711121082306

    127500	  0.118219	  0.117987	  0.127174		CURRENT LEARNING RATE: 0.13957583846105492
previous_iter_valid_loss : 0.12872964143753052

    127600	  0.129088	  0.128730	  0.127372		CURRENT LEARNING RATE: 0.13943633238725628
previous_iter_valid_loss : 0.11751227080821991

    127700	  0.117762	  0.117512	  0.127320		CURRENT LEARNING RATE: 0.13929696574980163
previous_iter_valid_loss : 0.11576514691114426

    127800	  0.115953	  0.115765	  0.127244		CURRENT LEARNING RATE: 0.13915773840932436
previous_iter_valid_loss : 0.10640173405408859

    127900	  0.106542	  0.106402	  0.127065		CURRENT LEARNING RATE: 0.13901865022659707
previous_iter_valid_loss : 0.11581845581531525

    128000	  0.116034	  0.115818	  0.126841		CURRENT LEARNING RATE: 0.1388797010625316
previous_iter_valid_loss : 0.12186022102832794

    128100	  0.122093	  0.121860	  0.126775		CURRENT LEARNING RATE: 0.13874089077817875
previous_iter_valid_loss : 0.13527631759643555

    128200	  0.135615	  0.135276	  0.126857		CURRENT LEARNING RATE: 0.1386022192347283
previous_iter_valid_loss : 0.18178890645503998

    128300	  0.182380	  0.181789	  0.126795		CURRENT LEARNING RATE: 0.13846368629350855
previous_iter_valid_loss : 0.1036216989159584

    128400	  0.103767	  0.103622	  0.126621		CURRENT LEARNING RATE: 0.1383252918159867
previous_iter_valid_loss : 0.10909412801265717

    128500	  0.109329	  0.109094	  0.126446		CURRENT LEARNING RATE: 0.13818703566376817
previous_iter_valid_loss : 0.09752624481916428

    128600	  0.097644	  0.097526	  0.126319		CURRENT LEARNING RATE: 0.1380489176985968
previous_iter_valid_loss : 0.09491732716560364

    128700	  0.094965	  0.094917	  0.125889		CURRENT LEARNING RATE: 0.13791093778235466
previous_iter_valid_loss : 0.11889542639255524

    128800	  0.119109	  0.118895	  0.125073		CURRENT LEARNING RATE: 0.1377730957770618
previous_iter_valid_loss : 0.1274736225605011

    128900	  0.127699	  0.127474	  0.125341		CURRENT LEARNING RATE: 0.13763539154487617
previous_iter_valid_loss : 0.10552455484867096

    129000	  0.105657	  0.105525	  0.125292		CURRENT LEARNING RATE: 0.13749782494809357
previous_iter_valid_loss : 0.1351998746395111

    129100	  0.135440	  0.135200	  0.125324		CURRENT LEARNING RATE: 0.13736039584914736
previous_iter_valid_loss : 0.09738190472126007

    129200	  0.097507	  0.097382	  0.125026		CURRENT LEARNING RATE: 0.13722310411060848
previous_iter_valid_loss : 0.20272767543792725

    129300	  0.203368	  0.202728	  0.125206		CURRENT LEARNING RATE: 0.1370859495951851
previous_iter_valid_loss : 0.12489543110132217

    129400	  0.125150	  0.124895	  0.125342		CURRENT LEARNING RATE: 0.1369489321657228
previous_iter_valid_loss : 0.11901169270277023

    129500	  0.119263	  0.119012	  0.124877		CURRENT LEARNING RATE: 0.13681205168520402
previous_iter_valid_loss : 0.095335453748703

    129600	  0.095472	  0.095335	  0.124972		CURRENT LEARNING RATE: 0.13667530801674838
previous_iter_valid_loss : 0.13446971774101257

    129700	  0.134795	  0.134470	  0.125256		CURRENT LEARNING RATE: 0.1365387010236121
previous_iter_valid_loss : 0.22764258086681366

    129800	  0.228192	  0.227643	  0.126413		CURRENT LEARNING RATE: 0.13640223056918824
previous_iter_valid_loss : 0.15818698704242706

    129900	  0.158636	  0.158187	  0.126522		CURRENT LEARNING RATE: 0.1362658965170063
previous_iter_valid_loss : 0.12914998829364777

    130000	  0.129478	  0.129150	  0.126155		CURRENT LEARNING RATE: 0.13612969873073225
previous_iter_valid_loss : 0.13072934746742249

    130100	  0.131085	  0.130729	  0.125708		CURRENT LEARNING RATE: 0.13599363707416826
previous_iter_valid_loss : 0.1509305089712143

    130200	  0.151305	  0.150931	  0.126010		CURRENT LEARNING RATE: 0.13585771141125272
previous_iter_valid_loss : 0.09803703427314758

    130300	  0.098147	  0.098037	  0.126129		CURRENT LEARNING RATE: 0.13572192160605986
previous_iter_valid_loss : 0.12436570227146149

    130400	  0.124598	  0.124366	  0.126349		CURRENT LEARNING RATE: 0.13558626752279995
previous_iter_valid_loss : 0.14748430252075195

    130500	  0.147856	  0.147484	  0.126719		CURRENT LEARNING RATE: 0.13545074902581883
previous_iter_valid_loss : 0.09961345791816711

    130600	  0.099706	  0.099613	  0.126215		CURRENT LEARNING RATE: 0.13531536597959806
previous_iter_valid_loss : 0.15794123709201813

    130700	  0.158397	  0.157941	  0.126602		CURRENT LEARNING RATE: 0.1351801182487545
previous_iter_valid_loss : 0.12433898448944092

    130800	  0.124665	  0.124339	  0.126726		CURRENT LEARNING RATE: 0.1350450056980405
previous_iter_valid_loss : 0.11214813590049744

    130900	  0.112348	  0.112148	  0.125949		CURRENT LEARNING RATE: 0.1349100281923434
previous_iter_valid_loss : 0.13372854888439178

    131000	  0.134016	  0.133729	  0.125985		CURRENT LEARNING RATE: 0.1347751855966858
previous_iter_valid_loss : 0.11950695514678955

    131100	  0.119738	  0.119507	  0.125942		CURRENT LEARNING RATE: 0.13464047777622498
previous_iter_valid_loss : 0.12724918127059937

    131200	  0.127495	  0.127249	  0.125990		CURRENT LEARNING RATE: 0.1345059045962532
previous_iter_valid_loss : 0.10320974141359329

    131300	  0.103379	  0.103210	  0.125225		CURRENT LEARNING RATE: 0.13437146592219718
previous_iter_valid_loss : 0.11426509916782379

    131400	  0.114517	  0.114265	  0.125153		CURRENT LEARNING RATE: 0.1342371616196183
previous_iter_valid_loss : 0.08872941881418228

    131500	  0.088760	  0.088729	  0.124830		CURRENT LEARNING RATE: 0.1341029915542122
previous_iter_valid_loss : 0.13377238810062408

    131600	  0.134069	  0.133772	  0.125167		CURRENT LEARNING RATE: 0.13396895559180888
previous_iter_valid_loss : 0.09000800549983978

    131700	  0.090134	  0.090008	  0.124706		CURRENT LEARNING RATE: 0.13383505359837228
previous_iter_valid_loss : 0.14114631712436676

    131800	  0.141498	  0.141146	  0.124726		CURRENT LEARNING RATE: 0.13370128544000046
previous_iter_valid_loss : 0.11897598206996918

    131900	  0.119309	  0.118976	  0.124410		CURRENT LEARNING RATE: 0.13356765098292517
previous_iter_valid_loss : 0.11453618854284286

    132000	  0.114737	  0.114536	  0.124630		CURRENT LEARNING RATE: 0.13343415009351206
previous_iter_valid_loss : 0.1331833302974701

    132100	  0.133518	  0.133183	  0.124853		CURRENT LEARNING RATE: 0.1333007826382601
previous_iter_valid_loss : 0.1503438502550125

    132200	  0.150729	  0.150344	  0.125314		CURRENT LEARNING RATE: 0.13316754848380194
previous_iter_valid_loss : 0.1444435864686966

    132300	  0.144810	  0.144444	  0.125664		CURRENT LEARNING RATE: 0.13303444749690332
previous_iter_valid_loss : 0.08754622936248779

    132400	  0.087586	  0.087546	  0.124987		CURRENT LEARNING RATE: 0.1329014795444633
previous_iter_valid_loss : 0.11684447526931763

    132500	  0.117016	  0.116844	  0.125071		CURRENT LEARNING RATE: 0.1327686444935139
previous_iter_valid_loss : 0.1337926834821701

    132600	  0.134070	  0.133793	  0.125256		CURRENT LEARNING RATE: 0.13263594221122005
previous_iter_valid_loss : 0.10969306528568268

    132700	  0.109874	  0.109693	  0.124594		CURRENT LEARNING RATE: 0.13250337256487946
previous_iter_valid_loss : 0.08944666385650635

    132800	  0.089474	  0.089447	  0.124272		CURRENT LEARNING RATE: 0.13237093542192252
previous_iter_valid_loss : 0.11679325252771378

    132900	  0.117037	  0.116793	  0.124157		CURRENT LEARNING RATE: 0.13223863064991198
previous_iter_valid_loss : 0.1371849626302719

    133000	  0.137554	  0.137185	  0.124239		CURRENT LEARNING RATE: 0.13210645811654315
previous_iter_valid_loss : 0.11895185708999634

    133100	  0.119141	  0.118952	  0.124070		CURRENT LEARNING RATE: 0.1319744176896434
previous_iter_valid_loss : 0.12441588938236237

    133200	  0.124647	  0.124416	  0.123749		CURRENT LEARNING RATE: 0.1318425092371724
previous_iter_valid_loss : 0.14736667275428772

    133300	  0.147709	  0.147367	  0.124076		CURRENT LEARNING RATE: 0.13171073262722155
previous_iter_valid_loss : 0.10877512395381927

    133400	  0.108954	  0.108775	  0.123681		CURRENT LEARNING RATE: 0.13157908772801435
previous_iter_valid_loss : 0.13728725910186768

    133500	  0.137612	  0.137287	  0.123990		CURRENT LEARNING RATE: 0.13144757440790583
previous_iter_valid_loss : 0.18551090359687805

    133600	  0.186051	  0.185511	  0.124868		CURRENT LEARNING RATE: 0.13131619253538268
previous_iter_valid_loss : 0.0984216034412384

    133700	  0.098621	  0.098422	  0.124841		CURRENT LEARNING RATE: 0.131184941979063
previous_iter_valid_loss : 0.10742484033107758

    133800	  0.107609	  0.107425	  0.124904		CURRENT LEARNING RATE: 0.13105382260769624
previous_iter_valid_loss : 0.11039259284734726

    133900	  0.110631	  0.110393	  0.124828		CURRENT LEARNING RATE: 0.130922834290163
previous_iter_valid_loss : 0.11574549227952957

    134000	  0.115985	  0.115745	  0.124467		CURRENT LEARNING RATE: 0.13079197689547498
previous_iter_valid_loss : 0.09678581357002258

    134100	  0.096885	  0.096786	  0.124430		CURRENT LEARNING RATE: 0.1306612502927747
previous_iter_valid_loss : 0.0971696749329567

    134200	  0.097274	  0.097170	  0.123994		CURRENT LEARNING RATE: 0.13053065435133565
previous_iter_valid_loss : 0.14664499461650848

    134300	  0.147079	  0.146645	  0.124415		CURRENT LEARNING RATE: 0.13040018894056182
previous_iter_valid_loss : 0.19236670434474945

    134400	  0.192903	  0.192367	  0.125151		CURRENT LEARNING RATE: 0.1302698539299878
previous_iter_valid_loss : 0.11799095571041107

    134500	  0.118266	  0.117991	  0.125219		CURRENT LEARNING RATE: 0.13013964918927856
previous_iter_valid_loss : 0.12915198504924774

    134600	  0.129405	  0.129152	  0.125507		CURRENT LEARNING RATE: 0.13000957458822937
previous_iter_valid_loss : 0.09844539314508438

    134700	  0.098561	  0.098445	  0.125290		CURRENT LEARNING RATE: 0.12987962999676556
previous_iter_valid_loss : 0.14158256351947784

    134800	  0.141978	  0.141583	  0.125524		CURRENT LEARNING RATE: 0.12974981528494262
previous_iter_valid_loss : 0.1236167624592781

    134900	  0.123843	  0.123617	  0.124722		CURRENT LEARNING RATE: 0.12962013032294575
previous_iter_valid_loss : 0.15824484825134277

    135000	  0.158671	  0.158245	  0.125375		CURRENT LEARNING RATE: 0.12949057498109
previous_iter_valid_loss : 0.12488722801208496

    135100	  0.125083	  0.124887	  0.124949		CURRENT LEARNING RATE: 0.12936114912982002
previous_iter_valid_loss : 0.09002815186977386

    135200	  0.090108	  0.090028	  0.124632		CURRENT LEARNING RATE: 0.12923185263971
previous_iter_valid_loss : 0.10473967343568802

    135300	  0.104897	  0.104740	  0.124187		CURRENT LEARNING RATE: 0.12910268538146333
previous_iter_valid_loss : 0.1026744470000267

    135400	  0.102820	  0.102674	  0.123743		CURRENT LEARNING RATE: 0.12897364722591284
previous_iter_valid_loss : 0.11494522541761398

    135500	  0.115122	  0.114945	  0.123622		CURRENT LEARNING RATE: 0.12884473804402027
previous_iter_valid_loss : 0.11642986536026001

    135600	  0.116664	  0.116430	  0.123653		CURRENT LEARNING RATE: 0.12871595770687655
previous_iter_valid_loss : 0.12522611021995544

    135700	  0.125491	  0.125226	  0.123691		CURRENT LEARNING RATE: 0.1285873060857012
previous_iter_valid_loss : 0.10471422970294952

    135800	  0.104864	  0.104714	  0.123405		CURRENT LEARNING RATE: 0.12845878305184272
previous_iter_valid_loss : 0.14860516786575317

    135900	  0.149009	  0.148605	  0.123808		CURRENT LEARNING RATE: 0.12833038847677794
previous_iter_valid_loss : 0.1449357569217682

    136000	  0.145287	  0.144936	  0.124131		CURRENT LEARNING RATE: 0.12820212223211236
previous_iter_valid_loss : 0.11832477897405624

    136100	  0.118657	  0.118325	  0.124105		CURRENT LEARNING RATE: 0.12807398418957966
previous_iter_valid_loss : 0.107134610414505

    136200	  0.107316	  0.107135	  0.123978		CURRENT LEARNING RATE: 0.12794597422104187
previous_iter_valid_loss : 0.12187795341014862

    136300	  0.122118	  0.121878	  0.124258		CURRENT LEARNING RATE: 0.12781809219848891
previous_iter_valid_loss : 0.12066608667373657

    136400	  0.120922	  0.120666	  0.124220		CURRENT LEARNING RATE: 0.12769033799403884
previous_iter_valid_loss : 0.1259130984544754

    136500	  0.126174	  0.125913	  0.123984		CURRENT LEARNING RATE: 0.1275627114799374
previous_iter_valid_loss : 0.16262377798557281

    136600	  0.162982	  0.162624	  0.123988		CURRENT LEARNING RATE: 0.12743521252855808
previous_iter_valid_loss : 0.11461346596479416

    136700	  0.114836	  0.114613	  0.123961		CURRENT LEARNING RATE: 0.12730784101240186
previous_iter_valid_loss : 0.15370459854602814

    136800	  0.154048	  0.153705	  0.124423		CURRENT LEARNING RATE: 0.12718059680409732
previous_iter_valid_loss : 0.15429359674453735

    136900	  0.154678	  0.154294	  0.124989		CURRENT LEARNING RATE: 0.12705347977640014
previous_iter_valid_loss : 0.14920352399349213

    137000	  0.149587	  0.149204	  0.125165		CURRENT LEARNING RATE: 0.12692648980219334
previous_iter_valid_loss : 0.16473937034606934

    137100	  0.165152	  0.164739	  0.125424		CURRENT LEARNING RATE: 0.12679962675448692
previous_iter_valid_loss : 0.10652052611112595

    137200	  0.106677	  0.106521	  0.124995		CURRENT LEARNING RATE: 0.12667289050641783
previous_iter_valid_loss : 0.1124633401632309

    137300	  0.112673	  0.112463	  0.125120		CURRENT LEARNING RATE: 0.12654628093124978
previous_iter_valid_loss : 0.12246332317590714

    137400	  0.122776	  0.122463	  0.125402		CURRENT LEARNING RATE: 0.12641979790237323
previous_iter_valid_loss : 0.15429328382015228

    137500	  0.154787	  0.154293	  0.125765		CURRENT LEARNING RATE: 0.12629344129330514
previous_iter_valid_loss : 0.11176881194114685

    137600	  0.111948	  0.111769	  0.125595		CURRENT LEARNING RATE: 0.12616721097768882
previous_iter_valid_loss : 0.1118231862783432

    137700	  0.112061	  0.111823	  0.125538		CURRENT LEARNING RATE: 0.126041106829294
previous_iter_valid_loss : 0.10773737728595734

    137800	  0.107885	  0.107737	  0.125458		CURRENT LEARNING RATE: 0.12591512872201652
previous_iter_valid_loss : 0.09550856798887253

    137900	  0.095584	  0.095509	  0.125349		CURRENT LEARNING RATE: 0.12578927652987826
previous_iter_valid_loss : 0.1274791806936264

    138000	  0.127824	  0.127479	  0.125466		CURRENT LEARNING RATE: 0.125663550127027
previous_iter_valid_loss : 0.14347496628761292

    138100	  0.143882	  0.143475	  0.125682		CURRENT LEARNING RATE: 0.12553794938773635
previous_iter_valid_loss : 0.0946011021733284

    138200	  0.094720	  0.094601	  0.125275		CURRENT LEARNING RATE: 0.12541247418640555
previous_iter_valid_loss : 0.1472020000219345

    138300	  0.147628	  0.147202	  0.124929		CURRENT LEARNING RATE: 0.1252871243975594
previous_iter_valid_loss : 0.11627061665058136

    138400	  0.116480	  0.116271	  0.125056		CURRENT LEARNING RATE: 0.12516189989584806
previous_iter_valid_loss : 0.12240979820489883

    138500	  0.122656	  0.122410	  0.125189		CURRENT LEARNING RATE: 0.12503680055604707
previous_iter_valid_loss : 0.10308016836643219

    138600	  0.103276	  0.103080	  0.125245		CURRENT LEARNING RATE: 0.12491182625305702
previous_iter_valid_loss : 0.1570909023284912

    138700	  0.157603	  0.157091	  0.125866		CURRENT LEARNING RATE: 0.12478697686190367
previous_iter_valid_loss : 0.12411821633577347

    138800	  0.124437	  0.124118	  0.125919		CURRENT LEARNING RATE: 0.12466225225773755
previous_iter_valid_loss : 0.0921974629163742

    138900	  0.092296	  0.092197	  0.125566		CURRENT LEARNING RATE: 0.12453765231583411
previous_iter_valid_loss : 0.16097724437713623

    139000	  0.161493	  0.160977	  0.126120		CURRENT LEARNING RATE: 0.12441317691159333
previous_iter_valid_loss : 0.15973006188869476

    139100	  0.160200	  0.159730	  0.126366		CURRENT LEARNING RATE: 0.12428882592053986
previous_iter_valid_loss : 0.10731978714466095

    139200	  0.107540	  0.107320	  0.126465		CURRENT LEARNING RATE: 0.12416459921832264
previous_iter_valid_loss : 0.09831668436527252

    139300	  0.098539	  0.098317	  0.125421		CURRENT LEARNING RATE: 0.12404049668071501
previous_iter_valid_loss : 0.12353783845901489

    139400	  0.123854	  0.123538	  0.125407		CURRENT LEARNING RATE: 0.12391651818361438
previous_iter_valid_loss : 0.1312965601682663

    139500	  0.131703	  0.131297	  0.125530		CURRENT LEARNING RATE: 0.12379266360304228
previous_iter_valid_loss : 0.10355023294687271

    139600	  0.103746	  0.103550	  0.125612		CURRENT LEARNING RATE: 0.12366893281514406
previous_iter_valid_loss : 0.1177544966340065

    139700	  0.118021	  0.117754	  0.125445		CURRENT LEARNING RATE: 0.123545325696189
previous_iter_valid_loss : 0.13641013205051422

    139800	  0.136826	  0.136410	  0.124533		CURRENT LEARNING RATE: 0.12342184212256992
previous_iter_valid_loss : 0.1059710904955864

    139900	  0.106123	  0.105971	  0.124011		CURRENT LEARNING RATE: 0.12329848197080324
previous_iter_valid_loss : 0.16503693163394928

    140000	  0.165533	  0.165037	  0.124370		CURRENT LEARNING RATE: 0.12317524511752881
previous_iter_valid_loss : 0.11394104361534119

    140100	  0.114140	  0.113941	  0.124202		CURRENT LEARNING RATE: 0.12305213143950977
previous_iter_valid_loss : 0.11694588512182236

    140200	  0.117180	  0.116946	  0.123862		CURRENT LEARNING RATE: 0.1229291408136324
previous_iter_valid_loss : 0.09063531458377838

    140300	  0.090706	  0.090635	  0.123788		CURRENT LEARNING RATE: 0.12280627311690613
previous_iter_valid_loss : 0.11262853443622589

    140400	  0.112839	  0.112629	  0.123670		CURRENT LEARNING RATE: 0.12268352822646317
previous_iter_valid_loss : 0.08501824736595154

    140500	  0.085042	  0.085018	  0.123046		CURRENT LEARNING RATE: 0.12256090601955869
previous_iter_valid_loss : 0.15935268998146057

    140600	  0.159774	  0.159353	  0.123643		CURRENT LEARNING RATE: 0.12243840637357041
previous_iter_valid_loss : 0.09626463055610657

    140700	  0.096410	  0.096265	  0.123026		CURRENT LEARNING RATE: 0.12231602916599875
previous_iter_valid_loss : 0.08415491878986359

    140800	  0.084208	  0.084155	  0.122625		CURRENT LEARNING RATE: 0.1221937742744664
previous_iter_valid_loss : 0.09606083482503891

    140900	  0.096222	  0.096061	  0.122464		CURRENT LEARNING RATE: 0.12207164157671856
previous_iter_valid_loss : 0.23750562965869904

    141000	  0.238123	  0.237506	  0.123501		CURRENT LEARNING RATE: 0.12194963095062242
previous_iter_valid_loss : 0.13232780992984772

    141100	  0.132675	  0.132328	  0.123630		CURRENT LEARNING RATE: 0.12182774227416743
previous_iter_valid_loss : 0.12879377603530884

    141200	  0.129144	  0.128794	  0.123645		CURRENT LEARNING RATE: 0.12170597542546484
previous_iter_valid_loss : 0.08947395533323288

    141300	  0.089490	  0.089474	  0.123508		CURRENT LEARNING RATE: 0.12158433028274783
previous_iter_valid_loss : 0.10197246819734573

    141400	  0.101786	  0.101972	  0.123385		CURRENT LEARNING RATE: 0.12146280672437122
previous_iter_valid_loss : 0.12799154222011566

    141500	  0.127773	  0.127992	  0.123777		CURRENT LEARNING RATE: 0.12134140462881149
previous_iter_valid_loss : 0.08643782138824463

    141600	  0.086359	  0.086438	  0.123304		CURRENT LEARNING RATE: 0.12122012387466646
previous_iter_valid_loss : 0.10454250872135162

    141700	  0.104332	  0.104543	  0.123449		CURRENT LEARNING RATE: 0.12109896434065545
previous_iter_valid_loss : 0.16221565008163452

    141800	  0.162057	  0.162216	  0.123660		CURRENT LEARNING RATE: 0.12097792590561883
previous_iter_valid_loss : 0.09139695018529892

    141900	  0.091342	  0.091397	  0.123384		CURRENT LEARNING RATE: 0.12085700844851822
previous_iter_valid_loss : 0.10486498475074768

    142000	  0.104659	  0.104865	  0.123288		CURRENT LEARNING RATE: 0.12073621184843612
previous_iter_valid_loss : 0.08737251907587051

    142100	  0.087374	  0.087373	  0.122830		CURRENT LEARNING RATE: 0.12061553598457596
previous_iter_valid_loss : 0.09833559393882751

    142200	  0.098164	  0.098336	  0.122309		CURRENT LEARNING RATE: 0.12049498073626179
previous_iter_valid_loss : 0.08820778876543045

    142300	  0.088257	  0.088208	  0.121747		CURRENT LEARNING RATE: 0.12037454598293844
previous_iter_valid_loss : 0.08547418564558029

    142400	  0.085390	  0.085474	  0.121726		CURRENT LEARNING RATE: 0.12025423160417106
previous_iter_valid_loss : 0.14934252202510834

    142500	  0.149126	  0.149343	  0.122051		CURRENT LEARNING RATE: 0.12013403747964535
previous_iter_valid_loss : 0.08923865854740143

    142600	  0.089273	  0.089239	  0.121606		CURRENT LEARNING RATE: 0.1200139634891671
previous_iter_valid_loss : 0.1025155633687973

    142700	  0.102291	  0.102516	  0.121534		CURRENT LEARNING RATE: 0.11989400951266235
previous_iter_valid_loss : 0.08373482525348663

    142800	  0.083828	  0.083735	  0.121477		CURRENT LEARNING RATE: 0.1197741754301771
previous_iter_valid_loss : 0.09165867418050766

    142900	  0.091572	  0.091659	  0.121226		CURRENT LEARNING RATE: 0.11965446112187728
previous_iter_valid_loss : 0.09112351387739182

    143000	  0.091004	  0.091124	  0.120765		CURRENT LEARNING RATE: 0.11953486646804852
previous_iter_valid_loss : 0.09556093066930771

    143100	  0.095537	  0.095561	  0.120531		CURRENT LEARNING RATE: 0.11941539134909622
previous_iter_valid_loss : 0.08770862221717834

    143200	  0.087792	  0.087709	  0.120164		CURRENT LEARNING RATE: 0.1192960356455452
previous_iter_valid_loss : 0.09581848233938217

    143300	  0.095770	  0.095818	  0.119649		CURRENT LEARNING RATE: 0.11917679923803978
previous_iter_valid_loss : 0.09591701626777649

    143400	  0.095723	  0.095917	  0.119520		CURRENT LEARNING RATE: 0.11905768200734351
previous_iter_valid_loss : 0.08180846273899078

    143500	  0.081782	  0.081808	  0.118965		CURRENT LEARNING RATE: 0.1189386838343392
previous_iter_valid_loss : 0.11009114235639572

    143600	  0.109834	  0.110091	  0.118211		CURRENT LEARNING RATE: 0.1188198046000286
previous_iter_valid_loss : 0.09583456069231033

    143700	  0.095690	  0.095835	  0.118185		CURRENT LEARNING RATE: 0.11870104418553254
previous_iter_valid_loss : 0.08430715650320053

    143800	  0.084377	  0.084307	  0.117954		CURRENT LEARNING RATE: 0.11858240247209052
previous_iter_valid_loss : 0.19726969301700592

    143900	  0.197089	  0.197270	  0.118823		CURRENT LEARNING RATE: 0.11846387934106088
previous_iter_valid_loss : 0.08914252370595932

    144000	  0.089025	  0.089143	  0.118557		CURRENT LEARNING RATE: 0.11834547467392044
previous_iter_valid_loss : 0.16447901725769043

    144100	  0.164962	  0.164479	  0.119234		CURRENT LEARNING RATE: 0.11822718835226455
previous_iter_valid_loss : 0.13732127845287323

    144200	  0.137668	  0.137321	  0.119635		CURRENT LEARNING RATE: 0.11810902025780684
previous_iter_valid_loss : 0.11403937637805939

    144300	  0.114228	  0.114039	  0.119309		CURRENT LEARNING RATE: 0.11799097027237926
previous_iter_valid_loss : 0.09943033754825592

    144400	  0.099458	  0.099430	  0.118380		CURRENT LEARNING RATE: 0.11787303827793176
previous_iter_valid_loss : 0.1133950874209404

    144500	  0.113565	  0.113395	  0.118334		CURRENT LEARNING RATE: 0.11775522415653238
previous_iter_valid_loss : 0.12129025906324387

    144600	  0.121498	  0.121290	  0.118255		CURRENT LEARNING RATE: 0.11763752779036694
previous_iter_valid_loss : 0.09019535779953003

    144700	  0.090271	  0.090195	  0.118173		CURRENT LEARNING RATE: 0.11751994906173914
previous_iter_valid_loss : 0.11088807880878448

    144800	  0.111068	  0.110888	  0.117866		CURRENT LEARNING RATE: 0.11740248785307016
previous_iter_valid_loss : 0.09040156751871109

    144900	  0.090450	  0.090402	  0.117533		CURRENT LEARNING RATE: 0.11728514404689883
previous_iter_valid_loss : 0.08840945363044739

    145000	  0.088324	  0.088409	  0.116835		CURRENT LEARNING RATE: 0.11716791752588131
previous_iter_valid_loss : 0.08461945503950119

    145100	  0.084690	  0.084619	  0.116432		CURRENT LEARNING RATE: 0.1170508081727911
previous_iter_valid_loss : 0.08848225325345993

    145200	  0.088466	  0.088482	  0.116417		CURRENT LEARNING RATE: 0.11693381587051878
previous_iter_valid_loss : 0.0851084515452385

    145300	  0.085167	  0.085108	  0.116221		CURRENT LEARNING RATE: 0.11681694050207211
previous_iter_valid_loss : 0.0912134125828743

    145400	  0.091186	  0.091213	  0.116106		CURRENT LEARNING RATE: 0.11670018195057566
previous_iter_valid_loss : 0.11001919209957123

    145500	  0.109820	  0.110019	  0.116057		CURRENT LEARNING RATE: 0.1165835400992709
previous_iter_valid_loss : 0.08225912600755692

    145600	  0.082248	  0.082259	  0.115715		CURRENT LEARNING RATE: 0.11646701483151593
previous_iter_valid_loss : 0.08582740277051926

    145700	  0.085808	  0.085827	  0.115321		CURRENT LEARNING RATE: 0.11635060603078554
previous_iter_valid_loss : 0.10992690920829773

    145800	  0.109797	  0.109927	  0.115373		CURRENT LEARNING RATE: 0.11623431358067082
previous_iter_valid_loss : 0.1314055621623993

    145900	  0.131227	  0.131406	  0.115201		CURRENT LEARNING RATE: 0.11611813736487941
previous_iter_valid_loss : 0.09484973549842834

    146000	  0.094729	  0.094850	  0.114700		CURRENT LEARNING RATE: 0.11600207726723502
previous_iter_valid_loss : 0.09922541677951813

    146100	  0.099523	  0.099225	  0.114509		CURRENT LEARNING RATE: 0.11588613317167758
previous_iter_valid_loss : 0.08551887422800064

    146200	  0.085514	  0.085519	  0.114293		CURRENT LEARNING RATE: 0.11577030496226295
previous_iter_valid_loss : 0.09004718065261841

    146300	  0.090169	  0.090047	  0.113975		CURRENT LEARNING RATE: 0.11565459252316296
previous_iter_valid_loss : 0.09590750932693481

    146400	  0.095716	  0.095908	  0.113727		CURRENT LEARNING RATE: 0.11553899573866509
previous_iter_valid_loss : 0.10432673990726471

    146500	  0.104172	  0.104327	  0.113511		CURRENT LEARNING RATE: 0.11542351449317262
previous_iter_valid_loss : 0.12857428193092346

    146600	  0.128548	  0.128574	  0.113171		CURRENT LEARNING RATE: 0.11530814867120424
previous_iter_valid_loss : 0.09582691639661789

    146700	  0.095717	  0.095827	  0.112983		CURRENT LEARNING RATE: 0.11519289815739417
previous_iter_valid_loss : 0.11235892027616501

    146800	  0.112183	  0.112359	  0.112570		CURRENT LEARNING RATE: 0.11507776283649182
previous_iter_valid_loss : 0.09416119754314423

    146900	  0.094217	  0.094161	  0.111968		CURRENT LEARNING RATE: 0.11496274259336192
previous_iter_valid_loss : 0.09120923280715942

    147000	  0.091300	  0.091209	  0.111388		CURRENT LEARNING RATE: 0.11484783731298417
previous_iter_valid_loss : 0.09344238042831421

    147100	  0.093548	  0.093442	  0.110675		CURRENT LEARNING RATE: 0.11473304688045334
previous_iter_valid_loss : 0.12091474235057831

    147200	  0.121104	  0.120915	  0.110819		CURRENT LEARNING RATE: 0.11461837118097892
previous_iter_valid_loss : 0.16979970037937164

    147300	  0.170193	  0.169800	  0.111393		CURRENT LEARNING RATE: 0.11450381009988525
previous_iter_valid_loss : 0.0986563041806221

    147400	  0.098808	  0.098656	  0.111155		CURRENT LEARNING RATE: 0.11438936352261121
previous_iter_valid_loss : 0.22236423194408417

    147500	  0.222033	  0.222364	  0.111835		CURRENT LEARNING RATE: 0.11427503133471024
previous_iter_valid_loss : 0.20693722367286682

    147600	  0.207598	  0.206937	  0.112787		CURRENT LEARNING RATE: 0.11416081342185011
previous_iter_valid_loss : 0.0933041125535965

    147700	  0.093510	  0.093304	  0.112602		CURRENT LEARNING RATE: 0.11404670966981294
previous_iter_valid_loss : 0.14079980552196503

    147800	  0.140586	  0.140800	  0.112932		CURRENT LEARNING RATE: 0.11393271996449492
previous_iter_valid_loss : 0.09412413835525513

    147900	  0.094197	  0.094124	  0.112919		CURRENT LEARNING RATE: 0.11381884419190637
previous_iter_valid_loss : 0.1315280646085739

    148000	  0.131714	  0.131528	  0.112959		CURRENT LEARNING RATE: 0.11370508223817148
previous_iter_valid_loss : 0.10652683675289154

    148100	  0.106442	  0.106527	  0.112590		CURRENT LEARNING RATE: 0.11359143398952833
previous_iter_valid_loss : 0.08881016820669174

    148200	  0.088832	  0.088810	  0.112532		CURRENT LEARNING RATE: 0.11347789933232862
previous_iter_valid_loss : 0.11326546967029572

    148300	  0.113506	  0.113265	  0.112192		CURRENT LEARNING RATE: 0.11336447815303771
previous_iter_valid_loss : 0.24519893527030945

    148400	  0.246134	  0.245199	  0.113482		CURRENT LEARNING RATE: 0.11325117033823437
previous_iter_valid_loss : 0.08506371080875397

    148500	  0.085118	  0.085064	  0.113108		CURRENT LEARNING RATE: 0.11313797577461085
previous_iter_valid_loss : 0.1475725620985031

    148600	  0.147897	  0.147573	  0.113553		CURRENT LEARNING RATE: 0.11302489434897249
previous_iter_valid_loss : 0.2661498188972473

    148700	  0.266867	  0.266150	  0.114644		CURRENT LEARNING RATE: 0.11291192594823793
previous_iter_valid_loss : 0.12947013974189758

    148800	  0.129188	  0.129470	  0.114697		CURRENT LEARNING RATE: 0.11279907045943871
previous_iter_valid_loss : 0.11598459631204605

    148900	  0.115753	  0.115985	  0.114935		CURRENT LEARNING RATE: 0.11268632776971936
previous_iter_valid_loss : 0.12843574583530426

    149000	  0.128271	  0.128436	  0.114610		CURRENT LEARNING RATE: 0.11257369776633716
previous_iter_valid_loss : 0.11029540747404099

    149100	  0.110419	  0.110295	  0.114115		CURRENT LEARNING RATE: 0.11246118033666212
previous_iter_valid_loss : 0.149470254778862

    149200	  0.149838	  0.149470	  0.114537		CURRENT LEARNING RATE: 0.11234877536817676
previous_iter_valid_loss : 0.10323433578014374

    149300	  0.103274	  0.103234	  0.114586		CURRENT LEARNING RATE: 0.11223648274847617
previous_iter_valid_loss : 0.09673624485731125

    149400	  0.096668	  0.096736	  0.114318		CURRENT LEARNING RATE: 0.11212430236526766
previous_iter_valid_loss : 0.1014247015118599

    149500	  0.101120	  0.101425	  0.114019		CURRENT LEARNING RATE: 0.11201223410637087
previous_iter_valid_loss : 0.12319118529558182

    149600	  0.123457	  0.123191	  0.114216		CURRENT LEARNING RATE: 0.1119002778597175
previous_iter_valid_loss : 0.21002322435379028

    149700	  0.210500	  0.210023	  0.115138		CURRENT LEARNING RATE: 0.11178843351335134
previous_iter_valid_loss : 0.12226900458335876

    149800	  0.122526	  0.122269	  0.114997		CURRENT LEARNING RATE: 0.11167670095542799
previous_iter_valid_loss : 0.08380751311779022

    149900	  0.083717	  0.083808	  0.114775		CURRENT LEARNING RATE: 0.11156508007421491
previous_iter_valid_loss : 0.14048217236995697

    150000	  0.140919	  0.140482	  0.114530		CURRENT LEARNING RATE: 0.11145357075809122
previous_iter_valid_loss : 0.1123344823718071

    150100	  0.112071	  0.112334	  0.114514		CURRENT LEARNING RATE: 0.11134217289554754
previous_iter_valid_loss : 0.10981947183609009

    150200	  0.110014	  0.109819	  0.114442		CURRENT LEARNING RATE: 0.11123088637518606
previous_iter_valid_loss : 0.15047071874141693

    150300	  0.150895	  0.150471	  0.115041		CURRENT LEARNING RATE: 0.1111197110857202
previous_iter_valid_loss : 0.08529787510633469

    150400	  0.085214	  0.085298	  0.114768		CURRENT LEARNING RATE: 0.11100864691597472
previous_iter_valid_loss : 0.15572437644004822

    150500	  0.156121	  0.155724	  0.115475		CURRENT LEARNING RATE: 0.11089769375488537
previous_iter_valid_loss : 0.11858388036489487

    150600	  0.118824	  0.118584	  0.115067		CURRENT LEARNING RATE: 0.11078685149149904
previous_iter_valid_loss : 0.2800901234149933

    150700	  0.280595	  0.280090	  0.116905		CURRENT LEARNING RATE: 0.11067612001497341
previous_iter_valid_loss : 0.08897099643945694

    150800	  0.089117	  0.088971	  0.116953		CURRENT LEARNING RATE: 0.11056549921457706
previous_iter_valid_loss : 0.15232263505458832

    150900	  0.152937	  0.152323	  0.117516		CURRENT LEARNING RATE: 0.1104549889796891
previous_iter_valid_loss : 0.12456465512514114

    151000	  0.125099	  0.124565	  0.116387		CURRENT LEARNING RATE: 0.11034458919979935
previous_iter_valid_loss : 0.2588430345058441

    151100	  0.259847	  0.258843	  0.117652		CURRENT LEARNING RATE: 0.11023429976450796
previous_iter_valid_loss : 0.10891517251729965

    151200	  0.108675	  0.108915	  0.117453		CURRENT LEARNING RATE: 0.11012412056352557
previous_iter_valid_loss : 0.11230426281690598

    151300	  0.112152	  0.112304	  0.117681		CURRENT LEARNING RATE: 0.11001405148667287
previous_iter_valid_loss : 0.16604594886302948

    151400	  0.166640	  0.166046	  0.118322		CURRENT LEARNING RATE: 0.10990409242388087
previous_iter_valid_loss : 0.14911086857318878

    151500	  0.149637	  0.149111	  0.118533		CURRENT LEARNING RATE: 0.10979424326519041
previous_iter_valid_loss : 0.08255674690008163

    151600	  0.082665	  0.082557	  0.118494		CURRENT LEARNING RATE: 0.1096845039007524
previous_iter_valid_loss : 0.12365198135375977

    151700	  0.124003	  0.123652	  0.118685		CURRENT LEARNING RATE: 0.1095748742208274
previous_iter_valid_loss : 0.0872003510594368

    151800	  0.087160	  0.087200	  0.117935		CURRENT LEARNING RATE: 0.10946535411578578
previous_iter_valid_loss : 0.08622368425130844

    151900	  0.086238	  0.086224	  0.117884		CURRENT LEARNING RATE: 0.10935594347610737
previous_iter_valid_loss : 0.09846164286136627

    152000	  0.098334	  0.098462	  0.117819		CURRENT LEARNING RATE: 0.10924664219238159
previous_iter_valid_loss : 0.09731370955705643

    152100	  0.097665	  0.097314	  0.117919		CURRENT LEARNING RATE: 0.10913745015530707
previous_iter_valid_loss : 0.17211675643920898

    152200	  0.172685	  0.172117	  0.118657		CURRENT LEARNING RATE: 0.10902836725569182
previous_iter_valid_loss : 0.22999247908592224

    152300	  0.229940	  0.229992	  0.120075		CURRENT LEARNING RATE: 0.10891939338445289
previous_iter_valid_loss : 0.09108807891607285

    152400	  0.091037	  0.091088	  0.120131		CURRENT LEARNING RATE: 0.10881052843261645
previous_iter_valid_loss : 0.08284451812505722

    152500	  0.082883	  0.082845	  0.119466		CURRENT LEARNING RATE: 0.10870177229131749
previous_iter_valid_loss : 0.1160011887550354

    152600	  0.115851	  0.116001	  0.119733		CURRENT LEARNING RATE: 0.10859312485179988
previous_iter_valid_loss : 0.08808859437704086

    152700	  0.088074	  0.088089	  0.119589		CURRENT LEARNING RATE: 0.10848458600541618
previous_iter_valid_loss : 0.08730851858854294

    152800	  0.087342	  0.087309	  0.119625		CURRENT LEARNING RATE: 0.10837615564362753
previous_iter_valid_loss : 0.08525600284337997

    152900	  0.085270	  0.085256	  0.119561		CURRENT LEARNING RATE: 0.10826783365800353
previous_iter_valid_loss : 0.08695637434720993

    153000	  0.087089	  0.086956	  0.119519		CURRENT LEARNING RATE: 0.10815961994022223
previous_iter_valid_loss : 0.17436575889587402

    153100	  0.174445	  0.174366	  0.120307		CURRENT LEARNING RATE: 0.10805151438206988
previous_iter_valid_loss : 0.09543696790933609

    153200	  0.095246	  0.095437	  0.120384		CURRENT LEARNING RATE: 0.10794351687544092
previous_iter_valid_loss : 0.0871506854891777

    153300	  0.087205	  0.087151	  0.120298		CURRENT LEARNING RATE: 0.10783562731233783
previous_iter_valid_loss : 0.13499397039413452

    153400	  0.134903	  0.134994	  0.120689		CURRENT LEARNING RATE: 0.10772784558487104
previous_iter_valid_loss : 0.16478557884693146

    153500	  0.165260	  0.164786	  0.121518		CURRENT LEARNING RATE: 0.10762017158525879
previous_iter_valid_loss : 0.08318842202425003

    153600	  0.083250	  0.083188	  0.121249		CURRENT LEARNING RATE: 0.10751260520582713
previous_iter_valid_loss : 0.1651606559753418

    153700	  0.164969	  0.165161	  0.121943		CURRENT LEARNING RATE: 0.1074051463390096
previous_iter_valid_loss : 0.09857932478189468

    153800	  0.098767	  0.098579	  0.122085		CURRENT LEARNING RATE: 0.10729779487734739
previous_iter_valid_loss : 0.1373634934425354

    153900	  0.137750	  0.137363	  0.121486		CURRENT LEARNING RATE: 0.10719055071348897
previous_iter_valid_loss : 0.10770989209413528

    154000	  0.107538	  0.107710	  0.121672		CURRENT LEARNING RATE: 0.10708341374019023
previous_iter_valid_loss : 0.08059181272983551

    154100	  0.080682	  0.080592	  0.120833		CURRENT LEARNING RATE: 0.10697638385031412
previous_iter_valid_loss : 0.08908621966838837

    154200	  0.089148	  0.089086	  0.120351		CURRENT LEARNING RATE: 0.1068694609368308
previous_iter_valid_loss : 0.18177972733974457

    154300	  0.182335	  0.181780	  0.121028		CURRENT LEARNING RATE: 0.1067626448928173
previous_iter_valid_loss : 0.09782807528972626

    154400	  0.097639	  0.097828	  0.121012		CURRENT LEARNING RATE: 0.10665593561145761
previous_iter_valid_loss : 0.23594707250595093

    154500	  0.235687	  0.235947	  0.122238		CURRENT LEARNING RATE: 0.10654933298604241
previous_iter_valid_loss : 0.09324759989976883

    154600	  0.093184	  0.093248	  0.121957		CURRENT LEARNING RATE: 0.10644283690996909
previous_iter_valid_loss : 0.12700283527374268

    154700	  0.127303	  0.127003	  0.122325		CURRENT LEARNING RATE: 0.10633644727674152
previous_iter_valid_loss : 0.09504729509353638

    154800	  0.095114	  0.095047	  0.122167		CURRENT LEARNING RATE: 0.10623016397997012
previous_iter_valid_loss : 0.1308911144733429

    154900	  0.131297	  0.130891	  0.122572		CURRENT LEARNING RATE: 0.10612398691337152
previous_iter_valid_loss : 0.2716584801673889

    155000	  0.271414	  0.271658	  0.124404		CURRENT LEARNING RATE: 0.1060179159707687
previous_iter_valid_loss : 0.11631101369857788

    155100	  0.116672	  0.116311	  0.124721		CURRENT LEARNING RATE: 0.10591195104609068
previous_iter_valid_loss : 0.14510098099708557

    155200	  0.144930	  0.145101	  0.125287		CURRENT LEARNING RATE: 0.10580609203337255
previous_iter_valid_loss : 0.0841171070933342

    155300	  0.084130	  0.084117	  0.125277		CURRENT LEARNING RATE: 0.10570033882675524
previous_iter_valid_loss : 0.149812713265419

    155400	  0.149674	  0.149813	  0.125863		CURRENT LEARNING RATE: 0.10559469132048559
previous_iter_valid_loss : 0.08853945136070251

    155500	  0.088424	  0.088539	  0.125649		CURRENT LEARNING RATE: 0.10548914940891603
previous_iter_valid_loss : 0.10698738694190979

    155600	  0.106745	  0.106987	  0.125896		CURRENT LEARNING RATE: 0.1053837129865047
previous_iter_valid_loss : 0.09353005886077881

    155700	  0.093360	  0.093530	  0.125973		CURRENT LEARNING RATE: 0.10527838194781512
previous_iter_valid_loss : 0.09697830677032471

    155800	  0.096774	  0.096978	  0.125843		CURRENT LEARNING RATE: 0.10517315618751627
previous_iter_valid_loss : 0.08615314215421677

    155900	  0.086023	  0.086153	  0.125391		CURRENT LEARNING RATE: 0.10506803560038236
previous_iter_valid_loss : 0.10658375173807144

    156000	  0.106765	  0.106584	  0.125508		CURRENT LEARNING RATE: 0.10496302008129282
previous_iter_valid_loss : 0.10257799923419952

    156100	  0.102474	  0.102578	  0.125542		CURRENT LEARNING RATE: 0.1048581095252321
previous_iter_valid_loss : 0.08803229033946991

    156200	  0.087881	  0.088032	  0.125567		CURRENT LEARNING RATE: 0.10475330382728966
previous_iter_valid_loss : 0.12280551344156265

    156300	  0.122978	  0.122806	  0.125894		CURRENT LEARNING RATE: 0.10464860288265976
previous_iter_valid_loss : 0.0882999449968338

    156400	  0.088151	  0.088300	  0.125818		CURRENT LEARNING RATE: 0.10454400658664147
previous_iter_valid_loss : 0.08244603127241135

    156500	  0.082323	  0.082446	  0.125600		CURRENT LEARNING RATE: 0.10443951483463847
previous_iter_valid_loss : 0.09027230739593506

    156600	  0.090041	  0.090272	  0.125216		CURRENT LEARNING RATE: 0.10433512752215902
previous_iter_valid_loss : 0.0876707062125206

    156700	  0.087642	  0.087671	  0.125135		CURRENT LEARNING RATE: 0.10423084454481576
previous_iter_valid_loss : 0.08839919418096542

    156800	  0.088363	  0.088399	  0.124895		CURRENT LEARNING RATE: 0.10412666579832577
previous_iter_valid_loss : 0.102012999355793

    156900	  0.102057	  0.102013	  0.124974		CURRENT LEARNING RATE: 0.10402259117851023
previous_iter_valid_loss : 0.09658269584178925

    157000	  0.096610	  0.096583	  0.125028		CURRENT LEARNING RATE: 0.10391862058129456
previous_iter_valid_loss : 0.14597737789154053

    157100	  0.146250	  0.145977	  0.125553		CURRENT LEARNING RATE: 0.1038147539027081
previous_iter_valid_loss : 0.12590375542640686

    157200	  0.125985	  0.125904	  0.125603		CURRENT LEARNING RATE: 0.10371099103888422
previous_iter_valid_loss : 0.1264515072107315

    157300	  0.126634	  0.126452	  0.125169		CURRENT LEARNING RATE: 0.10360733188606
previous_iter_valid_loss : 0.08591274172067642

    157400	  0.085518	  0.085913	  0.125042		CURRENT LEARNING RATE: 0.10350377634057632
previous_iter_valid_loss : 0.08845323324203491

    157500	  0.088334	  0.088453	  0.123703		CURRENT LEARNING RATE: 0.10340032429887758
previous_iter_valid_loss : 0.09014295786619186

    157600	  0.089952	  0.090143	  0.122535		CURRENT LEARNING RATE: 0.10329697565751178
previous_iter_valid_loss : 0.0838107094168663

    157700	  0.083593	  0.083811	  0.122440		CURRENT LEARNING RATE: 0.10319373031313023
previous_iter_valid_loss : 0.096425361931324

    157800	  0.096397	  0.096425	  0.121996		CURRENT LEARNING RATE: 0.10309058816248762
previous_iter_valid_loss : 0.11493300646543503

    157900	  0.114940	  0.114933	  0.122204		CURRENT LEARNING RATE: 0.10298754910244172
previous_iter_valid_loss : 0.1691078096628189

    158000	  0.169640	  0.169108	  0.122580		CURRENT LEARNING RATE: 0.10288461302995354
previous_iter_valid_loss : 0.11997487396001816

    158100	  0.120150	  0.119975	  0.122714		CURRENT LEARNING RATE: 0.10278177984208695
previous_iter_valid_loss : 0.08087614178657532

    158200	  0.080682	  0.080876	  0.122635		CURRENT LEARNING RATE: 0.10267904943600878
previous_iter_valid_loss : 0.11035093665122986

    158300	  0.110529	  0.110351	  0.122606		CURRENT LEARNING RATE: 0.10257642170898858
previous_iter_valid_loss : 0.08823179453611374

    158400	  0.088231	  0.088232	  0.121036		CURRENT LEARNING RATE: 0.10247389655839866
previous_iter_valid_loss : 0.13512203097343445

    158500	  0.135443	  0.135122	  0.121537		CURRENT LEARNING RATE: 0.10237147388171382
previous_iter_valid_loss : 0.1269206404685974

    158600	  0.127290	  0.126921	  0.121330		CURRENT LEARNING RATE: 0.1022691535765114
previous_iter_valid_loss : 0.08261453360319138

    158700	  0.082590	  0.082615	  0.119495		CURRENT LEARNING RATE: 0.10216693554047107
previous_iter_valid_loss : 0.10506347566843033

    158800	  0.104972	  0.105063	  0.119251		CURRENT LEARNING RATE: 0.10206481967137482
previous_iter_valid_loss : 0.10831105709075928

    158900	  0.108197	  0.108311	  0.119174		CURRENT LEARNING RATE: 0.10196280586710671
previous_iter_valid_loss : 0.09328273683786392

    159000	  0.093125	  0.093283	  0.118823		CURRENT LEARNING RATE: 0.101860894025653
previous_iter_valid_loss : 0.09101281315088272

    159100	  0.090976	  0.091013	  0.118630		CURRENT LEARNING RATE: 0.10175908404510177
previous_iter_valid_loss : 0.1126038059592247

    159200	  0.112458	  0.112604	  0.118261		CURRENT LEARNING RATE: 0.10165737582364309
previous_iter_valid_loss : 0.08572274446487427

    159300	  0.085742	  0.085723	  0.118086		CURRENT LEARNING RATE: 0.10155576925956869
previous_iter_valid_loss : 0.10994471609592438

    159400	  0.109681	  0.109945	  0.118218		CURRENT LEARNING RATE: 0.10145426425127203
previous_iter_valid_loss : 0.1280299723148346

    159500	  0.128023	  0.128030	  0.118484		CURRENT LEARNING RATE: 0.10135286069724805
previous_iter_valid_loss : 0.09589796513319016

    159600	  0.095818	  0.095898	  0.118211		CURRENT LEARNING RATE: 0.10125155849609324
previous_iter_valid_loss : 0.09265103191137314

    159700	  0.092474	  0.092651	  0.117038		CURRENT LEARNING RATE: 0.10115035754650535
previous_iter_valid_loss : 0.09650243073701859

    159800	  0.096698	  0.096502	  0.116780		CURRENT LEARNING RATE: 0.10104925774728345
previous_iter_valid_loss : 0.09265582263469696

    159900	  0.092784	  0.092656	  0.116868		CURRENT LEARNING RATE: 0.10094825899732769
previous_iter_valid_loss : 0.10133204609155655

    160000	  0.101202	  0.101332	  0.116477		CURRENT LEARNING RATE: 0.10084736119563938
previous_iter_valid_loss : 0.12697084248065948

    160100	  0.126812	  0.126971	  0.116623		CURRENT LEARNING RATE: 0.10074656424132063
previous_iter_valid_loss : 0.10041065514087677

    160200	  0.100322	  0.100411	  0.116529		CURRENT LEARNING RATE: 0.10064586803357455
previous_iter_valid_loss : 0.10935816168785095

    160300	  0.109309	  0.109358	  0.116118		CURRENT LEARNING RATE: 0.10054527247170486
previous_iter_valid_loss : 0.10754779726266861

    160400	  0.107467	  0.107548	  0.116341		CURRENT LEARNING RATE: 0.10044477745511604
previous_iter_valid_loss : 0.08353575319051743

    160500	  0.083534	  0.083536	  0.115619		CURRENT LEARNING RATE: 0.10034438288331303
previous_iter_valid_loss : 0.08920671045780182

    160600	  0.089092	  0.089207	  0.115325		CURRENT LEARNING RATE: 0.10024408865590129
previous_iter_valid_loss : 0.09820961207151413

    160700	  0.098150	  0.098210	  0.113506		CURRENT LEARNING RATE: 0.10014389467258653
previous_iter_valid_loss : 0.0941435694694519

    160800	  0.094059	  0.094144	  0.113558		CURRENT LEARNING RATE: 0.10004380083317481
previous_iter_valid_loss : 0.10101836174726486

    160900	  0.101200	  0.101018	  0.113045		CURRENT LEARNING RATE: 0.09994380703757225
previous_iter_valid_loss : 0.08455807715654373

    161000	  0.084702	  0.084558	  0.112645		CURRENT LEARNING RATE: 0.09984391318578506
previous_iter_valid_loss : 0.08875805884599686

    161100	  0.088895	  0.088758	  0.110944		CURRENT LEARNING RATE: 0.09974411917791937
previous_iter_valid_loss : 0.08845040947198868

    161200	  0.088533	  0.088450	  0.110739		CURRENT LEARNING RATE: 0.09964442491418118
previous_iter_valid_loss : 0.12277276813983917

    161300	  0.122758	  0.122773	  0.110844		CURRENT LEARNING RATE: 0.0995448302948762
previous_iter_valid_loss : 0.08183806389570236

    161400	  0.081831	  0.081838	  0.110002		CURRENT LEARNING RATE: 0.09944533522040981
previous_iter_valid_loss : 0.10133668780326843

    161500	  0.101291	  0.101337	  0.109524		CURRENT LEARNING RATE: 0.09934593959128693
previous_iter_valid_loss : 0.08149620145559311

    161600	  0.081547	  0.081496	  0.109513		CURRENT LEARNING RATE: 0.09924664330811193
previous_iter_valid_loss : 0.09670176357030869

    161700	  0.096918	  0.096702	  0.109244		CURRENT LEARNING RATE: 0.09914744627158849
previous_iter_valid_loss : 0.10477890074253082

    161800	  0.105053	  0.104779	  0.109420		CURRENT LEARNING RATE: 0.09904834838251961
previous_iter_valid_loss : 0.11241721361875534

    161900	  0.112769	  0.112417	  0.109682		CURRENT LEARNING RATE: 0.09894934954180733
previous_iter_valid_loss : 0.09354593604803085

    162000	  0.093709	  0.093546	  0.109633		CURRENT LEARNING RATE: 0.09885044965045287
previous_iter_valid_loss : 0.08475248515605927

    162100	  0.084983	  0.084752	  0.109507		CURRENT LEARNING RATE: 0.09875164860955628
previous_iter_valid_loss : 0.09547307342290878

    162200	  0.095667	  0.095473	  0.108740		CURRENT LEARNING RATE: 0.09865294632031654
previous_iter_valid_loss : 0.08555681258440018

    162300	  0.085649	  0.085557	  0.107296		CURRENT LEARNING RATE: 0.09855434268403132
previous_iter_valid_loss : 0.11295263469219208

    162400	  0.113247	  0.112953	  0.107515		CURRENT LEARNING RATE: 0.09845583760209703
previous_iter_valid_loss : 0.10134662687778473

    162500	  0.101637	  0.101347	  0.107700		CURRENT LEARNING RATE: 0.09835743097600853
previous_iter_valid_loss : 0.11601578444242477

    162600	  0.116313	  0.116016	  0.107700		CURRENT LEARNING RATE: 0.09825912270735919
previous_iter_valid_loss : 0.10050641745328903

    162700	  0.100449	  0.100506	  0.107824		CURRENT LEARNING RATE: 0.09816091269784077
previous_iter_valid_loss : 0.09439267218112946

    162800	  0.094385	  0.094393	  0.107895		CURRENT LEARNING RATE: 0.09806280084924321
previous_iter_valid_loss : 0.1214621439576149

    162900	  0.121651	  0.121462	  0.108257		CURRENT LEARNING RATE: 0.09796478706345468
previous_iter_valid_loss : 0.12801393866539001

    163000	  0.127941	  0.128014	  0.108668		CURRENT LEARNING RATE: 0.09786687124246136
previous_iter_valid_loss : 0.2408299744129181

    163100	  0.241432	  0.240830	  0.109332		CURRENT LEARNING RATE: 0.09776905328834747
previous_iter_valid_loss : 0.13830506801605225

    163200	  0.138648	  0.138305	  0.109761		CURRENT LEARNING RATE: 0.09767133310329498
previous_iter_valid_loss : 0.11773045361042023

    163300	  0.117976	  0.117730	  0.110067		CURRENT LEARNING RATE: 0.09757371058958376
previous_iter_valid_loss : 0.10844181478023529

    163400	  0.108605	  0.108442	  0.109801		CURRENT LEARNING RATE: 0.09747618564959125
previous_iter_valid_loss : 0.13743944466114044

    163500	  0.137668	  0.137439	  0.109528		CURRENT LEARNING RATE: 0.09737875818579252
previous_iter_valid_loss : 0.08801046758890152

    163600	  0.088022	  0.088010	  0.109576		CURRENT LEARNING RATE: 0.09728142810076007
previous_iter_valid_loss : 0.19262906908988953

    163700	  0.192505	  0.192629	  0.109851		CURRENT LEARNING RATE: 0.09718419529716385
previous_iter_valid_loss : 0.09963449835777283

    163800	  0.099549	  0.099634	  0.109861		CURRENT LEARNING RATE: 0.09708705967777101
previous_iter_valid_loss : 0.08366835862398148

    163900	  0.083666	  0.083668	  0.109324		CURRENT LEARNING RATE: 0.09699002114544596
previous_iter_valid_loss : 0.08733072876930237

    164000	  0.087385	  0.087331	  0.109120		CURRENT LEARNING RATE: 0.0968930796031501
previous_iter_valid_loss : 0.11323749274015427

    164100	  0.113110	  0.113237	  0.109447		CURRENT LEARNING RATE: 0.09679623495394196
previous_iter_valid_loss : 0.09179005771875381

    164200	  0.091727	  0.091790	  0.109474		CURRENT LEARNING RATE: 0.09669948710097681
previous_iter_valid_loss : 0.1483718752861023

    164300	  0.148680	  0.148372	  0.109140		CURRENT LEARNING RATE: 0.09660283594750685
previous_iter_valid_loss : 0.14300814270973206

    164400	  0.143200	  0.143008	  0.109592		CURRENT LEARNING RATE: 0.09650628139688086
previous_iter_valid_loss : 0.09190387278795242

    164500	  0.091921	  0.091904	  0.108151		CURRENT LEARNING RATE: 0.09640982335254432
previous_iter_valid_loss : 0.14777033030986786

    164600	  0.147608	  0.147770	  0.108696		CURRENT LEARNING RATE: 0.09631346171803916
previous_iter_valid_loss : 0.10594283044338226

    164700	  0.105806	  0.105943	  0.108486		CURRENT LEARNING RATE: 0.09621719639700375
previous_iter_valid_loss : 0.11392683535814285

    164800	  0.114284	  0.113927	  0.108675		CURRENT LEARNING RATE: 0.09612102729317275
previous_iter_valid_loss : 0.11803457140922546

    164900	  0.118165	  0.118035	  0.108546		CURRENT LEARNING RATE: 0.09602495431037707
previous_iter_valid_loss : 0.11908454447984695

    165000	  0.119130	  0.119085	  0.107020		CURRENT LEARNING RATE: 0.09592897735254367
previous_iter_valid_loss : 0.09288768470287323

    165100	  0.092988	  0.092888	  0.106786		CURRENT LEARNING RATE: 0.09583309632369565
previous_iter_valid_loss : 0.0863887369632721

    165200	  0.086285	  0.086389	  0.106199		CURRENT LEARNING RATE: 0.09573731112795192
previous_iter_valid_loss : 0.15142960846424103

    165300	  0.151175	  0.151430	  0.106872		CURRENT LEARNING RATE: 0.0956416216695273
previous_iter_valid_loss : 0.10027635842561722

    165400	  0.099889	  0.100276	  0.106377		CURRENT LEARNING RATE: 0.09554602785273232
previous_iter_valid_loss : 0.08600122481584549

    165500	  0.085995	  0.086001	  0.106351		CURRENT LEARNING RATE: 0.09545052958197317
previous_iter_valid_loss : 0.2600080668926239

    165600	  0.260525	  0.260008	  0.107882		CURRENT LEARNING RATE: 0.09535512676175152
previous_iter_valid_loss : 0.09683113545179367

    165700	  0.096866	  0.096831	  0.107915		CURRENT LEARNING RATE: 0.09525981929666462
previous_iter_valid_loss : 0.08652040362358093

    165800	  0.086367	  0.086520	  0.107810		CURRENT LEARNING RATE: 0.09516460709140492
previous_iter_valid_loss : 0.11995679885149002

    165900	  0.119818	  0.119957	  0.108148		CURRENT LEARNING RATE: 0.09506949005076028
previous_iter_valid_loss : 0.08973594009876251

    166000	  0.089886	  0.089736	  0.107980		CURRENT LEARNING RATE: 0.09497446807961357
previous_iter_valid_loss : 0.08182855695486069

    166100	  0.081856	  0.081829	  0.107772		CURRENT LEARNING RATE: 0.09487954108294289
previous_iter_valid_loss : 0.10667947679758072

    166200	  0.106571	  0.106679	  0.107959		CURRENT LEARNING RATE: 0.09478470896582117
previous_iter_valid_loss : 0.09705906361341476

    166300	  0.097075	  0.097059	  0.107701		CURRENT LEARNING RATE: 0.09468997163341634
previous_iter_valid_loss : 0.10261182487010956

    166400	  0.102769	  0.102612	  0.107844		CURRENT LEARNING RATE: 0.09459532899099102
previous_iter_valid_loss : 0.12595660984516144

    166500	  0.125775	  0.125957	  0.108279		CURRENT LEARNING RATE: 0.09450078094390257
previous_iter_valid_loss : 0.08232251554727554

    166600	  0.082286	  0.082323	  0.108200		CURRENT LEARNING RATE: 0.09440632739760295
previous_iter_valid_loss : 0.08625004440546036

    166700	  0.086290	  0.086250	  0.108186		CURRENT LEARNING RATE: 0.0943119682576386
previous_iter_valid_loss : 0.09264685958623886

    166800	  0.092496	  0.092647	  0.108228		CURRENT LEARNING RATE: 0.09421770342965034
previous_iter_valid_loss : 0.11875782907009125

    166900	  0.118884	  0.118758	  0.108396		CURRENT LEARNING RATE: 0.0941235328193734
previous_iter_valid_loss : 0.08651754260063171

    167000	  0.086353	  0.086518	  0.108295		CURRENT LEARNING RATE: 0.09402945633263708
previous_iter_valid_loss : 0.08289232105016708

    167100	  0.082782	  0.082892	  0.107664		CURRENT LEARNING RATE: 0.09393547387536497
previous_iter_valid_loss : 0.08357355743646622

    167200	  0.083530	  0.083574	  0.107241		CURRENT LEARNING RATE: 0.09384158535357452
previous_iter_valid_loss : 0.11065714061260223

    167300	  0.110833	  0.110657	  0.107083		CURRENT LEARNING RATE: 0.09374779067337728
previous_iter_valid_loss : 0.11162169277667999

    167400	  0.111791	  0.111622	  0.107340		CURRENT LEARNING RATE: 0.09365408974097851
previous_iter_valid_loss : 0.12419772148132324

    167500	  0.124441	  0.124198	  0.107697		CURRENT LEARNING RATE: 0.0935604824626773
previous_iter_valid_loss : 0.1031513437628746

    167600	  0.103235	  0.103151	  0.107827		CURRENT LEARNING RATE: 0.09346696874486632
previous_iter_valid_loss : 0.10168562084436417

    167700	  0.101546	  0.101686	  0.108006		CURRENT LEARNING RATE: 0.0933735484940319
previous_iter_valid_loss : 0.08304360508918762

    167800	  0.083065	  0.083044	  0.107872		CURRENT LEARNING RATE: 0.09328022161675374
previous_iter_valid_loss : 0.09272635728120804

    167900	  0.092811	  0.092726	  0.107650		CURRENT LEARNING RATE: 0.09318698801970499
previous_iter_valid_loss : 0.1232002004981041

    168000	  0.123446	  0.123200	  0.107191		CURRENT LEARNING RATE: 0.093093847609652
previous_iter_valid_loss : 0.08785154670476913

    168100	  0.087774	  0.087852	  0.106870		CURRENT LEARNING RATE: 0.0930008002934544
previous_iter_valid_loss : 0.09284190088510513

    168200	  0.092763	  0.092842	  0.106990		CURRENT LEARNING RATE: 0.09290784597806483
previous_iter_valid_loss : 0.08157827705144882

    168300	  0.081573	  0.081578	  0.106702		CURRENT LEARNING RATE: 0.09281498457052899
previous_iter_valid_loss : 0.0957726389169693

    168400	  0.095915	  0.095773	  0.106777		CURRENT LEARNING RATE: 0.09272221597798544
previous_iter_valid_loss : 0.2201520949602127

    168500	  0.220673	  0.220152	  0.107628		CURRENT LEARNING RATE: 0.09262954010766561
previous_iter_valid_loss : 0.09072092920541763

    168600	  0.090851	  0.090721	  0.107266		CURRENT LEARNING RATE: 0.09253695686689359
previous_iter_valid_loss : 0.1207776740193367

    168700	  0.120681	  0.120778	  0.107647		CURRENT LEARNING RATE: 0.09244446616308617
previous_iter_valid_loss : 0.13521887362003326

    168800	  0.135537	  0.135219	  0.107949		CURRENT LEARNING RATE: 0.0923520679037526
previous_iter_valid_loss : 0.11318174004554749

    168900	  0.113218	  0.113182	  0.107997		CURRENT LEARNING RATE: 0.09225976199649463
previous_iter_valid_loss : 0.11376740038394928

    169000	  0.113846	  0.113767	  0.108202		CURRENT LEARNING RATE: 0.09216754834900635
previous_iter_valid_loss : 0.0961410328745842

    169100	  0.096111	  0.096141	  0.108254		CURRENT LEARNING RATE: 0.0920754268690741
previous_iter_valid_loss : 0.09772796928882599

    169200	  0.097949	  0.097728	  0.108105		CURRENT LEARNING RATE: 0.09198339746457639
previous_iter_valid_loss : 0.1692209392786026

    169300	  0.169703	  0.169221	  0.108940		CURRENT LEARNING RATE: 0.09189146004348382
previous_iter_valid_loss : 0.08560853451490402

    169400	  0.085634	  0.085609	  0.108696		CURRENT LEARNING RATE: 0.09179961451385893
previous_iter_valid_loss : 0.0809313952922821

    169500	  0.080877	  0.080931	  0.108225		CURRENT LEARNING RATE: 0.09170786078385623
previous_iter_valid_loss : 0.1281690001487732

    169600	  0.128521	  0.128169	  0.108548		CURRENT LEARNING RATE: 0.09161619876172193
previous_iter_valid_loss : 0.10664200037717819

    169700	  0.106897	  0.106642	  0.108688		CURRENT LEARNING RATE: 0.09152462835579406
previous_iter_valid_loss : 0.08374462276697159

    169800	  0.083741	  0.083745	  0.108561		CURRENT LEARNING RATE: 0.09143314947450214
previous_iter_valid_loss : 0.07965992391109467

    169900	  0.079548	  0.079660	  0.108431		CURRENT LEARNING RATE: 0.09134176202636733
previous_iter_valid_loss : 0.09758549928665161

    170000	  0.097339	  0.097585	  0.108393		CURRENT LEARNING RATE: 0.09125046592000215
previous_iter_valid_loss : 0.08603276312351227

    170100	  0.086034	  0.086033	  0.107984		CURRENT LEARNING RATE: 0.0911592610641105
previous_iter_valid_loss : 0.08834880590438843

    170200	  0.088389	  0.088349	  0.107863		CURRENT LEARNING RATE: 0.0910681473674875
previous_iter_valid_loss : 0.13455811142921448

    170300	  0.134305	  0.134558	  0.108115		CURRENT LEARNING RATE: 0.09097712473901948
previous_iter_valid_loss : 0.08553336560726166

    170400	  0.085483	  0.085533	  0.107895		CURRENT LEARNING RATE: 0.09088619308768375
previous_iter_valid_loss : 0.08594456315040588

    170500	  0.085915	  0.085945	  0.107919		CURRENT LEARNING RATE: 0.0907953523225487
previous_iter_valid_loss : 0.10596061497926712

    170600	  0.106186	  0.105961	  0.108087		CURRENT LEARNING RATE: 0.09070460235277353
previous_iter_valid_loss : 0.10096340626478195

    170700	  0.100720	  0.100963	  0.108114		CURRENT LEARNING RATE: 0.0906139430876083
previous_iter_valid_loss : 0.08952803909778595

    170800	  0.089313	  0.089528	  0.108068		CURRENT LEARNING RATE: 0.09052337443639367
previous_iter_valid_loss : 0.08238622546195984

    170900	  0.082331	  0.082386	  0.107882		CURRENT LEARNING RATE: 0.09043289630856105
previous_iter_valid_loss : 0.09048157185316086

    171000	  0.090505	  0.090482	  0.107941		CURRENT LEARNING RATE: 0.09034250861363224
previous_iter_valid_loss : 0.0909939557313919

    171100	  0.090836	  0.090994	  0.107963		CURRENT LEARNING RATE: 0.09025221126121961
previous_iter_valid_loss : 0.15894752740859985

    171200	  0.159479	  0.158948	  0.108668		CURRENT LEARNING RATE: 0.09016200416102574
previous_iter_valid_loss : 0.09693032503128052

    171300	  0.096786	  0.096930	  0.108410		CURRENT LEARNING RATE: 0.09007188722284355
previous_iter_valid_loss : 0.11467205733060837

    171400	  0.114520	  0.114672	  0.108738		CURRENT LEARNING RATE: 0.08998186035655609
previous_iter_valid_loss : 0.09816896170377731

    171500	  0.098289	  0.098169	  0.108706		CURRENT LEARNING RATE: 0.08989192347213648
previous_iter_valid_loss : 0.10263942182064056

    171600	  0.102733	  0.102639	  0.108918		CURRENT LEARNING RATE: 0.08980207647964783
previous_iter_valid_loss : 0.09470390528440475

    171700	  0.095099	  0.094704	  0.108898		CURRENT LEARNING RATE: 0.08971231928924317
previous_iter_valid_loss : 0.1537085920572281

    171800	  0.153653	  0.153709	  0.109387		CURRENT LEARNING RATE: 0.08962265181116524
previous_iter_valid_loss : 0.09343093633651733

    171900	  0.093586	  0.093431	  0.109197		CURRENT LEARNING RATE: 0.08953307395574661
previous_iter_valid_loss : 0.1082405298948288

    172000	  0.108621	  0.108241	  0.109344		CURRENT LEARNING RATE: 0.08944358563340939
previous_iter_valid_loss : 0.11085668951272964

    172100	  0.110842	  0.110857	  0.109605		CURRENT LEARNING RATE: 0.08935418675466526
previous_iter_valid_loss : 0.08700603246688843

    172200	  0.087262	  0.087006	  0.109521		CURRENT LEARNING RATE: 0.08926487723011532
previous_iter_valid_loss : 0.09673888236284256

    172300	  0.096703	  0.096739	  0.109632		CURRENT LEARNING RATE: 0.08917565697045007
previous_iter_valid_loss : 0.08020071685314178

    172400	  0.080260	  0.080201	  0.109305		CURRENT LEARNING RATE: 0.0890865258864492
previous_iter_valid_loss : 0.09765898436307907

    172500	  0.097864	  0.097659	  0.109268		CURRENT LEARNING RATE: 0.08899748388898167
previous_iter_valid_loss : 0.08433626592159271

    172600	  0.084371	  0.084336	  0.108951		CURRENT LEARNING RATE: 0.08890853088900541
previous_iter_valid_loss : 0.09518732875585556

    172700	  0.095189	  0.095187	  0.108898		CURRENT LEARNING RATE: 0.08881966679756748
previous_iter_valid_loss : 0.0896119549870491

    172800	  0.089534	  0.089612	  0.108850		CURRENT LEARNING RATE: 0.08873089152580373
previous_iter_valid_loss : 0.08771137148141861

    172900	  0.087627	  0.087711	  0.108513		CURRENT LEARNING RATE: 0.08864220498493891
previous_iter_valid_loss : 0.10375312715768814

    173000	  0.103650	  0.103753	  0.108270		CURRENT LEARNING RATE: 0.08855360708628644
previous_iter_valid_loss : 0.0807587057352066

    173100	  0.080784	  0.080759	  0.106669		CURRENT LEARNING RATE: 0.08846509774124846
previous_iter_valid_loss : 0.10370375216007233

    173200	  0.104167	  0.103704	  0.106323		CURRENT LEARNING RATE: 0.08837667686131558
previous_iter_valid_loss : 0.08193803578615189

    173300	  0.082252	  0.081938	  0.105966		CURRENT LEARNING RATE: 0.08828834435806694
previous_iter_valid_loss : 0.09100315719842911

    173400	  0.091277	  0.091003	  0.105791		CURRENT LEARNING RATE: 0.08820010014317
previous_iter_valid_loss : 0.10586278885602951

    173500	  0.106205	  0.105863	  0.105475		CURRENT LEARNING RATE: 0.08811194412838055
previous_iter_valid_loss : 0.08396244049072266

    173600	  0.084154	  0.083962	  0.105435		CURRENT LEARNING RATE: 0.08802387622554259
previous_iter_valid_loss : 0.09509482979774475

    173700	  0.095324	  0.095095	  0.104460		CURRENT LEARNING RATE: 0.08793589634658819
previous_iter_valid_loss : 0.10304877907037735

    173800	  0.103239	  0.103049	  0.104494		CURRENT LEARNING RATE: 0.08784800440353743
previous_iter_valid_loss : 0.09564857929944992

    173900	  0.095792	  0.095649	  0.104613		CURRENT LEARNING RATE: 0.08776020030849843
previous_iter_valid_loss : 0.10510026663541794

    174000	  0.105432	  0.105100	  0.104791		CURRENT LEARNING RATE: 0.08767248397366705
previous_iter_valid_loss : 0.08061641454696655

    174100	  0.080620	  0.080616	  0.104465		CURRENT LEARNING RATE: 0.08758485531132694
previous_iter_valid_loss : 0.10597103089094162

    174200	  0.105814	  0.105971	  0.104607		CURRENT LEARNING RATE: 0.08749731423384943
previous_iter_valid_loss : 0.08121374249458313

    174300	  0.081189	  0.081214	  0.103935		CURRENT LEARNING RATE: 0.08740986065369347
previous_iter_valid_loss : 0.09472553431987762

    174400	  0.094741	  0.094726	  0.103452		CURRENT LEARNING RATE: 0.08732249448340543
previous_iter_valid_loss : 0.11377710849046707

    174500	  0.113718	  0.113777	  0.103671		CURRENT LEARNING RATE: 0.08723521563561916
previous_iter_valid_loss : 0.08761827647686005

    174600	  0.087518	  0.087618	  0.103070		CURRENT LEARNING RATE: 0.08714802402305578
previous_iter_valid_loss : 0.10441979765892029

    174700	  0.104304	  0.104420	  0.103054		CURRENT LEARNING RATE: 0.0870609195585237
previous_iter_valid_loss : 0.1180119514465332

    174800	  0.117895	  0.118012	  0.103095		CURRENT LEARNING RATE: 0.08697390215491842
previous_iter_valid_loss : 0.08936702460050583

    174900	  0.089322	  0.089367	  0.102809		CURRENT LEARNING RATE: 0.08688697172522257
previous_iter_valid_loss : 0.1102055087685585

    175000	  0.110174	  0.110206	  0.102720		CURRENT LEARNING RATE: 0.08680012818250567
previous_iter_valid_loss : 0.09321737289428711

    175100	  0.093153	  0.093217	  0.102723		CURRENT LEARNING RATE: 0.08671337143992418
previous_iter_valid_loss : 0.1037534549832344

    175200	  0.103663	  0.103753	  0.102897		CURRENT LEARNING RATE: 0.08662670141072136
previous_iter_valid_loss : 0.16882428526878357

    175300	  0.168629	  0.168824	  0.103071		CURRENT LEARNING RATE: 0.08654011800822717
previous_iter_valid_loss : 0.10119938105344772

    175400	  0.100991	  0.101199	  0.103080		CURRENT LEARNING RATE: 0.0864536211458582
previous_iter_valid_loss : 0.15682919323444366

    175500	  0.156691	  0.156829	  0.103788		CURRENT LEARNING RATE: 0.08636721073711758
previous_iter_valid_loss : 0.09369933605194092

    175600	  0.093538	  0.093699	  0.102125		CURRENT LEARNING RATE: 0.08628088669559489
previous_iter_valid_loss : 0.14318056404590607

    175700	  0.143051	  0.143181	  0.102589		CURRENT LEARNING RATE: 0.08619464893496609
previous_iter_valid_loss : 0.0850762203335762

    175800	  0.084984	  0.085076	  0.102574		CURRENT LEARNING RATE: 0.08610849736899341
previous_iter_valid_loss : 0.09040902554988861

    175900	  0.090326	  0.090409	  0.102279		CURRENT LEARNING RATE: 0.08602243191152527
previous_iter_valid_loss : 0.11560890823602676

    176000	  0.115445	  0.115609	  0.102537		CURRENT LEARNING RATE: 0.08593645247649621
previous_iter_valid_loss : 0.10518898069858551

    176100	  0.105460	  0.105189	  0.102771		CURRENT LEARNING RATE: 0.08585055897792679
previous_iter_valid_loss : 0.1696336418390274

    176200	  0.170382	  0.169634	  0.103401		CURRENT LEARNING RATE: 0.0857647513299235
previous_iter_valid_loss : 0.09504613280296326

    176300	  0.095311	  0.095046	  0.103380		CURRENT LEARNING RATE: 0.08567902944667868
previous_iter_valid_loss : 0.11022690683603287

    176400	  0.110572	  0.110227	  0.103457		CURRENT LEARNING RATE: 0.08559339324247048
previous_iter_valid_loss : 0.08225821703672409

    176500	  0.082305	  0.082258	  0.103020		CURRENT LEARNING RATE: 0.08550784263166261
previous_iter_valid_loss : 0.08578445017337799

    176600	  0.085773	  0.085784	  0.103054		CURRENT LEARNING RATE: 0.08542237752870453
previous_iter_valid_loss : 0.09913145005702972

    176700	  0.099055	  0.099131	  0.103183		CURRENT LEARNING RATE: 0.08533699784813108
previous_iter_valid_loss : 0.1472228765487671

    176800	  0.147057	  0.147223	  0.103729		CURRENT LEARNING RATE: 0.0852517035045626
previous_iter_valid_loss : 0.11716591566801071

    176900	  0.116987	  0.117166	  0.103713		CURRENT LEARNING RATE: 0.08516649441270471
previous_iter_valid_loss : 0.0860714390873909

    177000	  0.085977	  0.086071	  0.103708		CURRENT LEARNING RATE: 0.08508137048734836
previous_iter_valid_loss : 0.09653481096029282

    177100	  0.096525	  0.096535	  0.103845		CURRENT LEARNING RATE: 0.08499633164336956
previous_iter_valid_loss : 0.09672239422798157

    177200	  0.096593	  0.096722	  0.103976		CURRENT LEARNING RATE: 0.0849113777957295
previous_iter_valid_loss : 0.09095855057239532

    177300	  0.090994	  0.090959	  0.103779		CURRENT LEARNING RATE: 0.0848265088594743
previous_iter_valid_loss : 0.12244617193937302

    177400	  0.122985	  0.122446	  0.103888		CURRENT LEARNING RATE: 0.08474172474973506
previous_iter_valid_loss : 0.08998515456914902

    177500	  0.090225	  0.089985	  0.103545		CURRENT LEARNING RATE: 0.08465702538172759
previous_iter_valid_loss : 0.11227347701787949

    177600	  0.112543	  0.112273	  0.103637		CURRENT LEARNING RATE: 0.08457241067075259
previous_iter_valid_loss : 0.08666235953569412

    177700	  0.086864	  0.086662	  0.103486		CURRENT LEARNING RATE: 0.08448788053219529
previous_iter_valid_loss : 0.16002537310123444

    177800	  0.160575	  0.160025	  0.104256		CURRENT LEARNING RATE: 0.08440343488152557
previous_iter_valid_loss : 0.09885827451944351

    177900	  0.099084	  0.098858	  0.104318		CURRENT LEARNING RATE: 0.08431907363429775
previous_iter_valid_loss : 0.08092120289802551

    178000	  0.081016	  0.080921	  0.103895		CURRENT LEARNING RATE: 0.08423479670615061
previous_iter_valid_loss : 0.09673084318637848

    178100	  0.096937	  0.096731	  0.103984		CURRENT LEARNING RATE: 0.08415060401280719
previous_iter_valid_loss : 0.15216393768787384

    178200	  0.152560	  0.152164	  0.104577		CURRENT LEARNING RATE: 0.0840664954700748
previous_iter_valid_loss : 0.11462412029504776

    178300	  0.114943	  0.114624	  0.104907		CURRENT LEARNING RATE: 0.08398247099384487
previous_iter_valid_loss : 0.09016415476799011

    178400	  0.090367	  0.090164	  0.104851		CURRENT LEARNING RATE: 0.08389853050009295
previous_iter_valid_loss : 0.08287128061056137

    178500	  0.082947	  0.082871	  0.103478		CURRENT LEARNING RATE: 0.0838146739048785
previous_iter_valid_loss : 0.08507148176431656

    178600	  0.085205	  0.085071	  0.103422		CURRENT LEARNING RATE: 0.08373090112434496
previous_iter_valid_loss : 0.1146470308303833

    178700	  0.114900	  0.114647	  0.103361		CURRENT LEARNING RATE: 0.0836472120747195
previous_iter_valid_loss : 0.12000353634357452

    178800	  0.119829	  0.120004	  0.103208		CURRENT LEARNING RATE: 0.08356360667231312
previous_iter_valid_loss : 0.08479928225278854

    178900	  0.084750	  0.084799	  0.102925		CURRENT LEARNING RATE: 0.08348008483352035
previous_iter_valid_loss : 0.09208356589078903

    179000	  0.091895	  0.092084	  0.102708		CURRENT LEARNING RATE: 0.08339664647481938
previous_iter_valid_loss : 0.08232483267784119

    179100	  0.082323	  0.082325	  0.102570		CURRENT LEARNING RATE: 0.08331329151277182
previous_iter_valid_loss : 0.08081380277872086

    179200	  0.080793	  0.080814	  0.102400		CURRENT LEARNING RATE: 0.08323001986402274
previous_iter_valid_loss : 0.09064361453056335

    179300	  0.090733	  0.090644	  0.101615		CURRENT LEARNING RATE: 0.08314683144530044
previous_iter_valid_loss : 0.08713028579950333

    179400	  0.087258	  0.087130	  0.101630		CURRENT LEARNING RATE: 0.08306372617341654
previous_iter_valid_loss : 0.08062487840652466

    179500	  0.080711	  0.080625	  0.101627		CURRENT LEARNING RATE: 0.08298070396526569
previous_iter_valid_loss : 0.08857636898756027

    179600	  0.088730	  0.088576	  0.101231		CURRENT LEARNING RATE: 0.08289776473782576
previous_iter_valid_loss : 0.11067993938922882

    179700	  0.111021	  0.110680	  0.101271		CURRENT LEARNING RATE: 0.08281490840815746
previous_iter_valid_loss : 0.1144636869430542

    179800	  0.114741	  0.114464	  0.101578		CURRENT LEARNING RATE: 0.08273213489340447
previous_iter_valid_loss : 0.13535715639591217

    179900	  0.135772	  0.135357	  0.102135		CURRENT LEARNING RATE: 0.08264944411079327
previous_iter_valid_loss : 0.13404609262943268

    180000	  0.134468	  0.134046	  0.102500		CURRENT LEARNING RATE: 0.08256683597763308
previous_iter_valid_loss : 0.10839096456766129

    180100	  0.108138	  0.108391	  0.102724		CURRENT LEARNING RATE: 0.08248431041131572
previous_iter_valid_loss : 0.09285465627908707

    180200	  0.092954	  0.092855	  0.102769		CURRENT LEARNING RATE: 0.08240186732931568
previous_iter_valid_loss : 0.10201144963502884

    180300	  0.102186	  0.102011	  0.102443		CURRENT LEARNING RATE: 0.0823195066491898
previous_iter_valid_loss : 0.10455251485109329

    180400	  0.104400	  0.104553	  0.102633		CURRENT LEARNING RATE: 0.08223722828857745
previous_iter_valid_loss : 0.09579096734523773

    180500	  0.095888	  0.095791	  0.102732		CURRENT LEARNING RATE: 0.08215503216520023
previous_iter_valid_loss : 0.1284041553735733

    180600	  0.128799	  0.128404	  0.102956		CURRENT LEARNING RATE: 0.08207291819686204
previous_iter_valid_loss : 0.07887069880962372

    180700	  0.078883	  0.078871	  0.102735		CURRENT LEARNING RATE: 0.08199088630144886
previous_iter_valid_loss : 0.08425411581993103

    180800	  0.084149	  0.084254	  0.102683		CURRENT LEARNING RATE: 0.08190893639692884
previous_iter_valid_loss : 0.07788792252540588


Current valid loss: 0.07788792252540588;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    180900	  0.077847	  0.077888	  0.102638		CURRENT LEARNING RATE: 0.08182706840135202
previous_iter_valid_loss : 0.09919007867574692

    181000	  0.099397	  0.099190	  0.102725		CURRENT LEARNING RATE: 0.08174528223285045
previous_iter_valid_loss : 0.13060416281223297

    181100	  0.130994	  0.130604	  0.103121		CURRENT LEARNING RATE: 0.0816635778096379
previous_iter_valid_loss : 0.08629899471998215

    181200	  0.086462	  0.086299	  0.102394		CURRENT LEARNING RATE: 0.08158195505000998
previous_iter_valid_loss : 0.10331233590841293

    181300	  0.103520	  0.103312	  0.102458		CURRENT LEARNING RATE: 0.08150041387234389
previous_iter_valid_loss : 0.11690862476825714

    181400	  0.117108	  0.116909	  0.102481		CURRENT LEARNING RATE: 0.08141895419509848
previous_iter_valid_loss : 0.09724918752908707

    181500	  0.097442	  0.097249	  0.102471		CURRENT LEARNING RATE: 0.08133757593681404
previous_iter_valid_loss : 0.10648007690906525

    181600	  0.106592	  0.106480	  0.102510		CURRENT LEARNING RATE: 0.08125627901611233
previous_iter_valid_loss : 0.08083279430866241

    181700	  0.080819	  0.080833	  0.102371		CURRENT LEARNING RATE: 0.08117506335169639
previous_iter_valid_loss : 0.10818745195865631

    181800	  0.108434	  0.108187	  0.101916		CURRENT LEARNING RATE: 0.08109392886235059
previous_iter_valid_loss : 0.09055014699697495

    181900	  0.090281	  0.090550	  0.101887		CURRENT LEARNING RATE: 0.08101287546694037
previous_iter_valid_loss : 0.08061656355857849

    182000	  0.080528	  0.080617	  0.101611		CURRENT LEARNING RATE: 0.08093190308441241
previous_iter_valid_loss : 0.10545754432678223

    182100	  0.105689	  0.105458	  0.101557		CURRENT LEARNING RATE: 0.08085101163379425
previous_iter_valid_loss : 0.08277921378612518

    182200	  0.082691	  0.082779	  0.101515		CURRENT LEARNING RATE: 0.08077020103419448
previous_iter_valid_loss : 0.08663704991340637

    182300	  0.086679	  0.086637	  0.101414		CURRENT LEARNING RATE: 0.08068947120480247
previous_iter_valid_loss : 0.08341673016548157

    182400	  0.083445	  0.083417	  0.101446		CURRENT LEARNING RATE: 0.08060882206488838
previous_iter_valid_loss : 0.08612506091594696

    182500	  0.086213	  0.086125	  0.101330		CURRENT LEARNING RATE: 0.08052825353380308
previous_iter_valid_loss : 0.110481396317482

    182600	  0.110711	  0.110481	  0.101592		CURRENT LEARNING RATE: 0.08044776553097803
previous_iter_valid_loss : 0.09910567849874496

    182700	  0.099367	  0.099106	  0.101631		CURRENT LEARNING RATE: 0.08036735797592519
previous_iter_valid_loss : 0.08106720447540283

    182800	  0.080997	  0.081067	  0.101546		CURRENT LEARNING RATE: 0.08028703078823705
previous_iter_valid_loss : 0.10203859955072403

    182900	  0.102215	  0.102039	  0.101689		CURRENT LEARNING RATE: 0.08020678388758637
previous_iter_valid_loss : 0.08183819055557251

    183000	  0.081834	  0.081838	  0.101470		CURRENT LEARNING RATE: 0.08012661719372628
previous_iter_valid_loss : 0.09559761732816696

    183100	  0.095700	  0.095598	  0.101618		CURRENT LEARNING RATE: 0.08004653062649005
previous_iter_valid_loss : 0.07834174484014511

    183200	  0.078311	  0.078342	  0.101364		CURRENT LEARNING RATE: 0.07996652410579112
previous_iter_valid_loss : 0.13370059430599213

    183300	  0.134042	  0.133701	  0.101882		CURRENT LEARNING RATE: 0.07988659755162296
previous_iter_valid_loss : 0.07908200472593307

    183400	  0.078969	  0.079082	  0.101763		CURRENT LEARNING RATE: 0.07980675088405902
previous_iter_valid_loss : 0.07845713198184967

    183500	  0.078461	  0.078457	  0.101489		CURRENT LEARNING RATE: 0.07972698402325258
previous_iter_valid_loss : 0.11013233661651611

    183600	  0.110412	  0.110132	  0.101750		CURRENT LEARNING RATE: 0.07964729688943685
previous_iter_valid_loss : 0.0966886430978775

    183700	  0.096779	  0.096689	  0.101766		CURRENT LEARNING RATE: 0.07956768940292461
previous_iter_valid_loss : 0.08431614190340042

    183800	  0.084105	  0.084316	  0.101579		CURRENT LEARNING RATE: 0.07948816148410844
previous_iter_valid_loss : 0.08537601679563522

    183900	  0.085193	  0.085376	  0.101476		CURRENT LEARNING RATE: 0.07940871305346034
previous_iter_valid_loss : 0.08157362788915634

    184000	  0.081436	  0.081574	  0.101241		CURRENT LEARNING RATE: 0.07932934403153194
previous_iter_valid_loss : 0.10058989375829697

    184100	  0.100774	  0.100590	  0.101441		CURRENT LEARNING RATE: 0.07925005433895416
previous_iter_valid_loss : 0.08346923440694809

    184200	  0.083435	  0.083469	  0.101216		CURRENT LEARNING RATE: 0.07917084389643735
previous_iter_valid_loss : 0.11474397033452988

    184300	  0.114961	  0.114744	  0.101551		CURRENT LEARNING RATE: 0.07909171262477101
previous_iter_valid_loss : 0.08425068855285645

    184400	  0.084273	  0.084251	  0.101446		CURRENT LEARNING RATE: 0.0790126604448239
previous_iter_valid_loss : 0.0834708884358406

    184500	  0.083443	  0.083471	  0.101143		CURRENT LEARNING RATE: 0.0789336872775438
previous_iter_valid_loss : 0.10327157378196716

    184600	  0.103038	  0.103272	  0.101300		CURRENT LEARNING RATE: 0.07885479304395758
previous_iter_valid_loss : 0.11026665568351746

    184700	  0.110020	  0.110267	  0.101358		CURRENT LEARNING RATE: 0.07877597766517096
previous_iter_valid_loss : 0.1252269744873047

    184800	  0.125161	  0.125227	  0.101430		CURRENT LEARNING RATE: 0.07869724106236857
previous_iter_valid_loss : 0.10943280160427094

    184900	  0.109665	  0.109433	  0.101631		CURRENT LEARNING RATE: 0.0786185831568138
previous_iter_valid_loss : 0.11507637798786163

    185000	  0.115226	  0.115076	  0.101680		CURRENT LEARNING RATE: 0.07854000386984875
previous_iter_valid_loss : 0.08412464708089828

    185100	  0.083913	  0.084125	  0.101589		CURRENT LEARNING RATE: 0.0784615031228941
previous_iter_valid_loss : 0.09037665277719498

    185200	  0.090452	  0.090377	  0.101455		CURRENT LEARNING RATE: 0.07838308083744913
previous_iter_valid_loss : 0.08031155914068222

    185300	  0.080330	  0.080312	  0.100570		CURRENT LEARNING RATE: 0.0783047369350915
previous_iter_valid_loss : 0.09586640447378159

    185400	  0.095971	  0.095866	  0.100517		CURRENT LEARNING RATE: 0.07822647133747737
previous_iter_valid_loss : 0.08036307990550995

    185500	  0.080306	  0.080363	  0.099752		CURRENT LEARNING RATE: 0.07814828396634106
previous_iter_valid_loss : 0.08191700279712677

    185600	  0.081980	  0.081917	  0.099634		CURRENT LEARNING RATE: 0.07807017474349526
previous_iter_valid_loss : 0.08281931281089783

    185700	  0.082860	  0.082819	  0.099031		CURRENT LEARNING RATE: 0.07799214359083068
previous_iter_valid_loss : 0.08722512423992157

    185800	  0.087089	  0.087225	  0.099052		CURRENT LEARNING RATE: 0.07791419043031621
previous_iter_valid_loss : 0.08532340824604034

    185900	  0.085489	  0.085323	  0.099001		CURRENT LEARNING RATE: 0.07783631518399865
previous_iter_valid_loss : 0.09140768647193909

    186000	  0.091469	  0.091408	  0.098759		CURRENT LEARNING RATE: 0.07775851777400278
previous_iter_valid_loss : 0.12103714048862457

    186100	  0.121293	  0.121037	  0.098918		CURRENT LEARNING RATE: 0.07768079812253113
previous_iter_valid_loss : 0.08490876853466034

    186200	  0.084944	  0.084909	  0.098070		CURRENT LEARNING RATE: 0.07760315615186411
previous_iter_valid_loss : 0.09120608121156693

    186300	  0.091243	  0.091206	  0.098032		CURRENT LEARNING RATE: 0.07752559178435968
previous_iter_valid_loss : 0.0940374955534935

    186400	  0.094158	  0.094037	  0.097870		CURRENT LEARNING RATE: 0.07744810494245352
previous_iter_valid_loss : 0.09270087629556656

    186500	  0.092746	  0.092701	  0.097975		CURRENT LEARNING RATE: 0.07737069554865875
previous_iter_valid_loss : 0.09992551803588867

    186600	  0.100112	  0.099926	  0.098116		CURRENT LEARNING RATE: 0.07729336352556597
previous_iter_valid_loss : 0.09258153289556503

    186700	  0.092629	  0.092582	  0.098050		CURRENT LEARNING RATE: 0.07721610879584316
previous_iter_valid_loss : 0.08832195401191711

    186800	  0.088409	  0.088322	  0.097461		CURRENT LEARNING RATE: 0.07713893128223558
previous_iter_valid_loss : 0.09862414002418518

    186900	  0.098428	  0.098624	  0.097276		CURRENT LEARNING RATE: 0.0770618309075657
previous_iter_valid_loss : 0.11133775860071182

    187000	  0.111595	  0.111338	  0.097529		CURRENT LEARNING RATE: 0.07698480759473317
previous_iter_valid_loss : 0.07883700728416443

    187100	  0.078731	  0.078837	  0.097352		CURRENT LEARNING RATE: 0.07690786126671463
previous_iter_valid_loss : 0.07848397642374039

    187200	  0.078418	  0.078484	  0.097169		CURRENT LEARNING RATE: 0.07683099184656379
previous_iter_valid_loss : 0.0837012529373169

    187300	  0.083626	  0.083701	  0.097097		CURRENT LEARNING RATE: 0.07675419925741117
previous_iter_valid_loss : 0.07965343445539474

    187400	  0.079634	  0.079653	  0.096669		CURRENT LEARNING RATE: 0.07667748342246423
previous_iter_valid_loss : 0.0797044038772583

    187500	  0.079639	  0.079704	  0.096566		CURRENT LEARNING RATE: 0.0766008442650071
previous_iter_valid_loss : 0.07953469455242157

    187600	  0.079534	  0.079535	  0.096239		CURRENT LEARNING RATE: 0.0765242817084006
previous_iter_valid_loss : 0.1131562739610672

    187700	  0.113330	  0.113156	  0.096504		CURRENT LEARNING RATE: 0.0764477956760822
previous_iter_valid_loss : 0.09782677888870239

    187800	  0.097914	  0.097827	  0.095882		CURRENT LEARNING RATE: 0.07637138609156584
previous_iter_valid_loss : 0.07805415987968445

    187900	  0.077954	  0.078054	  0.095674		CURRENT LEARNING RATE: 0.07629505287844195
previous_iter_valid_loss : 0.10259454697370529

    188000	  0.102359	  0.102595	  0.095890		CURRENT LEARNING RATE: 0.07621879596037727
previous_iter_valid_loss : 0.08906061947345734

    188100	  0.089190	  0.089061	  0.095814		CURRENT LEARNING RATE: 0.07614261526111492
previous_iter_valid_loss : 0.08747168630361557

    188200	  0.087304	  0.087472	  0.095167		CURRENT LEARNING RATE: 0.07606651070447416
previous_iter_valid_loss : 0.08066199719905853

    188300	  0.080526	  0.080662	  0.094827		CURRENT LEARNING RATE: 0.07599048221435047
previous_iter_valid_loss : 0.11097214370965958

    188400	  0.110800	  0.110972	  0.095035		CURRENT LEARNING RATE: 0.0759145297147153
previous_iter_valid_loss : 0.08609860390424728

    188500	  0.085948	  0.086099	  0.095067		CURRENT LEARNING RATE: 0.0758386531296162
previous_iter_valid_loss : 0.09627453982830048

    188600	  0.096455	  0.096275	  0.095179		CURRENT LEARNING RATE: 0.07576285238317651
previous_iter_valid_loss : 0.08442562818527222

    188700	  0.084500	  0.084426	  0.094877		CURRENT LEARNING RATE: 0.07568712739959556
previous_iter_valid_loss : 0.11666478961706161

    188800	  0.116884	  0.116665	  0.094844		CURRENT LEARNING RATE: 0.07561147810314828
previous_iter_valid_loss : 0.08437921851873398

    188900	  0.084376	  0.084379	  0.094840		CURRENT LEARNING RATE: 0.07553590441818543
previous_iter_valid_loss : 0.10261519998311996

    189000	  0.102817	  0.102615	  0.094945		CURRENT LEARNING RATE: 0.07546040626913328
previous_iter_valid_loss : 0.07995687425136566

    189100	  0.079815	  0.079957	  0.094921		CURRENT LEARNING RATE: 0.0753849835804937
previous_iter_valid_loss : 0.08760948479175568

    189200	  0.087531	  0.087609	  0.094989		CURRENT LEARNING RATE: 0.07530963627684396
previous_iter_valid_loss : 0.08804550021886826

    189300	  0.088077	  0.088046	  0.094963		CURRENT LEARNING RATE: 0.0752343642828368
previous_iter_valid_loss : 0.11986097693443298

    189400	  0.120100	  0.119861	  0.095291		CURRENT LEARNING RATE: 0.07515916752320016
previous_iter_valid_loss : 0.08266495168209076

    189500	  0.082578	  0.082665	  0.095311		CURRENT LEARNING RATE: 0.07508404592273733
previous_iter_valid_loss : 0.08629106730222702

    189600	  0.086314	  0.086291	  0.095288		CURRENT LEARNING RATE: 0.07500899940632667
previous_iter_valid_loss : 0.0794294998049736

    189700	  0.079416	  0.079429	  0.094976		CURRENT LEARNING RATE: 0.07493402789892167
previous_iter_valid_loss : 0.09980794787406921

    189800	  0.099622	  0.099808	  0.094829		CURRENT LEARNING RATE: 0.07485913132555082
previous_iter_valid_loss : 0.10031206905841827

    189900	  0.100121	  0.100312	  0.094479		CURRENT LEARNING RATE: 0.07478430961131753
previous_iter_valid_loss : 0.08719000965356827

    190000	  0.087033	  0.087190	  0.094010		CURRENT LEARNING RATE: 0.07470956268140008
previous_iter_valid_loss : 0.08426442742347717

    190100	  0.084209	  0.084264	  0.093769		CURRENT LEARNING RATE: 0.07463489046105154
previous_iter_valid_loss : 0.07952212542295456

    190200	  0.079521	  0.079522	  0.093635		CURRENT LEARNING RATE: 0.07456029287559968
previous_iter_valid_loss : 0.07850436866283417

    190300	  0.078544	  0.078504	  0.093400		CURRENT LEARNING RATE: 0.07448576985044691
previous_iter_valid_loss : 0.09993443638086319

    190400	  0.100136	  0.099934	  0.093354		CURRENT LEARNING RATE: 0.07441132131107019
previous_iter_valid_loss : 0.08413168787956238

    190500	  0.083968	  0.084132	  0.093238		CURRENT LEARNING RATE: 0.074336947183021
previous_iter_valid_loss : 0.08003424853086472

    190600	  0.079925	  0.080034	  0.092754		CURRENT LEARNING RATE: 0.07426264739192516
previous_iter_valid_loss : 0.0812431052327156

    190700	  0.081184	  0.081243	  0.092778		CURRENT LEARNING RATE: 0.07418842186348293
previous_iter_valid_loss : 0.11631444096565247

    190800	  0.116107	  0.116314	  0.093098		CURRENT LEARNING RATE: 0.07411427052346872
previous_iter_valid_loss : 0.08251935988664627

    190900	  0.082460	  0.082519	  0.093145		CURRENT LEARNING RATE: 0.07404019329773123
previous_iter_valid_loss : 0.12334905564785004

    191000	  0.123691	  0.123349	  0.093386		CURRENT LEARNING RATE: 0.0739661901121932
previous_iter_valid_loss : 0.1037462055683136

    191100	  0.103935	  0.103746	  0.093118		CURRENT LEARNING RATE: 0.07389226089285145
previous_iter_valid_loss : 0.08054345846176147

    191200	  0.080508	  0.080543	  0.093060		CURRENT LEARNING RATE: 0.07381840556577673
previous_iter_valid_loss : 0.08040379732847214

    191300	  0.080432	  0.080404	  0.092831		CURRENT LEARNING RATE: 0.07374462405711375
previous_iter_valid_loss : 0.10670989751815796

    191400	  0.106905	  0.106710	  0.092729		CURRENT LEARNING RATE: 0.07367091629308097
previous_iter_valid_loss : 0.08472487330436707

    191500	  0.084777	  0.084725	  0.092604		CURRENT LEARNING RATE: 0.07359728219997062
previous_iter_valid_loss : 0.08273709565401077

    191600	  0.082735	  0.082737	  0.092366		CURRENT LEARNING RATE: 0.0735237217041486
previous_iter_valid_loss : 0.07927471399307251

    191700	  0.079234	  0.079275	  0.092351		CURRENT LEARNING RATE: 0.07345023473205442
previous_iter_valid_loss : 0.14051303267478943

    191800	  0.140292	  0.140513	  0.092674		CURRENT LEARNING RATE: 0.07337682121020107
previous_iter_valid_loss : 0.07882020622491837

    191900	  0.078746	  0.078820	  0.092557		CURRENT LEARNING RATE: 0.07330348106517508
previous_iter_valid_loss : 0.10034559667110443

    192000	  0.100096	  0.100346	  0.092754		CURRENT LEARNING RATE: 0.07323021422363624
previous_iter_valid_loss : 0.10330042988061905

    192100	  0.103111	  0.103300	  0.092732		CURRENT LEARNING RATE: 0.07315702061231773
previous_iter_valid_loss : 0.08590788394212723

    192200	  0.085882	  0.085908	  0.092764		CURRENT LEARNING RATE: 0.07308390015802592
previous_iter_valid_loss : 0.07823611795902252

    192300	  0.078137	  0.078236	  0.092680		CURRENT LEARNING RATE: 0.07301085278764037
previous_iter_valid_loss : 0.150675967335701

    192400	  0.150394	  0.150676	  0.093352		CURRENT LEARNING RATE: 0.07293787842811368
previous_iter_valid_loss : 0.09658904373645782

    192500	  0.096678	  0.096589	  0.093457		CURRENT LEARNING RATE: 0.07286497700647152
previous_iter_valid_loss : 0.07833452522754669

    192600	  0.078305	  0.078335	  0.093135		CURRENT LEARNING RATE: 0.07279214844981241
previous_iter_valid_loss : 0.0787518173456192

    192700	  0.078642	  0.078752	  0.092932		CURRENT LEARNING RATE: 0.07271939268530785
previous_iter_valid_loss : 0.07958769053220749

    192800	  0.079534	  0.079588	  0.092917		CURRENT LEARNING RATE: 0.072646709640202
previous_iter_valid_loss : 0.08522749692201614

    192900	  0.085182	  0.085227	  0.092749		CURRENT LEARNING RATE: 0.07257409924181187
previous_iter_valid_loss : 0.07976753264665604

    193000	  0.079753	  0.079768	  0.092728		CURRENT LEARNING RATE: 0.072501561417527
previous_iter_valid_loss : 0.09197210520505905

    193100	  0.092131	  0.091972	  0.092692		CURRENT LEARNING RATE: 0.07242909609480963
previous_iter_valid_loss : 0.07810351252555847

    193200	  0.078018	  0.078104	  0.092690		CURRENT LEARNING RATE: 0.07235670320119436
previous_iter_valid_loss : 0.08317279815673828

    193300	  0.083081	  0.083173	  0.092184		CURRENT LEARNING RATE: 0.07228438266428834
previous_iter_valid_loss : 0.08144351094961166

    193400	  0.081487	  0.081444	  0.092208		CURRENT LEARNING RATE: 0.07221213441177099
previous_iter_valid_loss : 0.08295293897390366

    193500	  0.082890	  0.082953	  0.092253		CURRENT LEARNING RATE: 0.07213995837139409
previous_iter_valid_loss : 0.08538206666707993

    193600	  0.085460	  0.085382	  0.092005		CURRENT LEARNING RATE: 0.07206785447098155
previous_iter_valid_loss : 0.11490361392498016

    193700	  0.115242	  0.114904	  0.092188		CURRENT LEARNING RATE: 0.0719958226384295
previous_iter_valid_loss : 0.1007634773850441

    193800	  0.100549	  0.100763	  0.092352		CURRENT LEARNING RATE: 0.07192386280170608
previous_iter_valid_loss : 0.08289758861064911

    193900	  0.082740	  0.082898	  0.092327		CURRENT LEARNING RATE: 0.07185197488885146
previous_iter_valid_loss : 0.0826195627450943

    194000	  0.082381	  0.082620	  0.092338		CURRENT LEARNING RATE: 0.07178015882797771
previous_iter_valid_loss : 0.08796754479408264

    194100	  0.088027	  0.087968	  0.092211		CURRENT LEARNING RATE: 0.07170841454726878
previous_iter_valid_loss : 0.07804097980260849

    194200	  0.077928	  0.078041	  0.092157		CURRENT LEARNING RATE: 0.07163674197498036
previous_iter_valid_loss : 0.07895559072494507

    194300	  0.078875	  0.078956	  0.091799		CURRENT LEARNING RATE: 0.07156514103943991
previous_iter_valid_loss : 0.07931586354970932

    194400	  0.079302	  0.079316	  0.091750		CURRENT LEARNING RATE: 0.07149361166904644
previous_iter_valid_loss : 0.10186745226383209

    194500	  0.101575	  0.101867	  0.091934		CURRENT LEARNING RATE: 0.07142215379227061
previous_iter_valid_loss : 0.07827957719564438

    194600	  0.078165	  0.078280	  0.091684		CURRENT LEARNING RATE: 0.07135076733765451
previous_iter_valid_loss : 0.08113455772399902

    194700	  0.081169	  0.081135	  0.091393		CURRENT LEARNING RATE: 0.07127945223381171
previous_iter_valid_loss : 0.08034910261631012

    194800	  0.080236	  0.080349	  0.090944		CURRENT LEARNING RATE: 0.07120820840942707
previous_iter_valid_loss : 0.08054440468549728

    194900	  0.080508	  0.080544	  0.090655		CURRENT LEARNING RATE: 0.0711370357932568
previous_iter_valid_loss : 0.08115947246551514

    195000	  0.081097	  0.081159	  0.090316		CURRENT LEARNING RATE: 0.0710659343141282
previous_iter_valid_loss : 0.08932904899120331

    195100	  0.089190	  0.089329	  0.090368		CURRENT LEARNING RATE: 0.07099490390093989
previous_iter_valid_loss : 0.07927410304546356

    195200	  0.079244	  0.079274	  0.090257		CURRENT LEARNING RATE: 0.07092394448266136
previous_iter_valid_loss : 0.1323622614145279

    195300	  0.132643	  0.132362	  0.090777		CURRENT LEARNING RATE: 0.07085305598833325
previous_iter_valid_loss : 0.08110713213682175

    195400	  0.080936	  0.081107	  0.090630		CURRENT LEARNING RATE: 0.07078223834706701
previous_iter_valid_loss : 0.08301644027233124

    195500	  0.083056	  0.083016	  0.090656		CURRENT LEARNING RATE: 0.07071149148804504
previous_iter_valid_loss : 0.07848838716745377

    195600	  0.078451	  0.078488	  0.090622		CURRENT LEARNING RATE: 0.07064081534052043
previous_iter_valid_loss : 0.09803926199674606

    195700	  0.097866	  0.098039	  0.090774		CURRENT LEARNING RATE: 0.07057020983381705
previous_iter_valid_loss : 0.10185898095369339

    195800	  0.102031	  0.101859	  0.090921		CURRENT LEARNING RATE: 0.07049967489732938
previous_iter_valid_loss : 0.09472634643316269

    195900	  0.094827	  0.094726	  0.091015		CURRENT LEARNING RATE: 0.0704292104605225
previous_iter_valid_loss : 0.09833947569131851

    196000	  0.098452	  0.098339	  0.091084		CURRENT LEARNING RATE: 0.07035881645293193
previous_iter_valid_loss : 0.08782390505075455

    196100	  0.087854	  0.087824	  0.090752		CURRENT LEARNING RATE: 0.0702884928041637
previous_iter_valid_loss : 0.11985252797603607

    196200	  0.119617	  0.119853	  0.091101		CURRENT LEARNING RATE: 0.0702182394438941
previous_iter_valid_loss : 0.09608282893896103

    196300	  0.096171	  0.096083	  0.091150		CURRENT LEARNING RATE: 0.07014805630186982
previous_iter_valid_loss : 0.08837869763374329

    196400	  0.088194	  0.088379	  0.091093		CURRENT LEARNING RATE: 0.07007794330790768
previous_iter_valid_loss : 0.08810367435216904

    196500	  0.088111	  0.088104	  0.091047		CURRENT LEARNING RATE: 0.0700079003918947
previous_iter_valid_loss : 0.07911376655101776

    196600	  0.078932	  0.079114	  0.090839		CURRENT LEARNING RATE: 0.06993792748378792
previous_iter_valid_loss : 0.13806632161140442

    196700	  0.138434	  0.138066	  0.091294		CURRENT LEARNING RATE: 0.06986802451361447
previous_iter_valid_loss : 0.13452699780464172

    196800	  0.134890	  0.134527	  0.091756		CURRENT LEARNING RATE: 0.06979819141147135
previous_iter_valid_loss : 0.08566898852586746

    196900	  0.085704	  0.085669	  0.091627		CURRENT LEARNING RATE: 0.06972842810752547
previous_iter_valid_loss : 0.07735144346952438


Current valid loss: 0.07735144346952438;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    197000	  0.077261	  0.077351	  0.091287		CURRENT LEARNING RATE: 0.06965873453201349
previous_iter_valid_loss : 0.07821033149957657

    197100	  0.078180	  0.078210	  0.091280		CURRENT LEARNING RATE: 0.06958911061524187
previous_iter_valid_loss : 0.08170292526483536

    197200	  0.081706	  0.081703	  0.091313		CURRENT LEARNING RATE: 0.06951955628758663
previous_iter_valid_loss : 0.08815287053585052

    197300	  0.088232	  0.088153	  0.091357		CURRENT LEARNING RATE: 0.0694500714794935
previous_iter_valid_loss : 0.09354006499052048

    197400	  0.093387	  0.093540	  0.091496		CURRENT LEARNING RATE: 0.06938065612147762
previous_iter_valid_loss : 0.07975821942090988

    197500	  0.079740	  0.079758	  0.091497		CURRENT LEARNING RATE: 0.06931131014412366
previous_iter_valid_loss : 0.08509258180856705

    197600	  0.085148	  0.085093	  0.091552		CURRENT LEARNING RATE: 0.06924203347808561
previous_iter_valid_loss : 0.08533165603876114

    197700	  0.085345	  0.085332	  0.091274		CURRENT LEARNING RATE: 0.06917282605408681
previous_iter_valid_loss : 0.12344584614038467

    197800	  0.123352	  0.123446	  0.091530		CURRENT LEARNING RATE: 0.06910368780291982
previous_iter_valid_loss : 0.10942696034908295

    197900	  0.109264	  0.109427	  0.091844		CURRENT LEARNING RATE: 0.06903461865544641
previous_iter_valid_loss : 0.07875173538923264

    198000	  0.078712	  0.078752	  0.091605		CURRENT LEARNING RATE: 0.06896561854259739
previous_iter_valid_loss : 0.14173604547977448

    198100	  0.141518	  0.141736	  0.092132		CURRENT LEARNING RATE: 0.06889668739537268
previous_iter_valid_loss : 0.08032775670289993

    198200	  0.080353	  0.080328	  0.092061		CURRENT LEARNING RATE: 0.06882782514484108
previous_iter_valid_loss : 0.12902021408081055

    198300	  0.129245	  0.129020	  0.092544		CURRENT LEARNING RATE: 0.06875903172214037
previous_iter_valid_loss : 0.07850977778434753

    198400	  0.078426	  0.078510	  0.092220		CURRENT LEARNING RATE: 0.06869030705847712
previous_iter_valid_loss : 0.07749374955892563

    198500	  0.077446	  0.077494	  0.092134		CURRENT LEARNING RATE: 0.06862165108512666
previous_iter_valid_loss : 0.10927082598209381

    198600	  0.109015	  0.109271	  0.092264		CURRENT LEARNING RATE: 0.06855306373343298
previous_iter_valid_loss : 0.11301232874393463

    198700	  0.112774	  0.113012	  0.092549		CURRENT LEARNING RATE: 0.06848454493480877
previous_iter_valid_loss : 0.08526718616485596

    198800	  0.085085	  0.085267	  0.092236		CURRENT LEARNING RATE: 0.06841609462073518
previous_iter_valid_loss : 0.08087765425443649

    198900	  0.080816	  0.080878	  0.092200		CURRENT LEARNING RATE: 0.06834771272276192
previous_iter_valid_loss : 0.08164142817258835

    199000	  0.081511	  0.081641	  0.091991		CURRENT LEARNING RATE: 0.06827939917250708
previous_iter_valid_loss : 0.07855595648288727

    199100	  0.078541	  0.078556	  0.091977		CURRENT LEARNING RATE: 0.06821115390165712
previous_iter_valid_loss : 0.08040817826986313

    199200	  0.080390	  0.080408	  0.091905		CURRENT LEARNING RATE: 0.06814297684196671
previous_iter_valid_loss : 0.08782322704792023

    199300	  0.087688	  0.087823	  0.091903		CURRENT LEARNING RATE: 0.06807486792525885
previous_iter_valid_loss : 0.1017376035451889

    199400	  0.101925	  0.101738	  0.091721		CURRENT LEARNING RATE: 0.06800682708342458
previous_iter_valid_loss : 0.0903502032160759

    199500	  0.090351	  0.090350	  0.091798		CURRENT LEARNING RATE: 0.06793885424842307
previous_iter_valid_loss : 0.12029898911714554

    199600	  0.120142	  0.120299	  0.092138		CURRENT LEARNING RATE: 0.06787094935228144
previous_iter_valid_loss : 0.10972153395414352

    199700	  0.109800	  0.109722	  0.092441		CURRENT LEARNING RATE: 0.06780311232709485
previous_iter_valid_loss : 0.07752827554941177

    199800	  0.077474	  0.077528	  0.092218		CURRENT LEARNING RATE: 0.06773534310502621
previous_iter_valid_loss : 0.11011791974306107

    199900	  0.109929	  0.110118	  0.092316		CURRENT LEARNING RATE: 0.06766764161830635
previous_iter_valid_loss : 0.09821254760026932

    200000	  0.098334	  0.098213	  0.092427		CURRENT LEARNING RATE: 0.06760000779923374
previous_iter_valid_loss : 0.09731271862983704

    200100	  0.097379	  0.097313	  0.092557		CURRENT LEARNING RATE: 0.06753244158017456
previous_iter_valid_loss : 0.10247158259153366

    200200	  0.102145	  0.102472	  0.092787		CURRENT LEARNING RATE: 0.06746494289356256
previous_iter_valid_loss : 0.08308719098567963

    200300	  0.083072	  0.083087	  0.092832		CURRENT LEARNING RATE: 0.0673975116718991
previous_iter_valid_loss : 0.08096714317798615

    200400	  0.080749	  0.080967	  0.092643		CURRENT LEARNING RATE: 0.06733014784775293
previous_iter_valid_loss : 0.11981888860464096

    200500	  0.119581	  0.119819	  0.093000		CURRENT LEARNING RATE: 0.06726285135376023
previous_iter_valid_loss : 0.11924143880605698

    200600	  0.118944	  0.119241	  0.093392		CURRENT LEARNING RATE: 0.06719562212262445
previous_iter_valid_loss : 0.1122891828417778

    200700	  0.112151	  0.112289	  0.093702		CURRENT LEARNING RATE: 0.06712846008711643
previous_iter_valid_loss : 0.10719680041074753

    200800	  0.106972	  0.107197	  0.093611		CURRENT LEARNING RATE: 0.06706136518007408
previous_iter_valid_loss : 0.08011113852262497

    200900	  0.080074	  0.080111	  0.093587		CURRENT LEARNING RATE: 0.06699433733440249
previous_iter_valid_loss : 0.1116095706820488

    201000	  0.111785	  0.111610	  0.093469		CURRENT LEARNING RATE: 0.06692737648307381
previous_iter_valid_loss : 0.08282556384801865

    201100	  0.082782	  0.082826	  0.093260		CURRENT LEARNING RATE: 0.0668604825591272
previous_iter_valid_loss : 0.1305825561285019

    201200	  0.130389	  0.130583	  0.093761		CURRENT LEARNING RATE: 0.06679365549566874
previous_iter_valid_loss : 0.07826615869998932

    201300	  0.078222	  0.078266	  0.093739		CURRENT LEARNING RATE: 0.06672689522587133
previous_iter_valid_loss : 0.09652084857225418

    201400	  0.096293	  0.096521	  0.093637		CURRENT LEARNING RATE: 0.06666020168297468
previous_iter_valid_loss : 0.09231055527925491

    201500	  0.092450	  0.092311	  0.093713		CURRENT LEARNING RATE: 0.0665935748002853
previous_iter_valid_loss : 0.11988106369972229

    201600	  0.120137	  0.119881	  0.094085		CURRENT LEARNING RATE: 0.06652701451117626
previous_iter_valid_loss : 0.08464029431343079

    201700	  0.084507	  0.084640	  0.094138		CURRENT LEARNING RATE: 0.06646052074908729
previous_iter_valid_loss : 0.14391741156578064

    201800	  0.143753	  0.143917	  0.094172		CURRENT LEARNING RATE: 0.06639409344752457
previous_iter_valid_loss : 0.08412884920835495

    201900	  0.084128	  0.084129	  0.094225		CURRENT LEARNING RATE: 0.06632773254006086
previous_iter_valid_loss : 0.09417498111724854

    202000	  0.094310	  0.094175	  0.094164		CURRENT LEARNING RATE: 0.06626143796033522
previous_iter_valid_loss : 0.08293724805116653

    202100	  0.082971	  0.082937	  0.093960		CURRENT LEARNING RATE: 0.06619520964205305
previous_iter_valid_loss : 0.09035515785217285

    202200	  0.090384	  0.090355	  0.094005		CURRENT LEARNING RATE: 0.06612904751898603
previous_iter_valid_loss : 0.09475187212228775

    202300	  0.094581	  0.094752	  0.094170		CURRENT LEARNING RATE: 0.06606295152497205
previous_iter_valid_loss : 0.08065153658390045

    202400	  0.080494	  0.080652	  0.093470		CURRENT LEARNING RATE: 0.06599692159391511
previous_iter_valid_loss : 0.10449323803186417

    202500	  0.104312	  0.104493	  0.093549		CURRENT LEARNING RATE: 0.06593095765978527
previous_iter_valid_loss : 0.09289314597845078

    202600	  0.092759	  0.092893	  0.093694		CURRENT LEARNING RATE: 0.06586505965661854
previous_iter_valid_loss : 0.07811899483203888

    202700	  0.078033	  0.078119	  0.093688		CURRENT LEARNING RATE: 0.065799227518517
previous_iter_valid_loss : 0.08549037575721741

    202800	  0.085488	  0.085490	  0.093747		CURRENT LEARNING RATE: 0.06573346117964844
previous_iter_valid_loss : 0.10299881547689438

    202900	  0.103098	  0.102999	  0.093925		CURRENT LEARNING RATE: 0.06566776057424656
previous_iter_valid_loss : 0.08859290927648544

    203000	  0.088625	  0.088593	  0.094013		CURRENT LEARNING RATE: 0.06560212563661068
previous_iter_valid_loss : 0.07834828644990921

    203100	  0.078383	  0.078348	  0.093877		CURRENT LEARNING RATE: 0.06553655630110594
previous_iter_valid_loss : 0.07942502945661545

    203200	  0.079370	  0.079425	  0.093890		CURRENT LEARNING RATE: 0.06547105250216297
previous_iter_valid_loss : 0.09023217111825943

    203300	  0.090037	  0.090232	  0.093960		CURRENT LEARNING RATE: 0.06540561417427794
previous_iter_valid_loss : 0.08161704242229462

    203400	  0.081608	  0.081617	  0.093962		CURRENT LEARNING RATE: 0.06534024125201252
previous_iter_valid_loss : 0.10246302932500839

    203500	  0.102527	  0.102463	  0.094157		CURRENT LEARNING RATE: 0.06527493366999382
previous_iter_valid_loss : 0.14190618693828583

    203600	  0.142234	  0.141906	  0.094722		CURRENT LEARNING RATE: 0.06520969136291424
previous_iter_valid_loss : 0.09998682141304016

    203700	  0.100028	  0.099987	  0.094573		CURRENT LEARNING RATE: 0.06514451426553144
previous_iter_valid_loss : 0.10929286479949951

    203800	  0.109090	  0.109293	  0.094659		CURRENT LEARNING RATE: 0.06507940231266832
previous_iter_valid_loss : 0.11973249912261963

    203900	  0.119490	  0.119732	  0.095027		CURRENT LEARNING RATE: 0.06501435543921295
previous_iter_valid_loss : 0.11233581602573395

    204000	  0.112570	  0.112336	  0.095324		CURRENT LEARNING RATE: 0.06494937358011846
previous_iter_valid_loss : 0.07931052148342133

    204100	  0.079274	  0.079311	  0.095238		CURRENT LEARNING RATE: 0.06488445667040293
previous_iter_valid_loss : 0.08147945255041122

    204200	  0.081350	  0.081479	  0.095272		CURRENT LEARNING RATE: 0.06481960464514948
previous_iter_valid_loss : 0.09294494241476059

    204300	  0.093105	  0.092945	  0.095412		CURRENT LEARNING RATE: 0.06475481743950609
previous_iter_valid_loss : 0.08180416375398636

    204400	  0.081658	  0.081804	  0.095437		CURRENT LEARNING RATE: 0.06469009498868554
previous_iter_valid_loss : 0.08538109809160233

    204500	  0.085190	  0.085381	  0.095272		CURRENT LEARNING RATE: 0.06462543722796536
previous_iter_valid_loss : 0.09958750754594803

    204600	  0.099769	  0.099588	  0.095485		CURRENT LEARNING RATE: 0.06456084409268778
previous_iter_valid_loss : 0.08180664479732513

    204700	  0.081621	  0.081807	  0.095492		CURRENT LEARNING RATE: 0.0644963155182597
previous_iter_valid_loss : 0.08184697479009628

    204800	  0.081753	  0.081847	  0.095507		CURRENT LEARNING RATE: 0.06443185144015251
previous_iter_valid_loss : 0.0823550745844841

    204900	  0.082273	  0.082355	  0.095525		CURRENT LEARNING RATE: 0.06436745179390212
previous_iter_valid_loss : 0.09816402941942215

    205000	  0.098076	  0.098164	  0.095695		CURRENT LEARNING RATE: 0.06430311651510887
previous_iter_valid_loss : 0.08286157995462418

    205100	  0.082751	  0.082862	  0.095630		CURRENT LEARNING RATE: 0.06423884553943751
previous_iter_valid_loss : 0.09761149436235428

    205200	  0.097695	  0.097611	  0.095813		CURRENT LEARNING RATE: 0.06417463880261706
previous_iter_valid_loss : 0.09234079718589783

    205300	  0.092350	  0.092341	  0.095413		CURRENT LEARNING RATE: 0.06411049624044075
previous_iter_valid_loss : 0.08726654946804047

    205400	  0.087052	  0.087267	  0.095475		CURRENT LEARNING RATE: 0.06404641778876599
previous_iter_valid_loss : 0.11721225082874298

    205500	  0.117043	  0.117212	  0.095817		CURRENT LEARNING RATE: 0.0639824033835144
previous_iter_valid_loss : 0.10418713837862015

    205600	  0.104017	  0.104187	  0.096074		CURRENT LEARNING RATE: 0.06391845296067152
previous_iter_valid_loss : 0.08393832296133041

    205700	  0.083762	  0.083938	  0.095933		CURRENT LEARNING RATE: 0.06385456645628691
previous_iter_valid_loss : 0.13103561103343964

    205800	  0.131286	  0.131036	  0.096225		CURRENT LEARNING RATE: 0.06379074380647407
previous_iter_valid_loss : 0.11318153142929077

    205900	  0.113075	  0.113182	  0.096409		CURRENT LEARNING RATE: 0.06372698494741037
previous_iter_valid_loss : 0.0880703330039978

    206000	  0.087989	  0.088070	  0.096306		CURRENT LEARNING RATE: 0.06366328981533693
previous_iter_valid_loss : 0.08568888902664185

    206100	  0.085637	  0.085689	  0.096285		CURRENT LEARNING RATE: 0.0635996583465586
previous_iter_valid_loss : 0.09100520610809326

    206200	  0.091129	  0.091005	  0.095997		CURRENT LEARNING RATE: 0.06353609047744391
previous_iter_valid_loss : 0.08502674102783203

    206300	  0.085040	  0.085027	  0.095886		CURRENT LEARNING RATE: 0.06347258614442501
previous_iter_valid_loss : 0.09745042026042938

    206400	  0.097455	  0.097450	  0.095977		CURRENT LEARNING RATE: 0.06340914528399755
previous_iter_valid_loss : 0.1432010531425476

    206500	  0.143437	  0.143201	  0.096528		CURRENT LEARNING RATE: 0.06334576783272065
previous_iter_valid_loss : 0.10936010628938675

    206600	  0.109506	  0.109360	  0.096830		CURRENT LEARNING RATE: 0.06328245372721683
previous_iter_valid_loss : 0.10487381368875504

    206700	  0.104816	  0.104874	  0.096498		CURRENT LEARNING RATE: 0.06321920290417204
previous_iter_valid_loss : 0.09343129396438599

    206800	  0.093532	  0.093431	  0.096087		CURRENT LEARNING RATE: 0.06315601530033543
previous_iter_valid_loss : 0.08193118870258331

    206900	  0.082032	  0.081931	  0.096050		CURRENT LEARNING RATE: 0.06309289085251939
previous_iter_valid_loss : 0.11744965612888336

    207000	  0.117715	  0.117450	  0.096451		CURRENT LEARNING RATE: 0.06302982949759942
previous_iter_valid_loss : 0.09667842090129852

    207100	  0.096925	  0.096678	  0.096636		CURRENT LEARNING RATE: 0.06296683117251423
previous_iter_valid_loss : 0.08769463002681732

    207200	  0.087544	  0.087695	  0.096696		CURRENT LEARNING RATE: 0.06290389581426546
previous_iter_valid_loss : 0.09603876620531082

    207300	  0.095910	  0.096039	  0.096774		CURRENT LEARNING RATE: 0.06284102335991774
previous_iter_valid_loss : 0.10508479177951813

    207400	  0.105300	  0.105085	  0.096890		CURRENT LEARNING RATE: 0.0627782137465986
previous_iter_valid_loss : 0.11057288944721222

    207500	  0.110824	  0.110573	  0.097198		CURRENT LEARNING RATE: 0.06271546691149846
previous_iter_valid_loss : 0.08438918739557266

    207600	  0.084342	  0.084389	  0.097191		CURRENT LEARNING RATE: 0.06265278279187046
previous_iter_valid_loss : 0.09488523751497269

    207700	  0.094982	  0.094885	  0.097286		CURRENT LEARNING RATE: 0.06259016132503047
previous_iter_valid_loss : 0.12286985665559769

    207800	  0.122886	  0.122870	  0.097281		CURRENT LEARNING RATE: 0.06252760244835699
previous_iter_valid_loss : 0.09688801318407059

    207900	  0.096812	  0.096888	  0.097155		CURRENT LEARNING RATE: 0.06246510609929121
previous_iter_valid_loss : 0.138795405626297

    208000	  0.139085	  0.138795	  0.097756		CURRENT LEARNING RATE: 0.06240267221533673
previous_iter_valid_loss : 0.09619411081075668

    208100	  0.096259	  0.096194	  0.097300		CURRENT LEARNING RATE: 0.062340300734059655
previous_iter_valid_loss : 0.08861754089593887

    208200	  0.088695	  0.088618	  0.097383		CURRENT LEARNING RATE: 0.06227799159308849
previous_iter_valid_loss : 0.09496814012527466

    208300	  0.094915	  0.094968	  0.097043		CURRENT LEARNING RATE: 0.06221574473011413
previous_iter_valid_loss : 0.09229114651679993

    208400	  0.092098	  0.092291	  0.097181		CURRENT LEARNING RATE: 0.06215356008288969
previous_iter_valid_loss : 0.0843241736292839

    208500	  0.084296	  0.084324	  0.097249		CURRENT LEARNING RATE: 0.06209143758923051
previous_iter_valid_loss : 0.08455849438905716

    208600	  0.084619	  0.084558	  0.097002		CURRENT LEARNING RATE: 0.06202937718701407
previous_iter_valid_loss : 0.0972171425819397

    208700	  0.097275	  0.097217	  0.096844		CURRENT LEARNING RATE: 0.06196737881418001
previous_iter_valid_loss : 0.09403392672538757

    208800	  0.094175	  0.094034	  0.096931		CURRENT LEARNING RATE: 0.06190544240872993
previous_iter_valid_loss : 0.09559587389230728

    208900	  0.095449	  0.095596	  0.097079		CURRENT LEARNING RATE: 0.061843567908727415
previous_iter_valid_loss : 0.08747244626283646

    209000	  0.087378	  0.087472	  0.097137		CURRENT LEARNING RATE: 0.06178175525229794
previous_iter_valid_loss : 0.08182907849550247

    209100	  0.081802	  0.081829	  0.097170		CURRENT LEARNING RATE: 0.06172000437762889
previous_iter_valid_loss : 0.08709942549467087

    209200	  0.086945	  0.087099	  0.097237		CURRENT LEARNING RATE: 0.061658315222969357
previous_iter_valid_loss : 0.0801440179347992

    209300	  0.080096	  0.080144	  0.097160		CURRENT LEARNING RATE: 0.06159668772663019
previous_iter_valid_loss : 0.08729482442140579

    209400	  0.087164	  0.087295	  0.097015		CURRENT LEARNING RATE: 0.061535121826983855
previous_iter_valid_loss : 0.1311320662498474

    209500	  0.130981	  0.131132	  0.097423		CURRENT LEARNING RATE: 0.0614736174624645
previous_iter_valid_loss : 0.0808100551366806

    209600	  0.080726	  0.080810	  0.097028		CURRENT LEARNING RATE: 0.06141217457156773
previous_iter_valid_loss : 0.12426596879959106

    209700	  0.124073	  0.124266	  0.097174		CURRENT LEARNING RATE: 0.06135079309285065
previous_iter_valid_loss : 0.08749565482139587

    209800	  0.087238	  0.087496	  0.097273		CURRENT LEARNING RATE: 0.06128947296493175
previous_iter_valid_loss : 0.1210830956697464

    209900	  0.120860	  0.121083	  0.097383		CURRENT LEARNING RATE: 0.06122821412649095
previous_iter_valid_loss : 0.08306397497653961

    210000	  0.083016	  0.083064	  0.097232		CURRENT LEARNING RATE: 0.06116701651626938
previous_iter_valid_loss : 0.0811009332537651

    210100	  0.081140	  0.081101	  0.097069		CURRENT LEARNING RATE: 0.06110588007306942
previous_iter_valid_loss : 0.08029349893331528

    210200	  0.080283	  0.080293	  0.096848		CURRENT LEARNING RATE: 0.0610448047357546
previous_iter_valid_loss : 0.07950333505868912

    210300	  0.079457	  0.079503	  0.096812		CURRENT LEARNING RATE: 0.06098379044324963
previous_iter_valid_loss : 0.10337997227907181

    210400	  0.103165	  0.103380	  0.097036		CURRENT LEARNING RATE: 0.06092283713454018
previous_iter_valid_loss : 0.09803786873817444

    210500	  0.097932	  0.098038	  0.096818		CURRENT LEARNING RATE: 0.060861944748672944
previous_iter_valid_loss : 0.11623641103506088

    210600	  0.116475	  0.116236	  0.096788		CURRENT LEARNING RATE: 0.0608011132247555
previous_iter_valid_loss : 0.08450090140104294

    210700	  0.084513	  0.084501	  0.096510		CURRENT LEARNING RATE: 0.06074034250195638
previous_iter_valid_loss : 0.13973000645637512

    210800	  0.139508	  0.139730	  0.096836		CURRENT LEARNING RATE: 0.060679632519504825
previous_iter_valid_loss : 0.08656435459852219

    210900	  0.086574	  0.086564	  0.096900		CURRENT LEARNING RATE: 0.06061898321669084
previous_iter_valid_loss : 0.0900760367512703

    211000	  0.090035	  0.090076	  0.096685		CURRENT LEARNING RATE: 0.0605583945328651
previous_iter_valid_loss : 0.08635054528713226

    211100	  0.086358	  0.086351	  0.096720		CURRENT LEARNING RATE: 0.06049786640743896
previous_iter_valid_loss : 0.08458726853132248

    211200	  0.084552	  0.084587	  0.096260		CURRENT LEARNING RATE: 0.06043739877988428
previous_iter_valid_loss : 0.10353463888168335

    211300	  0.103841	  0.103535	  0.096513		CURRENT LEARNING RATE: 0.060376991589733406
previous_iter_valid_loss : 0.08585843443870544

    211400	  0.085799	  0.085858	  0.096406		CURRENT LEARNING RATE: 0.06031664477657913
previous_iter_valid_loss : 0.0836353525519371

    211500	  0.083635	  0.083635	  0.096319		CURRENT LEARNING RATE: 0.06025635828007469
previous_iter_valid_loss : 0.11638056486845016

    211600	  0.116539	  0.116381	  0.096284		CURRENT LEARNING RATE: 0.06019613203993354
previous_iter_valid_loss : 0.07986502349376678

    211700	  0.079749	  0.079865	  0.096237		CURRENT LEARNING RATE: 0.060135965995929457
previous_iter_valid_loss : 0.09804500639438629

    211800	  0.097814	  0.098045	  0.095778		CURRENT LEARNING RATE: 0.060075860087896345
previous_iter_valid_loss : 0.08752768486738205

    211900	  0.087613	  0.087528	  0.095812		CURRENT LEARNING RATE: 0.06001581425572836
previous_iter_valid_loss : 0.08995941281318665

    212000	  0.089667	  0.089959	  0.095770		CURRENT LEARNING RATE: 0.05995582843937963
previous_iter_valid_loss : 0.08020278066396713

    212100	  0.080206	  0.080203	  0.095742		CURRENT LEARNING RATE: 0.05989590257886434
previous_iter_valid_loss : 0.12400154769420624

    212200	  0.123808	  0.124002	  0.096079		CURRENT LEARNING RATE: 0.059836036614256585
previous_iter_valid_loss : 0.08606947213411331

    212300	  0.085856	  0.086069	  0.095992		CURRENT LEARNING RATE: 0.05977623048569047
previous_iter_valid_loss : 0.10363548249006271

    212400	  0.103459	  0.103635	  0.096222		CURRENT LEARNING RATE: 0.05971648413335981
previous_iter_valid_loss : 0.07944518327713013

    212500	  0.079315	  0.079445	  0.095971		CURRENT LEARNING RATE: 0.05965679749751826
previous_iter_valid_loss : 0.10376904904842377

    212600	  0.103552	  0.103769	  0.096080		CURRENT LEARNING RATE: 0.05959717051847919
previous_iter_valid_loss : 0.08692052960395813

    212700	  0.086741	  0.086921	  0.096168		CURRENT LEARNING RATE: 0.05953760313661557
previous_iter_valid_loss : 0.11305270344018936

    212800	  0.113221	  0.113053	  0.096444		CURRENT LEARNING RATE: 0.059478095292360075
previous_iter_valid_loss : 0.09669912606477737

    212900	  0.096543	  0.096699	  0.096381		CURRENT LEARNING RATE: 0.05941864692620483
previous_iter_valid_loss : 0.08332599699497223

    213000	  0.083307	  0.083326	  0.096328		CURRENT LEARNING RATE: 0.05935925797870146
previous_iter_valid_loss : 0.08037912845611572

    213100	  0.080285	  0.080379	  0.096348		CURRENT LEARNING RATE: 0.05929992839046099
previous_iter_valid_loss : 0.08116617053747177

    213200	  0.081209	  0.081166	  0.096366		CURRENT LEARNING RATE: 0.05924065810215388
previous_iter_valid_loss : 0.18987984955310822

    213300	  0.190316	  0.189880	  0.097362		CURRENT LEARNING RATE: 0.05918144705450981
previous_iter_valid_loss : 0.08611880987882614

    213400	  0.085935	  0.086119	  0.097407		CURRENT LEARNING RATE: 0.05912229518831772
previous_iter_valid_loss : 0.09107355773448944

    213500	  0.090852	  0.091074	  0.097293		CURRENT LEARNING RATE: 0.05906320244442573
previous_iter_valid_loss : 0.08526479452848434

    213600	  0.085242	  0.085265	  0.096727		CURRENT LEARNING RATE: 0.05900416876374112
previous_iter_valid_loss : 0.08561641722917557

    213700	  0.085642	  0.085616	  0.096583		CURRENT LEARNING RATE: 0.0589451940872302
previous_iter_valid_loss : 0.08725225180387497

    213800	  0.087107	  0.087252	  0.096363		CURRENT LEARNING RATE: 0.058886278355918274
previous_iter_valid_loss : 0.08522020280361176

    213900	  0.085230	  0.085220	  0.096018		CURRENT LEARNING RATE: 0.05882742151088959
previous_iter_valid_loss : 0.0907965824007988

    214000	  0.090791	  0.090797	  0.095802		CURRENT LEARNING RATE: 0.05876862349328734
previous_iter_valid_loss : 0.08423398435115814

    214100	  0.084366	  0.084234	  0.095852		CURRENT LEARNING RATE: 0.05870988424431349
previous_iter_valid_loss : 0.11418498307466507

    214200	  0.114448	  0.114185	  0.096179		CURRENT LEARNING RATE: 0.05865120370522877
previous_iter_valid_loss : 0.08429428935050964

    214300	  0.084210	  0.084294	  0.096092		CURRENT LEARNING RATE: 0.058592581817352614
previous_iter_valid_loss : 0.180990532040596

    214400	  0.181407	  0.180991	  0.097084		CURRENT LEARNING RATE: 0.058534018522063185
previous_iter_valid_loss : 0.15981340408325195

    214500	  0.160182	  0.159813	  0.097828		CURRENT LEARNING RATE: 0.05847551376079716
previous_iter_valid_loss : 0.16363084316253662

    214600	  0.163375	  0.163631	  0.098469		CURRENT LEARNING RATE: 0.058417067475049766
previous_iter_valid_loss : 0.11382290720939636

    214700	  0.114164	  0.113823	  0.098789		CURRENT LEARNING RATE: 0.05835867960637469
previous_iter_valid_loss : 0.1401841938495636

    214800	  0.140582	  0.140184	  0.099372		CURRENT LEARNING RATE: 0.05830035009638411
previous_iter_valid_loss : 0.08985260874032974

    214900	  0.089973	  0.089853	  0.099447		CURRENT LEARNING RATE: 0.05824207888674848
previous_iter_valid_loss : 0.15003077685832977

    215000	  0.150335	  0.150031	  0.099966		CURRENT LEARNING RATE: 0.058183865919196595
previous_iter_valid_loss : 0.09364257007837296

    215100	  0.093448	  0.093643	  0.100074		CURRENT LEARNING RATE: 0.05812571113551546
previous_iter_valid_loss : 0.08322729915380478

    215200	  0.083175	  0.083227	  0.099930		CURRENT LEARNING RATE: 0.058067614477550315
previous_iter_valid_loss : 0.08944118767976761

    215300	  0.089311	  0.089441	  0.099901		CURRENT LEARNING RATE: 0.0580095758872045
previous_iter_valid_loss : 0.08423148095607758

    215400	  0.084236	  0.084231	  0.099871		CURRENT LEARNING RATE: 0.057951595306439396
previous_iter_valid_loss : 0.08238375186920166

    215500	  0.082389	  0.082384	  0.099522		CURRENT LEARNING RATE: 0.0578936726772744
previous_iter_valid_loss : 0.10040310025215149

    215600	  0.100217	  0.100403	  0.099484		CURRENT LEARNING RATE: 0.05783580794178694
previous_iter_valid_loss : 0.16354040801525116

    215700	  0.163896	  0.163540	  0.100280		CURRENT LEARNING RATE: 0.05777800104211224
previous_iter_valid_loss : 0.09397858381271362

    215800	  0.093771	  0.093979	  0.099910		CURRENT LEARNING RATE: 0.057720251920443395
previous_iter_valid_loss : 0.0813785120844841

    215900	  0.081436	  0.081379	  0.099592		CURRENT LEARNING RATE: 0.05766256051903126
previous_iter_valid_loss : 0.09456101804971695

    216000	  0.094544	  0.094561	  0.099657		CURRENT LEARNING RATE: 0.057604926780184466
previous_iter_valid_loss : 0.14231808483600616

    216100	  0.142608	  0.142318	  0.100223		CURRENT LEARNING RATE: 0.05754735064626926
previous_iter_valid_loss : 0.08646152168512344

    216200	  0.086312	  0.086462	  0.100178		CURRENT LEARNING RATE: 0.057489832059709485
previous_iter_valid_loss : 0.09288837015628815

    216300	  0.092836	  0.092888	  0.100256		CURRENT LEARNING RATE: 0.05743237096298654
previous_iter_valid_loss : 0.15551912784576416

    216400	  0.155137	  0.155519	  0.100837		CURRENT LEARNING RATE: 0.057374967298639376
previous_iter_valid_loss : 0.08931809663772583

    216500	  0.089026	  0.089318	  0.100298		CURRENT LEARNING RATE: 0.05731762100926429
previous_iter_valid_loss : 0.08307255059480667

    216600	  0.082923	  0.083073	  0.100035		CURRENT LEARNING RATE: 0.05726033203751499
previous_iter_valid_loss : 0.08550401777029037

    216700	  0.085310	  0.085504	  0.099842		CURRENT LEARNING RATE: 0.057203100326102464
previous_iter_valid_loss : 0.14937955141067505

    216800	  0.149743	  0.149380	  0.100401		CURRENT LEARNING RATE: 0.05714592581779507
previous_iter_valid_loss : 0.08788733184337616

    216900	  0.087808	  0.087887	  0.100461		CURRENT LEARNING RATE: 0.05708880845541825
previous_iter_valid_loss : 0.08380122482776642

    217000	  0.083504	  0.083801	  0.100124		CURRENT LEARNING RATE: 0.05703174818185464
previous_iter_valid_loss : 0.13055196404457092

    217100	  0.130699	  0.130552	  0.100463		CURRENT LEARNING RATE: 0.05697474494004394
previous_iter_valid_loss : 0.08053640276193619

    217200	  0.080205	  0.080536	  0.100391		CURRENT LEARNING RATE: 0.05691779867298296
previous_iter_valid_loss : 0.10095742344856262

    217300	  0.100646	  0.100957	  0.100440		CURRENT LEARNING RATE: 0.05686090932372539
previous_iter_valid_loss : 0.0879627987742424

    217400	  0.087648	  0.087963	  0.100269		CURRENT LEARNING RATE: 0.056804076835381884
previous_iter_valid_loss : 0.09843142330646515

    217500	  0.098000	  0.098431	  0.100148		CURRENT LEARNING RATE: 0.056747301151119915
previous_iter_valid_loss : 0.14403975009918213

    217600	  0.143605	  0.144040	  0.100744		CURRENT LEARNING RATE: 0.05669058221416386
previous_iter_valid_loss : 0.10658752918243408

    217700	  0.106538	  0.106588	  0.100861		CURRENT LEARNING RATE: 0.05663391996779474
previous_iter_valid_loss : 0.09389886260032654

    217800	  0.093492	  0.093899	  0.100572		CURRENT LEARNING RATE: 0.05657731435535031
previous_iter_valid_loss : 0.09194367378950119

    217900	  0.091602	  0.091944	  0.100522		CURRENT LEARNING RATE: 0.056520765320224924
previous_iter_valid_loss : 0.1669224351644516

    218000	  0.167183	  0.166922	  0.100803		CURRENT LEARNING RATE: 0.05646427280586959
previous_iter_valid_loss : 0.08315278589725494

    218100	  0.082791	  0.083153	  0.100673		CURRENT LEARNING RATE: 0.05640783675579177
previous_iter_valid_loss : 0.08950955420732498

    218200	  0.089331	  0.089510	  0.100682		CURRENT LEARNING RATE: 0.05635145711355541
previous_iter_valid_loss : 0.08164193481206894

    218300	  0.081379	  0.081642	  0.100549		CURRENT LEARNING RATE: 0.05629513382278083
previous_iter_valid_loss : 0.10994934290647507

    218400	  0.109513	  0.109949	  0.100725		CURRENT LEARNING RATE: 0.05623886682714479
previous_iter_valid_loss : 0.0849667638540268

    218500	  0.084930	  0.084967	  0.100732		CURRENT LEARNING RATE: 0.05618265607038026
previous_iter_valid_loss : 0.08386256545782089

    218600	  0.083672	  0.083863	  0.100725		CURRENT LEARNING RATE: 0.05612650149627649
previous_iter_valid_loss : 0.0815984234213829

    218700	  0.081380	  0.081598	  0.100569		CURRENT LEARNING RATE: 0.05607040304867886
previous_iter_valid_loss : 0.08546511083841324

    218800	  0.085224	  0.085465	  0.100483		CURRENT LEARNING RATE: 0.05601436067148898
previous_iter_valid_loss : 0.10145124793052673

    218900	  0.100995	  0.101451	  0.100541		CURRENT LEARNING RATE: 0.05595837430866444
previous_iter_valid_loss : 0.09471563994884491

    219000	  0.094596	  0.094716	  0.100614		CURRENT LEARNING RATE: 0.05590244390421887
previous_iter_valid_loss : 0.1126527488231659

    219100	  0.112576	  0.112653	  0.100922		CURRENT LEARNING RATE: 0.05584656940222184
previous_iter_valid_loss : 0.12041374295949936

    219200	  0.120760	  0.120414	  0.101255		CURRENT LEARNING RATE: 0.055790750746798894
previous_iter_valid_loss : 0.14931659400463104

    219300	  0.148867	  0.149317	  0.101947		CURRENT LEARNING RATE: 0.05573498788213134
previous_iter_valid_loss : 0.08495157957077026

    219400	  0.084744	  0.084952	  0.101924		CURRENT LEARNING RATE: 0.05567928075245631
previous_iter_valid_loss : 0.08367802947759628

    219500	  0.083406	  0.083678	  0.101449		CURRENT LEARNING RATE: 0.05562362930206665
previous_iter_valid_loss : 0.1179267093539238

    219600	  0.117599	  0.117927	  0.101820		CURRENT LEARNING RATE: 0.05556803347531095
previous_iter_valid_loss : 0.1256415843963623

    219700	  0.125928	  0.125642	  0.101834		CURRENT LEARNING RATE: 0.055512493216593364
previous_iter_valid_loss : 0.14434784650802612

    219800	  0.144045	  0.144348	  0.102402		CURRENT LEARNING RATE: 0.05545700847037361
previous_iter_valid_loss : 0.11780069023370743

    219900	  0.117435	  0.117801	  0.102370		CURRENT LEARNING RATE: 0.055401579181166935
previous_iter_valid_loss : 0.08478520065546036

    220000	  0.084566	  0.084785	  0.102387		CURRENT LEARNING RATE: 0.055346205293544073
previous_iter_valid_loss : 0.11631752550601959

    220100	  0.115878	  0.116318	  0.102739		CURRENT LEARNING RATE: 0.05529088675213112
previous_iter_valid_loss : 0.08294610679149628

    220200	  0.082720	  0.082946	  0.102765		CURRENT LEARNING RATE: 0.05523562350160953
previous_iter_valid_loss : 0.0873003751039505

    220300	  0.086986	  0.087300	  0.102843		CURRENT LEARNING RATE: 0.05518041548671601
previous_iter_valid_loss : 0.1356610208749771

    220400	  0.135692	  0.135661	  0.103166		CURRENT LEARNING RATE: 0.05512526265224261
previous_iter_valid_loss : 0.08112724125385284

    220500	  0.080919	  0.081127	  0.102997		CURRENT LEARNING RATE: 0.05507016494303645
previous_iter_valid_loss : 0.09582901746034622

    220600	  0.095771	  0.095829	  0.102793		CURRENT LEARNING RATE: 0.055015122303999825
previous_iter_valid_loss : 0.08623834699392319

    220700	  0.085904	  0.086238	  0.102810		CURRENT LEARNING RATE: 0.05496013468009006
previous_iter_valid_loss : 0.09603595733642578

    220800	  0.095594	  0.096036	  0.102374		CURRENT LEARNING RATE: 0.054905202016319585
previous_iter_valid_loss : 0.1686212569475174

    220900	  0.168157	  0.168621	  0.103194		CURRENT LEARNING RATE: 0.054850324257755705
previous_iter_valid_loss : 0.08141104131937027

    221000	  0.081124	  0.081411	  0.103107		CURRENT LEARNING RATE: 0.054795501349520645
previous_iter_valid_loss : 0.08665449172258377

    221100	  0.086365	  0.086654	  0.103110		CURRENT LEARNING RATE: 0.05474073323679148
previous_iter_valid_loss : 0.11942163854837418

    221200	  0.119635	  0.119422	  0.103459		CURRENT LEARNING RATE: 0.05468601986480014
previous_iter_valid_loss : 0.0898984968662262

    221300	  0.089916	  0.089898	  0.103322		CURRENT LEARNING RATE: 0.054631361178833215
previous_iter_valid_loss : 0.10087371617555618

    221400	  0.100946	  0.100874	  0.103473		CURRENT LEARNING RATE: 0.05457675712423203
previous_iter_valid_loss : 0.09133940935134888

    221500	  0.091127	  0.091339	  0.103550		CURRENT LEARNING RATE: 0.05452220764639249
previous_iter_valid_loss : 0.0840078741312027

    221600	  0.084023	  0.084008	  0.103226		CURRENT LEARNING RATE: 0.05446771269076516
previous_iter_valid_loss : 0.08732727915048599

    221700	  0.087072	  0.087327	  0.103301		CURRENT LEARNING RATE: 0.054413272202855065
previous_iter_valid_loss : 0.08417780697345734

    221800	  0.083976	  0.084178	  0.103162		CURRENT LEARNING RATE: 0.054358886128221706
previous_iter_valid_loss : 0.08111001551151276

    221900	  0.080919	  0.081110	  0.103098		CURRENT LEARNING RATE: 0.05430455441247898
previous_iter_valid_loss : 0.11664870381355286

    222000	  0.117000	  0.116649	  0.103365		CURRENT LEARNING RATE: 0.05425027700129521
previous_iter_valid_loss : 0.08655895292758942

    222100	  0.086349	  0.086559	  0.103428		CURRENT LEARNING RATE: 0.05419605384039298
previous_iter_valid_loss : 0.08373284339904785

    222200	  0.083868	  0.083733	  0.103025		CURRENT LEARNING RATE: 0.05414188487554909
previous_iter_valid_loss : 0.0856231078505516

    222300	  0.085363	  0.085623	  0.103021		CURRENT LEARNING RATE: 0.05408777005259457
previous_iter_valid_loss : 0.08254563808441162

    222400	  0.082389	  0.082546	  0.102810		CURRENT LEARNING RATE: 0.05403370931741463
previous_iter_valid_loss : 0.09183239191770554

    222500	  0.091875	  0.091832	  0.102934		CURRENT LEARNING RATE: 0.05397970261594851
previous_iter_valid_loss : 0.10119537264108658

    222600	  0.101327	  0.101195	  0.102908		CURRENT LEARNING RATE: 0.05392574989418951
previous_iter_valid_loss : 0.08670786768198013

    222700	  0.086775	  0.086708	  0.102906		CURRENT LEARNING RATE: 0.053871851098184875
previous_iter_valid_loss : 0.1896323263645172

    222800	  0.189219	  0.189632	  0.103672		CURRENT LEARNING RATE: 0.05381800617403584
previous_iter_valid_loss : 0.10090138018131256

    222900	  0.100786	  0.100901	  0.103714		CURRENT LEARNING RATE: 0.053764215067897476
previous_iter_valid_loss : 0.09831198304891586

    223000	  0.097966	  0.098312	  0.103864		CURRENT LEARNING RATE: 0.05371047772597866
previous_iter_valid_loss : 0.13219809532165527

    223100	  0.132303	  0.132198	  0.104382		CURRENT LEARNING RATE: 0.053656794094542014
previous_iter_valid_loss : 0.08808781951665878

    223200	  0.087884	  0.088088	  0.104451		CURRENT LEARNING RATE: 0.053603164119903964
previous_iter_valid_loss : 0.08381304889917374

    223300	  0.083585	  0.083813	  0.103391		CURRENT LEARNING RATE: 0.0535495877484345
previous_iter_valid_loss : 0.08277281373739243

    223400	  0.082667	  0.082773	  0.103357		CURRENT LEARNING RATE: 0.053496064926557244
previous_iter_valid_loss : 0.07975629717111588

    223500	  0.079712	  0.079756	  0.103244		CURRENT LEARNING RATE: 0.05344259560074935
previous_iter_valid_loss : 0.09063726663589478

    223600	  0.090683	  0.090637	  0.103298		CURRENT LEARNING RATE: 0.05338917971754153
previous_iter_valid_loss : 0.09763966500759125

    223700	  0.097442	  0.097640	  0.103418		CURRENT LEARNING RATE: 0.05333581722351788
previous_iter_valid_loss : 0.08054279536008835

    223800	  0.080406	  0.080543	  0.103351		CURRENT LEARNING RATE: 0.0532825080653159
previous_iter_valid_loss : 0.12834285199642181

    223900	  0.128485	  0.128343	  0.103782		CURRENT LEARNING RATE: 0.0532292521896264
previous_iter_valid_loss : 0.08653435111045837

    224000	  0.086506	  0.086534	  0.103739		CURRENT LEARNING RATE: 0.05317604954319355
previous_iter_valid_loss : 0.0848032534122467

    224100	  0.084693	  0.084803	  0.103745		CURRENT LEARNING RATE: 0.05312290007281467
previous_iter_valid_loss : 0.10454868525266647

    224200	  0.104413	  0.104549	  0.103649		CURRENT LEARNING RATE: 0.0530698037253403
previous_iter_valid_loss : 0.10320645570755005

    224300	  0.103048	  0.103206	  0.103838		CURRENT LEARNING RATE: 0.05301676044767405
previous_iter_valid_loss : 0.08304941654205322

    224400	  0.082878	  0.083049	  0.102858		CURRENT LEARNING RATE: 0.05296377018677268
previous_iter_valid_loss : 0.11541717499494553

    224500	  0.115130	  0.115417	  0.102414		CURRENT LEARNING RATE: 0.052910832889645924
previous_iter_valid_loss : 0.09298619627952576

    224600	  0.092886	  0.092986	  0.101708		CURRENT LEARNING RATE: 0.052857948503356456
previous_iter_valid_loss : 0.09253542125225067

    224700	  0.092527	  0.092535	  0.101495		CURRENT LEARNING RATE: 0.052805116975019877
previous_iter_valid_loss : 0.08256997913122177

    224800	  0.082529	  0.082570	  0.100919		CURRENT LEARNING RATE: 0.0527523382518047
previous_iter_valid_loss : 0.09137507528066635

    224900	  0.091291	  0.091375	  0.100934		CURRENT LEARNING RATE: 0.052699612280932166
previous_iter_valid_loss : 0.09483352303504944

    225000	  0.094617	  0.094834	  0.100382		CURRENT LEARNING RATE: 0.05264693900967631
previous_iter_valid_loss : 0.08716963976621628

    225100	  0.087174	  0.087170	  0.100318		CURRENT LEARNING RATE: 0.052594318385363846
previous_iter_valid_loss : 0.08305664360523224

    225200	  0.083040	  0.083057	  0.100316		CURRENT LEARNING RATE: 0.05254175035537413
previous_iter_valid_loss : 0.10176832973957062

    225300	  0.101546	  0.101768	  0.100439		CURRENT LEARNING RATE: 0.05248923486713917
previous_iter_valid_loss : 0.095889613032341

    225400	  0.095716	  0.095890	  0.100556		CURRENT LEARNING RATE: 0.05243677186814345
previous_iter_valid_loss : 0.0834256038069725

    225500	  0.083473	  0.083426	  0.100566		CURRENT LEARNING RATE: 0.05238436130592397
previous_iter_valid_loss : 0.09136584401130676

    225600	  0.091101	  0.091366	  0.100476		CURRENT LEARNING RATE: 0.05233200312807013
previous_iter_valid_loss : 0.07780071347951889

    225700	  0.077776	  0.077801	  0.099618		CURRENT LEARNING RATE: 0.052279697282223814
previous_iter_valid_loss : 0.0816052258014679

    225800	  0.081497	  0.081605	  0.099495		CURRENT LEARNING RATE: 0.05222744371607913
previous_iter_valid_loss : 0.08018562197685242

    225900	  0.080068	  0.080186	  0.099483		CURRENT LEARNING RATE: 0.05217524237738252
previous_iter_valid_loss : 0.0861293151974678

    226000	  0.086039	  0.086129	  0.099398		CURRENT LEARNING RATE: 0.05212309321393261
previous_iter_valid_loss : 0.08773624897003174

    226100	  0.087684	  0.087736	  0.098852		CURRENT LEARNING RATE: 0.052070996173580276
previous_iter_valid_loss : 0.08635534346103668

    226200	  0.086223	  0.086355	  0.098851		CURRENT LEARNING RATE: 0.05201895120422846
previous_iter_valid_loss : 0.12356414645910263

    226300	  0.123928	  0.123564	  0.099158		CURRENT LEARNING RATE: 0.05196695825383218
previous_iter_valid_loss : 0.1223331168293953

    226400	  0.122148	  0.122333	  0.098826		CURRENT LEARNING RATE: 0.05191501727039846
previous_iter_valid_loss : 0.08354486525058746

    226500	  0.083659	  0.083545	  0.098769		CURRENT LEARNING RATE: 0.051863128201986367
previous_iter_valid_loss : 0.08868755400180817

    226600	  0.088577	  0.088688	  0.098825		CURRENT LEARNING RATE: 0.05181129099670679
previous_iter_valid_loss : 0.1824917197227478

    226700	  0.182296	  0.182492	  0.099795		CURRENT LEARNING RATE: 0.05175950560272253
previous_iter_valid_loss : 0.07813891768455505

    226800	  0.078128	  0.078139	  0.099082		CURRENT LEARNING RATE: 0.05170777196824817
previous_iter_valid_loss : 0.09726016223430634

    226900	  0.097437	  0.097260	  0.099176		CURRENT LEARNING RATE: 0.0516560900415501
previous_iter_valid_loss : 0.08858795464038849

    227000	  0.088673	  0.088588	  0.099224		CURRENT LEARNING RATE: 0.05160445977094638
previous_iter_valid_loss : 0.12809863686561584

    227100	  0.128339	  0.128099	  0.099199		CURRENT LEARNING RATE: 0.05155288110480673
previous_iter_valid_loss : 0.07947033643722534

    227200	  0.079474	  0.079470	  0.099189		CURRENT LEARNING RATE: 0.05150135399155246
previous_iter_valid_loss : 0.10156598687171936

    227300	  0.101365	  0.101566	  0.099195		CURRENT LEARNING RATE: 0.0514498783796565
previous_iter_valid_loss : 0.08287164568901062

    227400	  0.082912	  0.082872	  0.099144		CURRENT LEARNING RATE: 0.0513984542176432
previous_iter_valid_loss : 0.10516444593667984

    227500	  0.105315	  0.105164	  0.099211		CURRENT LEARNING RATE: 0.0513470814540884
previous_iter_valid_loss : 0.09661879390478134

    227600	  0.096498	  0.096619	  0.098737		CURRENT LEARNING RATE: 0.05129576003761931
previous_iter_valid_loss : 0.08449740707874298

    227700	  0.084393	  0.084497	  0.098516		CURRENT LEARNING RATE: 0.05124448991691456
previous_iter_valid_loss : 0.10981918126344681

    227800	  0.109670	  0.109819	  0.098675		CURRENT LEARNING RATE: 0.051193271040704
previous_iter_valid_loss : 0.08141832798719406

    227900	  0.081339	  0.081418	  0.098570		CURRENT LEARNING RATE: 0.05114210335776874
previous_iter_valid_loss : 0.11427780985832214

    228000	  0.114093	  0.114278	  0.098044		CURRENT LEARNING RATE: 0.05109098681694108
previous_iter_valid_loss : 0.09983552992343903

    228100	  0.099660	  0.099836	  0.098210		CURRENT LEARNING RATE: 0.051039921367104515
previous_iter_valid_loss : 0.09284316748380661

    228200	  0.093073	  0.092843	  0.098244		CURRENT LEARNING RATE: 0.050988906957193575
previous_iter_valid_loss : 0.08889234066009521

    228300	  0.088964	  0.088892	  0.098316		CURRENT LEARNING RATE: 0.05093794353619384
previous_iter_valid_loss : 0.08370832353830338

    228400	  0.083800	  0.083708	  0.098054		CURRENT LEARNING RATE: 0.05088703105314186
previous_iter_valid_loss : 0.09275027364492416

    228500	  0.092944	  0.092750	  0.098132		CURRENT LEARNING RATE: 0.0508361694571252
previous_iter_valid_loss : 0.10222646594047546

    228600	  0.102033	  0.102226	  0.098315		CURRENT LEARNING RATE: 0.050785358697282235
previous_iter_valid_loss : 0.0955333486199379

    228700	  0.095650	  0.095533	  0.098455		CURRENT LEARNING RATE: 0.05073459872280219
previous_iter_valid_loss : 0.09047762304544449

    228800	  0.090313	  0.090478	  0.098505		CURRENT LEARNING RATE: 0.050683889482925085
previous_iter_valid_loss : 0.0865243598818779

    228900	  0.086620	  0.086524	  0.098355		CURRENT LEARNING RATE: 0.0506332309269417
previous_iter_valid_loss : 0.08086714893579483

    229000	  0.080868	  0.080867	  0.098217		CURRENT LEARNING RATE: 0.050582623004193465
previous_iter_valid_loss : 0.09951417148113251

    229100	  0.099723	  0.099514	  0.098086		CURRENT LEARNING RATE: 0.05053206566407245
previous_iter_valid_loss : 0.08038794994354248

    229200	  0.080516	  0.080388	  0.097685		CURRENT LEARNING RATE: 0.050481558856021284
previous_iter_valid_loss : 0.08009125292301178

    229300	  0.080004	  0.080091	  0.096993		CURRENT LEARNING RATE: 0.05043110252953321
previous_iter_valid_loss : 0.0783834233880043

    229400	  0.078264	  0.078383	  0.096927		CURRENT LEARNING RATE: 0.050380696634151866
previous_iter_valid_loss : 0.08413752913475037

    229500	  0.084002	  0.084138	  0.096932		CURRENT LEARNING RATE: 0.05033034111947135
previous_iter_valid_loss : 0.08562570065259933

    229600	  0.085622	  0.085626	  0.096609		CURRENT LEARNING RATE: 0.05028003593513613
previous_iter_valid_loss : 0.10438670217990875

    229700	  0.104167	  0.104387	  0.096396		CURRENT LEARNING RATE: 0.05022978103084105
previous_iter_valid_loss : 0.07832187414169312

    229800	  0.078367	  0.078322	  0.095736		CURRENT LEARNING RATE: 0.0501795763563312
previous_iter_valid_loss : 0.07905671745538712

    229900	  0.079000	  0.079057	  0.095349		CURRENT LEARNING RATE: 0.050129421861401874
previous_iter_valid_loss : 0.08496445417404175

    230000	  0.084723	  0.084964	  0.095351		CURRENT LEARNING RATE: 0.05007931749589857
previous_iter_valid_loss : 0.10072437673807144

    230100	  0.100521	  0.100724	  0.095195		CURRENT LEARNING RATE: 0.05002926320971696
previous_iter_valid_loss : 0.13735069334506989

    230200	  0.137645	  0.137351	  0.095739		CURRENT LEARNING RATE: 0.04997925895280273
previous_iter_valid_loss : 0.09023584425449371

    230300	  0.090008	  0.090236	  0.095768		CURRENT LEARNING RATE: 0.049929304675151616
previous_iter_valid_loss : 0.08054105937480927

    230400	  0.080491	  0.080541	  0.095217		CURRENT LEARNING RATE: 0.04987940032680931
previous_iter_valid_loss : 0.07851111143827438

    230500	  0.078537	  0.078511	  0.095191		CURRENT LEARNING RATE: 0.04982954585787151
previous_iter_valid_loss : 0.07957927882671356

    230600	  0.079537	  0.079579	  0.095028		CURRENT LEARNING RATE: 0.049779741218483727
previous_iter_valid_loss : 0.08231160789728165

    230700	  0.082324	  0.082312	  0.094989		CURRENT LEARNING RATE: 0.04972998635884131
previous_iter_valid_loss : 0.09072715044021606

    230800	  0.090798	  0.090727	  0.094936		CURRENT LEARNING RATE: 0.049680281229189376
previous_iter_valid_loss : 0.08978304266929626

    230900	  0.089647	  0.089783	  0.094147		CURRENT LEARNING RATE: 0.04963062577982283
previous_iter_valid_loss : 0.08566443622112274

    231000	  0.085767	  0.085664	  0.094190		CURRENT LEARNING RATE: 0.049581019961086194
previous_iter_valid_loss : 0.07938871532678604

    231100	  0.079282	  0.079389	  0.094117		CURRENT LEARNING RATE: 0.04953146372337366
previous_iter_valid_loss : 0.10860200226306915

    231200	  0.108910	  0.108602	  0.094009		CURRENT LEARNING RATE: 0.04948195701712895
previous_iter_valid_loss : 0.07859309017658234

    231300	  0.078479	  0.078593	  0.093896		CURRENT LEARNING RATE: 0.049432499792845405
previous_iter_valid_loss : 0.08060858398675919

    231400	  0.080659	  0.080609	  0.093693		CURRENT LEARNING RATE: 0.04938309200106577
previous_iter_valid_loss : 0.08297421038150787

    231500	  0.082836	  0.082974	  0.093610		CURRENT LEARNING RATE: 0.04933373359238225
previous_iter_valid_loss : 0.07760099321603775

    231600	  0.077609	  0.077601	  0.093546		CURRENT LEARNING RATE: 0.04928442451743641
previous_iter_valid_loss : 0.09032467007637024

    231700	  0.090105	  0.090325	  0.093576		CURRENT LEARNING RATE: 0.049235164726919224
previous_iter_valid_loss : 0.07836827635765076

    231800	  0.078394	  0.078368	  0.093518		CURRENT LEARNING RATE: 0.049185954171570866
previous_iter_valid_loss : 0.08778370171785355

    231900	  0.087689	  0.087784	  0.093584		CURRENT LEARNING RATE: 0.04913679280218077
previous_iter_valid_loss : 0.09535130113363266

    232000	  0.095142	  0.095351	  0.093371		CURRENT LEARNING RATE: 0.04908768056958756
previous_iter_valid_loss : 0.07959620654582977

    232100	  0.079403	  0.079596	  0.093302		CURRENT LEARNING RATE: 0.04903861742467903
previous_iter_valid_loss : 0.08475323021411896

    232200	  0.084822	  0.084753	  0.093312		CURRENT LEARNING RATE: 0.04898960331839201
previous_iter_valid_loss : 0.08401483297348022

    232300	  0.083874	  0.084015	  0.093296		CURRENT LEARNING RATE: 0.048940638201712384
previous_iter_valid_loss : 0.09620371460914612

    232400	  0.096004	  0.096204	  0.093432		CURRENT LEARNING RATE: 0.04889172202567502
previous_iter_valid_loss : 0.08712886273860931

    232500	  0.087195	  0.087129	  0.093385		CURRENT LEARNING RATE: 0.04884285474136378
previous_iter_valid_loss : 0.10297355055809021

    232600	  0.103091	  0.102974	  0.093403		CURRENT LEARNING RATE: 0.04879403629991135
previous_iter_valid_loss : 0.08192699402570724

    232700	  0.081893	  0.081927	  0.093355		CURRENT LEARNING RATE: 0.04874526665249929
previous_iter_valid_loss : 0.09172862023115158

    232800	  0.091519	  0.091729	  0.092376		CURRENT LEARNING RATE: 0.04869654575035792
previous_iter_valid_loss : 0.07909848541021347

    232900	  0.079054	  0.079098	  0.092158		CURRENT LEARNING RATE: 0.04864787354476638
previous_iter_valid_loss : 0.0893169716000557

    233000	  0.089074	  0.089317	  0.092068		CURRENT LEARNING RATE: 0.04859924998705244
previous_iter_valid_loss : 0.09522305428981781

    233100	  0.095116	  0.095223	  0.091699		CURRENT LEARNING RATE: 0.04855067502859254
previous_iter_valid_loss : 0.08422597497701645

    233200	  0.084330	  0.084226	  0.091660		CURRENT LEARNING RATE: 0.04850214862081168
previous_iter_valid_loss : 0.08124296367168427

    233300	  0.081317	  0.081243	  0.091634		CURRENT LEARNING RATE: 0.04845367071518352
previous_iter_valid_loss : 0.1055670902132988

    233400	  0.105733	  0.105567	  0.091862		CURRENT LEARNING RATE: 0.048405241263230106
previous_iter_valid_loss : 0.09994392842054367

    233500	  0.100078	  0.099944	  0.092064		CURRENT LEARNING RATE: 0.04835686021652199
previous_iter_valid_loss : 0.09113974124193192

    233600	  0.091232	  0.091140	  0.092069		CURRENT LEARNING RATE: 0.048308527526678094
previous_iter_valid_loss : 0.09726787358522415

    233700	  0.097360	  0.097268	  0.092065		CURRENT LEARNING RATE: 0.048260243145365776
previous_iter_valid_loss : 0.07839662581682205

    233800	  0.078415	  0.078397	  0.092044		CURRENT LEARNING RATE: 0.048212007024300625
previous_iter_valid_loss : 0.08115018904209137

    233900	  0.081206	  0.081150	  0.091572		CURRENT LEARNING RATE: 0.04816381911524652
previous_iter_valid_loss : 0.07850329577922821

    234000	  0.078440	  0.078503	  0.091492		CURRENT LEARNING RATE: 0.04811567937001551
previous_iter_valid_loss : 0.10622860491275787

    234100	  0.106004	  0.106229	  0.091706		CURRENT LEARNING RATE: 0.04806758774046791
previous_iter_valid_loss : 0.08178776502609253

    234200	  0.081669	  0.081788	  0.091478		CURRENT LEARNING RATE: 0.04801954417851206
previous_iter_valid_loss : 0.09342081099748611

    234300	  0.093454	  0.093421	  0.091380		CURRENT LEARNING RATE: 0.04797154863610439
previous_iter_valid_loss : 0.09779153764247894

    234400	  0.097980	  0.097792	  0.091528		CURRENT LEARNING RATE: 0.04792360106524932
previous_iter_valid_loss : 0.09332043677568436

    234500	  0.093392	  0.093320	  0.091307		CURRENT LEARNING RATE: 0.04787570141799934
previous_iter_valid_loss : 0.0789351537823677

    234600	  0.078843	  0.078935	  0.091166		CURRENT LEARNING RATE: 0.04782784964645477
previous_iter_valid_loss : 0.08492989093065262

    234700	  0.084817	  0.084930	  0.091090		CURRENT LEARNING RATE: 0.047780045702763826
previous_iter_valid_loss : 0.08412867039442062

    234800	  0.084170	  0.084129	  0.091106		CURRENT LEARNING RATE: 0.04773228953912255
previous_iter_valid_loss : 0.07982512563467026

    234900	  0.079808	  0.079825	  0.090990		CURRENT LEARNING RATE: 0.04768458110777481
previous_iter_valid_loss : 0.10680238157510757

    235000	  0.107002	  0.106802	  0.091110		CURRENT LEARNING RATE: 0.04763692036101214
previous_iter_valid_loss : 0.11160019785165787

    235100	  0.111357	  0.111600	  0.091354		CURRENT LEARNING RATE: 0.047589307251173815
previous_iter_valid_loss : 0.08084925264120102

    235200	  0.080877	  0.080849	  0.091332		CURRENT LEARNING RATE: 0.04754174173064669
previous_iter_valid_loss : 0.08437871932983398

    235300	  0.084472	  0.084379	  0.091158		CURRENT LEARNING RATE: 0.04749422375186527
previous_iter_valid_loss : 0.08001028001308441

    235400	  0.080069	  0.080010	  0.091000		CURRENT LEARNING RATE: 0.047446753267311556
previous_iter_valid_loss : 0.10297643393278122

    235500	  0.103202	  0.102976	  0.091195		CURRENT LEARNING RATE: 0.04739933022951507
previous_iter_valid_loss : 0.08972977846860886

    235600	  0.089530	  0.089730	  0.091179		CURRENT LEARNING RATE: 0.047351954591052736
previous_iter_valid_loss : 0.07932291179895401

    235700	  0.079299	  0.079323	  0.091194		CURRENT LEARNING RATE: 0.047304626304548965
previous_iter_valid_loss : 0.08853725343942642

    235800	  0.088366	  0.088537	  0.091263		CURRENT LEARNING RATE: 0.04725734532267544
previous_iter_valid_loss : 0.07875008881092072

    235900	  0.078723	  0.078750	  0.091249		CURRENT LEARNING RATE: 0.047210111598151173
previous_iter_valid_loss : 0.08296605199575424

    236000	  0.082790	  0.082966	  0.091217		CURRENT LEARNING RATE: 0.047162925083742424
previous_iter_valid_loss : 0.08874139189720154

    236100	  0.088802	  0.088741	  0.091227		CURRENT LEARNING RATE: 0.0471157857322627
previous_iter_valid_loss : 0.10495084524154663

    236200	  0.104721	  0.104951	  0.091413		CURRENT LEARNING RATE: 0.047068693496572646
previous_iter_valid_loss : 0.09571629762649536

    236300	  0.095532	  0.095716	  0.091135		CURRENT LEARNING RATE: 0.04702164832958
previous_iter_valid_loss : 0.08605364710092545

    236400	  0.086069	  0.086054	  0.090772		CURRENT LEARNING RATE: 0.04697465018423959
previous_iter_valid_loss : 0.08091871440410614

    236500	  0.080900	  0.080919	  0.090746		CURRENT LEARNING RATE: 0.046927699013553294
previous_iter_valid_loss : 0.1212737187743187

    236600	  0.121066	  0.121274	  0.091072		CURRENT LEARNING RATE: 0.04688079477056993
previous_iter_valid_loss : 0.07787018269300461

    236700	  0.077776	  0.077870	  0.090025		CURRENT LEARNING RATE: 0.046833937408385234
previous_iter_valid_loss : 0.09896126389503479

    236800	  0.099047	  0.098961	  0.090234		CURRENT LEARNING RATE: 0.04678712688014183
previous_iter_valid_loss : 0.08110823482275009

    236900	  0.080938	  0.081108	  0.090072		CURRENT LEARNING RATE: 0.04674036313902923
previous_iter_valid_loss : 0.08480580151081085

    237000	  0.084650	  0.084806	  0.090034		CURRENT LEARNING RATE: 0.04669364613828366
previous_iter_valid_loss : 0.0911116674542427

    237100	  0.090913	  0.091112	  0.089664		CURRENT LEARNING RATE: 0.046646975831188126
previous_iter_valid_loss : 0.11787967383861542

    237200	  0.118123	  0.117880	  0.090049		CURRENT LEARNING RATE: 0.046600352171072286
previous_iter_valid_loss : 0.19819040596485138

    237300	  0.197939	  0.198190	  0.091015		CURRENT LEARNING RATE: 0.04655377511131252
previous_iter_valid_loss : 0.08685575425624847

    237400	  0.086866	  0.086856	  0.091055		CURRENT LEARNING RATE: 0.046507244605331746
previous_iter_valid_loss : 0.0783853754401207

    237500	  0.078357	  0.078385	  0.090787		CURRENT LEARNING RATE: 0.04646076060659945
previous_iter_valid_loss : 0.08514536172151566

    237600	  0.085248	  0.085145	  0.090672		CURRENT LEARNING RATE: 0.046414323068631635
previous_iter_valid_loss : 0.07971970736980438

    237700	  0.079620	  0.079720	  0.090624		CURRENT LEARNING RATE: 0.04636793194499073
previous_iter_valid_loss : 0.08369357883930206

    237800	  0.083669	  0.083694	  0.090363		CURRENT LEARNING RATE: 0.04632158718928566
previous_iter_valid_loss : 0.09104037284851074

    237900	  0.090885	  0.091040	  0.090459		CURRENT LEARNING RATE: 0.046275288755171645
previous_iter_valid_loss : 0.11420635879039764

    238000	  0.114004	  0.114206	  0.090459		CURRENT LEARNING RATE: 0.046229036596350234
previous_iter_valid_loss : 0.09093303978443146

    238100	  0.091038	  0.090933	  0.090370		CURRENT LEARNING RATE: 0.04618283066656925
previous_iter_valid_loss : 0.0838492140173912

    238200	  0.083718	  0.083849	  0.090280		CURRENT LEARNING RATE: 0.046136670919622806
previous_iter_valid_loss : 0.07849399745464325

    238300	  0.078453	  0.078494	  0.090176		CURRENT LEARNING RATE: 0.046090557309351125
previous_iter_valid_loss : 0.08174789696931839

    238400	  0.081794	  0.081748	  0.090156		CURRENT LEARNING RATE: 0.0460444897896406
previous_iter_valid_loss : 0.09366614371538162

    238500	  0.093748	  0.093666	  0.090165		CURRENT LEARNING RATE: 0.04599846831442367
previous_iter_valid_loss : 0.09300167113542557

    238600	  0.093107	  0.093002	  0.090073		CURRENT LEARNING RATE: 0.04595249283767892
previous_iter_valid_loss : 0.07914698123931885

    238700	  0.079061	  0.079147	  0.089909		CURRENT LEARNING RATE: 0.04590656331343083
previous_iter_valid_loss : 0.08111635595560074

    238800	  0.081000	  0.081116	  0.089815		CURRENT LEARNING RATE: 0.045860679695749876
previous_iter_valid_loss : 0.08037296682596207

    238900	  0.080365	  0.080373	  0.089754		CURRENT LEARNING RATE: 0.04581484193875242
previous_iter_valid_loss : 0.08309673517942429

    239000	  0.083107	  0.083097	  0.089776		CURRENT LEARNING RATE: 0.045769049996600746
previous_iter_valid_loss : 0.0866597592830658

    239100	  0.086465	  0.086660	  0.089648		CURRENT LEARNING RATE: 0.04572330382350288
previous_iter_valid_loss : 0.10161332041025162

    239200	  0.101695	  0.101613	  0.089860		CURRENT LEARNING RATE: 0.04567760337371265
previous_iter_valid_loss : 0.1214202493429184

    239300	  0.121579	  0.121420	  0.090273		CURRENT LEARNING RATE: 0.045631948601529575
previous_iter_valid_loss : 0.13917407393455505

    239400	  0.139472	  0.139174	  0.090881		CURRENT LEARNING RATE: 0.045586339461298926
previous_iter_valid_loss : 0.08216830343008041

    239500	  0.082056	  0.082168	  0.090861		CURRENT LEARNING RATE: 0.04554077590741154
previous_iter_valid_loss : 0.07981432974338531

    239600	  0.079789	  0.079814	  0.090803		CURRENT LEARNING RATE: 0.04549525789430386
previous_iter_valid_loss : 0.08218740671873093

    239700	  0.082169	  0.082187	  0.090581		CURRENT LEARNING RATE: 0.04544978537645784
previous_iter_valid_loss : 0.12847959995269775

    239800	  0.128254	  0.128480	  0.091083		CURRENT LEARNING RATE: 0.045404358308401
previous_iter_valid_loss : 0.08779248595237732

    239900	  0.087585	  0.087792	  0.091170		CURRENT LEARNING RATE: 0.045358976644706256
previous_iter_valid_loss : 0.09743359684944153

    240000	  0.097509	  0.097434	  0.091295		CURRENT LEARNING RATE: 0.04531364033999194
previous_iter_valid_loss : 0.10343598574399948

    240100	  0.103637	  0.103436	  0.091322		CURRENT LEARNING RATE: 0.04526834934892172
previous_iter_valid_loss : 0.07826251536607742

    240200	  0.078219	  0.078263	  0.090731		CURRENT LEARNING RATE: 0.04522310362620463
previous_iter_valid_loss : 0.11009050160646439

    240300	  0.110262	  0.110091	  0.090930		CURRENT LEARNING RATE: 0.04517790312659495
previous_iter_valid_loss : 0.07881557196378708

    240400	  0.078742	  0.078816	  0.090913		CURRENT LEARNING RATE: 0.045132747804892154
previous_iter_valid_loss : 0.09046060591936111

    240500	  0.090435	  0.090461	  0.091032		CURRENT LEARNING RATE: 0.0450876376159409
previous_iter_valid_loss : 0.13989727199077606

    240600	  0.140202	  0.139897	  0.091635		CURRENT LEARNING RATE: 0.04504257251463105
previous_iter_valid_loss : 0.09890107810497284

    240700	  0.099076	  0.098901	  0.091801		CURRENT LEARNING RATE: 0.044997552455897455
previous_iter_valid_loss : 0.08244751393795013

    240800	  0.082474	  0.082448	  0.091718		CURRENT LEARNING RATE: 0.04495257739472008
previous_iter_valid_loss : 0.08260013163089752

    240900	  0.082450	  0.082600	  0.091646		CURRENT LEARNING RATE: 0.044907647286123814
previous_iter_valid_loss : 0.08583145588636398

    241000	  0.085893	  0.085831	  0.091648		CURRENT LEARNING RATE: 0.0448627620851786
previous_iter_valid_loss : 0.09078526496887207

    241100	  0.090597	  0.090785	  0.091762		CURRENT LEARNING RATE: 0.04481792174699921
previous_iter_valid_loss : 0.08636834472417831

    241200	  0.086212	  0.086368	  0.091540		CURRENT LEARNING RATE: 0.044773126226745306
previous_iter_valid_loss : 0.142823725938797

    241300	  0.143145	  0.142824	  0.092182		CURRENT LEARNING RATE: 0.044728375479621336
previous_iter_valid_loss : 0.07787150144577026

    241400	  0.077835	  0.077872	  0.092155		CURRENT LEARNING RATE: 0.044683669460876596
previous_iter_valid_loss : 0.07947852462530136

    241500	  0.079352	  0.079479	  0.092120		CURRENT LEARNING RATE: 0.044639008125805034
previous_iter_valid_loss : 0.08527375012636185

    241600	  0.085183	  0.085274	  0.092196		CURRENT LEARNING RATE: 0.04459439142974532
previous_iter_valid_loss : 0.08069518953561783

    241700	  0.080603	  0.080695	  0.092100		CURRENT LEARNING RATE: 0.044549819328080734
previous_iter_valid_loss : 0.13674701750278473

    241800	  0.137052	  0.136747	  0.092684		CURRENT LEARNING RATE: 0.044505291776239214
previous_iter_valid_loss : 0.078876793384552

    241900	  0.078874	  0.078877	  0.092595		CURRENT LEARNING RATE: 0.04446080872969317
previous_iter_valid_loss : 0.07877573370933533

    242000	  0.078743	  0.078776	  0.092429		CURRENT LEARNING RATE: 0.04441637014395956
previous_iter_valid_loss : 0.08835440129041672

    242100	  0.088372	  0.088354	  0.092517		CURRENT LEARNING RATE: 0.044371975974599784
previous_iter_valid_loss : 0.08615782856941223

    242200	  0.085948	  0.086158	  0.092531		CURRENT LEARNING RATE: 0.04432762617721969
previous_iter_valid_loss : 0.1533665955066681

    242300	  0.153671	  0.153367	  0.093224		CURRENT LEARNING RATE: 0.04428332070746948
previous_iter_valid_loss : 0.09781970083713531

    242400	  0.097600	  0.097820	  0.093240		CURRENT LEARNING RATE: 0.04423905952104366
previous_iter_valid_loss : 0.07809262722730637

    242500	  0.077981	  0.078093	  0.093150		CURRENT LEARNING RATE: 0.044194842573681024
previous_iter_valid_loss : 0.09399589896202087

    242600	  0.094050	  0.093996	  0.093060		CURRENT LEARNING RATE: 0.044150669821164674
previous_iter_valid_loss : 0.08922170847654343

    242700	  0.089239	  0.089222	  0.093133		CURRENT LEARNING RATE: 0.04410654121932182
previous_iter_valid_loss : 0.0836523249745369

    242800	  0.083500	  0.083652	  0.093052		CURRENT LEARNING RATE: 0.044062456724023855
previous_iter_valid_loss : 0.07797354459762573

    242900	  0.077952	  0.077974	  0.093041		CURRENT LEARNING RATE: 0.044018416291186274
previous_iter_valid_loss : 0.07783893495798111

    243000	  0.077741	  0.077839	  0.092926		CURRENT LEARNING RATE: 0.043974419876768665
previous_iter_valid_loss : 0.07976064831018448

    243100	  0.079645	  0.079761	  0.092772		CURRENT LEARNING RATE: 0.0439304674367746
previous_iter_valid_loss : 0.0788552388548851

    243200	  0.078836	  0.078855	  0.092718		CURRENT LEARNING RATE: 0.043886558927251636
previous_iter_valid_loss : 0.08626560121774673

    243300	  0.086089	  0.086266	  0.092768		CURRENT LEARNING RATE: 0.04384269430429124
previous_iter_valid_loss : 0.09650509059429169

    243400	  0.096583	  0.096505	  0.092678		CURRENT LEARNING RATE: 0.043798873524028815
previous_iter_valid_loss : 0.07810946553945541

    243500	  0.078099	  0.078109	  0.092459		CURRENT LEARNING RATE: 0.04375509654264356
previous_iter_valid_loss : 0.07985491305589676

    243600	  0.079723	  0.079855	  0.092347		CURRENT LEARNING RATE: 0.043711363316358505
previous_iter_valid_loss : 0.07742886245250702

    243700	  0.077387	  0.077429	  0.092148		CURRENT LEARNING RATE: 0.04366767380144038
previous_iter_valid_loss : 0.07703008502721786


Current valid loss: 0.07703008502721786;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    243800	  0.076942	  0.077030	  0.092134		CURRENT LEARNING RATE: 0.04362402795419972
previous_iter_valid_loss : 0.08648395538330078

    243900	  0.086481	  0.086484	  0.092188		CURRENT LEARNING RATE: 0.04358042573099065
previous_iter_valid_loss : 0.0802408903837204

    244000	  0.080227	  0.080241	  0.092205		CURRENT LEARNING RATE: 0.04353686708821094
previous_iter_valid_loss : 0.07812578231096268

    244100	  0.078067	  0.078126	  0.091924		CURRENT LEARNING RATE: 0.04349335198230193
previous_iter_valid_loss : 0.10284588485956192

    244200	  0.102630	  0.102846	  0.092135		CURRENT LEARNING RATE: 0.04344988036974854
previous_iter_valid_loss : 0.08120211958885193

    244300	  0.081206	  0.081202	  0.092013		CURRENT LEARNING RATE: 0.043406452207079144
previous_iter_valid_loss : 0.08114935457706451

    244400	  0.081000	  0.081149	  0.091846		CURRENT LEARNING RATE: 0.04336306745086557
previous_iter_valid_loss : 0.0833534225821495

    244500	  0.083347	  0.083353	  0.091746		CURRENT LEARNING RATE: 0.04331972605772305
previous_iter_valid_loss : 0.08151502162218094

    244600	  0.081394	  0.081515	  0.091772		CURRENT LEARNING RATE: 0.04327642798431021
previous_iter_valid_loss : 0.1242627426981926

    244700	  0.124022	  0.124263	  0.092166		CURRENT LEARNING RATE: 0.04323317318732896
previous_iter_valid_loss : 0.08243419975042343

    244800	  0.082346	  0.082434	  0.092149		CURRENT LEARNING RATE: 0.0431899616235245
previous_iter_valid_loss : 0.08336301147937775

    244900	  0.083323	  0.083363	  0.092184		CURRENT LEARNING RATE: 0.04314679324968525
previous_iter_valid_loss : 0.08554603904485703

    245000	  0.085640	  0.085546	  0.091971		CURRENT LEARNING RATE: 0.04310366802264286
previous_iter_valid_loss : 0.07961759716272354

    245100	  0.079492	  0.079618	  0.091652		CURRENT LEARNING RATE: 0.04306058589927208
previous_iter_valid_loss : 0.07866654545068741

    245200	  0.078550	  0.078667	  0.091630		CURRENT LEARNING RATE: 0.04301754683649079
previous_iter_valid_loss : 0.09788828343153

    245300	  0.098004	  0.097888	  0.091765		CURRENT LEARNING RATE: 0.042974550791259905
previous_iter_valid_loss : 0.1941274106502533

    245400	  0.194643	  0.194127	  0.092906		CURRENT LEARNING RATE: 0.042931597720583414
previous_iter_valid_loss : 0.11745229363441467

    245500	  0.117267	  0.117452	  0.093051		CURRENT LEARNING RATE: 0.04288868758150822
previous_iter_valid_loss : 0.08358960598707199

    245600	  0.083708	  0.083590	  0.092989		CURRENT LEARNING RATE: 0.042845820331124176
previous_iter_valid_loss : 0.09357837587594986

    245700	  0.093436	  0.093578	  0.093132		CURRENT LEARNING RATE: 0.042802995926564016
previous_iter_valid_loss : 0.08752428740262985

    245800	  0.087565	  0.087524	  0.093122		CURRENT LEARNING RATE: 0.04276021432500337
previous_iter_valid_loss : 0.10355670005083084

    245900	  0.103717	  0.103557	  0.093370		CURRENT LEARNING RATE: 0.042717475483660616
previous_iter_valid_loss : 0.109913669526577

    246000	  0.110119	  0.109914	  0.093639		CURRENT LEARNING RATE: 0.042674779359796904
previous_iter_valid_loss : 0.07747375965118408

    246100	  0.077488	  0.077474	  0.093527		CURRENT LEARNING RATE: 0.042632125910716086
previous_iter_valid_loss : 0.08174880594015121

    246200	  0.081637	  0.081749	  0.093295		CURRENT LEARNING RATE: 0.04258951509376475
previous_iter_valid_loss : 0.08866557478904724

    246300	  0.088771	  0.088666	  0.093224		CURRENT LEARNING RATE: 0.04254694686633206
previous_iter_valid_loss : 0.08847946673631668

    246400	  0.088605	  0.088479	  0.093248		CURRENT LEARNING RATE: 0.04250442118584978
previous_iter_valid_loss : 0.07884100079536438

    246500	  0.078883	  0.078841	  0.093228		CURRENT LEARNING RATE: 0.042461938009792206
previous_iter_valid_loss : 0.09054234623908997

    246600	  0.090692	  0.090542	  0.092920		CURRENT LEARNING RATE: 0.042419497295676206
previous_iter_valid_loss : 0.07990463823080063

    246700	  0.079880	  0.079905	  0.092941		CURRENT LEARNING RATE: 0.042377099001061035
previous_iter_valid_loss : 0.07769323885440826

    246800	  0.077617	  0.077693	  0.092728		CURRENT LEARNING RATE: 0.04233474308354839
previous_iter_valid_loss : 0.0773480013012886

    246900	  0.077312	  0.077348	  0.092690		CURRENT LEARNING RATE: 0.042292429500782346
previous_iter_valid_loss : 0.07926752418279648

    247000	  0.079328	  0.079268	  0.092635		CURRENT LEARNING RATE: 0.04225015821044934
previous_iter_valid_loss : 0.09806991368532181

    247100	  0.097909	  0.098070	  0.092705		CURRENT LEARNING RATE: 0.04220792917027807
previous_iter_valid_loss : 0.08046094328165054

    247200	  0.080394	  0.080461	  0.092330		CURRENT LEARNING RATE: 0.042165742338039484
previous_iter_valid_loss : 0.10749233514070511

    247300	  0.107724	  0.107492	  0.091423		CURRENT LEARNING RATE: 0.042123597671546734
previous_iter_valid_loss : 0.08575928211212158

    247400	  0.085889	  0.085759	  0.091412		CURRENT LEARNING RATE: 0.04208149512865518
previous_iter_valid_loss : 0.07697205245494843


Current valid loss: 0.07697205245494843;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    247500	  0.076939	  0.076972	  0.091398		CURRENT LEARNING RATE: 0.04203943466726227
previous_iter_valid_loss : 0.08559131622314453

    247600	  0.085691	  0.085591	  0.091403		CURRENT LEARNING RATE: 0.04199741624530752
previous_iter_valid_loss : 0.07961950451135635

    247700	  0.079681	  0.079620	  0.091402		CURRENT LEARNING RATE: 0.0419554398207725
previous_iter_valid_loss : 0.07728025317192078

    247800	  0.077251	  0.077280	  0.091338		CURRENT LEARNING RATE: 0.04191350535168082
previous_iter_valid_loss : 0.1021413579583168

    247900	  0.102351	  0.102141	  0.091449		CURRENT LEARNING RATE: 0.04187161279609798
previous_iter_valid_loss : 0.08486168831586838

    248000	  0.084685	  0.084862	  0.091155		CURRENT LEARNING RATE: 0.04182976211213143
previous_iter_valid_loss : 0.07947152853012085

    248100	  0.079515	  0.079472	  0.091041		CURRENT LEARNING RATE: 0.04178795325793045
previous_iter_valid_loss : 0.07740946859121323

    248200	  0.077465	  0.077409	  0.090976		CURRENT LEARNING RATE: 0.04174618619168624
previous_iter_valid_loss : 0.0972910225391388

    248300	  0.097422	  0.097291	  0.091164		CURRENT LEARNING RATE: 0.041704460871631696
previous_iter_valid_loss : 0.0801822692155838

    248400	  0.080100	  0.080182	  0.091149		CURRENT LEARNING RATE: 0.0416627772560415
previous_iter_valid_loss : 0.13569478690624237

    248500	  0.136023	  0.135695	  0.091569		CURRENT LEARNING RATE: 0.041621135303232006
previous_iter_valid_loss : 0.079655721783638

    248600	  0.079750	  0.079656	  0.091435		CURRENT LEARNING RATE: 0.04157953497156131
previous_iter_valid_loss : 0.07901709526777267

    248700	  0.079034	  0.079017	  0.091434		CURRENT LEARNING RATE: 0.04153797621942905
previous_iter_valid_loss : 0.09233806282281876

    248800	  0.092294	  0.092338	  0.091546		CURRENT LEARNING RATE: 0.041496459005276466
previous_iter_valid_loss : 0.08933758735656738

    248900	  0.089412	  0.089338	  0.091636		CURRENT LEARNING RATE: 0.04145498328758633
previous_iter_valid_loss : 0.07737725973129272

    249000	  0.077389	  0.077377	  0.091579		CURRENT LEARNING RATE: 0.04141354902488296
previous_iter_valid_loss : 0.07868918776512146

    249100	  0.078667	  0.078689	  0.091499		CURRENT LEARNING RATE: 0.04137215617573206
previous_iter_valid_loss : 0.09725415706634521

    249200	  0.097104	  0.097254	  0.091455		CURRENT LEARNING RATE: 0.041330804698740786
previous_iter_valid_loss : 0.12406431138515472

    249300	  0.123849	  0.124064	  0.091482		CURRENT LEARNING RATE: 0.041289494552557635
previous_iter_valid_loss : 0.08068393915891647

    249400	  0.080630	  0.080684	  0.090897		CURRENT LEARNING RATE: 0.04124822569587249
previous_iter_valid_loss : 0.0830264687538147

    249500	  0.082921	  0.083026	  0.090906		CURRENT LEARNING RATE: 0.041206998087416485
previous_iter_valid_loss : 0.09278827905654907

    249600	  0.092876	  0.092788	  0.091035		CURRENT LEARNING RATE: 0.041165811685962006
previous_iter_valid_loss : 0.07796984165906906

    249700	  0.077915	  0.077970	  0.090993		CURRENT LEARNING RATE: 0.04112466645032262
previous_iter_valid_loss : 0.0813470184803009

    249800	  0.081213	  0.081347	  0.090522		CURRENT LEARNING RATE: 0.041083562339353126
previous_iter_valid_loss : 0.13331127166748047

    249900	  0.133094	  0.133311	  0.090977		CURRENT LEARNING RATE: 0.0410424993119494
previous_iter_valid_loss : 0.07830938696861267

    250000	  0.078274	  0.078309	  0.090786		CURRENT LEARNING RATE: 0.041001477327048404
previous_iter_valid_loss : 0.08656519651412964

    250100	  0.086623	  0.086565	  0.090617		CURRENT LEARNING RATE: 0.04096049634362815
previous_iter_valid_loss : 0.09365131705999374

    250200	  0.093471	  0.093651	  0.090771		CURRENT LEARNING RATE: 0.04091955632070764
previous_iter_valid_loss : 0.08618366718292236

    250300	  0.086236	  0.086184	  0.090532		CURRENT LEARNING RATE: 0.040878657217346875
previous_iter_valid_loss : 0.08555712550878525

    250400	  0.085654	  0.085557	  0.090599		CURRENT LEARNING RATE: 0.04083779899264673
previous_iter_valid_loss : 0.0785057321190834

    250500	  0.078546	  0.078506	  0.090480		CURRENT LEARNING RATE: 0.04079698160574899
previous_iter_valid_loss : 0.0789748877286911

    250600	  0.079006	  0.078975	  0.089870		CURRENT LEARNING RATE: 0.04075620501583623
previous_iter_valid_loss : 0.0807768702507019

    250700	  0.080682	  0.080777	  0.089689		CURRENT LEARNING RATE: 0.040715469182131904
previous_iter_valid_loss : 0.07855615764856339

    250800	  0.078597	  0.078556	  0.089650		CURRENT LEARNING RATE: 0.04067477406390015
previous_iter_valid_loss : 0.07782205194234848

    250900	  0.077798	  0.077822	  0.089603		CURRENT LEARNING RATE: 0.04063411962044585
previous_iter_valid_loss : 0.08849453926086426

    251000	  0.088394	  0.088495	  0.089629		CURRENT LEARNING RATE: 0.040593505811114546
previous_iter_valid_loss : 0.08370323479175568

    251100	  0.083802	  0.083703	  0.089558		CURRENT LEARNING RATE: 0.04055293259529245
previous_iter_valid_loss : 0.0894223153591156

    251200	  0.089464	  0.089422	  0.089589		CURRENT LEARNING RATE: 0.04051239993240632
previous_iter_valid_loss : 0.07719605416059494

    251300	  0.077173	  0.077196	  0.088933		CURRENT LEARNING RATE: 0.040471907781923507
previous_iter_valid_loss : 0.08202462643384933

    251400	  0.082037	  0.082025	  0.088974		CURRENT LEARNING RATE: 0.04043145610335183
previous_iter_valid_loss : 0.09224463999271393

    251500	  0.092134	  0.092245	  0.089102		CURRENT LEARNING RATE: 0.04039104485623964
previous_iter_valid_loss : 0.0786251574754715

    251600	  0.078591	  0.078625	  0.089035		CURRENT LEARNING RATE: 0.040350674000175675
previous_iter_valid_loss : 0.08116511255502701

    251700	  0.081206	  0.081165	  0.089040		CURRENT LEARNING RATE: 0.040310343494789076
previous_iter_valid_loss : 0.09098649024963379

    251800	  0.090860	  0.090986	  0.088582		CURRENT LEARNING RATE: 0.04027005329974931
previous_iter_valid_loss : 0.08358573913574219

    251900	  0.083402	  0.083586	  0.088630		CURRENT LEARNING RATE: 0.04022980337476622
previous_iter_valid_loss : 0.07858991622924805

    252000	  0.078610	  0.078590	  0.088628		CURRENT LEARNING RATE: 0.04018959367958985
previous_iter_valid_loss : 0.09590576589107513

    252100	  0.095981	  0.095906	  0.088703		CURRENT LEARNING RATE: 0.04014942417401051
previous_iter_valid_loss : 0.09584925323724747

    252200	  0.095961	  0.095849	  0.088800		CURRENT LEARNING RATE: 0.04010929481785868
previous_iter_valid_loss : 0.08301516622304916

    252300	  0.083125	  0.083015	  0.088097		CURRENT LEARNING RATE: 0.040069205571005025
previous_iter_valid_loss : 0.0788104236125946

    252400	  0.078728	  0.078810	  0.087906		CURRENT LEARNING RATE: 0.04002915639336027
previous_iter_valid_loss : 0.080010324716568

    252500	  0.080062	  0.080010	  0.087926		CURRENT LEARNING RATE: 0.039989147244875255
previous_iter_valid_loss : 0.1007860004901886

    252600	  0.100570	  0.100786	  0.087994		CURRENT LEARNING RATE: 0.0399491780855408
previous_iter_valid_loss : 0.09611625969409943

    252700	  0.096209	  0.096116	  0.088062		CURRENT LEARNING RATE: 0.03990924887538777
previous_iter_valid_loss : 0.0806419849395752

    252800	  0.080655	  0.080642	  0.088032		CURRENT LEARNING RATE: 0.03986935957448695
previous_iter_valid_loss : 0.07850755006074905

    252900	  0.078483	  0.078508	  0.088038		CURRENT LEARNING RATE: 0.03982951014294902
previous_iter_valid_loss : 0.10202666372060776

    253000	  0.102133	  0.102027	  0.088280		CURRENT LEARNING RATE: 0.03978970054092454
previous_iter_valid_loss : 0.07703986018896103

    253100	  0.076987	  0.077040	  0.088252		CURRENT LEARNING RATE: 0.03974993072860393
previous_iter_valid_loss : 0.0779479369521141

    253200	  0.077905	  0.077948	  0.088243		CURRENT LEARNING RATE: 0.03971020066621736
previous_iter_valid_loss : 0.09225399792194366

    253300	  0.092051	  0.092254	  0.088303		CURRENT LEARNING RATE: 0.03967051031403477
previous_iter_valid_loss : 0.07954036444425583

    253400	  0.079541	  0.079540	  0.088134		CURRENT LEARNING RATE: 0.039630859632365775
previous_iter_valid_loss : 0.07676736265420914


Current valid loss: 0.07676736265420914;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    253500	  0.076716	  0.076767	  0.088120		CURRENT LEARNING RATE: 0.03959124858155974
previous_iter_valid_loss : 0.10032340884208679

    253600	  0.100505	  0.100323	  0.088325		CURRENT LEARNING RATE: 0.03955167712200559
previous_iter_valid_loss : 0.07786480337381363

    253700	  0.077806	  0.077865	  0.088329		CURRENT LEARNING RATE: 0.03951214521413184
previous_iter_valid_loss : 0.08818163722753525

    253800	  0.088310	  0.088182	  0.088441		CURRENT LEARNING RATE: 0.039472652818406596
previous_iter_valid_loss : 0.08566026389598846

    253900	  0.085702	  0.085660	  0.088432		CURRENT LEARNING RATE: 0.03943319989533747
previous_iter_valid_loss : 0.08431966602802277

    254000	  0.084339	  0.084320	  0.088473		CURRENT LEARNING RATE: 0.03939378640547153
previous_iter_valid_loss : 0.07700781524181366

    254100	  0.076981	  0.077008	  0.088462		CURRENT LEARNING RATE: 0.03935441230939527
previous_iter_valid_loss : 0.0783199667930603

    254200	  0.078342	  0.078320	  0.088217		CURRENT LEARNING RATE: 0.03931507756773459
previous_iter_valid_loss : 0.08955331891775131

    254300	  0.089585	  0.089553	  0.088300		CURRENT LEARNING RATE: 0.03927578214115477
previous_iter_valid_loss : 0.097080759704113

    254400	  0.097166	  0.097081	  0.088460		CURRENT LEARNING RATE: 0.039236525990360364
previous_iter_valid_loss : 0.11957909166812897

    254500	  0.119319	  0.119579	  0.088822		CURRENT LEARNING RATE: 0.03919730907609521
previous_iter_valid_loss : 0.09038827568292618

    254600	  0.090257	  0.090388	  0.088911		CURRENT LEARNING RATE: 0.039158131359142395
previous_iter_valid_loss : 0.0791318342089653

    254700	  0.079115	  0.079132	  0.088459		CURRENT LEARNING RATE: 0.03911899280032421
previous_iter_valid_loss : 0.07830700278282166

    254800	  0.078255	  0.078307	  0.088418		CURRENT LEARNING RATE: 0.03907989336050209
previous_iter_valid_loss : 0.08566726744174957

    254900	  0.085517	  0.085667	  0.088441		CURRENT LEARNING RATE: 0.039040833000576584
previous_iter_valid_loss : 0.08179312944412231

    255000	  0.081793	  0.081793	  0.088404		CURRENT LEARNING RATE: 0.039001811681487315
previous_iter_valid_loss : 0.08479592204093933

    255100	  0.084867	  0.084796	  0.088455		CURRENT LEARNING RATE: 0.038962829364213
previous_iter_valid_loss : 0.0791698470711708

    255200	  0.079118	  0.079170	  0.088460		CURRENT LEARNING RATE: 0.038923886009771286
previous_iter_valid_loss : 0.07777806371450424

    255300	  0.077725	  0.077778	  0.088259		CURRENT LEARNING RATE: 0.03888498157921883
previous_iter_valid_loss : 0.07935675233602524

    255400	  0.079356	  0.079357	  0.087112		CURRENT LEARNING RATE: 0.03884611603365118
previous_iter_valid_loss : 0.10104461759328842

    255500	  0.100855	  0.101045	  0.086947		CURRENT LEARNING RATE: 0.03880728933420281
previous_iter_valid_loss : 0.07782751321792603

    255600	  0.077772	  0.077828	  0.086890		CURRENT LEARNING RATE: 0.03876850144204702
previous_iter_valid_loss : 0.07889822870492935

    255700	  0.078841	  0.078898	  0.086743		CURRENT LEARNING RATE: 0.03872975231839589
previous_iter_valid_loss : 0.07691245526075363

    255800	  0.076912	  0.076912	  0.086637		CURRENT LEARNING RATE: 0.0386910419245003
previous_iter_valid_loss : 0.07888512313365936

    255900	  0.078793	  0.078885	  0.086390		CURRENT LEARNING RATE: 0.03865237022164987
previous_iter_valid_loss : 0.11147420108318329

    256000	  0.111334	  0.111474	  0.086406		CURRENT LEARNING RATE: 0.038613737171172884
previous_iter_valid_loss : 0.07817836850881577

    256100	  0.078227	  0.078178	  0.086413		CURRENT LEARNING RATE: 0.03857514273443629
previous_iter_valid_loss : 0.07727660238742828

    256200	  0.077253	  0.077277	  0.086368		CURRENT LEARNING RATE: 0.03853658687284562
previous_iter_valid_loss : 0.08037211000919342

    256300	  0.080235	  0.080372	  0.086285		CURRENT LEARNING RATE: 0.03849806954784506
previous_iter_valid_loss : 0.08201567083597183

    256400	  0.082088	  0.082016	  0.086221		CURRENT LEARNING RATE: 0.03845959072091725
previous_iter_valid_loss : 0.07966355234384537

    256500	  0.079673	  0.079664	  0.086229		CURRENT LEARNING RATE: 0.038421150353583365
previous_iter_valid_loss : 0.07849876582622528

    256600	  0.078484	  0.078499	  0.086108		CURRENT LEARNING RATE: 0.03838274840740302
previous_iter_valid_loss : 0.07750793546438217

    256700	  0.077471	  0.077508	  0.086084		CURRENT LEARNING RATE: 0.0383443848439743
previous_iter_valid_loss : 0.08910486102104187

    256800	  0.089222	  0.089105	  0.086199		CURRENT LEARNING RATE: 0.03830605962493362
previous_iter_valid_loss : 0.09328505396842957

    256900	  0.093377	  0.093285	  0.086358		CURRENT LEARNING RATE: 0.03826777271195576
previous_iter_valid_loss : 0.12296109646558762

    257000	  0.123137	  0.122961	  0.086795		CURRENT LEARNING RATE: 0.03822952406675378
previous_iter_valid_loss : 0.08019082993268967

    257100	  0.080084	  0.080191	  0.086616		CURRENT LEARNING RATE: 0.03819131365107906
previous_iter_valid_loss : 0.08291014283895493

    257200	  0.082991	  0.082910	  0.086641		CURRENT LEARNING RATE: 0.03815314142672119
previous_iter_valid_loss : 0.09264849126338959

    257300	  0.092486	  0.092648	  0.086492		CURRENT LEARNING RATE: 0.038115007355507914
previous_iter_valid_loss : 0.07813877612352371

    257400	  0.078144	  0.078139	  0.086416		CURRENT LEARNING RATE: 0.03807691139930516
previous_iter_valid_loss : 0.08016610145568848

    257500	  0.080148	  0.080166	  0.086448		CURRENT LEARNING RATE: 0.03803885352001699
previous_iter_valid_loss : 0.1161983534693718

    257600	  0.116388	  0.116198	  0.086754		CURRENT LEARNING RATE: 0.03800083367958552
previous_iter_valid_loss : 0.10388247668743134

    257700	  0.104007	  0.103882	  0.086997		CURRENT LEARNING RATE: 0.03796285183999089
previous_iter_valid_loss : 0.1012929379940033

    257800	  0.101092	  0.101293	  0.087237		CURRENT LEARNING RATE: 0.03792490796325124
previous_iter_valid_loss : 0.07810467481613159

    257900	  0.078023	  0.078105	  0.086996		CURRENT LEARNING RATE: 0.03788700201142274
previous_iter_valid_loss : 0.10903517156839371

    258000	  0.109247	  0.109035	  0.087238		CURRENT LEARNING RATE: 0.0378491339465994
previous_iter_valid_loss : 0.08216416835784912

    258100	  0.082153	  0.082164	  0.087265		CURRENT LEARNING RATE: 0.03781130373091317
previous_iter_valid_loss : 0.07837281376123428

    258200	  0.078337	  0.078373	  0.087275		CURRENT LEARNING RATE: 0.0377735113265338
previous_iter_valid_loss : 0.141387477517128

    258300	  0.141196	  0.141387	  0.087716		CURRENT LEARNING RATE: 0.03773575669566892
previous_iter_valid_loss : 0.08425702899694443

    258400	  0.084324	  0.084257	  0.087756		CURRENT LEARNING RATE: 0.03769803980056388
previous_iter_valid_loss : 0.08316470682621002

    258500	  0.083037	  0.083165	  0.087231		CURRENT LEARNING RATE: 0.03766036060350179
previous_iter_valid_loss : 0.07911514490842819

    258600	  0.079030	  0.079115	  0.087226		CURRENT LEARNING RATE: 0.037622719066803416
previous_iter_valid_loss : 0.08142794668674469

    258700	  0.081511	  0.081428	  0.087250		CURRENT LEARNING RATE: 0.03758511515282727
previous_iter_valid_loss : 0.0942791998386383

    258800	  0.094084	  0.094279	  0.087269		CURRENT LEARNING RATE: 0.0375475488239694
previous_iter_valid_loss : 0.1490360051393509

    258900	  0.149402	  0.149036	  0.087866		CURRENT LEARNING RATE: 0.03751002004266349
previous_iter_valid_loss : 0.08924607932567596

    259000	  0.089127	  0.089246	  0.087985		CURRENT LEARNING RATE: 0.03747252877138072
previous_iter_valid_loss : 0.07690156996250153

    259100	  0.076898	  0.076902	  0.087967		CURRENT LEARNING RATE: 0.03743507497262987
previous_iter_valid_loss : 0.0777667760848999

    259200	  0.077729	  0.077767	  0.087772		CURRENT LEARNING RATE: 0.037397658608957114
previous_iter_valid_loss : 0.07727576792240143

    259300	  0.077211	  0.077276	  0.087304		CURRENT LEARNING RATE: 0.03736027964294608
previous_iter_valid_loss : 0.07822934538125992

    259400	  0.078210	  0.078229	  0.087280		CURRENT LEARNING RATE: 0.037322938037217784
previous_iter_valid_loss : 0.07708367705345154

    259500	  0.077013	  0.077084	  0.087220		CURRENT LEARNING RATE: 0.037285633754430655
previous_iter_valid_loss : 0.07823972404003143

    259600	  0.078118	  0.078240	  0.087075		CURRENT LEARNING RATE: 0.03724836675728039
previous_iter_valid_loss : 0.08323436230421066

    259700	  0.083268	  0.083234	  0.087127		CURRENT LEARNING RATE: 0.03721113700849998
previous_iter_valid_loss : 0.07784163951873779

    259800	  0.077807	  0.077842	  0.087092		CURRENT LEARNING RATE: 0.037173944470859664
previous_iter_valid_loss : 0.0796007439494133

    259900	  0.079459	  0.079601	  0.086555		CURRENT LEARNING RATE: 0.03713678910716694
previous_iter_valid_loss : 0.0856042206287384

    260000	  0.085411	  0.085604	  0.086628		CURRENT LEARNING RATE: 0.03709967088026641
previous_iter_valid_loss : 0.07721475511789322

    260100	  0.077195	  0.077215	  0.086535		CURRENT LEARNING RATE: 0.03706258975303985
previous_iter_valid_loss : 0.0896906852722168

    260200	  0.089501	  0.089691	  0.086495		CURRENT LEARNING RATE: 0.037025545688406124
previous_iter_valid_loss : 0.09368126094341278

    260300	  0.093514	  0.093681	  0.086570		CURRENT LEARNING RATE: 0.03698853864932118
previous_iter_valid_loss : 0.0889509841799736

    260400	  0.089031	  0.088951	  0.086604		CURRENT LEARNING RATE: 0.036951568598777976
previous_iter_valid_loss : 0.07692867517471313

    260500	  0.076881	  0.076929	  0.086588		CURRENT LEARNING RATE: 0.03691463549980645
previous_iter_valid_loss : 0.07757895439863205

    260600	  0.077513	  0.077579	  0.086574		CURRENT LEARNING RATE: 0.03687773931547347
previous_iter_valid_loss : 0.08150158077478409

    260700	  0.081341	  0.081502	  0.086581		CURRENT LEARNING RATE: 0.036840880008882915
previous_iter_valid_loss : 0.07680130004882812

    260800	  0.076740	  0.076801	  0.086564		CURRENT LEARNING RATE: 0.03680405754317542
previous_iter_valid_loss : 0.0959494560956955

    260900	  0.096006	  0.095949	  0.086745		CURRENT LEARNING RATE: 0.03676727188152855
previous_iter_valid_loss : 0.09748226404190063

    261000	  0.097610	  0.097482	  0.086835		CURRENT LEARNING RATE: 0.0367305229871566
previous_iter_valid_loss : 0.0813281238079071

    261100	  0.081230	  0.081328	  0.086811		CURRENT LEARNING RATE: 0.03669381082331072
previous_iter_valid_loss : 0.0799240991473198

    261200	  0.079946	  0.079924	  0.086716		CURRENT LEARNING RATE: 0.03665713535327872
previous_iter_valid_loss : 0.08421825617551804

    261300	  0.084242	  0.084218	  0.086787		CURRENT LEARNING RATE: 0.03662049654038512
previous_iter_valid_loss : 0.09086662530899048

    261400	  0.090970	  0.090867	  0.086875		CURRENT LEARNING RATE: 0.03658389434799111
previous_iter_valid_loss : 0.08440535515546799

    261500	  0.084310	  0.084405	  0.086797		CURRENT LEARNING RATE: 0.036547328739494504
previous_iter_valid_loss : 0.07920163869857788

    261600	  0.079168	  0.079202	  0.086802		CURRENT LEARNING RATE: 0.03651079967832968
previous_iter_valid_loss : 0.07700230926275253

    261700	  0.076934	  0.077002	  0.086761		CURRENT LEARNING RATE: 0.036474307127967585
previous_iter_valid_loss : 0.08159738779067993

    261800	  0.081466	  0.081597	  0.086667		CURRENT LEARNING RATE: 0.03643785105191564
previous_iter_valid_loss : 0.10650476068258286

    261900	  0.106261	  0.106505	  0.086896		CURRENT LEARNING RATE: 0.036401431413717794
previous_iter_valid_loss : 0.07752520591020584

    262000	  0.077478	  0.077525	  0.086885		CURRENT LEARNING RATE: 0.0363650481769544
previous_iter_valid_loss : 0.07789263874292374

    262100	  0.077808	  0.077893	  0.086705		CURRENT LEARNING RATE: 0.03632870130524221
previous_iter_valid_loss : 0.08506768941879272

    262200	  0.085108	  0.085068	  0.086597		CURRENT LEARNING RATE: 0.03629239076223434
previous_iter_valid_loss : 0.08308940380811691

    262300	  0.083066	  0.083089	  0.086598		CURRENT LEARNING RATE: 0.036256116511620265
previous_iter_valid_loss : 0.07960975915193558

    262400	  0.079504	  0.079610	  0.086606		CURRENT LEARNING RATE: 0.03621987851712573
previous_iter_valid_loss : 0.1350916177034378

    262500	  0.135333	  0.135092	  0.087157		CURRENT LEARNING RATE: 0.03618367674251273
previous_iter_valid_loss : 0.07868287712335587

    262600	  0.078661	  0.078683	  0.086936		CURRENT LEARNING RATE: 0.036147511151579485
previous_iter_valid_loss : 0.09081059694290161

    262700	  0.090843	  0.090811	  0.086883		CURRENT LEARNING RATE: 0.03611138170816039
previous_iter_valid_loss : 0.10426471382379532

    262800	  0.104370	  0.104265	  0.087119		CURRENT LEARNING RATE: 0.03607528837612603
previous_iter_valid_loss : 0.07878805696964264

    262900	  0.078663	  0.078788	  0.087122		CURRENT LEARNING RATE: 0.03603923111938305
previous_iter_valid_loss : 0.07676003873348236


Current valid loss: 0.07676003873348236;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    263000	  0.076702	  0.076760	  0.086869		CURRENT LEARNING RATE: 0.036003209901874195
previous_iter_valid_loss : 0.09818529337644577

    263100	  0.098032	  0.098185	  0.087081		CURRENT LEARNING RATE: 0.03596722468757822
previous_iter_valid_loss : 0.07956172525882721

    263200	  0.079528	  0.079562	  0.087097		CURRENT LEARNING RATE: 0.035931275440509954
previous_iter_valid_loss : 0.083356112241745

    263300	  0.083348	  0.083356	  0.087008		CURRENT LEARNING RATE: 0.03589536212472012
previous_iter_valid_loss : 0.07853639125823975

    263400	  0.078520	  0.078536	  0.086998		CURRENT LEARNING RATE: 0.035859484704295404
previous_iter_valid_loss : 0.0866670310497284

    263500	  0.086768	  0.086667	  0.087097		CURRENT LEARNING RATE: 0.03582364314335836
previous_iter_valid_loss : 0.07987052202224731

    263600	  0.079766	  0.079871	  0.086892		CURRENT LEARNING RATE: 0.03578783740606746
previous_iter_valid_loss : 0.07679508626461029

    263700	  0.076752	  0.076795	  0.086882		CURRENT LEARNING RATE: 0.03575206745661695
previous_iter_valid_loss : 0.0888778492808342

    263800	  0.088769	  0.088878	  0.086889		CURRENT LEARNING RATE: 0.03571633325923688
previous_iter_valid_loss : 0.07734926789999008

    263900	  0.077283	  0.077349	  0.086805		CURRENT LEARNING RATE: 0.03568063477819303
previous_iter_valid_loss : 0.0788983404636383

    264000	  0.078814	  0.078898	  0.086751		CURRENT LEARNING RATE: 0.03564497197778694
previous_iter_valid_loss : 0.08371836692094803

    264100	  0.083768	  0.083718	  0.086818		CURRENT LEARNING RATE: 0.0356093448223558
previous_iter_valid_loss : 0.12679563462734222

    264200	  0.126560	  0.126796	  0.087303		CURRENT LEARNING RATE: 0.035573753276272456
previous_iter_valid_loss : 0.07846584916114807

    264300	  0.078344	  0.078466	  0.087192		CURRENT LEARNING RATE: 0.03553819730394533
previous_iter_valid_loss : 0.07774224132299423

    264400	  0.077794	  0.077742	  0.086999		CURRENT LEARNING RATE: 0.03550267686981849
previous_iter_valid_loss : 0.07776200771331787

    264500	  0.077684	  0.077762	  0.086581		CURRENT LEARNING RATE: 0.03546719193837147
previous_iter_valid_loss : 0.08716543763875961

    264600	  0.087189	  0.087165	  0.086548		CURRENT LEARNING RATE: 0.03543174247411936
previous_iter_valid_loss : 0.10457497835159302

    264700	  0.104378	  0.104575	  0.086803		CURRENT LEARNING RATE: 0.03539632844161265
previous_iter_valid_loss : 0.08897339552640915

    264800	  0.089003	  0.088973	  0.086909		CURRENT LEARNING RATE: 0.035360949805437344
previous_iter_valid_loss : 0.07661378383636475


Current valid loss: 0.07661378383636475;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    264900	  0.076530	  0.076614	  0.086819		CURRENT LEARNING RATE: 0.0353256065302148
previous_iter_valid_loss : 0.08578663319349289

    265000	  0.085624	  0.085787	  0.086859		CURRENT LEARNING RATE: 0.035290298580601724
previous_iter_valid_loss : 0.08726897090673447

    265100	  0.087339	  0.087269	  0.086884		CURRENT LEARNING RATE: 0.03525502592129015
previous_iter_valid_loss : 0.07734613120555878

    265200	  0.077304	  0.077346	  0.086865		CURRENT LEARNING RATE: 0.03521978851700746
previous_iter_valid_loss : 0.07675281912088394

    265300	  0.076710	  0.076753	  0.086855		CURRENT LEARNING RATE: 0.03518458633251621
previous_iter_valid_loss : 0.07841474562883377

    265400	  0.078281	  0.078415	  0.086846		CURRENT LEARNING RATE: 0.035149419332614236
previous_iter_valid_loss : 0.07807788997888565

    265500	  0.078083	  0.078078	  0.086616		CURRENT LEARNING RATE: 0.03511428748213451
previous_iter_valid_loss : 0.0771278515458107

    265600	  0.077116	  0.077128	  0.086609		CURRENT LEARNING RATE: 0.0350791907459452
previous_iter_valid_loss : 0.07972121983766556

    265700	  0.079566	  0.079721	  0.086617		CURRENT LEARNING RATE: 0.035044129088949556
previous_iter_valid_loss : 0.16207148134708405

    265800	  0.162414	  0.162071	  0.087469		CURRENT LEARNING RATE: 0.03500910247608593
previous_iter_valid_loss : 0.07819658517837524

    265900	  0.078186	  0.078197	  0.087462		CURRENT LEARNING RATE: 0.03497411087232768
previous_iter_valid_loss : 0.08064502477645874

    266000	  0.080517	  0.080645	  0.087154		CURRENT LEARNING RATE: 0.03493915424268323
previous_iter_valid_loss : 0.08773059397935867

    266100	  0.087815	  0.087731	  0.087249		CURRENT LEARNING RATE: 0.034904232552195935
previous_iter_valid_loss : 0.0809980258345604

    266200	  0.080821	  0.080998	  0.087286		CURRENT LEARNING RATE: 0.034869345765944096
previous_iter_valid_loss : 0.08362256735563278

    266300	  0.083662	  0.083623	  0.087319		CURRENT LEARNING RATE: 0.03483449384904092
previous_iter_valid_loss : 0.08692557364702225

    266400	  0.086772	  0.086926	  0.087368		CURRENT LEARNING RATE: 0.03479967676663451
previous_iter_valid_loss : 0.08007705211639404

    266500	  0.079927	  0.080077	  0.087372		CURRENT LEARNING RATE: 0.034764894483907766
previous_iter_valid_loss : 0.0769280269742012

    266600	  0.076885	  0.076928	  0.087356		CURRENT LEARNING RATE: 0.0347301469660784
previous_iter_valid_loss : 0.08055204898118973

    266700	  0.080449	  0.080552	  0.087387		CURRENT LEARNING RATE: 0.03469543417839888
previous_iter_valid_loss : 0.09395837038755417

    266800	  0.093825	  0.093958	  0.087435		CURRENT LEARNING RATE: 0.03466075608615645
previous_iter_valid_loss : 0.07712677121162415

    266900	  0.077108	  0.077127	  0.087274		CURRENT LEARNING RATE: 0.034626112654673
previous_iter_valid_loss : 0.08487532287836075

    267000	  0.084926	  0.084875	  0.086893		CURRENT LEARNING RATE: 0.03459150384930507
previous_iter_valid_loss : 0.08726434409618378

    267100	  0.087346	  0.087264	  0.086964		CURRENT LEARNING RATE: 0.03455692963544387
previous_iter_valid_loss : 0.09680723398923874

    267200	  0.096689	  0.096807	  0.087103		CURRENT LEARNING RATE: 0.03452238997851521
previous_iter_valid_loss : 0.07867743819952011

    267300	  0.078612	  0.078677	  0.086963		CURRENT LEARNING RATE: 0.03448788484397939
previous_iter_valid_loss : 0.07971539348363876

    267400	  0.079627	  0.079715	  0.086979		CURRENT LEARNING RATE: 0.03445341419733129
previous_iter_valid_loss : 0.0769406259059906

    267500	  0.076924	  0.076941	  0.086946		CURRENT LEARNING RATE: 0.034418978004100244
previous_iter_valid_loss : 0.07721693813800812

    267600	  0.077109	  0.077217	  0.086557		CURRENT LEARNING RATE: 0.03438457622985009
previous_iter_valid_loss : 0.08486106246709824

    267700	  0.084882	  0.084861	  0.086366		CURRENT LEARNING RATE: 0.034350208840179024
previous_iter_valid_loss : 0.08154267072677612

    267800	  0.081522	  0.081543	  0.086169		CURRENT LEARNING RATE: 0.03431587580071967
previous_iter_valid_loss : 0.08061765879392624

    267900	  0.080482	  0.080618	  0.086194		CURRENT LEARNING RATE: 0.034281577077138956
previous_iter_valid_loss : 0.09448891133069992

    268000	  0.094330	  0.094489	  0.086049		CURRENT LEARNING RATE: 0.03424731263513819
previous_iter_valid_loss : 0.07912353426218033

    268100	  0.079139	  0.079124	  0.086018		CURRENT LEARNING RATE: 0.034213082440452916
previous_iter_valid_loss : 0.0861596167087555

    268200	  0.086038	  0.086160	  0.086096		CURRENT LEARNING RATE: 0.03417888645885293
previous_iter_valid_loss : 0.09116235375404358

    268300	  0.091256	  0.091162	  0.085594		CURRENT LEARNING RATE: 0.03414472465614224
previous_iter_valid_loss : 0.09360124915838242

    268400	  0.093461	  0.093601	  0.085687		CURRENT LEARNING RATE: 0.03411059699815906
previous_iter_valid_loss : 0.07807226479053497

    268500	  0.077972	  0.078072	  0.085636		CURRENT LEARNING RATE: 0.03407650345077573
previous_iter_valid_loss : 0.0802498310804367

    268600	  0.080142	  0.080250	  0.085648		CURRENT LEARNING RATE: 0.03404244397989868
previous_iter_valid_loss : 0.07815060764551163

    268700	  0.078158	  0.078151	  0.085615		CURRENT LEARNING RATE: 0.03400841855146844
previous_iter_valid_loss : 0.08458559960126877

    268800	  0.084361	  0.084586	  0.085518		CURRENT LEARNING RATE: 0.0339744271314596
previous_iter_valid_loss : 0.0763724222779274


Current valid loss: 0.0763724222779274;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    268900	  0.076262	  0.076372	  0.084791		CURRENT LEARNING RATE: 0.03394046968588072
previous_iter_valid_loss : 0.09080210328102112

    269000	  0.090864	  0.090802	  0.084807		CURRENT LEARNING RATE: 0.033906546180774357
previous_iter_valid_loss : 0.08614303171634674

    269100	  0.086226	  0.086143	  0.084899		CURRENT LEARNING RATE: 0.033872656582216984
previous_iter_valid_loss : 0.07769788801670074

    269200	  0.077674	  0.077698	  0.084899		CURRENT LEARNING RATE: 0.033838800856319025
previous_iter_valid_loss : 0.08078160881996155

    269300	  0.080806	  0.080782	  0.084934		CURRENT LEARNING RATE: 0.03380497896922475
previous_iter_valid_loss : 0.10574205964803696

    269400	  0.105555	  0.105742	  0.085209		CURRENT LEARNING RATE: 0.03377119088711226
previous_iter_valid_loss : 0.12235220521688461

    269500	  0.122177	  0.122352	  0.085662		CURRENT LEARNING RATE: 0.03373743657619345
previous_iter_valid_loss : 0.08284518122673035

    269600	  0.082664	  0.082845	  0.085708		CURRENT LEARNING RATE: 0.03370371600271405
previous_iter_valid_loss : 0.0941658467054367

    269700	  0.093993	  0.094166	  0.085817		CURRENT LEARNING RATE: 0.03367002913295346
previous_iter_valid_loss : 0.0865892842411995

    269800	  0.086426	  0.086589	  0.085904		CURRENT LEARNING RATE: 0.033636375933224806
previous_iter_valid_loss : 0.10795927792787552

    269900	  0.107734	  0.107959	  0.086188		CURRENT LEARNING RATE: 0.03360275636987488
previous_iter_valid_loss : 0.07913370430469513

    270000	  0.079101	  0.079134	  0.086123		CURRENT LEARNING RATE: 0.03356917040928413
previous_iter_valid_loss : 0.10560985654592514

    270100	  0.105395	  0.105610	  0.086407		CURRENT LEARNING RATE: 0.03353561801786659
previous_iter_valid_loss : 0.08502763509750366

    270200	  0.084840	  0.085028	  0.086361		CURRENT LEARNING RATE: 0.033502099162069865
previous_iter_valid_loss : 0.09538712352514267

    270300	  0.095242	  0.095387	  0.086378		CURRENT LEARNING RATE: 0.033468613808375076
previous_iter_valid_loss : 0.08421707153320312

    270400	  0.084044	  0.084217	  0.086330		CURRENT LEARNING RATE: 0.0334351619232969
previous_iter_valid_loss : 0.08426547050476074

    270500	  0.084108	  0.084265	  0.086404		CURRENT LEARNING RATE: 0.033401743473383434
previous_iter_valid_loss : 0.07717648148536682

    270600	  0.077125	  0.077176	  0.086400		CURRENT LEARNING RATE: 0.03336835842521623
previous_iter_valid_loss : 0.11755774915218353

    270700	  0.117718	  0.117558	  0.086760		CURRENT LEARNING RATE: 0.03333500674541021
previous_iter_valid_loss : 0.07925863564014435

    270800	  0.079246	  0.079259	  0.086785		CURRENT LEARNING RATE: 0.03330168840061373
previous_iter_valid_loss : 0.07681150734424591

    270900	  0.076702	  0.076812	  0.086593		CURRENT LEARNING RATE: 0.03326840335750843
previous_iter_valid_loss : 0.076911062002182

    271000	  0.076796	  0.076911	  0.086388		CURRENT LEARNING RATE: 0.03323515158280925
previous_iter_valid_loss : 0.07828369736671448

    271100	  0.078216	  0.078284	  0.086357		CURRENT LEARNING RATE: 0.033201933043264416
previous_iter_valid_loss : 0.08722995221614838

    271200	  0.087260	  0.087230	  0.086430		CURRENT LEARNING RATE: 0.03316874770565541
previous_iter_valid_loss : 0.07703503221273422

    271300	  0.076933	  0.077035	  0.086358		CURRENT LEARNING RATE: 0.03313559553679686
previous_iter_valid_loss : 0.08050502091646194

    271400	  0.080387	  0.080505	  0.086255		CURRENT LEARNING RATE: 0.03310247650353662
previous_iter_valid_loss : 0.08733750879764557

    271500	  0.087179	  0.087338	  0.086284		CURRENT LEARNING RATE: 0.033069390572755625
previous_iter_valid_loss : 0.08475305885076523

    271600	  0.084652	  0.084753	  0.086340		CURRENT LEARNING RATE: 0.03303633771136797
previous_iter_valid_loss : 0.08085360378026962

    271700	  0.080712	  0.080854	  0.086378		CURRENT LEARNING RATE: 0.03300331788632078
previous_iter_valid_loss : 0.08986018598079681

    271800	  0.089688	  0.089860	  0.086461		CURRENT LEARNING RATE: 0.03297033106459423
previous_iter_valid_loss : 0.08117309957742691

    271900	  0.080989	  0.081173	  0.086207		CURRENT LEARNING RATE: 0.032937377213201474
previous_iter_valid_loss : 0.07604563981294632


Current valid loss: 0.07604563981294632;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    272000	  0.075991	  0.076046	  0.086193		CURRENT LEARNING RATE: 0.03290445629918869
previous_iter_valid_loss : 0.09081847220659256

    272100	  0.090891	  0.090818	  0.086322		CURRENT LEARNING RATE: 0.03287156828963495
previous_iter_valid_loss : 0.08071800321340561

    272200	  0.080684	  0.080718	  0.086278		CURRENT LEARNING RATE: 0.03283871315165224
previous_iter_valid_loss : 0.13595744967460632

    272300	  0.135672	  0.135957	  0.086807		CURRENT LEARNING RATE: 0.0328058908523854
previous_iter_valid_loss : 0.1268470138311386

    272400	  0.126623	  0.126847	  0.087280		CURRENT LEARNING RATE: 0.032773101359012166
previous_iter_valid_loss : 0.08261220157146454

    272500	  0.082622	  0.082612	  0.086755		CURRENT LEARNING RATE: 0.032740344638743014
previous_iter_valid_loss : 0.07655445486307144

    272600	  0.076526	  0.076554	  0.086733		CURRENT LEARNING RATE: 0.03270762065882123
previous_iter_valid_loss : 0.0778266116976738

    272700	  0.077685	  0.077827	  0.086604		CURRENT LEARNING RATE: 0.032674929386522826
previous_iter_valid_loss : 0.07689273357391357

    272800	  0.076786	  0.076893	  0.086330		CURRENT LEARNING RATE: 0.03264227078915654
previous_iter_valid_loss : 0.07855603843927383

    272900	  0.078538	  0.078556	  0.086328		CURRENT LEARNING RATE: 0.03260964483406376
previous_iter_valid_loss : 0.08645334839820862

    273000	  0.086260	  0.086453	  0.086424		CURRENT LEARNING RATE: 0.03257705148861854
previous_iter_valid_loss : 0.08070310950279236

    273100	  0.080681	  0.080703	  0.086250		CURRENT LEARNING RATE: 0.0325444907202275
previous_iter_valid_loss : 0.08560404181480408

    273200	  0.085466	  0.085604	  0.086310		CURRENT LEARNING RATE: 0.03251196249632991
previous_iter_valid_loss : 0.08021031320095062

    273300	  0.080067	  0.080210	  0.086279		CURRENT LEARNING RATE: 0.032479466784397525
previous_iter_valid_loss : 0.0862959772348404

    273400	  0.086068	  0.086296	  0.086356		CURRENT LEARNING RATE: 0.03244700355193463
previous_iter_valid_loss : 0.10514149814844131

    273500	  0.104885	  0.105141	  0.086541		CURRENT LEARNING RATE: 0.03241457276647798
previous_iter_valid_loss : 0.08661716431379318

    273600	  0.086669	  0.086617	  0.086608		CURRENT LEARNING RATE: 0.03238217439559681
previous_iter_valid_loss : 0.07689846307039261

    273700	  0.076861	  0.076898	  0.086609		CURRENT LEARNING RATE: 0.032349808406892736
previous_iter_valid_loss : 0.0779862105846405

    273800	  0.077824	  0.077986	  0.086501		CURRENT LEARNING RATE: 0.03231747476799977
previous_iter_valid_loss : 0.08083155006170273

    273900	  0.080785	  0.080832	  0.086535		CURRENT LEARNING RATE: 0.032285173446584235
previous_iter_valid_loss : 0.07921869307756424

    274000	  0.079053	  0.079219	  0.086539		CURRENT LEARNING RATE: 0.03225290441034486
previous_iter_valid_loss : 0.07725939899682999

    274100	  0.077152	  0.077259	  0.086474		CURRENT LEARNING RATE: 0.03222066762701259
previous_iter_valid_loss : 0.0766574963927269

    274200	  0.076586	  0.076657	  0.085973		CURRENT LEARNING RATE: 0.03218846306435062
previous_iter_valid_loss : 0.07638717442750931

    274300	  0.076253	  0.076387	  0.085952		CURRENT LEARNING RATE: 0.03215629069015439
previous_iter_valid_loss : 0.09058396518230438

    274400	  0.090430	  0.090584	  0.086080		CURRENT LEARNING RATE: 0.03212415047225154
previous_iter_valid_loss : 0.07865110784769058

    274500	  0.078592	  0.078651	  0.086089		CURRENT LEARNING RATE: 0.03209204237850184
previous_iter_valid_loss : 0.0763486698269844

    274600	  0.076247	  0.076349	  0.085981		CURRENT LEARNING RATE: 0.0320599663767972
previous_iter_valid_loss : 0.07646913826465607

    274700	  0.076382	  0.076469	  0.085700		CURRENT LEARNING RATE: 0.032027922435061584
previous_iter_valid_loss : 0.08114240318536758

    274800	  0.080927	  0.081142	  0.085622		CURRENT LEARNING RATE: 0.03199591052125109
previous_iter_valid_loss : 0.07668640464544296

    274900	  0.076622	  0.076686	  0.085622		CURRENT LEARNING RATE: 0.031963930603353785
previous_iter_valid_loss : 0.07738105207681656

    275000	  0.077261	  0.077381	  0.085538		CURRENT LEARNING RATE: 0.03193198264938975
previous_iter_valid_loss : 0.09017471224069595

    275100	  0.090216	  0.090175	  0.085567		CURRENT LEARNING RATE: 0.03190006662741102
previous_iter_valid_loss : 0.0882558599114418

    275200	  0.088320	  0.088256	  0.085676		CURRENT LEARNING RATE: 0.03186818250550156
previous_iter_valid_loss : 0.07632021605968475

    275300	  0.076241	  0.076320	  0.085672		CURRENT LEARNING RATE: 0.03183633025177728
previous_iter_valid_loss : 0.078700952231884

    275400	  0.078564	  0.078701	  0.085675		CURRENT LEARNING RATE: 0.0318045098343859
previous_iter_valid_loss : 0.09240606427192688

    275500	  0.092435	  0.092406	  0.085818		CURRENT LEARNING RATE: 0.03177272122150701
previous_iter_valid_loss : 0.11681599915027618

    275600	  0.116563	  0.116816	  0.086215		CURRENT LEARNING RATE: 0.031740964381351974
previous_iter_valid_loss : 0.07762013375759125

    275700	  0.077579	  0.077620	  0.086194		CURRENT LEARNING RATE: 0.03170923928216398
previous_iter_valid_loss : 0.0761205181479454

    275800	  0.076040	  0.076121	  0.085335		CURRENT LEARNING RATE: 0.031677545892217905
previous_iter_valid_loss : 0.078104168176651

    275900	  0.077979	  0.078104	  0.085334		CURRENT LEARNING RATE: 0.031645884179820366
previous_iter_valid_loss : 0.0859140157699585

    276000	  0.085900	  0.085914	  0.085386		CURRENT LEARNING RATE: 0.031614254113309634
previous_iter_valid_loss : 0.0786256492137909

    276100	  0.078533	  0.078626	  0.085295		CURRENT LEARNING RATE: 0.031582655661055656
previous_iter_valid_loss : 0.09232234954833984

    276200	  0.092374	  0.092322	  0.085409		CURRENT LEARNING RATE: 0.03155108879145997
previous_iter_valid_loss : 0.09101693332195282

    276300	  0.090830	  0.091017	  0.085482		CURRENT LEARNING RATE: 0.031519553472955715
previous_iter_valid_loss : 0.07934662699699402

    276400	  0.079333	  0.079347	  0.085407		CURRENT LEARNING RATE: 0.031488049674007534
previous_iter_valid_loss : 0.0767873078584671

    276500	  0.076721	  0.076787	  0.085374		CURRENT LEARNING RATE: 0.03145657736311167
previous_iter_valid_loss : 0.08719170093536377

    276600	  0.086996	  0.087192	  0.085476		CURRENT LEARNING RATE: 0.031425136508795797
previous_iter_valid_loss : 0.0901808962225914

    276700	  0.090020	  0.090181	  0.085573		CURRENT LEARNING RATE: 0.031393727079619044
previous_iter_valid_loss : 0.0866456925868988

    276800	  0.086513	  0.086646	  0.085500		CURRENT LEARNING RATE: 0.031362349044171976
previous_iter_valid_loss : 0.09711789339780807

    276900	  0.097190	  0.097118	  0.085699		CURRENT LEARNING RATE: 0.031331002371076576
previous_iter_valid_loss : 0.07908037304878235

    277000	  0.078921	  0.079080	  0.085642		CURRENT LEARNING RATE: 0.03129968702898616
previous_iter_valid_loss : 0.08070380985736847

    277100	  0.080545	  0.080704	  0.085576		CURRENT LEARNING RATE: 0.031268402986585384
previous_iter_valid_loss : 0.0761164203286171

    277200	  0.076008	  0.076116	  0.085369		CURRENT LEARNING RATE: 0.03123715021259018
previous_iter_valid_loss : 0.08630315959453583

    277300	  0.086327	  0.086303	  0.085445		CURRENT LEARNING RATE: 0.03120592867574781
previous_iter_valid_loss : 0.0783180296421051

    277400	  0.078288	  0.078318	  0.085431		CURRENT LEARNING RATE: 0.031174738344836715
previous_iter_valid_loss : 0.08327675610780716

    277500	  0.083130	  0.083277	  0.085495		CURRENT LEARNING RATE: 0.031143579188666563
previous_iter_valid_loss : 0.07798915356397629

    277600	  0.077977	  0.077989	  0.085502		CURRENT LEARNING RATE: 0.03111245117607818
previous_iter_valid_loss : 0.08360323309898376

    277700	  0.083606	  0.083603	  0.085490		CURRENT LEARNING RATE: 0.031081354275943582
previous_iter_valid_loss : 0.07699542492628098

    277800	  0.076948	  0.076995	  0.085444		CURRENT LEARNING RATE: 0.03105028845716585
previous_iter_valid_loss : 0.0835212916135788

    277900	  0.083501	  0.083521	  0.085473		CURRENT LEARNING RATE: 0.03101925368867916
previous_iter_valid_loss : 0.07643954455852509

    278000	  0.076355	  0.076440	  0.085293		CURRENT LEARNING RATE: 0.030988249939448733
previous_iter_valid_loss : 0.0792766734957695

    278100	  0.079204	  0.079277	  0.085294		CURRENT LEARNING RATE: 0.030957277178470837
previous_iter_valid_loss : 0.07749134302139282

    278200	  0.077410	  0.077491	  0.085208		CURRENT LEARNING RATE: 0.030926335374772705
previous_iter_valid_loss : 0.12515372037887573

    278300	  0.125319	  0.125154	  0.085548		CURRENT LEARNING RATE: 0.030895424497412522
previous_iter_valid_loss : 0.07857611030340195

    278400	  0.078436	  0.078576	  0.085397		CURRENT LEARNING RATE: 0.030864544515479396
previous_iter_valid_loss : 0.07925695180892944

    278500	  0.079129	  0.079257	  0.085409		CURRENT LEARNING RATE: 0.030833695398093372
previous_iter_valid_loss : 0.07681390643119812

    278600	  0.076753	  0.076814	  0.085375		CURRENT LEARNING RATE: 0.030802877114405318
previous_iter_valid_loss : 0.08383144438266754

    278700	  0.083682	  0.083831	  0.085432		CURRENT LEARNING RATE: 0.030772089633596945
previous_iter_valid_loss : 0.07664082199335098

    278800	  0.076584	  0.076641	  0.085352		CURRENT LEARNING RATE: 0.03074133292488075
previous_iter_valid_loss : 0.08315922319889069

    278900	  0.082976	  0.083159	  0.085420		CURRENT LEARNING RATE: 0.030710606957500063
previous_iter_valid_loss : 0.07876333594322205

    279000	  0.078670	  0.078763	  0.085300		CURRENT LEARNING RATE: 0.03067991170072889
previous_iter_valid_loss : 0.07665956765413284

    279100	  0.076552	  0.076660	  0.085205		CURRENT LEARNING RATE: 0.030649247123871976
previous_iter_valid_loss : 0.0763259083032608

    279200	  0.076237	  0.076326	  0.085191		CURRENT LEARNING RATE: 0.030618613196264723
previous_iter_valid_loss : 0.0966193899512291

    279300	  0.096757	  0.096619	  0.085350		CURRENT LEARNING RATE: 0.030588009887273233
previous_iter_valid_loss : 0.07668903470039368

    279400	  0.076613	  0.076689	  0.085059		CURRENT LEARNING RATE: 0.03055743716629418
previous_iter_valid_loss : 0.08030521869659424

    279500	  0.080241	  0.080305	  0.084639		CURRENT LEARNING RATE: 0.03052689500275484
previous_iter_valid_loss : 0.07595418393611908


Current valid loss: 0.07595418393611908;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    279600	  0.075851	  0.075954	  0.084570		CURRENT LEARNING RATE: 0.03049638336611303
previous_iter_valid_loss : 0.0871729627251625

    279700	  0.086973	  0.087173	  0.084500		CURRENT LEARNING RATE: 0.030465902225857145
previous_iter_valid_loss : 0.0821947380900383

    279800	  0.082194	  0.082195	  0.084456		CURRENT LEARNING RATE: 0.030435451551506024
previous_iter_valid_loss : 0.07668714970350266

    279900	  0.076527	  0.076687	  0.084143		CURRENT LEARNING RATE: 0.030405031312608986
previous_iter_valid_loss : 0.07937213778495789

    280000	  0.079256	  0.079372	  0.084145		CURRENT LEARNING RATE: 0.030374641478745787
previous_iter_valid_loss : 0.12447618693113327

    280100	  0.124298	  0.124476	  0.084334		CURRENT LEARNING RATE: 0.03034428201952661
previous_iter_valid_loss : 0.08665001392364502

    280200	  0.086720	  0.086650	  0.084350		CURRENT LEARNING RATE: 0.03031395290459198
previous_iter_valid_loss : 0.08181947469711304

    280300	  0.081649	  0.081819	  0.084215		CURRENT LEARNING RATE: 0.030283654103612778
previous_iter_valid_loss : 0.09224054962396622

    280400	  0.092303	  0.092241	  0.084295		CURRENT LEARNING RATE: 0.030253385586290194
previous_iter_valid_loss : 0.07905155420303345

    280500	  0.079054	  0.079052	  0.084243		CURRENT LEARNING RATE: 0.03022314732235573
previous_iter_valid_loss : 0.08016915619373322

    280600	  0.080150	  0.080169	  0.084273		CURRENT LEARNING RATE: 0.030192939281571105
previous_iter_valid_loss : 0.07614053040742874

    280700	  0.076100	  0.076141	  0.083859		CURRENT LEARNING RATE: 0.030162761433728282
previous_iter_valid_loss : 0.07866481691598892

    280800	  0.078532	  0.078665	  0.083853		CURRENT LEARNING RATE: 0.030132613748649388
previous_iter_valid_loss : 0.0792359784245491

    280900	  0.079244	  0.079236	  0.083877		CURRENT LEARNING RATE: 0.03010249619618677
previous_iter_valid_loss : 0.0793854296207428

    281000	  0.079238	  0.079385	  0.083902		CURRENT LEARNING RATE: 0.030072408746222856
previous_iter_valid_loss : 0.09840060025453568

    281100	  0.098189	  0.098401	  0.084103		CURRENT LEARNING RATE: 0.030042351368670193
previous_iter_valid_loss : 0.10409210622310638

    281200	  0.104265	  0.104092	  0.084271		CURRENT LEARNING RATE: 0.030012324033471392
previous_iter_valid_loss : 0.07700085639953613

    281300	  0.076927	  0.077001	  0.084271		CURRENT LEARNING RATE: 0.029982326710599135
previous_iter_valid_loss : 0.07705756276845932

    281400	  0.076999	  0.077058	  0.084237		CURRENT LEARNING RATE: 0.02995235937005609
previous_iter_valid_loss : 0.07655788213014603

    281500	  0.076492	  0.076558	  0.084129		CURRENT LEARNING RATE: 0.029922421981874912
previous_iter_valid_loss : 0.08542953431606293

    281600	  0.085446	  0.085430	  0.084135		CURRENT LEARNING RATE: 0.029892514516118192
previous_iter_valid_loss : 0.07858125120401382

    281700	  0.078486	  0.078581	  0.084113		CURRENT LEARNING RATE: 0.029862636942878495
previous_iter_valid_loss : 0.09050773084163666

    281800	  0.090551	  0.090508	  0.084119		CURRENT LEARNING RATE: 0.02983278923227823
previous_iter_valid_loss : 0.08681328594684601

    281900	  0.086834	  0.086813	  0.084176		CURRENT LEARNING RATE: 0.029802971354469684
previous_iter_valid_loss : 0.09878206998109818

    282000	  0.098563	  0.098782	  0.084403		CURRENT LEARNING RATE: 0.02977318327963496
previous_iter_valid_loss : 0.09264196455478668

    282100	  0.092430	  0.092642	  0.084421		CURRENT LEARNING RATE: 0.02974342497798601
previous_iter_valid_loss : 0.08029145002365112

    282200	  0.080143	  0.080291	  0.084417		CURRENT LEARNING RATE: 0.02971369641976452
previous_iter_valid_loss : 0.07710418850183487

    282300	  0.077026	  0.077104	  0.083828		CURRENT LEARNING RATE: 0.029683997575241924
previous_iter_valid_loss : 0.07774551212787628

    282400	  0.077586	  0.077746	  0.083337		CURRENT LEARNING RATE: 0.02965432841471936
previous_iter_valid_loss : 0.07650549709796906

    282500	  0.076392	  0.076505	  0.083276		CURRENT LEARNING RATE: 0.0296246889085277
previous_iter_valid_loss : 0.0785554051399231

    282600	  0.078448	  0.078555	  0.083296		CURRENT LEARNING RATE: 0.029595079027027415
previous_iter_valid_loss : 0.08881080895662308

    282700	  0.088661	  0.088811	  0.083406		CURRENT LEARNING RATE: 0.029565498740608626
previous_iter_valid_loss : 0.07598519325256348

    282800	  0.075889	  0.075985	  0.083397		CURRENT LEARNING RATE: 0.029535948019691026
previous_iter_valid_loss : 0.0792076364159584

    282900	  0.079071	  0.079208	  0.083404		CURRENT LEARNING RATE: 0.02950642683472392
previous_iter_valid_loss : 0.0759284496307373


Current valid loss: 0.0759284496307373;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    283000	  0.075811	  0.075928	  0.083298		CURRENT LEARNING RATE: 0.02947693515618611
previous_iter_valid_loss : 0.07991638779640198

    283100	  0.079758	  0.079916	  0.083291		CURRENT LEARNING RATE: 0.02944747295458591
previous_iter_valid_loss : 0.10007672756910324

    283200	  0.100212	  0.100077	  0.083435		CURRENT LEARNING RATE: 0.029418040200461106
previous_iter_valid_loss : 0.10456764698028564

    283300	  0.104327	  0.104568	  0.083679		CURRENT LEARNING RATE: 0.029388636864378967
previous_iter_valid_loss : 0.077595554292202

    283400	  0.077512	  0.077596	  0.083592		CURRENT LEARNING RATE: 0.029359262916936142
previous_iter_valid_loss : 0.07741596549749374

    283500	  0.077252	  0.077416	  0.083315		CURRENT LEARNING RATE: 0.02932991832875868
previous_iter_valid_loss : 0.0849958136677742

    283600	  0.084993	  0.084996	  0.083298		CURRENT LEARNING RATE: 0.029300603070501977
previous_iter_valid_loss : 0.11101171374320984

    283700	  0.110794	  0.111012	  0.083640		CURRENT LEARNING RATE: 0.0292713171128508
previous_iter_valid_loss : 0.0820973739027977

    283800	  0.082128	  0.082097	  0.083681		CURRENT LEARNING RATE: 0.029242060426519174
previous_iter_valid_loss : 0.1004251092672348

    283900	  0.100240	  0.100425	  0.083877		CURRENT LEARNING RATE: 0.029212832982250414
previous_iter_valid_loss : 0.08341699838638306

    284000	  0.083408	  0.083417	  0.083919		CURRENT LEARNING RATE: 0.029183634750817058
previous_iter_valid_loss : 0.09866710007190704

    284100	  0.098766	  0.098667	  0.084133		CURRENT LEARNING RATE: 0.029154465703020896
previous_iter_valid_loss : 0.07944796979427338

    284200	  0.079286	  0.079448	  0.084161		CURRENT LEARNING RATE: 0.029125325809692865
previous_iter_valid_loss : 0.08372434228658676

    284300	  0.083517	  0.083724	  0.084234		CURRENT LEARNING RATE: 0.029096215041693074
previous_iter_valid_loss : 0.07907333225011826

    284400	  0.079036	  0.079073	  0.084119		CURRENT LEARNING RATE: 0.029067133369910732
previous_iter_valid_loss : 0.08960604667663574

    284500	  0.089411	  0.089606	  0.084228		CURRENT LEARNING RATE: 0.0290380807652642
previous_iter_valid_loss : 0.08476725220680237

    284600	  0.084766	  0.084767	  0.084313		CURRENT LEARNING RATE: 0.029009057198700852
previous_iter_valid_loss : 0.08660247176885605

    284700	  0.086628	  0.086602	  0.084414		CURRENT LEARNING RATE: 0.028980062641197117
previous_iter_valid_loss : 0.0781799778342247

    284800	  0.078028	  0.078180	  0.084384		CURRENT LEARNING RATE: 0.028951097063758428
previous_iter_valid_loss : 0.07731840759515762

    284900	  0.077160	  0.077318	  0.084391		CURRENT LEARNING RATE: 0.028922160437419228
previous_iter_valid_loss : 0.07883214205503464

    285000	  0.078687	  0.078832	  0.084405		CURRENT LEARNING RATE: 0.028893252733242877
previous_iter_valid_loss : 0.07843920588493347

    285100	  0.078356	  0.078439	  0.084288		CURRENT LEARNING RATE: 0.028864373922321666
previous_iter_valid_loss : 0.07652057707309723

    285200	  0.076425	  0.076521	  0.084170		CURRENT LEARNING RATE: 0.028835523975776767
previous_iter_valid_loss : 0.07825005799531937

    285300	  0.078182	  0.078250	  0.084190		CURRENT LEARNING RATE: 0.02880670286475826
previous_iter_valid_loss : 0.0810546725988388

    285400	  0.080876	  0.081055	  0.084213		CURRENT LEARNING RATE: 0.028777910560445024
previous_iter_valid_loss : 0.07587894052267075


Current valid loss: 0.07587894052267075;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    285500	  0.075774	  0.075879	  0.084048		CURRENT LEARNING RATE: 0.028749147034044742
previous_iter_valid_loss : 0.07908472418785095

    285600	  0.079026	  0.079085	  0.083671		CURRENT LEARNING RATE: 0.02872041225679388
previous_iter_valid_loss : 0.07635956257581711

    285700	  0.076240	  0.076360	  0.083658		CURRENT LEARNING RATE: 0.028691706199957676
previous_iter_valid_loss : 0.07745661586523056

    285800	  0.077356	  0.077457	  0.083671		CURRENT LEARNING RATE: 0.028663028834830066
previous_iter_valid_loss : 0.07590959221124649

    285900	  0.075813	  0.075910	  0.083649		CURRENT LEARNING RATE: 0.02863438013273368
previous_iter_valid_loss : 0.10195237398147583

    286000	  0.102109	  0.101952	  0.083810		CURRENT LEARNING RATE: 0.0286057600650198
previous_iter_valid_loss : 0.07687003165483475

    286100	  0.076816	  0.076870	  0.083792		CURRENT LEARNING RATE: 0.02857716860306838
previous_iter_valid_loss : 0.07633223384618759

    286200	  0.076260	  0.076332	  0.083632		CURRENT LEARNING RATE: 0.028548605718287944
previous_iter_valid_loss : 0.0762072205543518

    286300	  0.076131	  0.076207	  0.083484		CURRENT LEARNING RATE: 0.028520071382115608
previous_iter_valid_loss : 0.07838154584169388

    286400	  0.078291	  0.078382	  0.083475		CURRENT LEARNING RATE: 0.028491565566017018
previous_iter_valid_loss : 0.09156107902526855

    286500	  0.091618	  0.091561	  0.083622		CURRENT LEARNING RATE: 0.028463088241486377
previous_iter_valid_loss : 0.08019079267978668

    286600	  0.080155	  0.080191	  0.083552		CURRENT LEARNING RATE: 0.028434639380046353
previous_iter_valid_loss : 0.08910145610570908

    286700	  0.089117	  0.089101	  0.083542		CURRENT LEARNING RATE: 0.028406218953248078
previous_iter_valid_loss : 0.07620903104543686

    286800	  0.076108	  0.076209	  0.083437		CURRENT LEARNING RATE: 0.028377826932671112
previous_iter_valid_loss : 0.08246076107025146

    286900	  0.082450	  0.082461	  0.083291		CURRENT LEARNING RATE: 0.02834946328992345
previous_iter_valid_loss : 0.07921449840068817

    287000	  0.079068	  0.079214	  0.083292		CURRENT LEARNING RATE: 0.028321127996641448
previous_iter_valid_loss : 0.09460876882076263

    287100	  0.094422	  0.094609	  0.083431		CURRENT LEARNING RATE: 0.028292821024489802
previous_iter_valid_loss : 0.08855076879262924

    287200	  0.088364	  0.088551	  0.083555		CURRENT LEARNING RATE: 0.02826454234516152
previous_iter_valid_loss : 0.08394608646631241

    287300	  0.083993	  0.083946	  0.083532		CURRENT LEARNING RATE: 0.028236291930377955
previous_iter_valid_loss : 0.08491848409175873

    287400	  0.084709	  0.084918	  0.083598		CURRENT LEARNING RATE: 0.028208069751888675
previous_iter_valid_loss : 0.08692321926355362

    287500	  0.086736	  0.086923	  0.083634		CURRENT LEARNING RATE: 0.028179875781471495
previous_iter_valid_loss : 0.08774320781230927

    287600	  0.087820	  0.087743	  0.083732		CURRENT LEARNING RATE: 0.028151709990932444
previous_iter_valid_loss : 0.07772104442119598

    287700	  0.077573	  0.077721	  0.083673		CURRENT LEARNING RATE: 0.02812357235210572
previous_iter_valid_loss : 0.08068031817674637

    287800	  0.080472	  0.080680	  0.083710		CURRENT LEARNING RATE: 0.028095462836853703
previous_iter_valid_loss : 0.07658447325229645

    287900	  0.076569	  0.076584	  0.083640		CURRENT LEARNING RATE: 0.028067381417066863
previous_iter_valid_loss : 0.07643254846334457

    288000	  0.076390	  0.076433	  0.083640		CURRENT LEARNING RATE: 0.02803932806466378
previous_iter_valid_loss : 0.0855359360575676

    288100	  0.085333	  0.085536	  0.083703		CURRENT LEARNING RATE: 0.028011302751591086
previous_iter_valid_loss : 0.08366689085960388

    288200	  0.083540	  0.083667	  0.083765		CURRENT LEARNING RATE: 0.02798330544982349
previous_iter_valid_loss : 0.08036010712385178

    288300	  0.080264	  0.080360	  0.083317		CURRENT LEARNING RATE: 0.027955336131363678
previous_iter_valid_loss : 0.08689922839403152

    288400	  0.086752	  0.086899	  0.083400		CURRENT LEARNING RATE: 0.027927394768242325
previous_iter_valid_loss : 0.08161803334951401

    288500	  0.081650	  0.081618	  0.083424		CURRENT LEARNING RATE: 0.027899481332518055
previous_iter_valid_loss : 0.07751975208520889

    288600	  0.077413	  0.077520	  0.083431		CURRENT LEARNING RATE: 0.027871595796277453
previous_iter_valid_loss : 0.07622285932302475

    288700	  0.076118	  0.076223	  0.083355		CURRENT LEARNING RATE: 0.027843738131634974
previous_iter_valid_loss : 0.08311698585748672

    288800	  0.083110	  0.083117	  0.083419		CURRENT LEARNING RATE: 0.027815908310732943
previous_iter_valid_loss : 0.07679939270019531

    288900	  0.076714	  0.076799	  0.083356		CURRENT LEARNING RATE: 0.02778810630574153
previous_iter_valid_loss : 0.08027614653110504

    289000	  0.080136	  0.080276	  0.083371		CURRENT LEARNING RATE: 0.027760332088858752
previous_iter_valid_loss : 0.09202495217323303

    289100	  0.092162	  0.092025	  0.083525		CURRENT LEARNING RATE: 0.027732585632310375
previous_iter_valid_loss : 0.07774427533149719

    289200	  0.077702	  0.077744	  0.083539		CURRENT LEARNING RATE: 0.027704866908349942
previous_iter_valid_loss : 0.08319618552923203

    289300	  0.083238	  0.083196	  0.083404		CURRENT LEARNING RATE: 0.027677175889258714
previous_iter_valid_loss : 0.09356802701950073

    289400	  0.093406	  0.093568	  0.083573		CURRENT LEARNING RATE: 0.02764951254734569
previous_iter_valid_loss : 0.07628408074378967

    289500	  0.076259	  0.076284	  0.083533		CURRENT LEARNING RATE: 0.027621876854947523
previous_iter_valid_loss : 0.07593120634555817

    289600	  0.075902	  0.075931	  0.083533		CURRENT LEARNING RATE: 0.02759426878442851
previous_iter_valid_loss : 0.08657748252153397

    289700	  0.086423	  0.086577	  0.083527		CURRENT LEARNING RATE: 0.02756668830818057
previous_iter_valid_loss : 0.07634014636278152

    289800	  0.076294	  0.076340	  0.083468		CURRENT LEARNING RATE: 0.027539135398623245
previous_iter_valid_loss : 0.08471208065748215

    289900	  0.084800	  0.084712	  0.083549		CURRENT LEARNING RATE: 0.027511610028203615
previous_iter_valid_loss : 0.07644364982843399

    290000	  0.076380	  0.076444	  0.083519		CURRENT LEARNING RATE: 0.027484112169396308
previous_iter_valid_loss : 0.08418698608875275

    290100	  0.084262	  0.084187	  0.083116		CURRENT LEARNING RATE: 0.027456641794703446
previous_iter_valid_loss : 0.0773746594786644

    290200	  0.077362	  0.077375	  0.083024		CURRENT LEARNING RATE: 0.02742919887665468
previous_iter_valid_loss : 0.07761292159557343

    290300	  0.077479	  0.077613	  0.082982		CURRENT LEARNING RATE: 0.027401783387807077
previous_iter_valid_loss : 0.0860588401556015

    290400	  0.086137	  0.086059	  0.082920		CURRENT LEARNING RATE: 0.027374395300745143
previous_iter_valid_loss : 0.07813787460327148

    290500	  0.078044	  0.078138	  0.082911		CURRENT LEARNING RATE: 0.02734703458808078
previous_iter_valid_loss : 0.09253831952810287

    290600	  0.092639	  0.092538	  0.083034		CURRENT LEARNING RATE: 0.027319701222453297
previous_iter_valid_loss : 0.10456795245409012

    290700	  0.104378	  0.104568	  0.083319		CURRENT LEARNING RATE: 0.027292395176529313
previous_iter_valid_loss : 0.07712982594966888

    290800	  0.077020	  0.077130	  0.083303		CURRENT LEARNING RATE: 0.02726511642300278
previous_iter_valid_loss : 0.09815426915884018

    290900	  0.097915	  0.098154	  0.083492		CURRENT LEARNING RATE: 0.02723786493459493
previous_iter_valid_loss : 0.08612525463104248

    291000	  0.086173	  0.086125	  0.083560		CURRENT LEARNING RATE: 0.027210640684054294
previous_iter_valid_loss : 0.0838451161980629

    291100	  0.083643	  0.083845	  0.083414		CURRENT LEARNING RATE: 0.02718344364415661
previous_iter_valid_loss : 0.08333392441272736

    291200	  0.083367	  0.083334	  0.083207		CURRENT LEARNING RATE: 0.02715627378770484
previous_iter_valid_loss : 0.0759822353720665

    291300	  0.075892	  0.075982	  0.083196		CURRENT LEARNING RATE: 0.027129131087529106
previous_iter_valid_loss : 0.08701571822166443

    291400	  0.087082	  0.087016	  0.083296		CURRENT LEARNING RATE: 0.027102015516486732
previous_iter_valid_loss : 0.07999676465988159

    291500	  0.079993	  0.079997	  0.083330		CURRENT LEARNING RATE: 0.027074927047462134
previous_iter_valid_loss : 0.08389674127101898

    291600	  0.083731	  0.083897	  0.083315		CURRENT LEARNING RATE: 0.027047865653366837
previous_iter_valid_loss : 0.07725050300359726

    291700	  0.077222	  0.077251	  0.083302		CURRENT LEARNING RATE: 0.027020831307139438
previous_iter_valid_loss : 0.08386411517858505

    291800	  0.083700	  0.083864	  0.083235		CURRENT LEARNING RATE: 0.02699382398174561
previous_iter_valid_loss : 0.07789363712072372

    291900	  0.077771	  0.077894	  0.083146		CURRENT LEARNING RATE: 0.02696684365017801
previous_iter_valid_loss : 0.07605426758527756

    292000	  0.076014	  0.076054	  0.082919		CURRENT LEARNING RATE: 0.02693989028545631
previous_iter_valid_loss : 0.08155679702758789

    292100	  0.081575	  0.081557	  0.082808		CURRENT LEARNING RATE: 0.026912963860627127
previous_iter_valid_loss : 0.07649573683738708

    292200	  0.076445	  0.076496	  0.082770		CURRENT LEARNING RATE: 0.02688606434876406
previous_iter_valid_loss : 0.11375239491462708

    292300	  0.113954	  0.113752	  0.083137		CURRENT LEARNING RATE: 0.026859191722967583
previous_iter_valid_loss : 0.07701275497674942

    292400	  0.076957	  0.077013	  0.083129		CURRENT LEARNING RATE: 0.026832345956365068
previous_iter_valid_loss : 0.07601671665906906

    292500	  0.075899	  0.076017	  0.083124		CURRENT LEARNING RATE: 0.026805527022110733
previous_iter_valid_loss : 0.07601580023765564

    292600	  0.075954	  0.076016	  0.083099		CURRENT LEARNING RATE: 0.026778734893385663
previous_iter_valid_loss : 0.08021696656942368

    292700	  0.080247	  0.080217	  0.083013		CURRENT LEARNING RATE: 0.02675196954339772
previous_iter_valid_loss : 0.09544916450977325

    292800	  0.095540	  0.095449	  0.083208		CURRENT LEARNING RATE: 0.02672523094538155
previous_iter_valid_loss : 0.07639876008033752

    292900	  0.076303	  0.076399	  0.083180		CURRENT LEARNING RATE: 0.026698519072598542
previous_iter_valid_loss : 0.08643190562725067

    293000	  0.086445	  0.086432	  0.083285		CURRENT LEARNING RATE: 0.02667183389833684
previous_iter_valid_loss : 0.0796876922249794

    293100	  0.079522	  0.079688	  0.083282		CURRENT LEARNING RATE: 0.026645175395911262
previous_iter_valid_loss : 0.08663289248943329

    293200	  0.086382	  0.086633	  0.083148		CURRENT LEARNING RATE: 0.0266185435386633
previous_iter_valid_loss : 0.07638827711343765

    293300	  0.076360	  0.076388	  0.082866		CURRENT LEARNING RATE: 0.02659193829996108
previous_iter_valid_loss : 0.08114363253116608

    293400	  0.080976	  0.081144	  0.082902		CURRENT LEARNING RATE: 0.02656535965319939
previous_iter_valid_loss : 0.0756835862994194


Current valid loss: 0.0756835862994194;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    293500	  0.075596	  0.075684	  0.082884		CURRENT LEARNING RATE: 0.026538807571799567
previous_iter_valid_loss : 0.0782080814242363

    293600	  0.078052	  0.078208	  0.082816		CURRENT LEARNING RATE: 0.02651228202920953
previous_iter_valid_loss : 0.07642098516225815

    293700	  0.076320	  0.076421	  0.082470		CURRENT LEARNING RATE: 0.026485782998903713
previous_iter_valid_loss : 0.12378642708063126

    293800	  0.123561	  0.123786	  0.082887		CURRENT LEARNING RATE: 0.026459310454383118
previous_iter_valid_loss : 0.07619038969278336

    293900	  0.076095	  0.076190	  0.082645		CURRENT LEARNING RATE: 0.026432864369175184
previous_iter_valid_loss : 0.07629626989364624

    294000	  0.076213	  0.076296	  0.082574		CURRENT LEARNING RATE: 0.02640644471683382
previous_iter_valid_loss : 0.07717776298522949

    294100	  0.077150	  0.077178	  0.082359		CURRENT LEARNING RATE: 0.026380051470939362
previous_iter_valid_loss : 0.07686015963554382

    294200	  0.076803	  0.076860	  0.082333		CURRENT LEARNING RATE: 0.026353684605098585
previous_iter_valid_loss : 0.07762042433023453

    294300	  0.077559	  0.077620	  0.082272		CURRENT LEARNING RATE: 0.026327344092944606
previous_iter_valid_loss : 0.07853061705827713

    294400	  0.078433	  0.078531	  0.082267		CURRENT LEARNING RATE: 0.02630102990813692
previous_iter_valid_loss : 0.08187538385391235

    294500	  0.081697	  0.081875	  0.082189		CURRENT LEARNING RATE: 0.02627474202436132
previous_iter_valid_loss : 0.07903540879487991

    294600	  0.079024	  0.079035	  0.082132		CURRENT LEARNING RATE: 0.02624848041532994
previous_iter_valid_loss : 0.07724183797836304

    294700	  0.077220	  0.077242	  0.082038		CURRENT LEARNING RATE: 0.02622224505478117
previous_iter_valid_loss : 0.11136527359485626

    294800	  0.111113	  0.111365	  0.082370		CURRENT LEARNING RATE: 0.026196035916479638
previous_iter_valid_loss : 0.07623648643493652

    294900	  0.076136	  0.076236	  0.082359		CURRENT LEARNING RATE: 0.02616985297421619
previous_iter_valid_loss : 0.09676603227853775

    295000	  0.096556	  0.096766	  0.082539		CURRENT LEARNING RATE: 0.026143696201807915
previous_iter_valid_loss : 0.07569185644388199

    295100	  0.075605	  0.075692	  0.082511		CURRENT LEARNING RATE: 0.026117565573098016
previous_iter_valid_loss : 0.08500559628009796

    295200	  0.084813	  0.085006	  0.082596		CURRENT LEARNING RATE: 0.026091461061955867
previous_iter_valid_loss : 0.0964544340968132

    295300	  0.096224	  0.096454	  0.082778		CURRENT LEARNING RATE: 0.026065382642276945
previous_iter_valid_loss : 0.07597554475069046

    295400	  0.075908	  0.075976	  0.082727		CURRENT LEARNING RATE: 0.026039330287982845
previous_iter_valid_loss : 0.08200470358133316

    295500	  0.081803	  0.082005	  0.082789		CURRENT LEARNING RATE: 0.026013303973021207
previous_iter_valid_loss : 0.07570814341306686

    295600	  0.075615	  0.075708	  0.082755		CURRENT LEARNING RATE: 0.02598730367136571
previous_iter_valid_loss : 0.08441880345344543

    295700	  0.084445	  0.084419	  0.082835		CURRENT LEARNING RATE: 0.025961329357016037
previous_iter_valid_loss : 0.07590721547603607

    295800	  0.075792	  0.075907	  0.082820		CURRENT LEARNING RATE: 0.025935381003997893
previous_iter_valid_loss : 0.07695908099412918

    295900	  0.076817	  0.076959	  0.082830		CURRENT LEARNING RATE: 0.025909458586362916
previous_iter_valid_loss : 0.07741117477416992

    296000	  0.077354	  0.077411	  0.082585		CURRENT LEARNING RATE: 0.025883562078188687
previous_iter_valid_loss : 0.08858713507652283

    296100	  0.088706	  0.088587	  0.082702		CURRENT LEARNING RATE: 0.02585769145357868
previous_iter_valid_loss : 0.10001526772975922

    296200	  0.099814	  0.100015	  0.082939		CURRENT LEARNING RATE: 0.02583184668666229
previous_iter_valid_loss : 0.07751240581274033

    296300	  0.077397	  0.077512	  0.082952		CURRENT LEARNING RATE: 0.025806027751594744
previous_iter_valid_loss : 0.13049496710300446

    296400	  0.130251	  0.130495	  0.083473		CURRENT LEARNING RATE: 0.0257802346225571
previous_iter_valid_loss : 0.0754973515868187


Current valid loss: 0.0754973515868187;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    296500	  0.075414	  0.075497	  0.083313		CURRENT LEARNING RATE: 0.025754467273756212
previous_iter_valid_loss : 0.07884715497493744

    296600	  0.078822	  0.078847	  0.083299		CURRENT LEARNING RATE: 0.025728725679424757
previous_iter_valid_loss : 0.0822729840874672

    296700	  0.082052	  0.082273	  0.083231		CURRENT LEARNING RATE: 0.025703009813821127
previous_iter_valid_loss : 0.0948043167591095

    296800	  0.094578	  0.094804	  0.083417		CURRENT LEARNING RATE: 0.025677319651229453
previous_iter_valid_loss : 0.07869190722703934

    296900	  0.078560	  0.078692	  0.083379		CURRENT LEARNING RATE: 0.025651655165959554
previous_iter_valid_loss : 0.07885950803756714

    297000	  0.078867	  0.078860	  0.083376		CURRENT LEARNING RATE: 0.025626016332346974
previous_iter_valid_loss : 0.09272392839193344

    297100	  0.092506	  0.092724	  0.083357		CURRENT LEARNING RATE: 0.02560040312475286
previous_iter_valid_loss : 0.0900997519493103

    297200	  0.089887	  0.090100	  0.083372		CURRENT LEARNING RATE: 0.025574815517564006
previous_iter_valid_loss : 0.07623066753149033

    297300	  0.076131	  0.076231	  0.083295		CURRENT LEARNING RATE: 0.02554925348519279
previous_iter_valid_loss : 0.17911185324192047

    297400	  0.179480	  0.179112	  0.084237		CURRENT LEARNING RATE: 0.025523717002077197
previous_iter_valid_loss : 0.07768859714269638

    297500	  0.077658	  0.077689	  0.084145		CURRENT LEARNING RATE: 0.025498206042680733
previous_iter_valid_loss : 0.08283203095197678

    297600	  0.082821	  0.082832	  0.084096		CURRENT LEARNING RATE: 0.02547272058149244
previous_iter_valid_loss : 0.07714680582284927

    297700	  0.077006	  0.077147	  0.084090		CURRENT LEARNING RATE: 0.025447260593026835
previous_iter_valid_loss : 0.08419231325387955

    297800	  0.084033	  0.084192	  0.084125		CURRENT LEARNING RATE: 0.02542182605182396
previous_iter_valid_loss : 0.07601504772901535

    297900	  0.075935	  0.076015	  0.084119		CURRENT LEARNING RATE: 0.02539641693244925
previous_iter_valid_loss : 0.08090425282716751

    298000	  0.080695	  0.080904	  0.084164		CURRENT LEARNING RATE: 0.025371033209493594
previous_iter_valid_loss : 0.09071949124336243

    298100	  0.090511	  0.090719	  0.084216		CURRENT LEARNING RATE: 0.025345674857573247
previous_iter_valid_loss : 0.07848146557807922

    298200	  0.078432	  0.078481	  0.084164		CURRENT LEARNING RATE: 0.02532034185132988
previous_iter_valid_loss : 0.07810164242982864

    298300	  0.078098	  0.078102	  0.084141		CURRENT LEARNING RATE: 0.02529503416543048
previous_iter_valid_loss : 0.09577971696853638

    298400	  0.095873	  0.095780	  0.084230		CURRENT LEARNING RATE: 0.025269751774567348
previous_iter_valid_loss : 0.07709396630525589

    298500	  0.076967	  0.077094	  0.084185		CURRENT LEARNING RATE: 0.025244494653458086
previous_iter_valid_loss : 0.07833567261695862

    298600	  0.078180	  0.078336	  0.084193		CURRENT LEARNING RATE: 0.025219262776845594
previous_iter_valid_loss : 0.08045241236686707

    298700	  0.080289	  0.080452	  0.084235		CURRENT LEARNING RATE: 0.02519405611949798
previous_iter_valid_loss : 0.0776195302605629

    298800	  0.077482	  0.077620	  0.084180		CURRENT LEARNING RATE: 0.025168874656208585
previous_iter_valid_loss : 0.0804000049829483

    298900	  0.080190	  0.080400	  0.084216		CURRENT LEARNING RATE: 0.025143718361795932
previous_iter_valid_loss : 0.08001699298620224

    299000	  0.079823	  0.080017	  0.084214		CURRENT LEARNING RATE: 0.025118587211103747
previous_iter_valid_loss : 0.08098437637090683

    299100	  0.080942	  0.080984	  0.084103		CURRENT LEARNING RATE: 0.025093481179000867
previous_iter_valid_loss : 0.07861359417438507

    299200	  0.078553	  0.078614	  0.084112		CURRENT LEARNING RATE: 0.025068400240381258
previous_iter_valid_loss : 0.08811034262180328

    299300	  0.087871	  0.088110	  0.084161		CURRENT LEARNING RATE: 0.025043344370163964
previous_iter_valid_loss : 0.08015625923871994

    299400	  0.080138	  0.080156	  0.084027		CURRENT LEARNING RATE: 0.02501831354329314
previous_iter_valid_loss : 0.07961387187242508

    299500	  0.079614	  0.079614	  0.084060		CURRENT LEARNING RATE: 0.024993307734737947
previous_iter_valid_loss : 0.08326946943998337

    299600	  0.083244	  0.083269	  0.084134		CURRENT LEARNING RATE: 0.024968326919492568
previous_iter_valid_loss : 0.0771164521574974

    299700	  0.076960	  0.077116	  0.084039		CURRENT LEARNING RATE: 0.02494337107257618
previous_iter_valid_loss : 0.09090743958950043

    299800	  0.090703	  0.090907	  0.084185		CURRENT LEARNING RATE: 0.02491844016903295
previous_iter_valid_loss : 0.08010401576757431

    299900	  0.079960	  0.080104	  0.084139		CURRENT LEARNING RATE: 0.024893534183931972
previous_iter_valid_loss : 0.07595817744731903

    300000	  0.075844	  0.075958	  0.084134		CURRENT LEARNING RATE: 0.02486865309236725
previous_iter_valid_loss : 0.09379834681749344

    300100	  0.093863	  0.093798	  0.084230		CURRENT LEARNING RATE: 0.02484379686945769
previous_iter_valid_loss : 0.08062851428985596

    300200	  0.080630	  0.080629	  0.084263		CURRENT LEARNING RATE: 0.024818965490347063
previous_iter_valid_loss : 0.11351268738508224

    300300	  0.113736	  0.113513	  0.084622		CURRENT LEARNING RATE: 0.024794158930204
previous_iter_valid_loss : 0.12449931353330612

    300400	  0.124291	  0.124499	  0.085006		CURRENT LEARNING RATE: 0.02476937716422194
previous_iter_valid_loss : 0.07870800048112869

    300500	  0.078535	  0.078708	  0.085012		CURRENT LEARNING RATE: 0.024744620167619105
previous_iter_valid_loss : 0.07573531568050385

    300600	  0.075624	  0.075735	  0.084844		CURRENT LEARNING RATE: 0.024719887915638488
previous_iter_valid_loss : 0.08636901527643204

    300700	  0.086396	  0.086369	  0.084662		CURRENT LEARNING RATE: 0.024695180383547857
previous_iter_valid_loss : 0.07944945245981216

    300800	  0.079403	  0.079449	  0.084685		CURRENT LEARNING RATE: 0.02467049754663967
previous_iter_valid_loss : 0.07586687803268433

    300900	  0.075782	  0.075867	  0.084462		CURRENT LEARNING RATE: 0.024645839380231085
previous_iter_valid_loss : 0.08287734538316727

    301000	  0.082882	  0.082877	  0.084430		CURRENT LEARNING RATE: 0.024621205859663924
previous_iter_valid_loss : 0.07590591907501221

    301100	  0.075872	  0.075906	  0.084350		CURRENT LEARNING RATE: 0.02459659696030468
previous_iter_valid_loss : 0.08194247633218765

    301200	  0.081931	  0.081942	  0.084336		CURRENT LEARNING RATE: 0.024572012657544454
previous_iter_valid_loss : 0.07743539661169052

    301300	  0.077276	  0.077435	  0.084351		CURRENT LEARNING RATE: 0.024547452926798927
previous_iter_valid_loss : 0.08547163754701614

    301400	  0.085534	  0.085472	  0.084335		CURRENT LEARNING RATE: 0.024522917743508364
previous_iter_valid_loss : 0.07776138931512833

    301500	  0.077649	  0.077761	  0.084313		CURRENT LEARNING RATE: 0.0244984070831376
previous_iter_valid_loss : 0.08755599707365036

    301600	  0.087379	  0.087556	  0.084350		CURRENT LEARNING RATE: 0.024473920921175958
previous_iter_valid_loss : 0.10860715806484222

    301700	  0.108785	  0.108607	  0.084663		CURRENT LEARNING RATE: 0.02444945923313728
previous_iter_valid_loss : 0.0756627768278122

    301800	  0.075620	  0.075663	  0.084581		CURRENT LEARNING RATE: 0.02442502199455986
previous_iter_valid_loss : 0.07663843035697937

    301900	  0.076515	  0.076638	  0.084569		CURRENT LEARNING RATE: 0.02440060918100648
previous_iter_valid_loss : 0.08073654770851135

    302000	  0.080571	  0.080737	  0.084615		CURRENT LEARNING RATE: 0.024376220768064314
previous_iter_valid_loss : 0.07945409417152405

    302100	  0.079438	  0.079454	  0.084594		CURRENT LEARNING RATE: 0.024351856731344948
previous_iter_valid_loss : 0.0857367217540741

    302200	  0.085744	  0.085737	  0.084687		CURRENT LEARNING RATE: 0.024327517046484334
previous_iter_valid_loss : 0.07605490833520889

    302300	  0.075914	  0.076055	  0.084310		CURRENT LEARNING RATE: 0.024303201689142802
previous_iter_valid_loss : 0.07548584789037704


Current valid loss: 0.07548584789037704;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    302400	  0.075385	  0.075486	  0.084294		CURRENT LEARNING RATE: 0.024278910635004987
previous_iter_valid_loss : 0.07624485343694687

    302500	  0.076189	  0.076245	  0.084297		CURRENT LEARNING RATE: 0.024254643859779827
previous_iter_valid_loss : 0.09260747581720352

    302600	  0.092434	  0.092607	  0.084463		CURRENT LEARNING RATE: 0.024230401339200538
previous_iter_valid_loss : 0.08143066614866257

    302700	  0.081398	  0.081431	  0.084475		CURRENT LEARNING RATE: 0.02420618304902462
previous_iter_valid_loss : 0.07713732123374939

    302800	  0.077106	  0.077137	  0.084292		CURRENT LEARNING RATE: 0.024181988965033766
previous_iter_valid_loss : 0.07578916847705841

    302900	  0.075720	  0.075789	  0.084286		CURRENT LEARNING RATE: 0.024157819063033895
previous_iter_valid_loss : 0.08274193108081818

    303000	  0.082723	  0.082742	  0.084249		CURRENT LEARNING RATE: 0.024133673318855086
previous_iter_valid_loss : 0.09141193330287933

    303100	  0.091470	  0.091412	  0.084366		CURRENT LEARNING RATE: 0.02410955170835162
previous_iter_valid_loss : 0.09029322117567062

    303200	  0.090096	  0.090293	  0.084403		CURRENT LEARNING RATE: 0.024085454207401873
previous_iter_valid_loss : 0.07673899084329605

    303300	  0.076725	  0.076739	  0.084406		CURRENT LEARNING RATE: 0.024061380791908338
previous_iter_valid_loss : 0.07745949923992157

    303400	  0.077324	  0.077459	  0.084369		CURRENT LEARNING RATE: 0.02403733143779759
previous_iter_valid_loss : 0.07716726511716843

    303500	  0.077149	  0.077167	  0.084384		CURRENT LEARNING RATE: 0.024013306121020293
previous_iter_valid_loss : 0.08775699883699417

    303600	  0.087586	  0.087757	  0.084480		CURRENT LEARNING RATE: 0.023989304817551117
previous_iter_valid_loss : 0.07952465862035751

    303700	  0.079431	  0.079525	  0.084511		CURRENT LEARNING RATE: 0.02396532750338876
previous_iter_valid_loss : 0.07714342325925827

    303800	  0.077030	  0.077143	  0.084044		CURRENT LEARNING RATE: 0.02394137415455589
previous_iter_valid_loss : 0.07624050229787827

    303900	  0.076199	  0.076241	  0.084045		CURRENT LEARNING RATE: 0.023917444747099184
previous_iter_valid_loss : 0.07543335855007172


Current valid loss: 0.07543335855007172;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    304000	  0.075364	  0.075433	  0.084036		CURRENT LEARNING RATE: 0.023893539257089216
previous_iter_valid_loss : 0.07826574891805649

    304100	  0.078159	  0.078266	  0.084047		CURRENT LEARNING RATE: 0.023869657660620498
previous_iter_valid_loss : 0.0868062674999237

    304200	  0.086842	  0.086806	  0.084146		CURRENT LEARNING RATE: 0.023845799933811418
previous_iter_valid_loss : 0.07715734094381332

    304300	  0.077129	  0.077157	  0.084142		CURRENT LEARNING RATE: 0.023821966052804268
previous_iter_valid_loss : 0.07858522236347198

    304400	  0.078459	  0.078585	  0.084142		CURRENT LEARNING RATE: 0.02379815599376516
previous_iter_valid_loss : 0.09300871938467026

    304500	  0.093120	  0.093009	  0.084254		CURRENT LEARNING RATE: 0.023774369732884024
previous_iter_valid_loss : 0.08689122647047043

    304600	  0.086718	  0.086891	  0.084332		CURRENT LEARNING RATE: 0.023750607246374594
previous_iter_valid_loss : 0.07612721621990204

    304700	  0.076039	  0.076127	  0.084321		CURRENT LEARNING RATE: 0.0237268685104744
previous_iter_valid_loss : 0.08080392330884933

    304800	  0.080643	  0.080804	  0.084015		CURRENT LEARNING RATE: 0.023703153501444696
previous_iter_valid_loss : 0.11733854562044144

    304900	  0.117569	  0.117339	  0.084426		CURRENT LEARNING RATE: 0.023679462195570464
previous_iter_valid_loss : 0.08224157989025116

    305000	  0.082061	  0.082242	  0.084281		CURRENT LEARNING RATE: 0.023655794569160393
previous_iter_valid_loss : 0.07864189147949219

    305100	  0.078496	  0.078642	  0.084311		CURRENT LEARNING RATE: 0.023632150598546873
previous_iter_valid_loss : 0.07992129772901535

    305200	  0.079953	  0.079921	  0.084260		CURRENT LEARNING RATE: 0.02360853026008592
previous_iter_valid_loss : 0.08544861525297165

    305300	  0.085513	  0.085449	  0.084150		CURRENT LEARNING RATE: 0.023584933530157195
previous_iter_valid_loss : 0.07850492745637894

    305400	  0.078350	  0.078505	  0.084175		CURRENT LEARNING RATE: 0.023561360385163956
previous_iter_valid_loss : 0.07622278481721878

    305500	  0.076125	  0.076223	  0.084117		CURRENT LEARNING RATE: 0.023537810801533075
previous_iter_valid_loss : 0.09471803903579712

    305600	  0.094510	  0.094718	  0.084307		CURRENT LEARNING RATE: 0.02351428475571496
previous_iter_valid_loss : 0.07606763392686844

    305700	  0.075925	  0.076068	  0.084224		CURRENT LEARNING RATE: 0.023490782224183555
previous_iter_valid_loss : 0.07848583161830902

    305800	  0.078441	  0.078486	  0.084250		CURRENT LEARNING RATE: 0.023467303183436324
previous_iter_valid_loss : 0.08812452107667923

    305900	  0.087936	  0.088125	  0.084361		CURRENT LEARNING RATE: 0.023443847609994243
previous_iter_valid_loss : 0.08705402910709381

    306000	  0.086870	  0.087054	  0.084458		CURRENT LEARNING RATE: 0.023420415480401725
previous_iter_valid_loss : 0.08245889842510223

    306100	  0.082292	  0.082459	  0.084396		CURRENT LEARNING RATE: 0.02339700677122664
previous_iter_valid_loss : 0.09272418171167374

    306200	  0.092547	  0.092724	  0.084324		CURRENT LEARNING RATE: 0.023373621459060263
previous_iter_valid_loss : 0.08786387741565704

    306300	  0.087686	  0.087864	  0.084427		CURRENT LEARNING RATE: 0.023350259520517305
previous_iter_valid_loss : 0.07553993165493011

    306400	  0.075443	  0.075540	  0.083877		CURRENT LEARNING RATE: 0.023326920932235814
previous_iter_valid_loss : 0.07642506062984467

    306500	  0.076294	  0.076425	  0.083887		CURRENT LEARNING RATE: 0.0233036056708772
previous_iter_valid_loss : 0.07854647934436798

    306600	  0.078514	  0.078546	  0.083884		CURRENT LEARNING RATE: 0.023280313713126187
previous_iter_valid_loss : 0.0804484412074089

    306700	  0.080431	  0.080448	  0.083866		CURRENT LEARNING RATE: 0.023257045035690836
previous_iter_valid_loss : 0.08963081240653992

    306800	  0.089466	  0.089631	  0.083814		CURRENT LEARNING RATE: 0.02323379961530246
previous_iter_valid_loss : 0.07680346816778183

    306900	  0.076751	  0.076803	  0.083795		CURRENT LEARNING RATE: 0.023210577428715636
previous_iter_valid_loss : 0.07630151510238647

    307000	  0.076261	  0.076302	  0.083769		CURRENT LEARNING RATE: 0.023187378452708164
previous_iter_valid_loss : 0.07554470747709274

    307100	  0.075453	  0.075545	  0.083598		CURRENT LEARNING RATE: 0.023164202664081087
previous_iter_valid_loss : 0.07766745239496231

    307200	  0.077532	  0.077667	  0.083473		CURRENT LEARNING RATE: 0.023141050039658606
previous_iter_valid_loss : 0.09235794097185135

    307300	  0.092440	  0.092358	  0.083634		CURRENT LEARNING RATE: 0.023117920556288092
previous_iter_valid_loss : 0.07709542661905289

    307400	  0.077089	  0.077095	  0.082614		CURRENT LEARNING RATE: 0.02309481419084005
previous_iter_valid_loss : 0.09810739010572433

    307500	  0.098260	  0.098107	  0.082819		CURRENT LEARNING RATE: 0.023071730920208134
previous_iter_valid_loss : 0.08626262843608856

    307600	  0.086310	  0.086263	  0.082853		CURRENT LEARNING RATE: 0.02304867072130906
previous_iter_valid_loss : 0.0762069821357727

    307700	  0.076163	  0.076207	  0.082843		CURRENT LEARNING RATE: 0.023025633571082633
previous_iter_valid_loss : 0.07775208353996277

    307800	  0.077685	  0.077752	  0.082779		CURRENT LEARNING RATE: 0.02300261944649168
previous_iter_valid_loss : 0.07877584546804428

    307900	  0.078601	  0.078776	  0.082807		CURRENT LEARNING RATE: 0.022979628324522102
previous_iter_valid_loss : 0.07870184630155563

    308000	  0.078592	  0.078702	  0.082785		CURRENT LEARNING RATE: 0.022956660182182766
previous_iter_valid_loss : 0.0822700634598732

    308100	  0.082274	  0.082270	  0.082700		CURRENT LEARNING RATE: 0.02293371499650552
previous_iter_valid_loss : 0.07889986038208008

    308200	  0.078881	  0.078900	  0.082704		CURRENT LEARNING RATE: 0.022910792744545178
previous_iter_valid_loss : 0.10063061863183975

    308300	  0.100787	  0.100631	  0.082930		CURRENT LEARNING RATE: 0.022887893403379496
previous_iter_valid_loss : 0.08162016421556473

    308400	  0.081412	  0.081620	  0.082788		CURRENT LEARNING RATE: 0.022865016950109125
previous_iter_valid_loss : 0.07567927986383438

    308500	  0.075567	  0.075679	  0.082774		CURRENT LEARNING RATE: 0.02284216336185761
previous_iter_valid_loss : 0.07542572915554047


Current valid loss: 0.07542572915554047;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    308600	  0.075323	  0.075426	  0.082745		CURRENT LEARNING RATE: 0.02281933261577135
previous_iter_valid_loss : 0.07844268530607224

    308700	  0.078286	  0.078443	  0.082725		CURRENT LEARNING RATE: 0.022796524689019618
previous_iter_valid_loss : 0.08310512453317642

    308800	  0.082974	  0.083105	  0.082780		CURRENT LEARNING RATE: 0.022773739558794474
previous_iter_valid_loss : 0.07994559407234192

    308900	  0.079821	  0.079946	  0.082775		CURRENT LEARNING RATE: 0.022750977202310785
previous_iter_valid_loss : 0.08306858688592911

    309000	  0.082914	  0.083069	  0.082805		CURRENT LEARNING RATE: 0.022728237596806186
previous_iter_valid_loss : 0.08099837601184845

    309100	  0.080853	  0.080998	  0.082806		CURRENT LEARNING RATE: 0.02270552071954109
previous_iter_valid_loss : 0.09799530357122421

    309200	  0.098139	  0.097995	  0.082999		CURRENT LEARNING RATE: 0.0226828265477986
previous_iter_valid_loss : 0.09569087624549866

    309300	  0.095530	  0.095691	  0.083075		CURRENT LEARNING RATE: 0.022660155058884555
previous_iter_valid_loss : 0.09441899508237839

    309400	  0.094563	  0.094419	  0.083218		CURRENT LEARNING RATE: 0.022637506230127443
previous_iter_valid_loss : 0.09059759974479675

    309500	  0.090716	  0.090598	  0.083328		CURRENT LEARNING RATE: 0.02261488003887846
previous_iter_valid_loss : 0.1013977974653244

    309600	  0.101192	  0.101398	  0.083509		CURRENT LEARNING RATE: 0.0225922764625114
previous_iter_valid_loss : 0.0771198645234108

    309700	  0.077081	  0.077120	  0.083509		CURRENT LEARNING RATE: 0.02256969547842268
previous_iter_valid_loss : 0.08524380624294281

    309800	  0.085079	  0.085244	  0.083452		CURRENT LEARNING RATE: 0.022547137064031313
previous_iter_valid_loss : 0.07565194368362427

    309900	  0.075576	  0.075652	  0.083408		CURRENT LEARNING RATE: 0.0225246011967789
previous_iter_valid_loss : 0.08238238841295242

    310000	  0.082224	  0.082382	  0.083472		CURRENT LEARNING RATE: 0.022502087854129563
previous_iter_valid_loss : 0.07553824782371521

    310100	  0.075430	  0.075538	  0.083290		CURRENT LEARNING RATE: 0.02247959701356995
previous_iter_valid_loss : 0.07843827456235886

    310200	  0.078398	  0.078438	  0.083268		CURRENT LEARNING RATE: 0.022457128652609216
previous_iter_valid_loss : 0.08052301406860352

    310300	  0.080353	  0.080523	  0.082938		CURRENT LEARNING RATE: 0.022434682748779015
previous_iter_valid_loss : 0.07898392528295517

    310400	  0.078853	  0.078984	  0.082483		CURRENT LEARNING RATE: 0.022412259279633435
previous_iter_valid_loss : 0.13863980770111084

    310500	  0.138487	  0.138640	  0.083082		CURRENT LEARNING RATE: 0.022389858222749002
previous_iter_valid_loss : 0.0805145874619484

    310600	  0.080340	  0.080515	  0.083130		CURRENT LEARNING RATE: 0.022367479555724646
previous_iter_valid_loss : 0.08200943470001221

    310700	  0.081903	  0.082009	  0.083086		CURRENT LEARNING RATE: 0.02234512325618172
previous_iter_valid_loss : 0.08656548708677292

    310800	  0.086620	  0.086565	  0.083157		CURRENT LEARNING RATE: 0.02232278930176391
previous_iter_valid_loss : 0.07618960738182068

    310900	  0.076035	  0.076190	  0.083160		CURRENT LEARNING RATE: 0.022300477670137268
previous_iter_valid_loss : 0.08420407772064209

    311000	  0.084269	  0.084204	  0.083174		CURRENT LEARNING RATE: 0.02227818833899014
previous_iter_valid_loss : 0.09570343792438507

    311100	  0.095495	  0.095703	  0.083372		CURRENT LEARNING RATE: 0.02225592128603322
previous_iter_valid_loss : 0.07593999803066254

    311200	  0.075875	  0.075940	  0.083312		CURRENT LEARNING RATE: 0.02223367648899944
previous_iter_valid_loss : 0.08226559311151505

    311300	  0.082142	  0.082266	  0.083360		CURRENT LEARNING RATE: 0.022211453925643998
previous_iter_valid_loss : 0.07549721002578735

    311400	  0.075415	  0.075497	  0.083260		CURRENT LEARNING RATE: 0.022189253573744325
previous_iter_valid_loss : 0.07881259173154831

    311500	  0.078676	  0.078813	  0.083271		CURRENT LEARNING RATE: 0.022167075411100086
previous_iter_valid_loss : 0.11935216933488846

    311600	  0.119105	  0.119352	  0.083589		CURRENT LEARNING RATE: 0.022144919415533107
previous_iter_valid_loss : 0.07898497581481934

    311700	  0.078869	  0.078985	  0.083293		CURRENT LEARNING RATE: 0.022122785564887386
previous_iter_valid_loss : 0.0774056538939476

    311800	  0.077283	  0.077406	  0.083310		CURRENT LEARNING RATE: 0.022100673837029065
previous_iter_valid_loss : 0.08153744041919708

    311900	  0.081562	  0.081537	  0.083359		CURRENT LEARNING RATE: 0.02207858420984643
previous_iter_valid_loss : 0.07712872326374054

    312000	  0.077086	  0.077129	  0.083323		CURRENT LEARNING RATE: 0.022056516661249848
previous_iter_valid_loss : 0.08970914781093597

    312100	  0.089533	  0.089709	  0.083425		CURRENT LEARNING RATE: 0.022034471169171763
previous_iter_valid_loss : 0.08850763738155365

    312200	  0.088605	  0.088508	  0.083453		CURRENT LEARNING RATE: 0.022012447711566675
previous_iter_valid_loss : 0.08120772242546082

    312300	  0.081036	  0.081208	  0.083505		CURRENT LEARNING RATE: 0.021990446266411143
previous_iter_valid_loss : 0.07556576281785965

    312400	  0.075460	  0.075566	  0.083505		CURRENT LEARNING RATE: 0.02196846681170371
previous_iter_valid_loss : 0.08085109293460846

    312500	  0.080908	  0.080851	  0.083552		CURRENT LEARNING RATE: 0.02194650932546492
previous_iter_valid_loss : 0.10163713991641998

    312600	  0.101840	  0.101637	  0.083642		CURRENT LEARNING RATE: 0.02192457378573728
previous_iter_valid_loss : 0.07939495146274567

    312700	  0.079455	  0.079395	  0.083621		CURRENT LEARNING RATE: 0.021902660170585245
previous_iter_valid_loss : 0.0825890526175499

    312800	  0.082622	  0.082589	  0.083676		CURRENT LEARNING RATE: 0.021880768458095216
previous_iter_valid_loss : 0.09045682847499847

    312900	  0.090293	  0.090457	  0.083823		CURRENT LEARNING RATE: 0.02185889862637547
previous_iter_valid_loss : 0.09259684383869171

    313000	  0.092710	  0.092597	  0.083921		CURRENT LEARNING RATE: 0.02183705065355617
previous_iter_valid_loss : 0.07749960571527481

    313100	  0.077362	  0.077500	  0.083782		CURRENT LEARNING RATE: 0.021815224517789337
previous_iter_valid_loss : 0.07792729884386063

    313200	  0.077811	  0.077927	  0.083658		CURRENT LEARNING RATE: 0.021793420197248847
previous_iter_valid_loss : 0.07781844586133957

    313300	  0.077825	  0.077818	  0.083669		CURRENT LEARNING RATE: 0.02177163767013037
previous_iter_valid_loss : 0.07749728113412857

    313400	  0.077478	  0.077497	  0.083670		CURRENT LEARNING RATE: 0.021749876914651377
previous_iter_valid_loss : 0.0771595910191536

    313500	  0.077090	  0.077160	  0.083669		CURRENT LEARNING RATE: 0.021728137909051103
previous_iter_valid_loss : 0.08405382931232452

    313600	  0.084155	  0.084054	  0.083632		CURRENT LEARNING RATE: 0.021706420631590558
previous_iter_valid_loss : 0.07591001689434052

    313700	  0.075825	  0.075910	  0.083596		CURRENT LEARNING RATE: 0.021684725060552454
previous_iter_valid_loss : 0.10977234691381454

    313800	  0.109984	  0.109772	  0.083923		CURRENT LEARNING RATE: 0.021663051174241214
previous_iter_valid_loss : 0.08380681276321411

    313900	  0.083836	  0.083807	  0.083998		CURRENT LEARNING RATE: 0.021641398950982948
previous_iter_valid_loss : 0.0797128900885582

    314000	  0.079719	  0.079713	  0.084041		CURRENT LEARNING RATE: 0.021619768369125443
previous_iter_valid_loss : 0.08274216204881668

    314100	  0.082603	  0.082742	  0.084086		CURRENT LEARNING RATE: 0.02159815940703811
previous_iter_valid_loss : 0.08515078574419022

    314200	  0.084952	  0.085151	  0.084069		CURRENT LEARNING RATE: 0.021576572043111985
previous_iter_valid_loss : 0.08573001623153687

    314300	  0.085540	  0.085730	  0.084155		CURRENT LEARNING RATE: 0.021555006255759693
previous_iter_valid_loss : 0.07609176635742188

    314400	  0.076039	  0.076092	  0.084130		CURRENT LEARNING RATE: 0.02153346202341546
previous_iter_valid_loss : 0.091019406914711

    314500	  0.091103	  0.091019	  0.084110		CURRENT LEARNING RATE: 0.021511939324535045
previous_iter_valid_loss : 0.08244962990283966

    314600	  0.082481	  0.082450	  0.084066		CURRENT LEARNING RATE: 0.021490438137595748
previous_iter_valid_loss : 0.08611135929822922

    314700	  0.085953	  0.086111	  0.084166		CURRENT LEARNING RATE: 0.021468958441096368
previous_iter_valid_loss : 0.07700472325086594

    314800	  0.076891	  0.077005	  0.084128		CURRENT LEARNING RATE: 0.02144750021355723
previous_iter_valid_loss : 0.07727494835853577

    314900	  0.077230	  0.077275	  0.083727		CURRENT LEARNING RATE: 0.021426063433520093
previous_iter_valid_loss : 0.07590846717357635

    315000	  0.075797	  0.075908	  0.083664		CURRENT LEARNING RATE: 0.021404648079548172
previous_iter_valid_loss : 0.07976030558347702

    315100	  0.079742	  0.079760	  0.083675		CURRENT LEARNING RATE: 0.02138325413022611
previous_iter_valid_loss : 0.09086780995130539

    315200	  0.090695	  0.090868	  0.083784		CURRENT LEARNING RATE: 0.021361881564159965
previous_iter_valid_loss : 0.0838003158569336

    315300	  0.083630	  0.083800	  0.083768		CURRENT LEARNING RATE: 0.021340530359977166
previous_iter_valid_loss : 0.07906842976808548

    315400	  0.078912	  0.079068	  0.083773		CURRENT LEARNING RATE: 0.021319200496326504
previous_iter_valid_loss : 0.07695192843675613

    315500	  0.076880	  0.076952	  0.083781		CURRENT LEARNING RATE: 0.021297891951878107
previous_iter_valid_loss : 0.07567887753248215

    315600	  0.075575	  0.075679	  0.083590		CURRENT LEARNING RATE: 0.021276604705323447
previous_iter_valid_loss : 0.07536888122558594


Current valid loss: 0.07536888122558594;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    315700	  0.075261	  0.075369	  0.083583		CURRENT LEARNING RATE: 0.021255338735375263
previous_iter_valid_loss : 0.08319225162267685

    315800	  0.083222	  0.083192	  0.083630		CURRENT LEARNING RATE: 0.021234094020767588
previous_iter_valid_loss : 0.0757913589477539

    315900	  0.075694	  0.075791	  0.083507		CURRENT LEARNING RATE: 0.021212870540255693
previous_iter_valid_loss : 0.07676780968904495

    316000	  0.076627	  0.076768	  0.083404		CURRENT LEARNING RATE: 0.021191668272616114
previous_iter_valid_loss : 0.07557780295610428

    316100	  0.075487	  0.075578	  0.083335		CURRENT LEARNING RATE: 0.021170487196646572
previous_iter_valid_loss : 0.10281023383140564

    316200	  0.102977	  0.102810	  0.083436		CURRENT LEARNING RATE: 0.021149327291165997
previous_iter_valid_loss : 0.07743413746356964

    316300	  0.077287	  0.077434	  0.083332		CURRENT LEARNING RATE: 0.021128188535014462
previous_iter_valid_loss : 0.08221538364887238

    316400	  0.082055	  0.082215	  0.083399		CURRENT LEARNING RATE: 0.021107070907053233
previous_iter_valid_loss : 0.09408394247293472

    316500	  0.094225	  0.094084	  0.083575		CURRENT LEARNING RATE: 0.021085974386164667
previous_iter_valid_loss : 0.08865845948457718

    316600	  0.088738	  0.088658	  0.083676		CURRENT LEARNING RATE: 0.02106489895125225
previous_iter_valid_loss : 0.07720965892076492

    316700	  0.077073	  0.077210	  0.083644		CURRENT LEARNING RATE: 0.021043844581240527
previous_iter_valid_loss : 0.07565142959356308

    316800	  0.075569	  0.075651	  0.083504		CURRENT LEARNING RATE: 0.021022811255075147
previous_iter_valid_loss : 0.07621625810861588

    316900	  0.076122	  0.076216	  0.083498		CURRENT LEARNING RATE: 0.021001798951722776
previous_iter_valid_loss : 0.10970835387706757

    317000	  0.109901	  0.109708	  0.083832		CURRENT LEARNING RATE: 0.020980807650171105
previous_iter_valid_loss : 0.08067786693572998

    317100	  0.080494	  0.080678	  0.083884		CURRENT LEARNING RATE: 0.020959837329428826
previous_iter_valid_loss : 0.08297013491392136

    317200	  0.082985	  0.082970	  0.083937		CURRENT LEARNING RATE: 0.02093888796852563
previous_iter_valid_loss : 0.07536792010068893


Current valid loss: 0.07536792010068893;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    317300	  0.075271	  0.075368	  0.083767		CURRENT LEARNING RATE: 0.02091795954651215
previous_iter_valid_loss : 0.09024497121572495

    317400	  0.090362	  0.090245	  0.083898		CURRENT LEARNING RATE: 0.02089705204245996
previous_iter_valid_loss : 0.09761321544647217

    317500	  0.097701	  0.097613	  0.083893		CURRENT LEARNING RATE: 0.02087616543546154
previous_iter_valid_loss : 0.08191782981157303

    317600	  0.081757	  0.081918	  0.083850		CURRENT LEARNING RATE: 0.02085529970463031
previous_iter_valid_loss : 0.08100996166467667

    317700	  0.081011	  0.081010	  0.083898		CURRENT LEARNING RATE: 0.02083445482910052
previous_iter_valid_loss : 0.0877087339758873

    317800	  0.087501	  0.087709	  0.083998		CURRENT LEARNING RATE: 0.020813630788027292
previous_iter_valid_loss : 0.09409229457378387

    317900	  0.094159	  0.094092	  0.084151		CURRENT LEARNING RATE: 0.02079282756058658
previous_iter_valid_loss : 0.07592692226171494

    318000	  0.075794	  0.075927	  0.084123		CURRENT LEARNING RATE: 0.02077204512597517
previous_iter_valid_loss : 0.07675597071647644

    318100	  0.076704	  0.076756	  0.084068		CURRENT LEARNING RATE: 0.02075128346341062
previous_iter_valid_loss : 0.0791286900639534

    318200	  0.078995	  0.079129	  0.084070		CURRENT LEARNING RATE: 0.020730542552131262
previous_iter_valid_loss : 0.08261065930128098

    318300	  0.082447	  0.082611	  0.083890		CURRENT LEARNING RATE: 0.020709822371396173
previous_iter_valid_loss : 0.07655426114797592

    318400	  0.076526	  0.076554	  0.083839		CURRENT LEARNING RATE: 0.02068912290048519
previous_iter_valid_loss : 0.11059717833995819

    318500	  0.110827	  0.110597	  0.084189		CURRENT LEARNING RATE: 0.020668444118698833
previous_iter_valid_loss : 0.07604770362377167

    318600	  0.076018	  0.076048	  0.084195		CURRENT LEARNING RATE: 0.020647786005358316
previous_iter_valid_loss : 0.09980762004852295

    318700	  0.099943	  0.099808	  0.084408		CURRENT LEARNING RATE: 0.020627148539805514
previous_iter_valid_loss : 0.08080971986055374

    318800	  0.080836	  0.080810	  0.084385		CURRENT LEARNING RATE: 0.02060653170140298
previous_iter_valid_loss : 0.102690190076828

    318900	  0.102836	  0.102690	  0.084613		CURRENT LEARNING RATE: 0.02058593546953387
previous_iter_valid_loss : 0.07807864993810654

    319000	  0.077947	  0.078079	  0.084563		CURRENT LEARNING RATE: 0.020565359823601942
previous_iter_valid_loss : 0.07915899902582169

    319100	  0.079023	  0.079159	  0.084545		CURRENT LEARNING RATE: 0.02054480474303154
previous_iter_valid_loss : 0.07738860696554184

    319200	  0.077250	  0.077389	  0.084339		CURRENT LEARNING RATE: 0.020524270207267603
previous_iter_valid_loss : 0.07907890528440475

    319300	  0.079091	  0.079079	  0.084172		CURRENT LEARNING RATE: 0.020503756195775585
previous_iter_valid_loss : 0.07569700479507446

    319400	  0.075591	  0.075697	  0.083985		CURRENT LEARNING RATE: 0.020483262688041473
previous_iter_valid_loss : 0.07597614079713821

    319500	  0.075876	  0.075976	  0.083839		CURRENT LEARNING RATE: 0.020462789663571745
previous_iter_valid_loss : 0.07555738836526871

    319600	  0.075429	  0.075557	  0.083581		CURRENT LEARNING RATE: 0.020442337101893394
previous_iter_valid_loss : 0.07776245474815369

    319700	  0.077608	  0.077762	  0.083587		CURRENT LEARNING RATE: 0.02042190498255385
previous_iter_valid_loss : 0.07559327036142349

    319800	  0.075522	  0.075593	  0.083490		CURRENT LEARNING RATE: 0.02040149328512099
previous_iter_valid_loss : 0.0752255991101265


Current valid loss: 0.0752255991101265;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    319900	  0.075139	  0.075226	  0.083486		CURRENT LEARNING RATE: 0.020381101989183106
previous_iter_valid_loss : 0.07592983543872833

    320000	  0.075848	  0.075930	  0.083422		CURRENT LEARNING RATE: 0.020360731074348916
previous_iter_valid_loss : 0.09988929331302643

    320100	  0.100041	  0.099889	  0.083665		CURRENT LEARNING RATE: 0.0203403805202475
previous_iter_valid_loss : 0.07630115747451782

    320200	  0.076157	  0.076301	  0.083644		CURRENT LEARNING RATE: 0.0203200503065283
previous_iter_valid_loss : 0.0819394588470459

    320300	  0.081906	  0.081939	  0.083658		CURRENT LEARNING RATE: 0.02029974041286109
previous_iter_valid_loss : 0.09578303247690201

    320400	  0.095864	  0.095783	  0.083826		CURRENT LEARNING RATE: 0.020279450818935993
previous_iter_valid_loss : 0.08494265377521515

    320500	  0.085026	  0.084943	  0.083289		CURRENT LEARNING RATE: 0.020259181504463403
previous_iter_valid_loss : 0.09897619485855103

    320600	  0.099084	  0.098976	  0.083474		CURRENT LEARNING RATE: 0.020238932449174008
previous_iter_valid_loss : 0.08004401624202728

    320700	  0.079891	  0.080044	  0.083454		CURRENT LEARNING RATE: 0.02021870363281874
previous_iter_valid_loss : 0.0754372626543045

    320800	  0.075361	  0.075437	  0.083343		CURRENT LEARNING RATE: 0.0201984950351688
previous_iter_valid_loss : 0.07713408768177032

    320900	  0.076976	  0.077134	  0.083352		CURRENT LEARNING RATE: 0.020178306636015574
previous_iter_valid_loss : 0.09135598689317703

    321000	  0.091398	  0.091356	  0.083424		CURRENT LEARNING RATE: 0.020158138415170668
previous_iter_valid_loss : 0.07691574096679688

    321100	  0.076778	  0.076916	  0.083236		CURRENT LEARNING RATE: 0.02013799035246585
previous_iter_valid_loss : 0.07687017321586609

    321200	  0.076751	  0.076870	  0.083245		CURRENT LEARNING RATE: 0.02011786242775307
previous_iter_valid_loss : 0.08188890665769577

    321300	  0.081736	  0.081889	  0.083241		CURRENT LEARNING RATE: 0.020097754620904393
previous_iter_valid_loss : 0.07546380162239075

    321400	  0.075347	  0.075464	  0.083241		CURRENT LEARNING RATE: 0.020077666911812012
previous_iter_valid_loss : 0.07625127583742142

    321500	  0.076122	  0.076251	  0.083215		CURRENT LEARNING RATE: 0.020057599280388208
previous_iter_valid_loss : 0.07572740316390991

    321600	  0.075630	  0.075727	  0.082779		CURRENT LEARNING RATE: 0.020037551706565366
previous_iter_valid_loss : 0.07630512863397598

    321700	  0.076220	  0.076305	  0.082752		CURRENT LEARNING RATE: 0.020017524170295897
previous_iter_valid_loss : 0.0774620771408081

    321800	  0.077316	  0.077462	  0.082753		CURRENT LEARNING RATE: 0.01999751665155227
previous_iter_valid_loss : 0.07676729559898376

    321900	  0.076651	  0.076767	  0.082705		CURRENT LEARNING RATE: 0.019977529130326948
previous_iter_valid_loss : 0.08253396302461624

    322000	  0.082323	  0.082534	  0.082759		CURRENT LEARNING RATE: 0.019957561586632436
previous_iter_valid_loss : 0.09018915146589279

    322100	  0.090022	  0.090189	  0.082764		CURRENT LEARNING RATE: 0.019937614000501168
previous_iter_valid_loss : 0.07621518522500992

    322200	  0.076122	  0.076215	  0.082641		CURRENT LEARNING RATE: 0.019917686351985566
previous_iter_valid_loss : 0.0805436372756958

    322300	  0.080579	  0.080544	  0.082634		CURRENT LEARNING RATE: 0.019897778621157963
previous_iter_valid_loss : 0.09497813880443573

    322400	  0.094794	  0.094978	  0.082829		CURRENT LEARNING RATE: 0.019877890788110652
previous_iter_valid_loss : 0.07506971061229706


Current valid loss: 0.07506971061229706;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    322500	  0.074999	  0.075070	  0.082771		CURRENT LEARNING RATE: 0.019858022832955784
previous_iter_valid_loss : 0.08229438215494156

    322600	  0.082334	  0.082294	  0.082577		CURRENT LEARNING RATE: 0.0198381747358254
previous_iter_valid_loss : 0.07703670114278793

    322700	  0.076927	  0.077037	  0.082554		CURRENT LEARNING RATE: 0.0198183464768714
previous_iter_valid_loss : 0.07723632454872131

    322800	  0.077236	  0.077236	  0.082500		CURRENT LEARNING RATE: 0.01979853803626554
previous_iter_valid_loss : 0.07559233903884888

    322900	  0.075540	  0.075592	  0.082352		CURRENT LEARNING RATE: 0.019778749394199362
previous_iter_valid_loss : 0.07685864716768265

    323000	  0.076759	  0.076859	  0.082194		CURRENT LEARNING RATE: 0.019758980530884228
previous_iter_valid_loss : 0.076264888048172

    323100	  0.076136	  0.076265	  0.082182		CURRENT LEARNING RATE: 0.019739231426551263
previous_iter_valid_loss : 0.0762975811958313

    323200	  0.076244	  0.076298	  0.082166		CURRENT LEARNING RATE: 0.01971950206145138
previous_iter_valid_loss : 0.07860726863145828

    323300	  0.078399	  0.078607	  0.082173		CURRENT LEARNING RATE: 0.019699792415855195
previous_iter_valid_loss : 0.0783657506108284

    323400	  0.078226	  0.078366	  0.082182		CURRENT LEARNING RATE: 0.019680102470053074
previous_iter_valid_loss : 0.08005405962467194

    323500	  0.080083	  0.080054	  0.082211		CURRENT LEARNING RATE: 0.019660432204355052
previous_iter_valid_loss : 0.0775647833943367

    323600	  0.077421	  0.077565	  0.082146		CURRENT LEARNING RATE: 0.01964078159909088
previous_iter_valid_loss : 0.07512086629867554

    323700	  0.075042	  0.075121	  0.082138		CURRENT LEARNING RATE: 0.019621150634609945
previous_iter_valid_loss : 0.08562719076871872

    323800	  0.085690	  0.085627	  0.081897		CURRENT LEARNING RATE: 0.01960153929128128
previous_iter_valid_loss : 0.07631296664476395

    323900	  0.076278	  0.076313	  0.081822		CURRENT LEARNING RATE: 0.019581947549493533
previous_iter_valid_loss : 0.0829886943101883

    324000	  0.082820	  0.082989	  0.081855		CURRENT LEARNING RATE: 0.019562375389654975
previous_iter_valid_loss : 0.09487190842628479

    324100	  0.094993	  0.094872	  0.081976		CURRENT LEARNING RATE: 0.019542822792193434
previous_iter_valid_loss : 0.0755295604467392

    324200	  0.075503	  0.075530	  0.081880		CURRENT LEARNING RATE: 0.019523289737556317
previous_iter_valid_loss : 0.07834272086620331

    324300	  0.078344	  0.078343	  0.081806		CURRENT LEARNING RATE: 0.019503776206210556
previous_iter_valid_loss : 0.0968509316444397

    324400	  0.096682	  0.096851	  0.082013		CURRENT LEARNING RATE: 0.01948428217864263
previous_iter_valid_loss : 0.07713638246059418

    324500	  0.077018	  0.077136	  0.081875		CURRENT LEARNING RATE: 0.019464807635358513
previous_iter_valid_loss : 0.07810233533382416

    324600	  0.078079	  0.078102	  0.081831		CURRENT LEARNING RATE: 0.01944535255688365
previous_iter_valid_loss : 0.08020292967557907

    324700	  0.080210	  0.080203	  0.081772		CURRENT LEARNING RATE: 0.019425916923762956
previous_iter_valid_loss : 0.08297145366668701

    324800	  0.082795	  0.082971	  0.081832		CURRENT LEARNING RATE: 0.019406500716560814
previous_iter_valid_loss : 0.08071011304855347

    324900	  0.080560	  0.080710	  0.081866		CURRENT LEARNING RATE: 0.019387103915861004
previous_iter_valid_loss : 0.08929409086704254

    325000	  0.089446	  0.089294	  0.082000		CURRENT LEARNING RATE: 0.019367726502266727
previous_iter_valid_loss : 0.08250197023153305

    325100	  0.082284	  0.082502	  0.082027		CURRENT LEARNING RATE: 0.019348368456400568
previous_iter_valid_loss : 0.08020386844873428

    325200	  0.080172	  0.080204	  0.081921		CURRENT LEARNING RATE: 0.019329029758904465
previous_iter_valid_loss : 0.07708635926246643

    325300	  0.077049	  0.077086	  0.081854		CURRENT LEARNING RATE: 0.019309710390439744
previous_iter_valid_loss : 0.09044633060693741

    325400	  0.090218	  0.090446	  0.081967		CURRENT LEARNING RATE: 0.01929041033168702
previous_iter_valid_loss : 0.0781395360827446

    325500	  0.077984	  0.078140	  0.081979		CURRENT LEARNING RATE: 0.019271129563346236
previous_iter_valid_loss : 0.08718129992485046

    325600	  0.087260	  0.087181	  0.082094		CURRENT LEARNING RATE: 0.019251868066136612
previous_iter_valid_loss : 0.08416351675987244

    325700	  0.084212	  0.084164	  0.082182		CURRENT LEARNING RATE: 0.01923262582079667
previous_iter_valid_loss : 0.07826027274131775

    325800	  0.078104	  0.078260	  0.082133		CURRENT LEARNING RATE: 0.01921340280808415
previous_iter_valid_loss : 0.07698594778776169

    325900	  0.076903	  0.076986	  0.082145		CURRENT LEARNING RATE: 0.019194199008776038
previous_iter_valid_loss : 0.09661270678043365

    326000	  0.096416	  0.096613	  0.082343		CURRENT LEARNING RATE: 0.019175014403668526
previous_iter_valid_loss : 0.08313507586717606

    326100	  0.083153	  0.083135	  0.082419		CURRENT LEARNING RATE: 0.019155848973577024
previous_iter_valid_loss : 0.07666928321123123

    326200	  0.076605	  0.076669	  0.082157		CURRENT LEARNING RATE: 0.01913670269933609
previous_iter_valid_loss : 0.10167519748210907

    326300	  0.101833	  0.101675	  0.082400		CURRENT LEARNING RATE: 0.019117575561799455
previous_iter_valid_loss : 0.09115829318761826

    326400	  0.091257	  0.091158	  0.082489		CURRENT LEARNING RATE: 0.019098467541839963
previous_iter_valid_loss : 0.07553895562887192

    326500	  0.075502	  0.075539	  0.082304		CURRENT LEARNING RATE: 0.019079378620349613
previous_iter_valid_loss : 0.07614950090646744

    326600	  0.076077	  0.076150	  0.082179		CURRENT LEARNING RATE: 0.019060308778239474
previous_iter_valid_loss : 0.07720951735973358

    326700	  0.077179	  0.077210	  0.082179		CURRENT LEARNING RATE: 0.019041257996439704
previous_iter_valid_loss : 0.08711361885070801

    326800	  0.086914	  0.087114	  0.082293		CURRENT LEARNING RATE: 0.019022226255899506
previous_iter_valid_loss : 0.07692014425992966

    326900	  0.076831	  0.076920	  0.082300		CURRENT LEARNING RATE: 0.019003213537587157
previous_iter_valid_loss : 0.08110949397087097

    327000	  0.081130	  0.081109	  0.082014		CURRENT LEARNING RATE: 0.01898421982248993
previous_iter_valid_loss : 0.07554939389228821

    327100	  0.075452	  0.075549	  0.081963		CURRENT LEARNING RATE: 0.018965245091614107
previous_iter_valid_loss : 0.07519116252660751

    327200	  0.075113	  0.075191	  0.081885		CURRENT LEARNING RATE: 0.01894628932598495
previous_iter_valid_loss : 0.0756106972694397

    327300	  0.075521	  0.075611	  0.081888		CURRENT LEARNING RATE: 0.018927352506646705
previous_iter_valid_loss : 0.07661725580692291

    327400	  0.076614	  0.076617	  0.081751		CURRENT LEARNING RATE: 0.01890843461466254
previous_iter_valid_loss : 0.09604688733816147

    327500	  0.095863	  0.096047	  0.081736		CURRENT LEARNING RATE: 0.01888953563111457
previous_iter_valid_loss : 0.07688365131616592

    327600	  0.076783	  0.076884	  0.081685		CURRENT LEARNING RATE: 0.018870655537103796
previous_iter_valid_loss : 0.09539009630680084

    327700	  0.095220	  0.095390	  0.081829		CURRENT LEARNING RATE: 0.018851794313750142
previous_iter_valid_loss : 0.07544974237680435

    327800	  0.075384	  0.075450	  0.081707		CURRENT LEARNING RATE: 0.01883295194219237
previous_iter_valid_loss : 0.07672735303640366

    327900	  0.076622	  0.076727	  0.081533		CURRENT LEARNING RATE: 0.01881412840358811
previous_iter_valid_loss : 0.07659643888473511

    328000	  0.076550	  0.076596	  0.081540		CURRENT LEARNING RATE: 0.018795323679113813
previous_iter_valid_loss : 0.07548338919878006

    328100	  0.075372	  0.075483	  0.081527		CURRENT LEARNING RATE: 0.01877653774996477
previous_iter_valid_loss : 0.07617828249931335

    328200	  0.076024	  0.076178	  0.081498		CURRENT LEARNING RATE: 0.01875777059735504
previous_iter_valid_loss : 0.07534195482730865

    328300	  0.075283	  0.075342	  0.081425		CURRENT LEARNING RATE: 0.018739022202517473
previous_iter_valid_loss : 0.07519848644733429

    328400	  0.075114	  0.075198	  0.081411		CURRENT LEARNING RATE: 0.01872029254670366
previous_iter_valid_loss : 0.07999023795127869

    328500	  0.079831	  0.079990	  0.081105		CURRENT LEARNING RATE: 0.018701581611183963
previous_iter_valid_loss : 0.07590997964143753

    328600	  0.075793	  0.075910	  0.081104		CURRENT LEARNING RATE: 0.018682889377247436
previous_iter_valid_loss : 0.07719462364912033

    328700	  0.077139	  0.077195	  0.080878		CURRENT LEARNING RATE: 0.01866421582620184
previous_iter_valid_loss : 0.07533453404903412

    328800	  0.075269	  0.075335	  0.080823		CURRENT LEARNING RATE: 0.018645560939373623
previous_iter_valid_loss : 0.07580719888210297

    328900	  0.075758	  0.075807	  0.080554		CURRENT LEARNING RATE: 0.018626924698107904
previous_iter_valid_loss : 0.08300168067216873

    329000	  0.082842	  0.083002	  0.080603		CURRENT LEARNING RATE: 0.018608307083768434
previous_iter_valid_loss : 0.07740119099617004

    329100	  0.077367	  0.077401	  0.080586		CURRENT LEARNING RATE: 0.0185897080777376
previous_iter_valid_loss : 0.07671691477298737

    329200	  0.076615	  0.076717	  0.080579		CURRENT LEARNING RATE: 0.018571127661416387
previous_iter_valid_loss : 0.0824679359793663

    329300	  0.082314	  0.082468	  0.080613		CURRENT LEARNING RATE: 0.018552565816224387
previous_iter_valid_loss : 0.07591642439365387

    329400	  0.075889	  0.075916	  0.080615		CURRENT LEARNING RATE: 0.01853402252359975
previous_iter_valid_loss : 0.07729793339967728

    329500	  0.077264	  0.077298	  0.080628		CURRENT LEARNING RATE: 0.018515497764999184
previous_iter_valid_loss : 0.07989892363548279

    329600	  0.079900	  0.079899	  0.080672		CURRENT LEARNING RATE: 0.018496991521897918
previous_iter_valid_loss : 0.11078107357025146

    329700	  0.111026	  0.110781	  0.081002		CURRENT LEARNING RATE: 0.01847850377578972
previous_iter_valid_loss : 0.09406345337629318

    329800	  0.093912	  0.094063	  0.081187		CURRENT LEARNING RATE: 0.01846003450818684
previous_iter_valid_loss : 0.07542584836483002

    329900	  0.075359	  0.075426	  0.081189		CURRENT LEARNING RATE: 0.018441583700620007
previous_iter_valid_loss : 0.07785629481077194

    330000	  0.077800	  0.077856	  0.081208		CURRENT LEARNING RATE: 0.018423151334638403
previous_iter_valid_loss : 0.08402904868125916

    330100	  0.084108	  0.084029	  0.081049		CURRENT LEARNING RATE: 0.018404737391809676
previous_iter_valid_loss : 0.09574497491121292

    330200	  0.095634	  0.095745	  0.081244		CURRENT LEARNING RATE: 0.018386341853719873
previous_iter_valid_loss : 0.07789555937051773

    330300	  0.077755	  0.077896	  0.081203		CURRENT LEARNING RATE: 0.01836796470197346
previous_iter_valid_loss : 0.09935101866722107

    330400	  0.099564	  0.099351	  0.081239		CURRENT LEARNING RATE: 0.018349605918193266
previous_iter_valid_loss : 0.10957106202840805

    330500	  0.109418	  0.109571	  0.081485		CURRENT LEARNING RATE: 0.01833126548402053
previous_iter_valid_loss : 0.07605116069316864

    330600	  0.075934	  0.076051	  0.081256		CURRENT LEARNING RATE: 0.018312943381114808
previous_iter_valid_loss : 0.07510332763195038

    330700	  0.075049	  0.075103	  0.081207		CURRENT LEARNING RATE: 0.018294639591153992
previous_iter_valid_loss : 0.08863070607185364

    330800	  0.088730	  0.088631	  0.081339		CURRENT LEARNING RATE: 0.018276354095834283
previous_iter_valid_loss : 0.07777472585439682

    330900	  0.077777	  0.077775	  0.081345		CURRENT LEARNING RATE: 0.0182580868768702
previous_iter_valid_loss : 0.08536724001169205

    331000	  0.085252	  0.085367	  0.081285		CURRENT LEARNING RATE: 0.018239837915994518
previous_iter_valid_loss : 0.07782953232526779

    331100	  0.077745	  0.077830	  0.081294		CURRENT LEARNING RATE: 0.01822160719495827
previous_iter_valid_loss : 0.07695282995700836

    331200	  0.076860	  0.076953	  0.081295		CURRENT LEARNING RATE: 0.018203394695530728
previous_iter_valid_loss : 0.08113053441047668

    331300	  0.081037	  0.081131	  0.081287		CURRENT LEARNING RATE: 0.018185200399499404
previous_iter_valid_loss : 0.07766276597976685

    331400	  0.077583	  0.077663	  0.081309		CURRENT LEARNING RATE: 0.018167024288669998
previous_iter_valid_loss : 0.0756572037935257

    331500	  0.075625	  0.075657	  0.081303		CURRENT LEARNING RATE: 0.018148866344866392
previous_iter_valid_loss : 0.0760037824511528

    331600	  0.075970	  0.076004	  0.081306		CURRENT LEARNING RATE: 0.01813072654993064
previous_iter_valid_loss : 0.07649070024490356

    331700	  0.076520	  0.076491	  0.081308		CURRENT LEARNING RATE: 0.018112604885722954
previous_iter_valid_loss : 0.0763484314084053

    331800	  0.076271	  0.076348	  0.081297		CURRENT LEARNING RATE: 0.01809450133412166
previous_iter_valid_loss : 0.07853756844997406

    331900	  0.078547	  0.078538	  0.081315		CURRENT LEARNING RATE: 0.018076415877023213
previous_iter_valid_loss : 0.07579703629016876

    332000	  0.075759	  0.075797	  0.081247		CURRENT LEARNING RATE: 0.01805834849634214
previous_iter_valid_loss : 0.09906835854053497

    332100	  0.098942	  0.099068	  0.081336		CURRENT LEARNING RATE: 0.018040299174011076
previous_iter_valid_loss : 0.08717379719018936

    332200	  0.087334	  0.087174	  0.081446		CURRENT LEARNING RATE: 0.01802226789198069
previous_iter_valid_loss : 0.08369262516498566

    332300	  0.083793	  0.083693	  0.081477		CURRENT LEARNING RATE: 0.018004254632219694
previous_iter_valid_loss : 0.07862243801355362

    332400	  0.078642	  0.078622	  0.081314		CURRENT LEARNING RATE: 0.017986259376714827
previous_iter_valid_loss : 0.0776139497756958

    332500	  0.077539	  0.077614	  0.081339		CURRENT LEARNING RATE: 0.01796828210747084
previous_iter_valid_loss : 0.07717660069465637

    332600	  0.077165	  0.077177	  0.081288		CURRENT LEARNING RATE: 0.01795032280651046
previous_iter_valid_loss : 0.07529406249523163

    332700	  0.075285	  0.075294	  0.081270		CURRENT LEARNING RATE: 0.01793238145587438
previous_iter_valid_loss : 0.08113822340965271

    332800	  0.081198	  0.081138	  0.081309		CURRENT LEARNING RATE: 0.017914458037621248
previous_iter_valid_loss : 0.09138014167547226

    332900	  0.091266	  0.091380	  0.081467		CURRENT LEARNING RATE: 0.01789655253382765
previous_iter_valid_loss : 0.07503334432840347


Current valid loss: 0.07503334432840347;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    333000	  0.075029	  0.075033	  0.081449		CURRENT LEARNING RATE: 0.017878664926588076
previous_iter_valid_loss : 0.10218968987464905

    333100	  0.102390	  0.102190	  0.081708		CURRENT LEARNING RATE: 0.017860795198014923
previous_iter_valid_loss : 0.07580143958330154

    333200	  0.075817	  0.075801	  0.081703		CURRENT LEARNING RATE: 0.017842943330238444
previous_iter_valid_loss : 0.08245601505041122

    333300	  0.082576	  0.082456	  0.081742		CURRENT LEARNING RATE: 0.017825109305406792
previous_iter_valid_loss : 0.07831920683383942

    333400	  0.078206	  0.078319	  0.081741		CURRENT LEARNING RATE: 0.01780729310568593
previous_iter_valid_loss : 0.08759485930204391

    333500	  0.087730	  0.087595	  0.081817		CURRENT LEARNING RATE: 0.01778949471325966
previous_iter_valid_loss : 0.08935720473527908

    333600	  0.089465	  0.089357	  0.081935		CURRENT LEARNING RATE: 0.017771714110329576
previous_iter_valid_loss : 0.08598217368125916

    333700	  0.085868	  0.085982	  0.082043		CURRENT LEARNING RATE: 0.017753951279115093
previous_iter_valid_loss : 0.07540348172187805

    333800	  0.075344	  0.075403	  0.081941		CURRENT LEARNING RATE: 0.017736206201853365
previous_iter_valid_loss : 0.0754043459892273

    333900	  0.075405	  0.075404	  0.081932		CURRENT LEARNING RATE: 0.01771847886079932
previous_iter_valid_loss : 0.07493618875741959


Current valid loss: 0.07493618875741959;  saved better model at /home/ali/Desktop/Pulled_Github_Repositories/torchQN/JupyterBook/Cluster/TRAIN/trained_models/Trained_IQNx4_RecoDatapT_3_layer16_hiddenLeakyReLU_activation1024_batchsize2000_Kiteration.dict
    334000	  0.074944	  0.074936	  0.081852		CURRENT LEARNING RATE: 0.017700769238225604
previous_iter_valid_loss : 0.07693131268024445

    334100	  0.076906	  0.076931	  0.081672		CURRENT LEARNING RATE: 0.01768307731642261
previous_iter_valid_loss : 0.07642513513565063

    334200	  0.076396	  0.076425	  0.081681		CURRENT LEARNING RATE: 0.017665403077698403
previous_iter_valid_loss : 0.08280011266469955

    334300	  0.082917	  0.082800	  0.081726		CURRENT LEARNING RATE: 0.017647746504378746
previous_iter_valid_loss : 0.08263523876667023

    334400	  0.082730	  0.082635	  0.081583		CURRENT LEARNING RATE: 0.01763010757880706
previous_iter_valid_loss : 0.07926557958126068

    334500	  0.079184	  0.079266	  0.081605		CURRENT LEARNING RATE: 0.017612486283344428
previous_iter_valid_loss : 0.07520284503698349

    334600	  0.075203	  0.075203	  0.081576		CURRENT LEARNING RATE: 0.017594882600369545
previous_iter_valid_loss : 0.08174343407154083

    334700	  0.081663	  0.081743	  0.081591		CURRENT LEARNING RATE: 0.01757729651227873
previous_iter_valid_loss : 0.07562746852636337

    334800	  0.075658	  0.075627	  0.081518		CURRENT LEARNING RATE: 0.017559728001485884
previous_iter_valid_loss : 0.07874248176813126

    334900	  0.078750	  0.078742	  0.081498		CURRENT LEARNING RATE: 0.017542177050422512
previous_iter_valid_loss : 0.08284333348274231

    335000	  0.082959	  0.082843	  0.081434		CURRENT LEARNING RATE: 0.017524643641537652
previous_iter_valid_loss : 0.07763353735208511

    335100	  0.077555	  0.077634	  0.081385		CURRENT LEARNING RATE: 0.017507127757297892
previous_iter_valid_loss : 0.12923945486545563

    335200	  0.129126	  0.129239	  0.081875		CURRENT LEARNING RATE: 0.017489629380187343
previous_iter_valid_loss : 0.07559973746538162

    335300	  0.075573	  0.075600	  0.081860		CURRENT LEARNING RATE: 0.017472148492707635
previous_iter_valid_loss : 0.08006922155618668

    335400	  0.079984	  0.080069	  0.081757		CURRENT LEARNING RATE: 0.01745468507737788
previous_iter_valid_loss : 0.08138654381036758

    335500	  0.081332	  0.081387	  0.081789		CURRENT LEARNING RATE: 0.017437239116734657
previous_iter_valid_loss : 0.07688163220882416

    335600	  0.076917	  0.076882	  0.081686		CURRENT LEARNING RATE: 0.017419810593331992
previous_iter_valid_loss : 0.07607094198465347

    335700	  0.076042	  0.076071	  0.081605		CURRENT LEARNING RATE: 0.017402399489741385
previous_iter_valid_loss : 0.08229579031467438

    335800	  0.082398	  0.082296	  0.081646		CURRENT LEARNING RATE: 0.017385005788551715
previous_iter_valid_loss : 0.0776200145483017

    335900	  0.077667	  0.077620	  0.081652		CURRENT LEARNING RATE: 0.01736762947236928
previous_iter_valid_loss : 0.07615181058645248

    336000	  0.076196	  0.076152	  0.081447		CURRENT LEARNING RATE: 0.01735027052381776
previous_iter_valid_loss : 0.07617809623479843

    336100	  0.076155	  0.076178	  0.081378		CURRENT LEARNING RATE: 0.01733292892553822
previous_iter_valid_loss : 0.07529821991920471

    336200	  0.075325	  0.075298	  0.081364		CURRENT LEARNING RATE: 0.017315604660189048
previous_iter_valid_loss : 0.08096762746572495

    336300	  0.081080	  0.080968	  0.081157		CURRENT LEARNING RATE: 0.017298297710445977
previous_iter_valid_loss : 0.08001623302698135

    336400	  0.080118	  0.080016	  0.081045		CURRENT LEARNING RATE: 0.01728100805900205
previous_iter_valid_loss : 0.07552862167358398

    336500	  0.075571	  0.075529	  0.081045		CURRENT LEARNING RATE: 0.017263735688567632
previous_iter_valid_loss : 0.07525020837783813

    336600	  0.075273	  0.075250	  0.081036		CURRENT LEARNING RATE: 0.01724648058187034
previous_iter_valid_loss : 0.09359248727560043

    336700	  0.093498	  0.093592	  0.081200		CURRENT LEARNING RATE: 0.017229242721655068
previous_iter_valid_loss : 0.07757601886987686

    336800	  0.077503	  0.077576	  0.081105		CURRENT LEARNING RATE: 0.017212022090683947
previous_iter_valid_loss : 0.08032199740409851

    336900	  0.080423	  0.080322	  0.081139		CURRENT LEARNING RATE: 0.017194818671736355
previous_iter_valid_loss : 0.07663428038358688

    337000	  0.076580	  0.076634	  0.081094		CURRENT LEARNING RATE: 0.01717763244760887
previous_iter_valid_loss : 0.07942944020032883

    337100	  0.079490	  0.079429	  0.081133		CURRENT LEARNING RATE: 0.017160463401115263
previous_iter_valid_loss : 0.08741404861211777

    337200	  0.087563	  0.087414	  0.081255		CURRENT LEARNING RATE: 0.017143311515086482
previous_iter_valid_loss : 0.07667403668165207

    337300	  0.076647	  0.076674	  0.081266		CURRENT LEARNING RATE: 0.01712617677237065
previous_iter_valid_loss : 0.0796755775809288

    337400	  0.079587	  0.079676	  0.081296		CURRENT LEARNING RATE: 0.017109059155833016
previous_iter_valid_loss : 0.08271022886037827

    337500	  0.082631	  0.082710	  0.081163		CURRENT LEARNING RATE: 0.017091958648355967
previous_iter_valid_loss : 0.07545140385627747

    337600	  0.075441	  0.075451	  0.081149		CURRENT LEARNING RATE: 0.01707487523283899
previous_iter_valid_loss : 0.10410112142562866

    337700	  0.104010	  0.104101	  0.081236		CURRENT LEARNING RATE: 0.01705780889219866
previous_iter_valid_loss : 0.08075470477342606

    337800	  0.080635	  0.080755	  0.081289		CURRENT LEARNING RATE: 0.017040759609368652
previous_iter_valid_loss : 0.07505335658788681

    337900	  0.075040	  0.075053	  0.081272		CURRENT LEARNING RATE: 0.017023727367299672
previous_iter_valid_loss : 0.0766904279589653

    338000	  0.076692	  0.076690	  0.081273		CURRENT LEARNING RATE: 0.01700671214895948
previous_iter_valid_loss : 0.08158010244369507

    338100	  0.081628	  0.081580	  0.081334		CURRENT LEARNING RATE: 0.016989713937332847
previous_iter_valid_loss : 0.08382601290941238

    338200	  0.083912	  0.083826	  0.081410		CURRENT LEARNING RATE: 0.016972732715421573
previous_iter_valid_loss : 0.08167115598917007

    338300	  0.081794	  0.081671	  0.081474		CURRENT LEARNING RATE: 0.016955768466244428
previous_iter_valid_loss : 0.08340946584939957

    338400	  0.083326	  0.083409	  0.081556		CURRENT LEARNING RATE: 0.016938821172837164
previous_iter_valid_loss : 0.07642634212970734

    338500	  0.076389	  0.076426	  0.081520		CURRENT LEARNING RATE: 0.016921890818252475
previous_iter_valid_loss : 0.11026239395141602

    338600	  0.110175	  0.110262	  0.081864		CURRENT LEARNING RATE: 0.016904977385560023
previous_iter_valid_loss : 0.07511641085147858

    338700	  0.075097	  0.075116	  0.081843		CURRENT LEARNING RATE: 0.016888080857846367
previous_iter_valid_loss : 0.07542891055345535

    338800	  0.075403	  0.075429	  0.081844		CURRENT LEARNING RATE: 0.016871201218214976
previous_iter_valid_loss : 0.07551935315132141

    338900	  0.075531	  0.075519	  0.081841		CURRENT LEARNING RATE: 0.016854338449786198
previous_iter_valid_loss : 0.08485209196805954

    339000	  0.084993	  0.084852	  0.081860		CURRENT LEARNING RATE: 0.016837492535697284
previous_iter_valid_loss : 0.09204574674367905

    339100	  0.092242	  0.092046	  0.082006		CURRENT LEARNING RATE: 0.01682066345910231
previous_iter_valid_loss : 0.0978970006108284

    339200	  0.097776	  0.097897	  0.082218		CURRENT LEARNING RATE: 0.016803851203172196
previous_iter_valid_loss : 0.08082281798124313

    339300	  0.080700	  0.080823	  0.082201		CURRENT LEARNING RATE: 0.016787055751094678
previous_iter_valid_loss : 0.07513777166604996

    339400	  0.075122	  0.075138	  0.082194		CURRENT LEARNING RATE: 0.016770277086074318
previous_iter_valid_loss : 0.07544562220573425

    339500	  0.075455	  0.075446	  0.082175		CURRENT LEARNING RATE: 0.01675351519133244
previous_iter_valid_loss : 0.09726618975400925

    339600	  0.097158	  0.097266	  0.082349		CURRENT LEARNING RATE: 0.016736770050107153
previous_iter_valid_loss : 0.07519156485795975

    339700	  0.075185	  0.075192	  0.081993		CURRENT LEARNING RATE: 0.0167200416456533
previous_iter_valid_loss : 0.07499626278877258

    339800	  0.075003	  0.074996	  0.081802		CURRENT LEARNING RATE: 0.016703329961242495
previous_iter_valid_loss : 0.07639086991548538

    339900	  0.076427	  0.076391	  0.081812		CURRENT LEARNING RATE: 0.01668663498016304
previous_iter_valid_loss : 0.07743562012910843

    340000	  0.077492	  0.077436	  0.081808		CURRENT LEARNING RATE: 0.01666995668571996
previous_iter_valid_loss : 0.08334019035100937

    340100	  0.083222	  0.083340	  0.081801		CURRENT LEARNING RATE: 0.016653295061234946
previous_iter_valid_loss : 0.08898915350437164

    340200	  0.089127	  0.088989	  0.081733		CURRENT LEARNING RATE: 0.016636650090046386
previous_iter_valid_loss : 0.07623566687107086

    340300	  0.076236	  0.076236	  0.081717		CURRENT LEARNING RATE: 0.016620021755509307
previous_iter_valid_loss : 0.09768613427877426

    340400	  0.097876	  0.097686	  0.081700		CURRENT LEARNING RATE: 0.016603410040995366
previous_iter_valid_loss : 0.07875534892082214

    340500	  0.078648	  0.078755	  0.081392		CURRENT LEARNING RATE: 0.01658681492989284
previous_iter_valid_loss : 0.07977402210235596

    340600	  0.079708	  0.079774	  0.081429		CURRENT LEARNING RATE: 0.016570236405606637
previous_iter_valid_loss : 0.08170481026172638

    340700	  0.081601	  0.081705	  0.081495		CURRENT LEARNING RATE: 0.01655367445155822
previous_iter_valid_loss : 0.08592108637094498

    340800	  0.085791	  0.085921	  0.081468		CURRENT LEARNING RATE: 0.016537129051185633
previous_iter_valid_loss : 0.08210799843072891

    340900	  0.081988	  0.082108	  0.081511		CURRENT LEARNING RATE: 0.016520600187943466
previous_iter_valid_loss : 0.10174165666103363

    341000	  0.101981	  0.101742	  0.081675		CURRENT LEARNING RATE: 0.016504087845302873
previous_iter_valid_loss : 0.09891936182975769

    341100	  0.099169	  0.098919	  0.081886		CURRENT LEARNING RATE: 0.0164875920067515
previous_iter_valid_loss : 0.08649422973394394

    341200	  0.086358	  0.086494	  0.081981		CURRENT LEARNING RATE: 0.01647111265579351
previous_iter_valid_loss : 0.08174029737710953

    341300	  0.081624	  0.081740	  0.081987		CURRENT LEARNING RATE: 0.01645464977594954
previous_iter_valid_loss : 0.08644181489944458

    341400	  0.086346	  0.086442	  0.082075		CURRENT LEARNING RATE: 0.016438203350756724
previous_iter_valid_loss : 0.07855236530303955

    341500	  0.078601	  0.078552	  0.082104		CURRENT LEARNING RATE: 0.01642177336376863
previous_iter_valid_loss : 0.07607672363519669

    341600	  0.076067	  0.076077	  0.082105		CURRENT LEARNING RATE: 0.016405359798555265
previous_iter_valid_loss : 0.07700517028570175

    341700	  0.076962	  0.077005	  0.082110		CURRENT LEARNING RATE: 0.016388962638703063
previous_iter_valid_loss : 0.07904640585184097

    341800	  0.079131	  0.079046	  0.082137		CURRENT LEARNING RATE: 0.01637258186781487
previous_iter_valid_loss : 0.08323952555656433

    341900	  0.083335	  0.083240	  0.082184		CURRENT LEARNING RATE: 0.01635621746950991
previous_iter_valid_loss : 0.07798022776842117

    342000	  0.077895	  0.077980	  0.082206		CURRENT LEARNING RATE: 0.01633986942742378
previous_iter_valid_loss : 0.08762180805206299

    342100	  0.087507	  0.087622	  0.082091		CURRENT LEARNING RATE: 0.016323537725208434
previous_iter_valid_loss : 0.09614510834217072

    342200	  0.096029	  0.096145	  0.082181		CURRENT LEARNING RATE: 0.01630722234653218
previous_iter_valid_loss : 0.08379483222961426

    342300	  0.083652	  0.083795	  0.082182		CURRENT LEARNING RATE: 0.01629092327507963
previous_iter_valid_loss : 0.10017746686935425

    342400	  0.100439	  0.100177	  0.082398		CURRENT LEARNING RATE: 0.016274640494551715
previous_iter_valid_loss : 0.07773178070783615

    342500	  0.077680	  0.077732	  0.082399		CURRENT LEARNING RATE: 0.016258373988665645
previous_iter_valid_loss : 0.07575321942567825

    342600	  0.075736	  0.075753	  0.082385		CURRENT LEARNING RATE: 0.016242123741154923
previous_iter_valid_loss : 0.07623295485973358

    342700	  0.076191	  0.076233	  0.082394		CURRENT LEARNING RATE: 0.016225889735769296
previous_iter_valid_loss : 0.07656126469373703

    342800	  0.076612	  0.076561	  0.082348		CURRENT LEARNING RATE: 0.016209671956274756
previous_iter_valid_loss : 0.0849897488951683

    342900	  0.085092	  0.084990	  0.082284		CURRENT LEARNING RATE: 0.01619347038645352
previous_iter_valid_loss : 0.07771655917167664

    343000	  0.077655	  0.077717	  0.082311		CURRENT LEARNING RATE: 0.01617728501010402
previous_iter_valid_loss : 0.075632244348526

    343100	  0.075618	  0.075632	  0.082046		CURRENT LEARNING RATE: 0.016161115811040884
previous_iter_valid_loss : 0.08051643520593643

    343200	  0.080381	  0.080516	  0.082093		CURRENT LEARNING RATE: 0.016144962773094906
previous_iter_valid_loss : 0.07542603462934494

    343300	  0.075436	  0.075426	  0.082022		CURRENT LEARNING RATE: 0.016128825880113037
previous_iter_valid_loss : 0.0848754420876503

    343400	  0.084760	  0.084875	  0.082088		CURRENT LEARNING RATE: 0.0161127051159584
previous_iter_valid_loss : 0.08166356384754181

    343500	  0.081705	  0.081664	  0.082029		CURRENT LEARNING RATE: 0.01609660046451022
previous_iter_valid_loss : 0.07537434250116348

    343600	  0.075385	  0.075374	  0.081889		CURRENT LEARNING RATE: 0.01608051190966385
previous_iter_valid_loss : 0.0880630761384964

    343700	  0.088230	  0.088063	  0.081910		CURRENT LEARNING RATE: 0.01606443943533072
previous_iter_valid_loss : 0.07932259142398834

    343800	  0.079255	  0.079323	  0.081949		CURRENT LEARNING RATE: 0.016048383025438373
previous_iter_valid_loss : 0.08032302558422089

    343900	  0.080227	  0.080323	  0.081998		CURRENT LEARNING RATE: 0.016032342663930384
previous_iter_valid_loss : 0.07842221111059189

    344000	  0.078500	  0.078422	  0.082033		CURRENT LEARNING RATE: 0.0160163183347664
previous_iter_valid_loss : 0.08886303007602692

    344100	  0.089052	  0.088863	  0.082152		CURRENT LEARNING RATE: 0.016000310021922075
previous_iter_valid_loss : 0.07528453320264816

    344200	  0.075290	  0.075285	  0.082141		CURRENT LEARNING RATE: 0.015984317709389115
previous_iter_valid_loss : 0.0754343569278717

    344300	  0.075399	  0.075434	  0.082067		CURRENT LEARNING RATE: 0.015968341381175196
previous_iter_valid_loss : 0.07713503390550613

    344400	  0.077223	  0.077135	  0.082012		CURRENT LEARNING RATE: 0.015952381021303988
previous_iter_valid_loss : 0.10552860051393509

    344500	  0.105822	  0.105529	  0.082275		CURRENT LEARNING RATE: 0.015936436613815122
previous_iter_valid_loss : 0.09634098410606384

    344600	  0.096248	  0.096341	  0.082486		CURRENT LEARNING RATE: 0.015920508142764207
previous_iter_valid_loss : 0.08980640769004822

    344700	  0.089989	  0.089806	  0.082567		CURRENT LEARNING RATE: 0.01590459559222276
previous_iter_valid_loss : 0.0757622942328453

    344800	  0.075748	  0.075762	  0.082568		CURRENT LEARNING RATE: 0.015888698946278233
previous_iter_valid_loss : 0.07770854979753494

    344900	  0.077778	  0.077709	  0.082558		CURRENT LEARNING RATE: 0.01587281818903397
previous_iter_valid_loss : 0.09690317511558533

    345000	  0.097106	  0.096903	  0.082698		CURRENT LEARNING RATE: 0.015856953304609223
previous_iter_valid_loss : 0.08368347585201263

    345100	  0.083819	  0.083683	  0.082759		CURRENT LEARNING RATE: 0.0158411042771391
previous_iter_valid_loss : 0.08355594426393509

    345200	  0.083676	  0.083556	  0.082302		CURRENT LEARNING RATE: 0.01582527109077458
previous_iter_valid_loss : 0.07622998207807541

    345300	  0.076171	  0.076230	  0.082308		CURRENT LEARNING RATE: 0.015809453729682458
previous_iter_valid_loss : 0.08399835973978043

    345400	  0.083891	  0.083998	  0.082348		CURRENT LEARNING RATE: 0.015793652178045393
previous_iter_valid_loss : 0.08431990444660187

    345500	  0.084233	  0.084320	  0.082377		CURRENT LEARNING RATE: 0.01577786642006182
previous_iter_valid_loss : 0.10025566071271896

    345600	  0.100523	  0.100256	  0.082611		CURRENT LEARNING RATE: 0.015762096439945982
previous_iter_valid_loss : 0.08816064149141312

    345700	  0.088032	  0.088161	  0.082732		CURRENT LEARNING RATE: 0.015746342221927893
previous_iter_valid_loss : 0.07983463257551193

    345800	  0.079920	  0.079835	  0.082707		CURRENT LEARNING RATE: 0.015730603750253345
previous_iter_valid_loss : 0.07727532833814621

    345900	  0.077188	  0.077275	  0.082704		CURRENT LEARNING RATE: 0.015714881009183855
previous_iter_valid_loss : 0.08729091286659241

    346000	  0.087430	  0.087291	  0.082815		CURRENT LEARNING RATE: 0.015699173982996684
previous_iter_valid_loss : 0.079845130443573

    346100	  0.079911	  0.079845	  0.082852		CURRENT LEARNING RATE: 0.0156834826559848
previous_iter_valid_loss : 0.07650165259838104

    346200	  0.076439	  0.076502	  0.082864		CURRENT LEARNING RATE: 0.015667807012456885
previous_iter_valid_loss : 0.07812634110450745

    346300	  0.078181	  0.078126	  0.082835		CURRENT LEARNING RATE: 0.01565214703673729
previous_iter_valid_loss : 0.08820790797472

    346400	  0.088372	  0.088208	  0.082917		CURRENT LEARNING RATE: 0.01563650271316603
previous_iter_valid_loss : 0.11523579061031342

    346500	  0.115107	  0.115236	  0.083314		CURRENT LEARNING RATE: 0.015620874026098783
previous_iter_valid_loss : 0.07526998221874237

    346600	  0.075249	  0.075270	  0.083314		CURRENT LEARNING RATE: 0.015605260959906872
previous_iter_valid_loss : 0.07692629098892212

    346700	  0.076962	  0.076926	  0.083148		CURRENT LEARNING RATE: 0.015589663498977219
previous_iter_valid_loss : 0.0756932869553566

    346800	  0.075627	  0.075693	  0.083129		CURRENT LEARNING RATE: 0.015574081627712365
previous_iter_valid_loss : 0.07705418020486832

    346900	  0.077094	  0.077054	  0.083096		CURRENT LEARNING RATE: 0.01555851533053043
previous_iter_valid_loss : 0.08541068434715271

    347000	  0.085287	  0.085411	  0.083184		CURRENT LEARNING RATE: 0.01554296459186513
previous_iter_valid_loss : 0.07600265741348267

    347100	  0.075915	  0.076003	  0.083150		CURRENT LEARNING RATE: 0.015527429396165715
previous_iter_valid_loss : 0.07608851045370102

    347200	  0.076031	  0.076089	  0.083036		CURRENT LEARNING RATE: 0.015511909727896992
previous_iter_valid_loss : 0.08711754530668259

    347300	  0.086981	  0.087118	  0.083141		CURRENT LEARNING RATE: 0.01549640557153928
previous_iter_valid_loss : 0.0772676169872284

    347400	  0.077189	  0.077268	  0.083117		CURRENT LEARNING RATE: 0.015480916911588441
previous_iter_valid_loss : 0.08398273587226868

    347500	  0.083888	  0.083983	  0.083130		CURRENT LEARNING RATE: 0.015465443732555801
previous_iter_valid_loss : 0.07675012946128845

    347600	  0.076703	  0.076750	  0.083143		CURRENT LEARNING RATE: 0.015449986018968184
previous_iter_valid_loss : 0.07564541697502136

    347700	  0.075667	  0.075645	  0.082858		CURRENT LEARNING RATE: 0.015434543755367866
previous_iter_valid_loss : 0.07632959634065628

    347800	  0.076320	  0.076330	  0.082814		CURRENT LEARNING RATE: 0.015419116926312596
previous_iter_valid_loss : 0.07562164962291718

    347900	  0.075651	  0.075622	  0.082819		CURRENT LEARNING RATE: 0.015403705516375538
previous_iter_valid_loss : 0.08134496957063675

    348000	  0.081405	  0.081345	  0.082866		CURRENT LEARNING RATE: 0.01538830951014528
previous_iter_valid_loss : 0.0775977075099945

    348100	  0.077568	  0.077598	  0.082826		CURRENT LEARNING RATE: 0.015372928892225808
previous_iter_valid_loss : 0.0781562477350235

    348200	  0.078218	  0.078156	  0.082769		CURRENT LEARNING RATE: 0.015357563647236516
previous_iter_valid_loss : 0.07908088713884354

    348300	  0.079114	  0.079081	  0.082744		CURRENT LEARNING RATE: 0.01534221375981215
previous_iter_valid_loss : 0.07616785168647766

    348400	  0.076103	  0.076168	  0.082671		CURRENT LEARNING RATE: 0.015326879214602823
previous_iter_valid_loss : 0.07627969980239868

    348500	  0.076223	  0.076280	  0.082670		CURRENT LEARNING RATE: 0.01531155999627398
previous_iter_valid_loss : 0.0763491839170456

    348600	  0.076300	  0.076349	  0.082331		CURRENT LEARNING RATE: 0.015296256089506417
previous_iter_valid_loss : 0.07505301386117935

    348700	  0.075056	  0.075053	  0.082330		CURRENT LEARNING RATE: 0.015280967478996219
previous_iter_valid_loss : 0.07704167068004608

    348800	  0.077065	  0.077042	  0.082346		CURRENT LEARNING RATE: 0.015265694149454773
previous_iter_valid_loss : 0.07502908259630203

    348900	  0.075017	  0.075029	  0.082341		CURRENT LEARNING RATE: 0.015250436085608741
previous_iter_valid_loss : 0.08438904583454132

    349000	  0.084256	  0.084389	  0.082337		CURRENT LEARNING RATE: 0.015235193272200073
previous_iter_valid_loss : 0.07795502245426178

    349100	  0.077995	  0.077955	  0.082196		CURRENT LEARNING RATE: 0.015219965693985945
previous_iter_valid_loss : 0.07519979029893875

    349200	  0.075221	  0.075200	  0.081969		CURRENT LEARNING RATE: 0.015204753335738782
previous_iter_valid_loss : 0.07569368928670883

    349300	  0.075651	  0.075694	  0.081917		CURRENT LEARNING RATE: 0.015189556182246215
previous_iter_valid_loss : 0.07644437998533249

    349400	  0.076459	  0.076444	  0.081930		CURRENT LEARNING RATE: 0.015174374218311101
previous_iter_valid_loss : 0.07673516869544983

    349500	  0.076765	  0.076735	  0.081943		CURRENT LEARNING RATE: 0.01515920742875147
previous_iter_valid_loss : 0.07534381002187729

    349600	  0.075295	  0.075344	  0.081724		CURRENT LEARNING RATE: 0.015144055798400531
previous_iter_valid_loss : 0.0755583718419075

    349700	  0.075521	  0.075558	  0.081728		CURRENT LEARNING RATE: 0.015128919312106647
previous_iter_valid_loss : 0.07732468098402023

    349800	  0.077270	  0.077325	  0.081751		CURRENT LEARNING RATE: 0.015113797954733341
previous_iter_valid_loss : 0.09998398274183273

    349900	  0.099868	  0.099984	  0.081987		CURRENT LEARNING RATE: 0.01509869171115925
previous_iter_valid_loss : 0.07740757614374161

    350000	  0.077430	  0.077408	  0.081987		CURRENT LEARNING RATE: 0.01508360056627813
previous_iter_valid_loss : 0.0759233608841896

    350100	  0.075906	  0.075923	  0.081913		CURRENT LEARNING RATE: 0.01506852450499883
previous_iter_valid_loss : 0.07981648296117783

    350200	  0.079914	  0.079816	  0.081821		CURRENT LEARNING RATE: 0.015053463512245286
previous_iter_valid_loss : 0.08142205327749252

    350300	  0.081527	  0.081422	  0.081873		CURRENT LEARNING RATE: 0.015038417572956516
previous_iter_valid_loss : 0.1120225042104721

    350400	  0.111886	  0.112023	  0.082016		CURRENT LEARNING RATE: 0.01502338667208657
previous_iter_valid_loss : 0.07617253065109253

    350500	  0.076175	  0.076173	  0.081990		CURRENT LEARNING RATE: 0.015008370794604549
previous_iter_valid_loss : 0.07591374963521957

    350600	  0.075853	  0.075914	  0.081952		CURRENT LEARNING RATE: 0.014993369925494568
previous_iter_valid_loss : 0.07889948785305023

    350700	  0.078895	  0.078899	  0.081924		CURRENT LEARNING RATE: 0.014978384049755766
previous_iter_valid_loss : 0.07642648369073868

    350800	  0.076363	  0.076426	  0.081829		CURRENT LEARNING RATE: 0.014963413152402264
previous_iter_valid_loss : 0.07536035776138306

    350900	  0.075328	  0.075360	  0.081761		CURRENT LEARNING RATE: 0.01494845721846316
previous_iter_valid_loss : 0.09441229701042175

    351000	  0.094616	  0.094412	  0.081688		CURRENT LEARNING RATE: 0.014933516232982514
previous_iter_valid_loss : 0.09358014911413193

    351100	  0.093470	  0.093580	  0.081634		CURRENT LEARNING RATE: 0.014918590181019353
previous_iter_valid_loss : 0.07674750685691833

    351200	  0.076756	  0.076748	  0.081537		CURRENT LEARNING RATE: 0.014903679047647616
previous_iter_valid_loss : 0.07950685173273087

    351300	  0.079390	  0.079507	  0.081515		CURRENT LEARNING RATE: 0.014888782817956168
previous_iter_valid_loss : 0.07750372588634491

    351400	  0.077515	  0.077504	  0.081425		CURRENT LEARNING RATE: 0.014873901477048772
previous_iter_valid_loss : 0.08386990427970886

    351500	  0.083983	  0.083870	  0.081478		CURRENT LEARNING RATE: 0.0148590350100441
previous_iter_valid_loss : 0.0764504224061966

    351600	  0.076395	  0.076450	  0.081482		CURRENT LEARNING RATE: 0.014844183402075675
previous_iter_valid_loss : 0.07813675701618195

    351700	  0.078069	  0.078137	  0.081493		CURRENT LEARNING RATE: 0.01482934663829189
previous_iter_valid_loss : 0.08035940676927567

    351800	  0.080270	  0.080359	  0.081507		CURRENT LEARNING RATE: 0.014814524703855973
previous_iter_valid_loss : 0.07645884156227112

    351900	  0.076413	  0.076459	  0.081439		CURRENT LEARNING RATE: 0.014799717583946
previous_iter_valid_loss : 0.07944592088460922

    352000	  0.079375	  0.079446	  0.081453		CURRENT LEARNING RATE: 0.014784925263754845
previous_iter_valid_loss : 0.07630077004432678

    352100	  0.076314	  0.076301	  0.081340		CURRENT LEARNING RATE: 0.014770147728490186
previous_iter_valid_loss : 0.08030211180448532

    352200	  0.080172	  0.080302	  0.081182		CURRENT LEARNING RATE: 0.014755384963374477
previous_iter_valid_loss : 0.09674139320850372

    352300	  0.096933	  0.096741	  0.081311		CURRENT LEARNING RATE: 0.01474063695364497
previous_iter_valid_loss : 0.08185117691755295

    352400	  0.081933	  0.081851	  0.081128		CURRENT LEARNING RATE: 0.014725903684553645
previous_iter_valid_loss : 0.07648368924856186

    352500	  0.076531	  0.076484	  0.081116		CURRENT LEARNING RATE: 0.014711185141367232
previous_iter_valid_loss : 0.08033861964941025

    352600	  0.080216	  0.080339	  0.081161		CURRENT LEARNING RATE: 0.014696481309367179
previous_iter_valid_loss : 0.07555156201124191

    352700	  0.075519	  0.075552	  0.081155		CURRENT LEARNING RATE: 0.014681792173849666
previous_iter_valid_loss : 0.08210813254117966

    352800	  0.082217	  0.082108	  0.081210		CURRENT LEARNING RATE: 0.014667117720125552
previous_iter_valid_loss : 0.0786842480301857

    352900	  0.078593	  0.078684	  0.081147		CURRENT LEARNING RATE: 0.01465245793352038
previous_iter_valid_loss : 0.07637649774551392

    353000	  0.076388	  0.076376	  0.081134		CURRENT LEARNING RATE: 0.014637812799374355
previous_iter_valid_loss : 0.07776202261447906

    353100	  0.077687	  0.077762	  0.081155		CURRENT LEARNING RATE: 0.014623182303042357
previous_iter_valid_loss : 0.07717426866292953

    353200	  0.077133	  0.077174	  0.081121		CURRENT LEARNING RATE: 0.014608566429893879
previous_iter_valid_loss : 0.07713409513235092

    353300	  0.077120	  0.077134	  0.081139		CURRENT LEARNING RATE: 0.01459396516531305
previous_iter_valid_loss : 0.07839988172054291

    353400	  0.078312	  0.078400	  0.081074		CURRENT LEARNING RATE: 0.014579378494698595
previous_iter_valid_loss : 0.0751456543803215

    353500	  0.075122	  0.075146	  0.081009		CURRENT LEARNING RATE: 0.014564806403463856
previous_iter_valid_loss : 0.08843084424734116

    353600	  0.088580	  0.088431	  0.081139		CURRENT LEARNING RATE: 0.014550248877036735
previous_iter_valid_loss : 0.07614045590162277

    353700	  0.076131	  0.076140	  0.081020		CURRENT LEARNING RATE: 0.014535705900859702
previous_iter_valid_loss : 0.08343833684921265

    353800	  0.083342	  0.083438	  0.081061		CURRENT LEARNING RATE: 0.014521177460389776
previous_iter_valid_loss : 0.07746421545743942

    353900	  0.077457	  0.077464	  0.081033		CURRENT LEARNING RATE: 0.014506663541098527
previous_iter_valid_loss : 0.07506340742111206

    354000	  0.075025	  0.075063	  0.080999		CURRENT LEARNING RATE: 0.014492164128472028
previous_iter_valid_loss : 0.07970467209815979

    354100	  0.079598	  0.079705	  0.080907		CURRENT LEARNING RATE: 0.014477679208010864
previous_iter_valid_loss : 0.07515037804841995

    354200	  0.075125	  0.075150	  0.080906		CURRENT LEARNING RATE: 0.014463208765230108
previous_iter_valid_loss : 0.07557620853185654

    354300	  0.075565	  0.075576	  0.080907		CURRENT LEARNING RATE: 0.014448752785659331
previous_iter_valid_loss : 0.07790787518024445

    354400	  0.077933	  0.077908	  0.080915		CURRENT LEARNING RATE: 0.014434311254842543
previous_iter_valid_loss : 0.10490588843822479

    354500	  0.105140	  0.104906	  0.080909		CURRENT LEARNING RATE: 0.014419884158338211
previous_iter_valid_loss : 0.08329218626022339

    354600	  0.083163	  0.083292	  0.080778		CURRENT LEARNING RATE: 0.014405471481719235
previous_iter_valid_loss : 0.07683189958333969

    354700	  0.076755	  0.076832	  0.080649		CURRENT LEARNING RATE: 0.014391073210572945
previous_iter_valid_loss : 0.08086629956960678

    354800	  0.080764	  0.080866	  0.080700		CURRENT LEARNING RATE: 0.014376689330501067
previous_iter_valid_loss : 0.0821618065237999

    354900	  0.082263	  0.082162	  0.080744		CURRENT LEARNING RATE: 0.014362319827119717
previous_iter_valid_loss : 0.078065425157547

    355000	  0.077985	  0.078065	  0.080556		CURRENT LEARNING RATE: 0.014347964686059384
previous_iter_valid_loss : 0.07841020077466965

    355100	  0.078322	  0.078410	  0.080503		CURRENT LEARNING RATE: 0.01433362389296494
previous_iter_valid_loss : 0.07611553370952606

    355200	  0.076072	  0.076116	  0.080429		CURRENT LEARNING RATE: 0.014319297433495583
previous_iter_valid_loss : 0.07607443630695343

    355300	  0.075995	  0.076074	  0.080427		CURRENT LEARNING RATE: 0.014304985293324853
previous_iter_valid_loss : 0.088303342461586

    355400	  0.088211	  0.088303	  0.080470		CURRENT LEARNING RATE: 0.014290687458140602
previous_iter_valid_loss : 0.07533884793519974

    355500	  0.075345	  0.075339	  0.080380		CURRENT LEARNING RATE: 0.014276403913645005
previous_iter_valid_loss : 0.07823198288679123

    355600	  0.078139	  0.078232	  0.080160		CURRENT LEARNING RATE: 0.014262134645554514
previous_iter_valid_loss : 0.0804409384727478

    355700	  0.080491	  0.080441	  0.080083		CURRENT LEARNING RATE: 0.014247879639599854
previous_iter_valid_loss : 0.07564198970794678

    355800	  0.075589	  0.075642	  0.080041		CURRENT LEARNING RATE: 0.014233638881526017
previous_iter_valid_loss : 0.07569917291402817

    355900	  0.075680	  0.075699	  0.080025		CURRENT LEARNING RATE: 0.014219412357092252
previous_iter_valid_loss : 0.08309898525476456

    356000	  0.083195	  0.083099	  0.079983		CURRENT LEARNING RATE: 0.01420520005207203
previous_iter_valid_loss : 0.07843576371669769

    356100	  0.078464	  0.078436	  0.079969		CURRENT LEARNING RATE: 0.014191001952253045
previous_iter_valid_loss : 0.07681722193956375

    356200	  0.076764	  0.076817	  0.079972		CURRENT LEARNING RATE: 0.014176818043437187
previous_iter_valid_loss : 0.07943243533372879

    356300	  0.079495	  0.079432	  0.079986		CURRENT LEARNING RATE: 0.01416264831144056
previous_iter_valid_loss : 0.07584097236394882

    356400	  0.075845	  0.075841	  0.079862		CURRENT LEARNING RATE: 0.014148492742093427
previous_iter_valid_loss : 0.07650627195835114

    356500	  0.076439	  0.076506	  0.079475		CURRENT LEARNING RATE: 0.014134351321240213
previous_iter_valid_loss : 0.07527723908424377

    356600	  0.075265	  0.075277	  0.079475		CURRENT LEARNING RATE: 0.014120224034739491
previous_iter_valid_loss : 0.07546491175889969

    356700	  0.075487	  0.075465	  0.079460		CURRENT LEARNING RATE: 0.01410611086846399
previous_iter_valid_loss : 0.07497788965702057

    356800	  0.074931	  0.074978	  0.079453		CURRENT LEARNING RATE: 0.01409201180830053
previous_iter_valid_loss : 0.08636931329965591

    356900	  0.086252	  0.086369	  0.079546		CURRENT LEARNING RATE: 0.014077926840150053
previous_iter_valid_loss : 0.08192526549100876

    357000	  0.082006	  0.081925	  0.079511		CURRENT LEARNING RATE: 0.014063855949927585
previous_iter_valid_loss : 0.07536152005195618

    357100	  0.075313	  0.075362	  0.079505		CURRENT LEARNING RATE: 0.014049799123562244
previous_iter_valid_loss : 0.08346233516931534

    357200	  0.083339	  0.083462	  0.079578		CURRENT LEARNING RATE: 0.014035756346997197
previous_iter_valid_loss : 0.08636021614074707

    357300	  0.086468	  0.086360	  0.079571		CURRENT LEARNING RATE: 0.014021727606189666
previous_iter_valid_loss : 0.07578925043344498

    357400	  0.075795	  0.075789	  0.079556		CURRENT LEARNING RATE: 0.014007712887110904
previous_iter_valid_loss : 0.07598965615034103

    357500	  0.075933	  0.075990	  0.079476		CURRENT LEARNING RATE: 0.013993712175746202
previous_iter_valid_loss : 0.08622943609952927

    357600	  0.086377	  0.086229	  0.079571		CURRENT LEARNING RATE: 0.013979725458094843
previous_iter_valid_loss : 0.0989198312163353

    357700	  0.099162	  0.098920	  0.079804		CURRENT LEARNING RATE: 0.013965752720170107
previous_iter_valid_loss : 0.09498269110918045

    357800	  0.094875	  0.094983	  0.079990		CURRENT LEARNING RATE: 0.013951793947999249
previous_iter_valid_loss : 0.09420254081487656

    357900	  0.094083	  0.094203	  0.080176		CURRENT LEARNING RATE: 0.013937849127623508
previous_iter_valid_loss : 0.07617562264204025

    358000	  0.076115	  0.076176	  0.080124		CURRENT LEARNING RATE: 0.013923918245098055
previous_iter_valid_loss : 0.08013167232275009

    358100	  0.080212	  0.080132	  0.080150		CURRENT LEARNING RATE: 0.013910001286492009
previous_iter_valid_loss : 0.07636060565710068

    358200	  0.076330	  0.076361	  0.080132		CURRENT LEARNING RATE: 0.0138960982378884
previous_iter_valid_loss : 0.0968688353896141

    358300	  0.097077	  0.096869	  0.080310		CURRENT LEARNING RATE: 0.013882209085384196
previous_iter_valid_loss : 0.07609287649393082

    358400	  0.076043	  0.076093	  0.080309		CURRENT LEARNING RATE: 0.013868333815090233
previous_iter_valid_loss : 0.08178719878196716

    358500	  0.081866	  0.081787	  0.080364		CURRENT LEARNING RATE: 0.01385447241313124
previous_iter_valid_loss : 0.0840587317943573

    358600	  0.083922	  0.084059	  0.080441		CURRENT LEARNING RATE: 0.01384062486564581
previous_iter_valid_loss : 0.07766101509332657

    358700	  0.077679	  0.077661	  0.080467		CURRENT LEARNING RATE: 0.013826791158786404
previous_iter_valid_loss : 0.07663507759571075

    358800	  0.076617	  0.076635	  0.080463		CURRENT LEARNING RATE: 0.013812971278719308
previous_iter_valid_loss : 0.09430564194917679

    358900	  0.094494	  0.094306	  0.080656		CURRENT LEARNING RATE: 0.013799165211624644
previous_iter_valid_loss : 0.07850098609924316

    359000	  0.078529	  0.078501	  0.080597		CURRENT LEARNING RATE: 0.013785372943696335
previous_iter_valid_loss : 0.07639676332473755

    359100	  0.076394	  0.076397	  0.080581		CURRENT LEARNING RATE: 0.013771594461142124
previous_iter_valid_loss : 0.09551124274730682

    359200	  0.095368	  0.095511	  0.080784		CURRENT LEARNING RATE: 0.013757829750183522
previous_iter_valid_loss : 0.07533165067434311

    359300	  0.075325	  0.075332	  0.080781		CURRENT LEARNING RATE: 0.013744078797055817
previous_iter_valid_loss : 0.09618992358446121

    359400	  0.096071	  0.096190	  0.080978		CURRENT LEARNING RATE: 0.013730341588008047
previous_iter_valid_loss : 0.07525008916854858

    359500	  0.075219	  0.075250	  0.080963		CURRENT LEARNING RATE: 0.013716618109303016
previous_iter_valid_loss : 0.09094787389039993

    359600	  0.090796	  0.090948	  0.081120		CURRENT LEARNING RATE: 0.013702908347217237
previous_iter_valid_loss : 0.07670978456735611

    359700	  0.076725	  0.076710	  0.081131		CURRENT LEARNING RATE: 0.013689212288040948
previous_iter_valid_loss : 0.07767634093761444

    359800	  0.077617	  0.077676	  0.081135		CURRENT LEARNING RATE: 0.013675529918078083
previous_iter_valid_loss : 0.08151347935199738

    359900	  0.081617	  0.081513	  0.080950		CURRENT LEARNING RATE: 0.01366186122364628
previous_iter_valid_loss : 0.12735013663768768

    360000	  0.127262	  0.127350	  0.081449		CURRENT LEARNING RATE: 0.013648206191076838
previous_iter_valid_loss : 0.07510598748922348

    360100	  0.075083	  0.075106	  0.081441		CURRENT LEARNING RATE: 0.013634564806714726
previous_iter_valid_loss : 0.08527779579162598

    360200	  0.085157	  0.085278	  0.081496		CURRENT LEARNING RATE: 0.013620937056918551
previous_iter_valid_loss : 0.07576701045036316

    360300	  0.075706	  0.075767	  0.081439		CURRENT LEARNING RATE: 0.013607322928060573
previous_iter_valid_loss : 0.07682610303163528

    360400	  0.076791	  0.076826	  0.081087		CURRENT LEARNING RATE: 0.013593722406526659
previous_iter_valid_loss : 0.07929141819477081

    360500	  0.079343	  0.079291	  0.081118		CURRENT LEARNING RATE: 0.013580135478716282
previous_iter_valid_loss : 0.07500693947076797

    360600	  0.074988	  0.075007	  0.081109		CURRENT LEARNING RATE: 0.013566562131042511
previous_iter_valid_loss : 0.07550685107707977

    360700	  0.075452	  0.075507	  0.081075		CURRENT LEARNING RATE: 0.013553002349932007
previous_iter_valid_loss : 0.09577182680368423

    360800	  0.095639	  0.095772	  0.081269		CURRENT LEARNING RATE: 0.013539456121824982
previous_iter_valid_loss : 0.07594245672225952

    360900	  0.075942	  0.075942	  0.081275		CURRENT LEARNING RATE: 0.013525923433175208
previous_iter_valid_loss : 0.08611425757408142

    361000	  0.085996	  0.086114	  0.081192		CURRENT LEARNING RATE: 0.013512404270449987
previous_iter_valid_loss : 0.07663365453481674

    361100	  0.076628	  0.076634	  0.081022		CURRENT LEARNING RATE: 0.01349889862013017
previous_iter_valid_loss : 0.0780937597155571

    361200	  0.078148	  0.078094	  0.081036		CURRENT LEARNING RATE: 0.013485406468710097
previous_iter_valid_loss : 0.07518129795789719

    361300	  0.075182	  0.075181	  0.080992		CURRENT LEARNING RATE: 0.013471927802697617
previous_iter_valid_loss : 0.07610954344272614

    361400	  0.076059	  0.076110	  0.080978		CURRENT LEARNING RATE: 0.013458462608614056
previous_iter_valid_loss : 0.08451557904481888

    361500	  0.084621	  0.084516	  0.080985		CURRENT LEARNING RATE: 0.013445010872994231
previous_iter_valid_loss : 0.08453090488910675

    361600	  0.084620	  0.084531	  0.081066		CURRENT LEARNING RATE: 0.013431572582386399
previous_iter_valid_loss : 0.07544349879026413

    361700	  0.075413	  0.075443	  0.081039		CURRENT LEARNING RATE: 0.01341814772335227
previous_iter_valid_loss : 0.0760221853852272

    361800	  0.075969	  0.076022	  0.080995		CURRENT LEARNING RATE: 0.013404736282466976
previous_iter_valid_loss : 0.07760386168956757

    361900	  0.077607	  0.077604	  0.081007		CURRENT LEARNING RATE: 0.013391338246319088
previous_iter_valid_loss : 0.07682152092456818

    362000	  0.076842	  0.076822	  0.080981		CURRENT LEARNING RATE: 0.013377953601510562
previous_iter_valid_loss : 0.07560117542743683

    362100	  0.075597	  0.075601	  0.080974		CURRENT LEARNING RATE: 0.01336458233465675
previous_iter_valid_loss : 0.07508611679077148

    362200	  0.075044	  0.075086	  0.080921		CURRENT LEARNING RATE: 0.013351224432386384
previous_iter_valid_loss : 0.090651735663414

    362300	  0.090532	  0.090652	  0.080861		CURRENT LEARNING RATE: 0.013337879881341566
previous_iter_valid_loss : 0.07744451612234116

    362400	  0.077386	  0.077445	  0.080817		CURRENT LEARNING RATE: 0.013324548668177743
previous_iter_valid_loss : 0.07508417963981628

    362500	  0.075056	  0.075084	  0.080803		CURRENT LEARNING RATE: 0.013311230779563699
previous_iter_valid_loss : 0.07677450031042099

    362600	  0.076769	  0.076775	  0.080767		CURRENT LEARNING RATE: 0.013297926202181542
previous_iter_valid_loss : 0.0761319026350975

    362700	  0.076159	  0.076132	  0.080773		CURRENT LEARNING RATE: 0.01328463492272669
previous_iter_valid_loss : 0.08063012361526489

    362800	  0.080701	  0.080630	  0.080758		CURRENT LEARNING RATE: 0.013271356927907874
previous_iter_valid_loss : 0.08806098997592926

    362900	  0.088242	  0.088061	  0.080852		CURRENT LEARNING RATE: 0.01325809220444709
previous_iter_valid_loss : 0.07541391998529434

    363000	  0.075379	  0.075414	  0.080842		CURRENT LEARNING RATE: 0.013244840739079618
previous_iter_valid_loss : 0.0805586725473404

    363100	  0.080617	  0.080559	  0.080870		CURRENT LEARNING RATE: 0.013231602518553981
previous_iter_valid_loss : 0.08430799841880798

    363200	  0.084202	  0.084308	  0.080941		CURRENT LEARNING RATE: 0.013218377529631972
previous_iter_valid_loss : 0.09596576541662216

    363300	  0.095853	  0.095966	  0.081130		CURRENT LEARNING RATE: 0.013205165759088594
previous_iter_valid_loss : 0.07618765532970428

    363400	  0.076207	  0.076188	  0.081108		CURRENT LEARNING RATE: 0.013191967193712077
previous_iter_valid_loss : 0.08283799141645432

    363500	  0.082740	  0.082838	  0.081184		CURRENT LEARNING RATE: 0.013178781820303844
previous_iter_valid_loss : 0.08121631294488907

    363600	  0.081294	  0.081216	  0.081112		CURRENT LEARNING RATE: 0.013165609625678538
previous_iter_valid_loss : 0.0772877186536789

    363700	  0.077307	  0.077288	  0.081124		CURRENT LEARNING RATE: 0.013152450596663954
previous_iter_valid_loss : 0.09575854986906052

    363800	  0.095957	  0.095759	  0.081247		CURRENT LEARNING RATE: 0.013139304720101063
previous_iter_valid_loss : 0.08986137062311172

    363900	  0.089746	  0.089861	  0.081371		CURRENT LEARNING RATE: 0.01312617198284398
previous_iter_valid_loss : 0.09935637563467026

    364000	  0.099223	  0.099356	  0.081614		CURRENT LEARNING RATE: 0.01311305237175998
previous_iter_valid_loss : 0.08197134733200073

    364100	  0.082053	  0.081971	  0.081637		CURRENT LEARNING RATE: 0.013099945873729445
previous_iter_valid_loss : 0.09868565201759338

    364200	  0.098908	  0.098686	  0.081872		CURRENT LEARNING RATE: 0.013086852475645876
previous_iter_valid_loss : 0.07733625918626785

    364300	  0.077368	  0.077336	  0.081890		CURRENT LEARNING RATE: 0.013073772164415867
previous_iter_valid_loss : 0.08314865827560425

    364400	  0.083221	  0.083149	  0.081942		CURRENT LEARNING RATE: 0.013060704926959116
previous_iter_valid_loss : 0.07578939199447632

    364500	  0.075781	  0.075789	  0.081651		CURRENT LEARNING RATE: 0.013047650750208382
previous_iter_valid_loss : 0.07658777385950089

    364600	  0.076578	  0.076588	  0.081584		CURRENT LEARNING RATE: 0.013034609621109486
previous_iter_valid_loss : 0.07801365852355957

    364700	  0.078061	  0.078014	  0.081596		CURRENT LEARNING RATE: 0.01302158152662129
previous_iter_valid_loss : 0.07527576386928558

    364800	  0.075233	  0.075276	  0.081540		CURRENT LEARNING RATE: 0.013008566453715713
previous_iter_valid_loss : 0.08060167729854584

    364900	  0.080491	  0.080602	  0.081524		CURRENT LEARNING RATE: 0.012995564389377674
previous_iter_valid_loss : 0.08067604154348373

    365000	  0.080746	  0.080676	  0.081550		CURRENT LEARNING RATE: 0.012982575320605105
previous_iter_valid_loss : 0.07877037674188614

    365100	  0.078797	  0.078770	  0.081554		CURRENT LEARNING RATE: 0.012969599234408935
previous_iter_valid_loss : 0.07503291964530945

    365200	  0.075003	  0.075033	  0.081543		CURRENT LEARNING RATE: 0.012956636117813084
previous_iter_valid_loss : 0.0870894342660904

    365300	  0.087212	  0.087089	  0.081653		CURRENT LEARNING RATE: 0.012943685957854433
previous_iter_valid_loss : 0.09748659282922745

    365400	  0.097358	  0.097487	  0.081745		CURRENT LEARNING RATE: 0.012930748741582817
previous_iter_valid_loss : 0.07935633510351181

    365500	  0.079263	  0.079356	  0.081785		CURRENT LEARNING RATE: 0.012917824456061013
previous_iter_valid_loss : 0.07514646649360657

    365600	  0.075136	  0.075146	  0.081754		CURRENT LEARNING RATE: 0.01290491308836475
previous_iter_valid_loss : 0.07565788179636002

    365700	  0.075648	  0.075658	  0.081706		CURRENT LEARNING RATE: 0.01289201462558265
previous_iter_valid_loss : 0.08069818466901779

    365800	  0.080734	  0.080698	  0.081757		CURRENT LEARNING RATE: 0.012879129054816248
previous_iter_valid_loss : 0.08070917427539825

    365900	  0.080621	  0.080709	  0.081807		CURRENT LEARNING RATE: 0.01286625636317997
previous_iter_valid_loss : 0.07783327251672745

    366000	  0.077892	  0.077833	  0.081754		CURRENT LEARNING RATE: 0.012853396537801133
previous_iter_valid_loss : 0.07540732622146606

    366100	  0.075410	  0.075407	  0.081724		CURRENT LEARNING RATE: 0.012840549565819906
previous_iter_valid_loss : 0.07817050814628601

    366200	  0.078197	  0.078171	  0.081738		CURRENT LEARNING RATE: 0.012827715434389313
previous_iter_valid_loss : 0.07910898327827454

    366300	  0.079041	  0.079109	  0.081734
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ali Al Kadhim<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>