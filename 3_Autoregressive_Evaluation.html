
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>IQNx4: Chapter 3: Autoregressive Evaluation &#8212; torchIQNx4</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Chapter 4: Optional Supplementary Material" href="4_SupplementaryMaterials_OptunaTuning_Theory_ML_Background.html" />
    <link rel="prev" title="IQNx4 Chapter 2: Train All Networks" href="2_Train_all_IQNs.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">torchIQNx4</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the torchIQNx4
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_Setup_and_Preprocess.html">
   IQNx4: Chapter 1. Setup and Preprocess
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2_Train_all_IQNs.html">
   IQNx4 Chapter 2: Train All Networks
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   IQNx4: Chapter 3: Autoregressive Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_SupplementaryMaterials_OptunaTuning_Theory_ML_Background.html">
   Chapter 4: Optional Supplementary Material
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/AliAlkadhim/torchQN/HEAD?labpath=JupyterBook/v2/gh/AliAlkadhim/torchQN/master?urlpath=tree/JupyterBook/3_Autoregressive_Evaluation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/3_Autoregressive_Evaluation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports-and-configurations">
   3.1: Imports and Configurations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#needed-functions">
   3.2: Needed Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluate-mass">
   3.3: Evaluate Mass
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluate-p-t">
   3.4: Evaluate
   <span class="math notranslate nohighlight">
    \(p_T\)
   </span>
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>IQNx4: Chapter 3: Autoregressive Evaluation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports-and-configurations">
   3.1: Imports and Configurations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#needed-functions">
   3.2: Needed Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluate-mass">
   3.3: Evaluate Mass
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluate-p-t">
   3.4: Evaluate
   <span class="math notranslate nohighlight">
    \(p_T\)
   </span>
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="iqnx4-chapter-3-autoregressive-evaluation">
<h1>IQNx4: Chapter 3: Autoregressive Evaluation<a class="headerlink" href="#iqnx4-chapter-3-autoregressive-evaluation" title="Permalink to this heading">#</a></h1>
<section id="imports-and-configurations">
<h2>3.1: Imports and Configurations<a class="headerlink" href="#imports-and-configurations" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import pandas as pd

# import scipy as sp; import scipy.stats as st
import torch
import torch.nn as nn

print(f&quot;using torch version {torch.__version__}&quot;)
# use numba&#39;s just-in-time compiler to speed things up
# from numba import njit
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib as mp

print(&quot;matplotlib version= &quot;, mp.__version__)

import matplotlib.pyplot as plt

# reset matplotlib stle/parameters
# reset matplotlib parameters to their defaults
# plt.style.use(&#39;seaborn-deep&#39;)
# mp.rcParams[&#39;agg.path.chunksize&#39;] = 10000
font_legend = 15
font_axes = 15
# %matplotlib inline
import sys
import os

# from IPython.display import Image, display
# from importlib import import_module
# import plotly
try:
    import optuna

    print(f&quot;using (optional) optuna version {optuna.__version__}&quot;)
except Exception:
    print(&quot;optuna is only used for hyperparameter tuning, not critical!&quot;)
    pass
import argparse
import time

# import sympy as sy
# import ipywidgets as wid;

# update fonts
font = {&quot;family&quot;: &quot;serif&quot;, &quot;size&quot;: 10}
mp.rc(&quot;font&quot;, **font)

# set usetex = False if LaTex is not
# available on your system or if the
# rendering is too slow
mp.rcParams.update({&quot;text.usetex&quot;: True})
# plt.rcParams[&#39;text.usetex&#39;] = True
mp.rcParams[&quot;text.latex.preamble&quot;] = [r&quot;\usepackage{amsmath}&quot;]  # for \text command


# set a seed to ensure reproducibility
# seed = 128
# rnd  = np.random.RandomState(seed)
# sometimes jupyter doesnt initialize MathJax automatically for latex, so do this:

try:
    IQN_BASE = os.environ[&quot;IQN_BASE&quot;]
    print(&quot;BASE directoy properly set = &quot;, IQN_BASE)
    utils_dir = os.path.join(IQN_BASE, &#39;utils/&#39;)
    sys.path.append(utils_dir)
    import utils

    # usually its not recommended to import everything from a module, but we know
    # whats in it so its fine
    # from utils import *
    print(&quot;DATA directory also properly set, in %s&quot; % os.environ[&quot;DATA_DIR&quot;])
except Exception:
    # IQN_BASE=os.getcwd()
    print(
        &quot;&quot;&quot;\nBASE directory not properly set. Read repo README.    If you need a function from utils, use the decorator below, or add utils to sys.path\n
    You can also do 
    os.environ[&#39;IQN_BASE&#39;]=&lt;ABSOLUTE PATH FOR THE IQN REPO&gt;
    or
    os.environ[&#39;IQN_BASE&#39;]=os.getcwd()&quot;&quot;&quot;
    )
    pass


IQN_BASE = os.environ[&quot;IQN_BASE&quot;]
print(&quot;BASE directoy properly set = &quot;, IQN_BASE)
utils_dir = os.path.join(IQN_BASE, &quot;utils/&quot;)
sys.path.append(utils_dir)
# usually its not recommended to import everything from a module, but we know
# whats in it so its fine

# or use joblib for caching on disk
from joblib import Memory


################################### CONFIGURATIONS ###################################
DATA_DIR = os.environ[&quot;DATA_DIR&quot;]
print(f&quot;using DATA_DIR={DATA_DIR}&quot;)
JUPYTER = False
use_subsample = False
# use_subsample=True
if use_subsample:
    SUBSAMPLE = int(
        1e5
    )  # subsample use for development - in production use whole dataset
else:
    SUBSAMPLE = None

memory = Memory(DATA_DIR)


###############################################################################################
y_label_dict = {
    &quot;RecoDatapT&quot;: &quot;$p(p_T)$&quot; + &quot; [ GeV&quot; + &quot;$^{-1} $&quot; + &quot;]&quot;,
    &quot;RecoDataeta&quot;: &quot;$p(\eta)$&quot;,
    &quot;RecoDataphi&quot;: &quot;$p(\phi)$&quot;,
    &quot;RecoDatam&quot;: &quot;$p(m)$&quot; + &quot; [ GeV&quot; + &quot;$^{-1} $&quot; + &quot;]&quot;,
}

loss_y_label_dict = {
    &quot;RecoDatapT&quot;: &quot;$p_T^{reco}$&quot;,
    &quot;RecoDataeta&quot;: &quot;$\eta^{reco}$&quot;,
    &quot;RecoDataphi&quot;: &quot;$\phi^{reco}$&quot;,
    &quot;RecoDatam&quot;: &quot;$m^{reco}$&quot;,
}


################################### SET DATA CONFIGURATIONS ###################################
X = [&quot;genDatapT&quot;, &quot;genDataeta&quot;, &quot;genDataphi&quot;, &quot;genDatam&quot;, &quot;tau&quot;]

# set order of training:
# pT_first: pT-&gt;&gt;m-&gt;eta-&gt;phi
# m_first: m-&gt;pT-&gt;eta-&gt;phi


ORDER = &quot;m_First&quot;

if ORDER == &quot;m_First&quot;:
    FIELDS = {
        &quot;RecoDatam&quot;: {
            &quot;inputs&quot;: X,
            &quot;xlabel&quot;: r&quot;$m$ (GeV)&quot;,
            &quot;ylabel&quot;: &quot;$m^{reco}$&quot;,
            &quot;xmin&quot;: 0,
            &quot;xmax&quot;: 25,
        },
        &quot;RecoDatapT&quot;: {
            &quot;inputs&quot;: [&quot;RecoDatam&quot;] + X,
            &quot;xlabel&quot;: r&quot;$p_T$ (GeV)&quot;,
            &quot;ylabel&quot;: &quot;$p_T^{reco}$&quot;,
            &quot;xmin&quot;: 20,
            &quot;xmax&quot;: 80,
        },
        &quot;RecoDataeta&quot;: {
            &quot;inputs&quot;: [&quot;RecoDatam&quot;, &quot;RecoDatapT&quot;] + X,
            &quot;xlabel&quot;: r&quot;$\eta$&quot;,
            &quot;ylabel&quot;: &quot;$\eta^{reco}$&quot;,
            &quot;xmin&quot;: -5,
            &quot;xmax&quot;: 5,
        },
        &quot;RecoDataphi&quot;: {
            &quot;inputs&quot;: [&quot;RecoDatam&quot;, &quot;RecoDatapT&quot;, &quot;RecoDataeta&quot;] + X,
            &quot;xlabel&quot;: r&quot;$\phi$&quot;,
            &quot;ylabel&quot;: &quot;$\phi^{reco}$&quot;,
            &quot;xmin&quot;: -3.2,
            &quot;xmax&quot;: 3.2,
        },
    }


# Load and explore raw (unscaled) dataframes


all_variable_cols = [
    &quot;genDatapT&quot;,
    &quot;genDataeta&quot;,
    &quot;genDataphi&quot;,
    &quot;genDatam&quot;,
    &quot;RecoDatapT&quot;,
    &quot;RecoDataeta&quot;,
    &quot;RecoDataphi&quot;,
    &quot;RecoDatam&quot;,
]
all_cols = [
    &quot;genDatapT&quot;,
    &quot;genDataeta&quot;,
    &quot;genDataphi&quot;,
    &quot;genDatam&quot;,
    &quot;RecoDatapT&quot;,
    &quot;RecoDataeta&quot;,
    &quot;RecoDataphi&quot;,
    &quot;RecoDatam&quot;,
    &quot;tau&quot;,
]

</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>using torch version 1.11.0.post2
matplotlib version=  3.5.3
using (optional) optuna version 3.0.0
BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN
using torch version 1.11.0.post2
matplotlib version=  3.5.3
using (optional) optuna version 3.0.0
BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN
DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data
DATA directory also properly set, in /home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data
BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/torchQN
using DATA_DIR=/home/ali/Desktop/Pulled_Github_Repositories/IQN_HEP/Davidson/data
</pre></div>
</div>
</div>
</div>
</section>
<section id="needed-functions">
<h2>3.2: Needed Functions<a class="headerlink" href="#needed-functions" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>target = &quot;RecoDatam&quot;
source = FIELDS[target]
features = source[&quot;inputs&quot;]
print(&quot;Training Features:\n&quot;, features)
print(&quot;\nTarget = &quot;, target)
AUTOREGRESSIVE_DIST_NAME = &quot;AUTOREGRESSIVE_m_Prime.csv&quot;
print(&quot;USING NEW DATASET\n&quot;)
######################################
USE_BRADEN_SCALING = False
#####################################
################################### CONFIGURATIONS ###################################

JUPYTER = True
use_subsample = False
# use_subsample = True
if use_subsample:
    SUBSAMPLE = int(
        1e5
    )  # subsample use for development - in production use whole dataset
else:
    SUBSAMPLE = None

########################################################################################
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Features:
 [&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;, &#39;tau&#39;]

Target =  RecoDatam
USING NEW DATASET
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>################################## Load unscaled dataframes ###################################
@memory.cache
def load_raw_data():
    &quot;&quot;&quot;Dont use AUTOREGRESSIVE_DIST_NAME for training of any variable. 
    For mass evaluation: dont use AUTOREGRESSIVE_DIST_NAME. For pT evaluation use AUTOREGRESSIVE_DIST_NAME 
    as the distribution predicted by mass, etc.  &quot;&quot;&quot;
    print(f&quot;\nSUBSAMPLE = {SUBSAMPLE}\n&quot;)
    raw_train_data = pd.read_csv(
        os.path.join(DATA_DIR, &quot;train_data_10M_2.csv&quot;),
        usecols=all_cols,
        nrows=SUBSAMPLE,
    )



    raw_valid_data = pd.read_csv(
        os.path.join(DATA_DIR, &quot;validation_data_10M_2.csv&quot;),
        usecols=all_cols,
        nrows=SUBSAMPLE,
    )
    

    
    raw_test_data = pd.read_csv(
    os.path.join(DATA_DIR, &quot;test_data_10M_2.csv&quot;), 
    usecols=all_cols, 
    nrows=SUBSAMPLE
    )

    print(&quot;\n RAW TRAIN DATA\n&quot;)
    print(raw_train_data.shape)
    raw_train_data.describe()  # unscaled
    print(&quot;\n RAW TEST DATA\n&quot;)
    print(raw_test_data.shape)
    raw_test_data.describe()  # unscaled

    return raw_train_data, raw_test_data, raw_valid_data


########## Generate scaled data###############
# scaled_train_data = L_scale_df(raw_train_data, title=&#39;scaled_train_data_10M_2.csv&#39;,
#                              save=True)
# print(&#39;\n\n&#39;)
# scaled_test_data = L_scale_df(raw_test_data,  title=&#39;scaled_test_data_10M_2.csv&#39;,
#                             save=True)
# print(&#39;\n\n&#39;)

# scaled_valid_data = L_scale_df(raw_valid_data,  title=&#39;scaled_valid_data_10M_2.csv&#39;,
#                             save=True)

# explore_data(df=scaled_train_data, title=&#39;Braden Kronheim-L-scaled Dataframe&#39;, scaled=True)

################ Load scaled data##############
@utils.time_type_of_func(tuning_or_training=&quot;loading&quot;)
# @memory.cache
def load_scaled_dataframes():
    print(&quot;SCALED TRAIN DATA&quot;)
    scaled_train_data = pd.read_csv(
        os.path.join(DATA_DIR, &quot;scaled_train_data_10M_2.csv&quot;),
        usecols=all_cols,
        nrows=SUBSAMPLE,
    )

    print(&quot;TRAINING FEATURES\n&quot;, scaled_train_data.head())

    scaled_test_data = pd.read_csv(
        os.path.join(DATA_DIR, &quot;scaled_test_data_10M_2.csv&quot;),
        usecols=all_cols,
        nrows=SUBSAMPLE,
    )

    scaled_valid_data = pd.read_csv(
        os.path.join(DATA_DIR, &quot;scaled_valid_data_10M_2.csv&quot;),
        usecols=all_cols,
        nrows=SUBSAMPLE,
    )
    return scaled_train_data, scaled_test_data, scaled_valid_data


#######################################
#
# # print(&#39;\nTESTING FEATURES\n&#39;, scaled_test_data.head())

# print(&#39;\ntrain set shape:&#39;,  scaled_train_data.shape)
# print(&#39;\ntest set shape:  &#39;, scaled_test_data.shape)
# # print(&#39;validation set shape:&#39;, valid_data.shape)
# @memory.cache
def get_train_scale_dict(USE_BRADEN_SCALING):
    if USE_BRADEN_SCALING==True:
        TRAIN_SCALE_DICT = utils.get_scaling_info(scaled_train_data)
        print(&quot;BRADEN SCALING DICTIONARY&quot;)
        print(TRAIN_SCALE_DICT)
        print(&quot;\n\n&quot;)
        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)
        # print(TEST_SCALE_DICT)
    else:
        print(&quot;NORMAL UNSCALED DICTIONARY&quot;)
        TRAIN_SCALE_DICT = utils.get_scaling_info(raw_train_data)
        print(TRAIN_SCALE_DICT)
        print(&quot;\n\n&quot;)
        # TEST_SCALE_DICT = get_scaling_info(scaled_test_data)
        # print(TEST_SCALE_DICT)
    return TRAIN_SCALE_DICT


################################ SPLIT###########
# @memory.cache
def T(variable, scaled_df):
    if variable == &quot;pT&quot;:
        L_pT_gen = scaled_df[&quot;genDatapT&quot;]
        L_pT_reco = scaled_df[&quot;RecoDatapT&quot;]
        target = (L_pT_reco + 10) / (L_pT_gen + 10)
    if variable == &quot;eta&quot;:
        L_eta_gen = scaled_df[&quot;genDataeta&quot;]
        L_eta_reco = scaled_df[&quot;RecoDataeta&quot;]
        target = (L_eta_reco + 10) / (L_eta_gen + 10)
    if variable == &quot;phi&quot;:
        L_phi_gen = scaled_df[&quot;genDataphi&quot;]
        L_phi_reco = scaled_df[&quot;RecoDataphi&quot;]
        target = (L_phi_reco + 10) / (L_phi_gen + 10)
    if variable == &quot;m&quot;:
        L_m_gen = scaled_df[&quot;genDatam&quot;]
        L_m_reco = scaled_df[&quot;RecoDatam&quot;]
        target = (L_m_reco + 10) / (L_m_gen + 10)

    return target


# @memory.cache
def split_t_x(df, target, input_features):
    &quot;&quot;&quot;Get teh target as the ratio, according to the T equation&quot;&quot;&quot;

    if target == &quot;RecoDatam&quot;:
        t = T(&quot;m&quot;, scaled_df=scaled_train_data)
    if target == &quot;RecoDatapT&quot;:
        t = T(&quot;pT&quot;, scaled_df=scaled_train_data)
    if target == &quot;RecoDataeta&quot;:
        t = T(&quot;eta&quot;, scaled_df=scaled_train_data)
    if target == &quot;RecoDataphi&quot;:
        t = T(&quot;phi&quot;, scaled_df=scaled_train_data)
    x = np.array(df[input_features])
    return np.array(t), x


# @memory.cache
def split_t_x_test(df, target, input_features):
    &quot;&quot;&quot;Get teh target as the ratio, according to the T equation&quot;&quot;&quot;

    if target == &quot;RecoDatam&quot;:
        t = T(&quot;m&quot;, scaled_df=scaled_test_data)
    if target == &quot;RecoDatapT&quot;:
        t = T(&quot;pT&quot;, scaled_df=scaled_test_data)
    if target == &quot;RecoDataeta&quot;:
        t = T(&quot;eta&quot;, scaled_df=scaled_test_data)
    if target == &quot;RecoDataphi&quot;:
        t = T(&quot;phi&quot;, scaled_df=scaled_test_data)
    x = np.array(df[input_features])
    return np.array(t), x


#########################################################################
# @memory.cache
def normal_split_t_x(df, target, input_features):
    # change from pandas dataframe format to a numpy
    # array of the specified types
    # t = np.array(df[target])
    t = np.array(df[target])
    x = np.array(df[input_features])
    return t, x


################ Apply Z scaling############
def z(x):
    eps = 1e-20
    return (x - np.mean(x)) / (np.std(x) + eps)


def z_inverse(xprime, x):
    return xprime * np.std(x) + np.mean(x)


# @memory.cache
def z2(x, mean, std):
    &quot;&quot;&quot;
    Args:
        x ([type]): [description]
        mean ([type]): [description]
        std ([type]): [description]

    Returns:
        [type]: [description]
    &quot;&quot;&quot;
    eps = 1e-20
    scaled = (x - mean) / (std + eps)
    return np.array(scaled, dtype=np.float64)


def z_inverse(xprime, x):
    unscaled = xprime * np.std(x) + np.mean(x)
    return np.array(unscaled, dtype=np.float64)


# @memory.cache
def z_inverse2(xprime, train_mean, train_std):
    &quot;&quot;&quot;mean original train mean, std: original. Probably not needed&quot;&quot;&quot;
    return xprime * train_std + train_mean


# @memory.cache
def apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x):
    &quot;&quot;&quot;TO ensure this z scaling is only applied once to the training features, we use a generator.
    This doesn&#39;t change the shapes of anything, just applies z to all the feature columns other than tau&quot;&quot;&quot;
    NFEATURES = train_x.shape[1]
    for i in range(NFEATURES - 1):
        variable = list(TRAIN_SCALE_DICT)[i]
        train_mean = float(TRAIN_SCALE_DICT[variable][&quot;mean&quot;])
        train_std = float(TRAIN_SCALE_DICT[variable][&quot;std&quot;])
        train_x[:, i] = z2(train_x[:, i], mean=train_mean, std=train_std)
        test_x[:, i] = z2(test_x[:, i], mean=train_mean, std=train_std)
        valid_x[:, i] = z2(valid_x[:, i], mean=train_mean, std=train_std)
    yield train_x
    yield test_x
    yield valid_x


# @memory.cache
def apply_z_to_targets(train_t, test_t, valid_t):
    train_mean = np.mean(train_t)
    train_std = np.std(train_t)
    train_t_ = z2(train_t, mean=train_mean, std=train_std)
    test_t_ = z2(test_t, mean=train_mean, std=train_std)
    valid_t_ = z2(valid_t, mean=train_mean, std=train_std)

    yield train_t_
    yield test_t_
    yield valid_t_


# @utils.debug
def save_model(model, PATH):
    print(model)
    torch.save(model.state_dict(), PATH)
    print(&quot;\ntrained model dictionary saved in %s&quot; % PATH)


# @utils.debug
def load_model(PATH, PARAMS):
    # n_layers = int(BEST_PARAMS[&quot;n_layers&quot;])
    # hidden_size = int(BEST_PARAMS[&quot;hidden_size&quot;])
    # dropout = float(BEST_PARAMS[&quot;dropout&quot;])
    # optimizer_name = BEST_PARAMS[&quot;optimizer_name&quot;].to_string().split()[1]
    # learning_rate =  float(BEST_PARAMS[&quot;learning_rate&quot;])
    # batch_size = int(BEST_PARAMS[&quot;batch_size&quot;])
    model = utils.RegularizedRegressionModel(
        nfeatures=NFEATURES,
        ntargets=1,
        nlayers=PARAMS[&quot;n_layers&quot;],
        hidden_size=PARAMS[&quot;hidden_size&quot;],
        dropout_1=PARAMS[&quot;dropout_1&quot;],
        dropout_2=PARAMS[&quot;dropout_2&quot;],
        activation=PARAMS[&quot;activation&quot;],
    )
    model.load_state_dict(torch.load(PATH))
    # OR
    # model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED! Also, use dictionary &quot;.pth&quot; which has both the model state dict and the PARAMS dict
    model.eval()
    print(model)
    return model


# @memory.cache
def simple_eval(model, test_x_z_scaled):
    model.eval()
    # evaluate on the scaled features
    valid_x_tensor = torch.from_numpy(test_x_z_scaled).float()
    # valid_x_tensor=torch.from_numpy(train_x).float()
    pred = model(valid_x_tensor)
    p = pred.detach().numpy()
    # if USE_BRADEN_SCALING:
    #     fig, ax = plt.subplots(1,1)
    #     label=FIELDS[target][&#39;ylabel&#39;]
    #     ax.hist(p, label=f&#39;Predicted post-z ratio for {label}&#39;, alpha=0.4, density=True)
    #     # orig_ratio = z(T(&#39;m&#39;, scaled_df=scaled_train_data))
    #     orig_ratio = z(T(&#39;m&#39;, scaled_df=scaled_test_data))
    #     print(orig_ratio[:5])
    #     ax.hist(orig_ratio, label = f&#39;original post-z ratio for {label}&#39;, alpha=0.4,density=True)
    #     ax.grid()
    #     set_axes(ax, xlabel=&#39;predicted $T$&#39;)
    # print(&#39;predicted ratio shape: &#39;, p.shape)
    return p

def get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME):
        
    print(f&#39;Test (evaluation) Data is Autoregressive, loading {AUTOREGRESSIVE_DIST_NAME}&#39;)
    eval_data = pd.read_csv(
        os.path.join(
            IQN_BASE,
            &quot;JupyterBook&quot;,
            &quot;Cluster&quot;,
            &quot;EVALUATE&quot;,
            AUTOREGRESSIVE_DIST_NAME,
        )
    )
    return eval_data


# @memory.cache
def get_hist(label):
    &quot;&quot;&quot;label could be &quot;pT&quot;, &quot;eta&quot;, &quot;phi&quot;, &quot;m&quot; &quot;&quot;&quot;
    predicted_label_counts, label_edges = np.histogram(
        JETS_DICT[&quot;Predicted_RecoData&quot; + label][&quot;dist&quot;],
        range=JETS_DICT[&quot;Predicted_RecoData&quot; + label][&quot;range&quot;],
        bins=bins,
    )
    real_label_counts, _ = np.histogram(
        JETS_DICT[&quot;Real_RecoData&quot; + label][&quot;dist&quot;],
        range=JETS_DICT[&quot;Real_RecoData&quot; + label][&quot;range&quot;],
        bins=bins,
    )
    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2

    return real_label_counts, predicted_label_counts, label_edges


# @memory.cache
def get_hist_simple(predicted_dist, target):
    
    range_ = (FIELDS[target][&quot;xmin&quot;], FIELDS[target][&quot;xmax&quot;])
    bins=50
    predicted_label_counts, label_edges = np.histogram(
        predicted_dist, range=range_, bins=bins
    )
    
    
    real_label_counts, _ = np.histogram(REAL_DIST, range=range_, bins=bins)
    label_edges = label_edges[1:] / 2 + label_edges[:-1] / 2
    return real_label_counts, predicted_label_counts, label_edges


# @memory.cache
def plot_one(
    target, real_edges, real_counts, predicted_counts, save_plot=False, PARAMS=None, JUPYTER=True
):
    fig, (ax1, ax2) = plt.subplots(
        2, 1, figsize=(3.5 * 3 / 2.5, 3.8), gridspec_kw={&quot;height_ratios&quot;: [2, 0.5]}
    )
    ax1.step(
        real_edges, real_counts / norm_data, where=&quot;mid&quot;, color=&quot;k&quot;, linewidth=0.5
    )  # step real_count_pt
    ax1.step(
        real_edges,
        predicted_counts / norm_IQN,
        where=&quot;mid&quot;,
        color=&quot;#D7301F&quot;,
        linewidth=0.5,
    )  # step predicted_count_pt
    ax1.scatter(
        real_edges,
        real_counts / norm_data,
        label=&quot;reco&quot;,
        color=&quot;k&quot;,
        facecolors=&quot;none&quot;,
        marker=&quot;o&quot;,
        s=5,
        linewidth=0.5,
    )
    ax1.scatter(
        real_edges,
        predicted_counts / norm_IQN,
        label=&quot;predicted&quot;,
        color=&quot;#D7301F&quot;,
        marker=&quot;x&quot;,
        s=5,
        linewidth=0.5,
    )
    ax1.set_xlim(range_)
    ax1.set_ylim(0, max(predicted_counts / norm_IQN) * 1.1)
    ax1.set_ylabel(&quot;counts&quot;)
    ax1.set_xticklabels([])
    ax1.legend(loc=&quot;upper right&quot;)

    ratio = (predicted_counts / norm_IQN) / (real_counts / norm_data)
    ax2.scatter(
        real_edges, ratio, color=&quot;r&quot;, marker=&quot;x&quot;, s=5, linewidth=0.5
    )  # PREDICTED (IQN)/Reco (Data)
    ax2.scatter(
        real_edges,
        ratio / ratio,
        color=&quot;k&quot;,
        marker=&quot;o&quot;,
        facecolors=&quot;none&quot;,
        s=5,
        linewidth=0.5,
    )
    ax2.set_xlim(range_)
    ax2.set_xlabel(FIELDS[target][&quot;xlabel&quot;])
    ax2.set_ylabel(
        r&quot;$\frac{\textnormal{predicted}}{\textnormal{reco}}$&quot;
        #    , fontsize=10
    )
    ax2.set_ylim((YLIM))
    ax2.set_xlim(range_)
    ax2.set_yticklabels([0.8, 1.0, 1.2])
    if JUPYTER==True:
        plt.show()
    else:
        plt.tight_layout()
        fig.subplots_adjust(wspace=0.5, hspace=0.2)
        fig.subplots_adjust(wspace=0.0, hspace=0.1)
        plt.axis(&#39;off&#39;)

    # plt.gca().set_position([0, 0, 1, 1])
    if save_plot:
        plot_filename = utils.get_model_filename(target, PARAMS).split(&quot;.dict&quot;)[0] + &quot;.png&quot;
        plt.savefig(
            os.path.join(IQN_BASE, &quot;JupyterBook&quot;, &quot;Cluster&quot;, &quot;EVALUATE&quot;, plot_filename)
        )

    
    # fig.show()
    # plt.show();
    # plt.axis(&quot;off&quot;)
    # plt.gca().set_position([0, 0, 1, 1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#load data only once, and with caching!
raw_train_data, raw_test_data, raw_valid_data = load_raw_data()

# Load scaled data
# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluate-mass">
<h2>3.3: Evaluate Mass<a class="headerlink" href="#evaluate-mass" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># TODO: plot the loss curves (train and valid) of all 4 networks on the same plot,
# and with the learning rate plotted on the same plot but on a different y axis 
# (x axis being iteration, with marks indicating epochs)

target = &quot;RecoDatam&quot;
source = FIELDS[target]
features = source[&quot;inputs&quot;]
print(&quot;Training Features:\n&quot;, features)
print(&quot;\nTarget = &quot;, target)
AUTOREGRESSIVE_DIST_NAME = &quot;AUTOREGRESSIVE_m_Prime.csv&quot;
print(&quot;USING NEW DATASET\n&quot;)
######################################
USE_BRADEN_SCALING = False
#####################################
################################### CONFIGURATIONS ###################################

JUPYTER = True
use_subsample = False
# use_subsample = True
if use_subsample:
    SUBSAMPLE = int(
        1e5
    )  # subsample use for development - in production use whole dataset
else:
    SUBSAMPLE = None

########################################################################################
# Get targets and features
if USE_BRADEN_SCALING==True:
    print(f&quot;spliting data for {target}&quot;)
    train_t, train_x = split_t_x(
        df=scaled_train_data, target=target, input_features=features
    )
    print(&quot;train_t shape = &quot;, train_t.shape, &quot;train_x shape = &quot;, train_x.shape)
    print(&quot;\n Training features:\n&quot;)
    print(train_x)
    valid_t, valid_x = split_t_x(
        df=scaled_valid_data, target=target, input_features=features
    )
    print(&quot;valid_t shape = &quot;, valid_t.shape, &quot;valid_x shape = &quot;, valid_x.shape)
    test_t, test_x = split_t_x(
        df=scaled_test_data, target=target, input_features=features
    )
    print(&quot;test_t shape = &quot;, test_t.shape, &quot;test_x shape = &quot;, test_x.shape)

else:
    print(f&quot;spliting data for {target}&quot;)
    train_t, train_x = normal_split_t_x(
        df=raw_train_data, target=target, input_features=features
    )
    print(&quot;train_t shape = &quot;, train_t.shape, &quot;train_x shape = &quot;, train_x.shape)
    print(&quot;\n Training features:\n&quot;)
    print(train_x)
    valid_t, valid_x = normal_split_t_x(
        df=raw_valid_data, target=target, input_features=features
    )
    print(&quot;valid_t shape = &quot;, valid_t.shape, &quot;valid_x shape = &quot;, valid_x.shape)
    test_t, test_x = normal_split_t_x(
        df=raw_test_data, target=target, input_features=features
    )
    print(&quot;test_t shape = &quot;, test_t.shape, &quot;test_x shape = &quot;, test_x.shape)

print(&quot;no need to train_test_split since we already have the split dataframes&quot;)
print(valid_x.mean(axis=0), valid_x.std(axis=0))
print(train_x.mean(axis=0), train_x.std(axis=0))
print(valid_t.mean(), valid_t.std())
print(train_t.mean(), train_t.std())
######################################################

# Apply z scaling to features and targets
# to features

NFEATURES = train_x.shape[1]
TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)
# to features
apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)
train_x_z_scaled = next(apply_z_generator)
test_x_z_scaled = next(apply_z_generator)
valid_x_z_scaled = next(apply_z_generator)
print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))
print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))
# to targets
apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)
train_t_z_scaled = next(apply_z_to_targets_generator)
test_t_z_scaled = next(apply_z_to_targets_generator)
valid_t_z_scaled = next(apply_z_to_targets_generator)
print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())
print(train_t_z_scaled.mean(), train_t_z_scaled.std())

###########################################################
# Get the  parameters for this model and training
PARAMS_m = {
&quot;n_layers&quot;: int(4),
&quot;hidden_size&quot;: int(6),
&quot;dropout_1&quot;: float(0.6),
&quot;dropout_2&quot;: float(0.1),
&quot;activation&quot;: &quot;LeakyReLU&quot;,
    &#39;optimizer_name&#39;:&#39;NAdam&#39;,
    &#39;starting_learning_rate&#39;:float(0.7),
    &#39;momentum&#39;:float(0.6),
    &#39;batch_size&#39;:int(1024),
    &#39;n_iterations&#39;: int(2e6),
}

optimizer_name = PARAMS_m[&quot;optimizer_name&quot;]
print(type(optimizer_name))
# optimizer_name = BEST_PARAMS[&quot;optimizer_name&quot;].to_string().split()[1]
NITERATIONS = PARAMS_m[&quot;n_iterations&quot;]
BATCHSIZE = PARAMS_m[&quot;batch_size&quot;]
comment = &quot;&quot;

# N_epochs X N_train_examples = N_iterations X batch_size
N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])
print(
    f&quot;This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs&quot;
)

# &#39;Trained_IQNx4_%s_TUNED.dict&#39; % target
filename_model = utils.get_model_filename(target, PARAMS_m)
# OR, if you know a model filename directly, you can also specify it, 
# BUT, if you pull a trained model explicitly, you have to make sure its parameters in the PARAMS dictionary above match
# Nominal one is &#39;Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict&#39;, also in backup
# filename_model=&#39;Trained_IQNx4_RecoDatam_ 8_layer5_hiddenLeakyReLU_activation1024_batchsize300_Kiteration.dict&#39;
# filename_model=&#39;Trained_IQNx4_RecoDatapT_10_layer6_hiddenLeakyReLU_activation512_batchsize300_Kiteration.dict&#39;
trained_models_dir = &quot;trained_models&quot;
utils.mkdir(trained_models_dir)
# on cluster, Im using another TRAIN directory
PATH_model = os.path.join(
    IQN_BASE,  # the loaction of the repo
    &quot;JupyterBook&quot;,  # up tp TRAIN could be combined in a srs dicretory
    &quot;Cluster&quot;,
    &quot;TRAIN&quot;,
    trained_models_dir,  # /trained_models
    filename_model,  # utils.get_model_filename has the saved file format
)

# Load trained model
IQN_m = load_model(PATH_model, PARAMS_m)
# Get predicted distribution
p = simple_eval(IQN_m, test_x_z_scaled)

range_ = (FIELDS[target][&quot;xmin&quot;], FIELDS[target][&quot;xmax&quot;])
bins = 50
REAL_RAW_DATA = raw_test_data

YLIM = (0.8, 1.2)
###########GET REAL DIST###########
REAL_RAW_DATA = REAL_RAW_DATA[
    [&quot;RecoDatapT&quot;, &quot;RecoDataeta&quot;, &quot;RecoDataphi&quot;, &quot;RecoDatam&quot;]
]
REAL_RAW_DATA.columns = [&quot;realpT&quot;, &quot;realeta&quot;, &quot;realphi&quot;, &quot;realm&quot;]
REAL_DIST = REAL_RAW_DATA[&quot;realm&quot;]
norm_data = REAL_RAW_DATA.shape[0]
#############GET EVALUATION DIST#############
raw_test_data.describe()
m_reco = raw_test_data[&quot;RecoDatam&quot;]
m_gen = raw_test_data[&quot;genDatam&quot;]
# plt.hist(m_reco,label=r&#39;$m_{gen}^{test \ data}$&#39;);plt.legend();plt.show()


def descale_Braden_scaled_prediction(label, p):
    &quot;&quot;&quot;Label could be m. p is the outcome of the model evaluation, e.g. 
    IQN_m = load_model(PATH_model, PARAMS_m)
    p = simple_eval(IQN_m, test_x_z_scaled)
    
    &quot;&quot;&quot;
    # make sure you&#39;ve set braden scaling global variable to use this function.
    assert USE_BRADEN_SCALING==True
    orig_ratio = T(label, scaled_df=scaled_train_data)
    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))
    L_obs = L(orig_observable=m_gen, label=label)
    z_inv_f = z_inv_f.flatten()
    print(z_inv_f.shape)

    factor = (z_inv_f * (L_obs + 10)) - 10
    label_pred = L_inverse(L_observable=factor, label=label)
    return label_pred
    
    
m_pred = z_inverse2(
    xprime=p,
    train_mean=TRAIN_SCALE_DICT[target][&quot;mean&quot;],
    train_std=TRAIN_SCALE_DICT[target][&quot;std&quot;],
)
m_pred = m_pred.flatten()

# Get histogram of predicted distribution
real_label_counts_m, predicted_label_counts_m, label_edges_m = get_hist_simple(
    predicted_dist=m_pred, target=target
)
# eval_data=pd.read_csv(DATA_DIR+&#39;/test_data_10M_2.csv&#39;)
# Get evaluation data
eval_data = pd.read_csv(DATA_DIR + &quot;/test_data_10M_2.csv&quot;)
ev_features = features
eval_data = eval_data[ev_features]
# save new distribution (m) in the eval data as autoregressive eval for next IQN
eval_data[target] = m_pred

new_cols = [target] + features
eval_data = eval_data.reindex(columns=new_cols)
print(&quot;EVALUATION DATA NEW INDEX\n&quot;, eval_data.head())

eval_data.to_csv(
    os.path.join(
        IQN_BASE, &quot;JupyterBook&quot;, &quot;Cluster&quot;, &quot;EVALUATE&quot;, AUTOREGRESSIVE_DIST_NAME
    )
)

# Load this saved predited autoregressive distribution
AUTOREGRESSIVE_DIST = pd.read_csv(
    os.path.join(
        IQN_BASE, &quot;JupyterBook&quot;, &quot;Cluster&quot;, &quot;EVALUATE&quot;, AUTOREGRESSIVE_DIST_NAME
    )
)

# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]
# get normalization values
norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]
norm_IQN = norm_autoregressive
print(
    &quot;norm_data&quot;,
    norm_data,
    &quot;\nnorm IQN&quot;,
    norm_IQN,
    &quot;\nnorm_autoregressive&quot;,
    norm_autoregressive,
)

# Finally, plot predicted distribution

plot_one(
    target=target,
    real_edges=label_edges_m,
    real_counts=real_label_counts_m,
    predicted_counts=predicted_label_counts_m,
    save_plot=True,
    PARAMS=PARAMS_m
)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Features:
 [&#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;, &#39;tau&#39;]

Target =  RecoDatam
USING NEW DATASET

spliting data for RecoDatam
train_t shape =  (8000000,) train_x shape =  (8000000, 5)

 Training features:

[[29.4452      0.828187    2.90213     2.85348     0.36130954]
 [24.3193     -1.16351     0.636469    5.83685     0.12689925]
 [24.3193     -1.16351     0.636469    5.83685     0.96230681]
 ...
 [41.4192     -2.23358    -2.81921     7.19348     0.08421659]
 [35.4637     -1.12318     0.356494    6.06597     0.05535172]
 [26.5586     -1.09427    -1.49334     4.25409     0.07489863]]
valid_t shape =  (1000000,) valid_x shape =  (1000000, 5)
test_t shape =  (1000000,) test_x shape =  (1000000, 5)
no need to train_test_split since we already have the split dataframes
[ 3.27223764e+01  6.98189368e-04 -8.95543973e-04  6.96116528e+00
  5.00485136e-01] [15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]
[ 3.26952341e+01 -1.78188172e-03 -3.83090331e-04  6.96299435e+00
  4.99915289e-01] [14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]
5.5514112643334546 2.664124544901276
5.555567451922438 2.664339857066051
NORMAL UNSCALED DICTIONARY
{&#39;genDatapT&#39;: {&#39;mean&#39;: 32.695234084987476, &#39;std&#39;: 14.937932540562551}, &#39;genDataeta&#39;: {&#39;mean&#39;: -0.0017818817154031672, &#39;std&#39;: 2.204309760627079}, &#39;genDataphi&#39;: {&#39;mean&#39;: -0.0003830903308450233, &#39;std&#39;: 1.8138251604791067}, &#39;genDatam&#39;: {&#39;mean&#39;: 6.962994352358474, &#39;std&#39;: 2.781332025286383}, &#39;RecoDatapT&#39;: {&#39;mean&#39;: 32.86720151648752, &#39;std&#39;: 15.829355769531851}, &#39;RecoDataeta&#39;: {&#39;mean&#39;: -0.0017898858568513964, &#39;std&#39;: 2.197968491495457}, &#39;RecoDataphi&#39;: {&#39;mean&#39;: -0.0004719170328962474, &#39;std&#39;: 1.8144739820043825}, &#39;RecoDatam&#39;: {&#39;mean&#39;: 5.555567451922438, &#39;std&#39;: 2.664339857066051}}



[ 1.81700902e-03  1.12510099e-03 -2.82526482e-04 -6.57626105e-04
  5.00485136e-01] [1.01748627 0.9999745  0.99989115 0.99987283 0.28852734]
[ 1.61650249e-15 -1.10134124e-18 -3.12905257e-17  5.01036723e-15
  4.99915289e-01] [1.         1.         1.         1.         0.28867295]
-0.0015599314696879494 0.9999191874249058
6.996216939114674e-16 1.0000000000000002
&lt;class &#39;str&#39;&gt;
This model was trained for 2000000 iteration, which is  256.0 epochs
RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.3)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.3)
    (5): Linear(in_features=6, out_features=6, bias=True)
    (6): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.3)
    (8): Linear(in_features=6, out_features=6, bias=True)
    (9): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.3)
    (11): Linear(in_features=6, out_features=1, bias=True)
  )
)
EVALUATION DATA NEW INDEX
    RecoDatam  genDatapT  genDataeta  genDataphi  genDatam       tau
0   4.840796    43.6113    0.824891    -1.26949   5.93310  0.250046
1   7.059293    43.6113    0.824891    -1.26949   5.93310  0.847493
2   5.725040    26.0153    3.529970     1.55495   7.41270  0.851995
3   3.605120    28.4944   -1.159650     1.82602   7.84157  0.052378
4   3.521224    21.9840    2.747660     2.03085   5.18315  0.542549
norm_data 1000000 
norm IQN 1000000 
norm_autoregressive 1000000
</pre></div>
</div>
<img alt="_images/3_Autoregressive_Evaluation_8_2.png" src="_images/3_Autoregressive_Evaluation_8_2.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 640x480 with 0 Axes&gt;
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluate-p-t">
<h2>3.4: Evaluate <span class="math notranslate nohighlight">\(p_T\)</span><a class="headerlink" href="#evaluate-p-t" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>eval_data_df = get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME=PREVIOUS_AUTOREGRESSIVE_DIST_NAME)
eval_data_df.head()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test (evaluation) Data is Autoregressive, loading AUTOREGRESSIVE_m_Prime.csv
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>RecoDatam</th>
      <th>genDatapT</th>
      <th>genDataeta</th>
      <th>genDataphi</th>
      <th>genDatam</th>
      <th>tau</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>4.797721</td>
      <td>43.6113</td>
      <td>0.824891</td>
      <td>-1.26949</td>
      <td>5.93310</td>
      <td>0.250046</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>6.829849</td>
      <td>43.6113</td>
      <td>0.824891</td>
      <td>-1.26949</td>
      <td>5.93310</td>
      <td>0.847493</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>5.645979</td>
      <td>26.0153</td>
      <td>3.529970</td>
      <td>1.55495</td>
      <td>7.41270</td>
      <td>0.851995</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>3.593726</td>
      <td>28.4944</td>
      <td>-1.159650</td>
      <td>1.82602</td>
      <td>7.84157</td>
      <td>0.052378</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>3.501870</td>
      <td>21.9840</td>
      <td>2.747660</td>
      <td>2.03085</td>
      <td>5.18315</td>
      <td>0.542549</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>target = &quot;RecoDatapT&quot;
source = FIELDS[target]
features = source[&quot;inputs&quot;]
print(&quot;Training Features:\n&quot;, features)
print(&quot;\nTarget = &quot;, target)
PREVIOUS_AUTOREGRESSIVE_DIST_NAME = &quot;AUTOREGRESSIVE_m_Prime.csv&quot;
AUTOREGRESSIVE_DIST_NAME = &quot;AUTOREGRESSIVE_m_Prime_pT_Prime.csv&quot;
print(&quot;USING NEW DATASET\n&quot;)
######################################
USE_BRADEN_SCALING = False

########################################################################################

raw_train_data, raw_test_data, raw_valid_data = load_raw_data()


# Load scaled data
# scaled_train_data, scaled_test_data, scaled_valid_data = load_scaled_dataframes()


################################################## Load Evaluation Data
#eval_data=pd.read_csv(DATA_DIR+&#39;/test_data_10M_2.csv&#39;)
# Or test on actual test (evaluation) data for development
# eval_data=pd.read_csv(DATA_DIR+&#39;/test_data_10M_2.csv&#39;)

# Get targets and features
if USE_BRADEN_SCALING == True:
    print(f&quot;spliting data for {target}&quot;)
    train_t, train_x = split_t_x(
        df=scaled_train_data, target=target, input_features=features
    )
    print(&quot;train_t shape = &quot;, train_t.shape, &quot;train_x shape = &quot;, train_x.shape)
    print(&quot;\n Training features:\n&quot;)
    print(train_x)
    valid_t, valid_x = split_t_x(
        df=scaled_valid_data, target=target, input_features=features
    )
    print(&quot;valid_t shape = &quot;, valid_t.shape, &quot;valid_x shape = &quot;, valid_x.shape)
        
    ##### WHAT MATTERS IS TEST (EVALUATION)

    test_t, test_x = split_t_x(
        df=scaled_test_data, target=target, input_features=features
    )
    print(&quot;test_t shape = &quot;, test_t.shape, &quot;test_x shape = &quot;, test_x.shape)

else:
    print(f&quot;spliting autoregressive evaluation data for {target}&quot;)
    train_t, train_x = normal_split_t_x(
            df=raw_train_data, target=target, input_features=features
        )
    print(&quot;train_t shape = &quot;, train_t.shape, &quot;train_x shape = &quot;, train_x.shape)
    print(&quot;\n Training features:\n&quot;)
    print(train_x)
    valid_t, valid_x = normal_split_t_x(
        df=raw_valid_data, target=target, input_features=features
    )
    print(&quot;valid_t shape = &quot;, valid_t.shape, &quot;valid_x shape = &quot;, valid_x.shape)
    ##### WHAT MATTERS IS TEST (EVALUATION)
    test_t, test_x = normal_split_t_x(
        df=raw_test_data, target=target, input_features=features
    )
    print(&quot;test_t shape = &quot;, test_t.shape, &quot;test_x shape = &quot;, test_x.shape)

print(&quot;no need to train_test_split since we already have the split dataframes&quot;)
print(valid_x.mean(axis=0), valid_x.std(axis=0))
print(train_x.mean(axis=0), train_x.std(axis=0))
print(valid_t.mean(), valid_t.std())
print(train_t.mean(), train_t.std())
######################################################
# Replace test_x with eval_data

# ev_features = features
# eval_data_df = eval_data[ev_features]
# eval_data = np.array(eval_data_df)
# test_x = eval_data

# eval_data=pd.read_csv(DATA_DIR+&#39;/test_data_10M_2.csv&#39;)
    
# eval_data=raw_train_data[:raw_test_data.shape[0]]
# test_x = np.array

# Apply z scaling to features and targets
# to features
#####################################################################
NFEATURES = train_x.shape[1]
# GET EVALUATION DATASET
# eval_data= get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME=PREVIOUS_AUTOREGRESSIVE_DIST_NAME)
# test_x = np.array(eval_data[features])



TRAIN_SCALE_DICT = get_train_scale_dict(USE_BRADEN_SCALING)
# to features
apply_z_generator = apply_z_to_features(TRAIN_SCALE_DICT, train_x, test_x, valid_x)
train_x_z_scaled = next(apply_z_generator)
test_x_z_scaled = next(apply_z_generator)
valid_x_z_scaled = next(apply_z_generator)
print(valid_x_z_scaled.mean(axis=0), valid_x_z_scaled.std(axis=0))
print(train_x_z_scaled.mean(axis=0), train_x_z_scaled.std(axis=0))
# to targets
apply_z_to_targets_generator = apply_z_to_targets(train_t, test_t, valid_t)
train_t_z_scaled = next(apply_z_to_targets_generator)
test_t_z_scaled = next(apply_z_to_targets_generator)
valid_t_z_scaled = next(apply_z_to_targets_generator)
print(valid_t_z_scaled.mean(), valid_t_z_scaled.std())
print(train_t_z_scaled.mean(), train_t_z_scaled.std())

###########################################################
# Get the  parameters for this model and training
PARAMS_pT = {
&quot;n_layers&quot;: int(3),
&quot;hidden_size&quot;: int(16),
&quot;dropout_1&quot;: float(0.6),
&quot;dropout_2&quot;: float(0.1),
&quot;activation&quot;: &quot;LeakyReLU&quot;,
    &#39;optimizer_name&#39;:&#39;NAdam&#39;,
    &#39;starting_learning_rate&#39;:float(0.5),
    &#39;momentum&#39;:float(0.6),
    &#39;batch_size&#39;:int(1024),
    &#39;n_iterations&#39;: int(2e6),
}

optimizer_name = PARAMS_pT[&quot;optimizer_name&quot;]
print(type(optimizer_name))
# optimizer_name = BEST_PARAMS[&quot;optimizer_name&quot;].to_string().split()[1]
NITERATIONS = PARAMS_pT[&quot;n_iterations&quot;]
BATCHSIZE = PARAMS_pT[&quot;batch_size&quot;]
comment = &quot;&quot;

# N_epochs X N_train_examples = N_iterations X batch_size
N_epochs = (NITERATIONS * BATCHSIZE) / int(train_x.shape[0])
print(
    f&quot;This model was trained for {NITERATIONS} iteration, which is  {N_epochs} epochs&quot;
)


filename_model = utils.get_model_filename(target, PARAMS_pT)
# filename_model = &#39;Trained_IQNx4_RecoDatapT_ 13_layer6_hiddenLeakyReLU_activation1024_batchsize200_Kiteration.dict&#39;
# filename_model = &#39;Trained_IQNx4_RecoDatapT_10_layer6_hiddenLeakyReLU_activation512_batchsize300_Kiteration.dict&#39;
trained_models_dir = &quot;trained_models&quot;
utils.mkdir(trained_models_dir)
# on cluster, Im using another TRAIN directory
PATH_model = os.path.join(
    IQN_BASE,  # the loaction of the repo
    &quot;JupyterBook&quot;,  # up tp TRAIN could be combined in a srs dicretory
    &quot;Cluster&quot;,
    &quot;TRAIN&quot;,
    trained_models_dir,  # /trained_models
    filename_model,  # utils.get_model_filename has the saved file format
)

# Load trained model
IQN_pT = load_model(PATH_model, PARAMS_pT)
# Get predicted distribution
p = simple_eval(IQN_pT, test_x_z_scaled)

range_ = (FIELDS[target][&quot;xmin&quot;], FIELDS[target][&quot;xmax&quot;])
bins = 50
REAL_RAW_DATA = raw_test_data

YLIM = (0.8, 1.2)
###########GET REAL DIST###########
REAL_RAW_DATA = REAL_RAW_DATA[
    [&quot;RecoDatapT&quot;, &quot;RecoDataeta&quot;, &quot;RecoDataphi&quot;, &quot;RecoDatam&quot;]
]
REAL_RAW_DATA.columns = [&quot;realpT&quot;, &quot;realeta&quot;, &quot;realphi&quot;, &quot;realm&quot;]
REAL_DIST = REAL_RAW_DATA[&quot;realpT&quot;]
norm_data = REAL_RAW_DATA.shape[0]
#############GET EVALUATION DIST#############
raw_test_data.describe()
pT_reco = raw_test_data[&quot;RecoDatapT&quot;]
pT_gen = raw_test_data[&quot;genDatapT&quot;]
# plt.hist(m_reco,label=r&#39;$m_{gen}^{test \ data}$&#39;);plt.legend();plt.show()

def descale_Braden_scaled_prediction(label, p):
    &quot;&quot;&quot;Label could be m. p is the outcome of the model evaluation, e.g. 
    IQN_m = load_model(PATH_model, PARAMS_m)
    p = simple_eval(IQN_m, test_x_z_scaled)

    &quot;&quot;&quot;
    # make sure you&#39;ve set braden scaling global variable to use this function.
    assert USE_BRADEN_SCALING==True
    orig_ratio = T(label, scaled_df=scaled_train_data)
    z_inv_f = z_inverse(xprime=p, mean=np.mean(orig_ratio), std=np.std(orig_ratio))
    L_obs = L(orig_observable=pT_gen, label=label)
    z_inv_f = z_inv_f.flatten()
    print(z_inv_f.shape)

    factor = (z_inv_f * (L_obs + 10)) - 10
    label_pred = L_inverse(L_observable=factor, label=label)
    return label_pred


pT_pred = z_inverse2(
    xprime=p,
    train_mean=TRAIN_SCALE_DICT[target][&quot;mean&quot;],
    train_std=TRAIN_SCALE_DICT[target][&quot;std&quot;],
)
pT_pred = pT_pred.flatten()

# Get histogram of predicted distribution
real_label_counts_pT, predicted_label_counts_pT, label_edges_pT = get_hist_simple(
    predicted_dist=pT_pred, target=target
)

# Get evaluation data as test data for development

# eval_data_df=pd.read_csv(DATA_DIR+&#39;/test_data_10M_2.csv&#39;)#[features]


eval_data_df = get_previous_autoregressive_dist(AUTOREGRESSIVE_DIST_NAME=PREVIOUS_AUTOREGRESSIVE_DIST_NAME)

ev_features = features
eval_data = eval_data_df[ev_features]
# save new distribution (pT) in the eval data as autoregressive eval for next IQN
eval_data_df[target] = pT_pred
#change order of columns
new_cols = [&quot;RecoDatam&quot;, target] + X
eval_data_df = eval_data_df.reindex(columns=new_cols)
print(&quot;EVALUATION DATA NEW INDEX\n&quot;, eval_data_df.head())
# save 
eval_data_df.to_csv(
    os.path.join(
        IQN_BASE, &quot;JupyterBook&quot;, &quot;Cluster&quot;, &quot;EVALUATE&quot;, AUTOREGRESSIVE_DIST_NAME
    )
)

# Load this saved predited autoregressive distribution
AUTOREGRESSIVE_DIST = pd.read_csv(
    os.path.join(
        IQN_BASE, &quot;JupyterBook&quot;, &quot;Cluster&quot;, &quot;EVALUATE&quot;, AUTOREGRESSIVE_DIST_NAME
    )
)

# norm_IQN=AUTOREGRESSIVE_DIST.shape[0]
# get normalization values
norm_autoregressive = AUTOREGRESSIVE_DIST.shape[0]
norm_IQN = norm_autoregressive
print(
    &quot;norm_data&quot;,
    norm_data,
    &quot;\nnorm IQN&quot;,
    norm_IQN,
    &quot;\nnorm_autoregressive&quot;,
    norm_autoregressive,
)

# Finally, plot predicted distribution
plot_one(
    target=target,
    real_edges=label_edges_pT,
    real_counts=real_label_counts_pT,
    predicted_counts=predicted_label_counts_pT,
    save_plot=True,
    PARAMS=PARAMS_pT,
)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Features:
 [&#39;RecoDatam&#39;, &#39;genDatapT&#39;, &#39;genDataeta&#39;, &#39;genDataphi&#39;, &#39;genDatam&#39;, &#39;tau&#39;]

Target =  RecoDatapT
USING NEW DATASET

spliting autoregressive evaluation data for RecoDatapT
train_t shape =  (8000000,) train_x shape =  (8000000, 6)

 Training features:

[[ 2.59587    29.4452      0.828187    2.90213     2.85348     0.36130954]
 [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.12689925]
 [ 5.35538    24.3193     -1.16351     0.636469    5.83685     0.96230681]
 ...
 [ 6.25659    41.4192     -2.23358    -2.81921     7.19348     0.08421659]
 [ 6.11213    35.4637     -1.12318     0.356494    6.06597     0.05535172]
 [ 4.17483    26.5586     -1.09427    -1.49334     4.25409     0.07489863]]
valid_t shape =  (1000000,) valid_x shape =  (1000000, 6)
test_t shape =  (1000000,) test_x shape =  (1000000, 6)
no need to train_test_split since we already have the split dataframes
[ 5.55141126e+00  3.27223764e+01  6.98189368e-04 -8.95543973e-04
  6.96116528e+00  5.00485136e-01] [ 2.66412454 15.19914133  2.20425356  1.81362773  2.78097831  0.28852734]
[ 5.55556745e+00  3.26952341e+01 -1.78188172e-03 -3.83090331e-04
  6.96299435e+00  4.99915289e-01] [ 2.66433986 14.93793254  2.20430976  1.81382516  2.78133203  0.28867295]
32.881453465999996 16.02400426348493
32.86720151648752 15.829355769531851
NORMAL UNSCALED DICTIONARY
{&#39;genDatapT&#39;: {&#39;mean&#39;: 32.695234084987476, &#39;std&#39;: 14.937932540562551}, &#39;genDataeta&#39;: {&#39;mean&#39;: -0.0017818817154031672, &#39;std&#39;: 2.204309760627079}, &#39;genDataphi&#39;: {&#39;mean&#39;: -0.0003830903308450233, &#39;std&#39;: 1.8138251604791067}, &#39;genDatam&#39;: {&#39;mean&#39;: 6.962994352358474, &#39;std&#39;: 2.781332025286383}, &#39;RecoDatapT&#39;: {&#39;mean&#39;: 32.86720151648752, &#39;std&#39;: 15.829355769531851}, &#39;RecoDataeta&#39;: {&#39;mean&#39;: -0.0017898858568513964, &#39;std&#39;: 2.197968491495457}, &#39;RecoDataphi&#39;: {&#39;mean&#39;: -0.0004719170328962474, &#39;std&#39;: 1.8144739820043825}, &#39;RecoDatam&#39;: {&#39;mean&#39;: 5.555567451922438, &#39;std&#39;: 2.664339857066051}}



[-1.81710707e+00  1.48455353e+01  5.96132264e-04 -2.50379668e+00
 -1.63658184e+00  5.00485136e-01] [0.17834627 6.89519305 1.2152514  0.65207164 0.17568487 0.28852734]
[-1.81682884e+00  1.48332220e+01 -7.71183141e-04 -2.50361243e+00
 -1.63646629e+00  4.99915289e-01] [0.17836068 6.77669391 1.21528238 0.65214262 0.17570722 0.28867295]
0.0009003493079555966 1.0122966781963252
-1.2048033681821834e-15 1.0000000000000002
&lt;class &#39;str&#39;&gt;
This model was trained for 2000000 iteration, which is  256.0 epochs
RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=16, bias=True)
    (1): LeakyReLU(negative_slope=0.3)
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.3)
    (5): Linear(in_features=16, out_features=16, bias=True)
    (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.3)
    (8): Linear(in_features=16, out_features=1, bias=True)
  )
)
Test (evaluation) Data is Autoregressive, loading AUTOREGRESSIVE_m_Prime.csv
EVALUATION DATA NEW INDEX
    RecoDatam  RecoDatapT  genDatapT  genDataeta  genDataphi  genDatam  \
0   4.840795   41.525097    43.6113    0.824891    -1.26949   5.93310   
1   7.059293   49.677708    43.6113    0.824891    -1.26949   5.93310   
2   5.725040   27.880127    26.0153    3.529970     1.55495   7.41270   
3   3.605120   26.332504    28.4944   -1.159650     1.82602   7.84157   
4   3.521224   22.635612    21.9840    2.747660     2.03085   5.18315   

        tau  
0  0.250046  
1  0.847493  
2  0.851995  
3  0.052378  
4  0.542549  
norm_data 1000000 
norm IQN 1000000 
norm_autoregressive 1000000
</pre></div>
</div>
<img alt="_images/3_Autoregressive_Evaluation_11_2.png" src="_images/3_Autoregressive_Evaluation_11_2.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 640x480 with 0 Axes&gt;
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="2_Train_all_IQNs.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">IQNx4 Chapter 2: Train All Networks</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="4_SupplementaryMaterials_OptunaTuning_Theory_ML_Background.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 4: Optional Supplementary Material</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ali Al Kadhim<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>